<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 6]
- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 55]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [A Comparative Study of EMG- and IMU-based Gesture Recognition at the Wrist and Forearm](https://arxiv.org/abs/2512.07997)
*Soroush Baghernezhad,Elaheh Mohammadreza,Vinicius Prado da Fonseca,Ting Zou,Xianta Jiang*

Main category: cs.HC

TL;DR: 该研究探索使用惯性测量单元（IMU）信号进行静态手势识别，发现IMU能够捕捉肌腱诱导的微运动，可作为独立传感器用于手势识别。


<details>
  <summary>Details</summary>
Motivation: 传统手势识别主要依赖视觉数据和表面肌电图（sEMG），而IMU作为较少探索的替代方案，能够提供关于细微肌肉运动的补充信息，这对于手势识别具有重要价值。

Method: 研究使用来自不同肌肉群的IMU信号来捕捉用户意图，比较不同肌肉群的质量，并分析肌腱诱导的微运动在静态手势识别中的作用。

Result: IMU信号包含足够信息，可作为静态手势识别的唯一输入传感器；肌腱诱导的微运动是静态手势识别的主要贡献因素；不同肌肉群在模式识别质量上存在差异。

Conclusion: 利用肌肉微运动信息可以增强假肢手臂的可用性，并为机器人、远程操作、手语翻译等领域的手势识别提供新的可能性。

Abstract: Gestures are an integral part of our daily interactions with the environment. Hand gesture recognition (HGR) is the process of interpreting human intent through various input modalities, such as visual data (images and videos) and bio-signals. Bio-signals are widely used in HGR due to their ability to be captured non-invasively via sensors placed on the arm. Among these, surface electromyography (sEMG), which measures the electrical activity of muscles, is the most extensively studied modality. However, less-explored alternatives such as inertial measurement units (IMUs) can provide complementary information on subtle muscle movements, which makes them valuable for gesture recognition. In this study, we investigate the potential of using IMU signals from different muscle groups to capture user intent. Our results demonstrate that IMU signals contain sufficient information to serve as the sole input sensor for static gesture recognition. Moreover, we compare different muscle groups and check the quality of pattern recognition on individual muscle groups. We further found that tendon-induced micro-movement captured by IMUs is a major contributor to static gesture recognition. We believe that leveraging muscle micro-movement information can enhance the usability of prosthetic arms for amputees. This approach also offers new possibilities for hand gesture recognition in fields such as robotics, teleoperation, sign language interpretation, and beyond.

</details>


### [2] [Joint Activity Design Heuristics for Enhancing Human-Machine Collaboration](https://arxiv.org/abs/2512.08036)
*Mohammadreza Jalaeian,Dane A. Morey,Michael F. Rayo*

Main category: cs.HC

TL;DR: 论文提出为联合活动设计技术的14条启发式原则，重点关注支持人机团队中的五个宏观认知功能，以提升协调效果。


<details>
  <summary>Details</summary>
Motivation: 当前技术设计主要关注可用性，但缺乏对联合活动中多智能体（人或机器）间协调依赖关系的专门支持。需要超越可用性设计，使技术能够成为有效的团队参与者。

Method: 从显示设计、人因工程、认知系统工程、认知心理学和计算机科学等相关文献中综合提炼出14条启发式原则，用于指导支持人机联合活动的技术设计、开发和评估。

Result: 提出了支持有效联合活动所需的五个核心宏观认知功能（事件检测、意义建构、适应性、视角转换和协调），并基于这些功能开发了14条具体的设计启发式原则。

Conclusion: 支持联合活动的设计与技术可用性同等重要。提出的启发式原则为设计能够有效支持人机团队协调的技术提供了系统化指导框架。

Abstract: Joint activity describes when more than one agent (human or machine) contributes to the completion of a task or activity. Designing for joint activity focuses on explicitly supporting the interdependencies between agents necessary for effective coordination among agents engaged in the joint activity. This builds and expands upon designing for usability to further address how technologies can be designed to act as effective team players. Effective joint activity requires supporting, at minimum, five primary macrocognitive functions within teams: Event Detection, Sensemaking, Adaptability, Perspective-Shifting, and Coordination. Supporting these functions is equally as important as making technologies usable. We synthesized fourteen heuristics from relevant literature including display design, human factors, cognitive systems engineering, cognitive psychology, and computer science to aid the design, development, and evaluation of technologies that support joint human-machine activity.

</details>


### [3] [Mediating Personal Relationships with Robotic Pets for Fostering Human-Human Interaction of Older Adults](https://arxiv.org/abs/2512.08426)
*Delong Du,Sara Gilda Amirhajlou,Akwasi Gyabaah,Richard Paluch,Claudia Müller*

Main category: cs.HC

TL;DR: 研究探索如何设计机器人宠物来促进老年人的人际关系，而非仅仅关注人机关系


<details>
  <summary>Details</summary>
Motivation: 良好的人际关系对幸福生活和心理健康至关重要，而现有研究主要关注老年人与机器人宠物的直接关系，忽略了机器人宠物如何促进人与人之间的关系

Method: 采用民族志研究方法，通过对六位老年人进行半结构化访谈，并进行主题分析

Result: 研究发现机器人宠物可以设计为远程机器人（telerobots），帮助人们远程连接他人，为促进人际关系提供新途径

Conclusion: 机器人宠物作为远程连接工具的设计理念，为未来促进心理健康的机器人系统开发提供了重要方向

Abstract: Good human relationships are important for us to have a happy life and maintain our well-being. Otherwise, we will be at risk of experiencing loneliness or depression. In human-computer interaction (HCI) and computer-supported cooperative work (CSCW), robotic systems offer nuanced approaches to foster human connection, providing interaction beyond the traditional mediums that smartphones and computers offer. However, many existing studies primarily focus on the humanrobot relationships that older adults form directly with robotic pets rather than exploring how these robotic pets can enhance human-human relationships. Our ethnographic study investigates how robotic pets can be designed to facilitate human relationships. Through semi-structured interviews with six older adults and thematic analysis, our empirical findings provide insights into how robotic pets can be designed as telerobots to connect with others remotely, thus contributing to advance future development of robotic systems for mental health.

</details>


### [4] [Time and Money Matters for Sustainability: Insights on User Preferences on Renewable Energy for Electric Vehicle Charging Stations](https://arxiv.org/abs/2512.08437)
*Delong Du,Apostolos Vavouris,Omid Veisi,Lu Jin,Gunnar Stevens,Lina Stankovic,Vladimir Stankovic,Alexander Boden*

Main category: cs.HC

TL;DR: 该研究探讨了在电动汽车充电站导航中整合动态可再生能源可用性如何影响用户选择，发现用户会根据驾驶场景优先考虑时间或金钱节省，并倾向于选择符合其优先级的可再生能源充电站。


<details>
  <summary>Details</summary>
Motivation: 使用可再生能源为电动汽车充电可以减少环境影响，但可再生能源的波动性影响了公共充电站的可持续性。附近充电站可能因微电网连接而使用不同能源来源（从完全可再生到非可再生或混合），突显了短距离内能源供应类型的显著变异性。

Method: 采用被试内设计调查了50名汽车用户，并对来自农村、郊区和城市地区的10名电动汽车用户进行了半结构化访谈，研究整合动态可再生能源可用性的充电站导航对用户选择的影响。

Result: 结果显示，在选择充电站时，驾驶员通常根据影响其消费价值的驾驶场景优先考虑时间节省或金钱节省。值得注意的是，当可再生能源充电站符合用户的主要优先级（无论是节省金钱还是时间）时，电动汽车用户倾向于选择它们。

Conclusion: 该研究为前端图形用户界面和后端排名算法的开发提供了终端用户见解，这些算法用于整合动态可再生能源可用性的导航推荐系统，以实现电动汽车的可持续使用。

Abstract: Charging electric vehicles (EVs) with renewable energy can lessen their environmental impact. However, the fluctuating availability of renewable energy affects the sustainability of public EV charging stations. Nearby public charging stations may utilize differing energy sources due to their microgrid connections - ranging from exclusively renewable to non-renewable or a combination of both - highlighting the substantial variability in energy supply types within short distances. This study investigates the near-future scenario of integrating dynamic renewable energy availability in charging station navigation to impact the choices of EV users towards renewable sources. We conducted a within-subjects design survey with 50 car users and semi-structured interviews with 10 EV users from rural, suburban, and urban areas. The results show that when choosing EV charging stations, drivers often prioritize either time savings or money savings based on the driving scenarios that influence drivers' consumer value. Notably, EV users tend to select renewable-powered stations when they align with their main priority, be it saving money or time. This study offers end-user insights into the front-end graphic user interface and the development of the back-end ranking algorithm for navigation recommender systems that integrate dynamic renewable energy availability for the sustainable use of electric vehicles.

</details>


### [5] [Exploring the Grassroots Understanding and Practices of Collective Memory Co-Contribution in a University Community](https://arxiv.org/abs/2512.08787)
*Zeyu Huang,Xinyi Cao,Yue Deng,Junze Li,Kangyu Yuan,Xiaojuan Ma*

Main category: cs.HC

TL;DR: 本研究从成员层面探讨集体记忆的形成过程，通过两种移动系统（定位叙事系统和在线论坛）进行实地研究，分析集体记忆的共建实践和解释模式。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要从整体社会学视角分析成熟的集体记忆，较少关注成员层面的集体记忆概念化以及共建实践的具体模式。本研究旨在填补这一空白，从成员视角探索集体记忆的解释和互动模式。

Method: 采用混合方法进行为期两周的实地研究（n=38），使用两种移动系统作为观察锚点：1）基于文献设计框架的定位叙事探索系统；2）代表当前实践的常规在线论坛。

Result: 研究发现核心争议在于：是回顾性思考当下作为未来历史，还是记录当下作为未来历史。这影响了叙事焦点、对集体记忆构成的期望，以及参与者从群体中寻求灵感的方式。研究提取了能更好包容集体记忆多样化概念化并连接不同社区成员的设计考量。

Conclusion: 通过反思设计，为社区驱动的用户生成内容平台提供了设计专用定位叙事体验的额外见解。研究强调了从成员层面理解集体记忆共建实践的重要性，并为相关系统设计提供了实用指导。

Abstract: Collective memory -- community members' interconnected memories and impressions of the group -- is essential to the community's culture and identity. Its development requires members' continuous participatory contribution and sensemaking. However, existing works mainly adopt a holistic sociological perspective to analyze well-developed collective memory, less focusing on member-level conceptualization of this possession or what the co-contribution practices can be. Therefore, this work alternatively adopts the latter perspective and probes such interpretative and interactional patterns with two mobile systems. With one being a locative narrative and exploration system condensed from existing literature's design frameworks, and the other being a conventional online forum representing current practices, they served as the anchors of observation for our two-week, mixed-methods field study (n=38) on a university campus. A core debate we have identified was to retrospectively contemplate or document the presence as a history for the future. This also subsequently impacted the narrative focuses, expectations of collective memory constituents, and the ways participants seek inspiration from the group. We further extracted design considerations that could better embrace the diverse conceptualizations of collective memory and bond different community members together. Lastly, revisiting and reflecting on our design, we provided extra insights on designing devoted locative narrative experiences for community-driven UGC platforms.

</details>


### [6] ["Nothing about us without us": Perspectives of Global Deaf and Hard-of-hearing Community Members on Sign Language Technologies](https://arxiv.org/abs/2512.08839)
*Katherine Atwell,Saki Imai,Danielle Bragg,Malihe Alikhani*

Main category: cs.HC

TL;DR: 全球多语言调查显示，聋人和重听者对现有手语技术持谨慎态度，担忧文化侵蚀、翻译不准确和听力主导的研究流程，强调聋人主导设计的重要性。


<details>
  <summary>Details</summary>
Motivation: 手语技术发展迅速，但聋人和重听者的观点在技术开发中被边缘化，特别是在西方以外和全球南方地区。研究旨在了解全球不同文化背景下聋人社群对手语技术的真实看法。

Method: 通过全球多语言调查，收集来自不同国家、使用不同手语和文化背景的聋人社群对手语技术的观点和意见。

Result: 参与者认识到手语技术有支持获取和独立的潜力，但普遍担忧文化侵蚀、翻译不准确和听力主导的研究流程。手语熟练度、政策接触度和聋人身份认同等因素影响对技术的看法。各地区参与者都强调聋人主导设计的重要性。

Conclusion: 研究提供了跨大洲、基于社区的手语技术分析，并为研究人员、技术开发者和政策制定者提出了可操作的建议，强调必须将聋人社群纳入技术决策过程以避免潜在危害。

Abstract: There is accelerating interest in sign language technologies (SLTs), with increasing attention from both industry and academia. However, the perspectives of Deaf and Hard-of-hearing (DHH) individuals remain marginalized in their development, particularly those outside of the West and in the global South. This paper presents findings from a global, multilingual survey capturing community views on SLTs across a wide range of countries, sign languages, and cultural contexts. While participants recognized the potential of SLTs to support access and independence, many expressed concerns about cultural erasure, inaccurate translation, and hearing-dominated research pipelines. Perceptions of SLTs were shaped by factors including sign language proficiency, policy exposure, and deaf identity. Across regions, participants emphasized the importance of DHH-led design, citing the risk of harm when DHH communities are excluded from technological decision-making. This study offers a novel cross-continental, community-informed analysis of SLTs and concludes with actionable recommendations for researchers, technologists, and policymakers.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [7] [Impact of Data-Oriented and Object-Oriented Design on Performance and Cache Utilization with Artificial Intelligence Algorithms in Multi-Threaded CPUs](https://arxiv.org/abs/2512.07841)
*Gabriel M. Arantes,Richard F. Pinto,Bruno L. Dalmazo,Eduardo N. Borges,Giancarlo Lucca,Viviane L. D. de Mattos,Fabian C. Cardoso,Rafael A. Berri*

Main category: cs.AI

TL;DR: 该研究对比了数据导向设计(DOD)和传统面向对象设计(OOD)在多线程环境下的性能表现，通过A*搜索算法的四种实现版本进行测试，发现DOD在多线程场景下具有更好的缓存利用和性能表现。


<details>
  <summary>Details</summary>
Motivation: 随着多核CPU与主内存之间性能差距的增大，需要硬件感知的软件设计范式。本研究旨在全面分析数据导向设计(DOD)与传统面向对象设计(OOD)在缓存利用和多线程环境效率方面的性能差异。

Method: 开发并比较了A*搜索算法的四个版本：单线程OOD(ST-OOD)、单线程DOD(ST-DOD)、多线程OOD(MT-OOD)和多线程DOD(MT-DOD)。评估基于执行时间、内存使用和CPU缓存未命中等指标。

Result: 在多线程测试中，DOD实现表现出显著的性能提升，执行时间更快，原始系统调用和缓存未命中更少。对于A*算法这样的细粒度任务，线程管理开销导致单线程版本在两个范式中都显著优于多线程版本。

Conclusion: 即使在简单算法中性能差异看似细微，但DOD在关键指标上的一致优势凸显了其基础架构的优越性，表明它是复杂、大规模AI和并行计算任务中最大化硬件效率的更有效方法。

Abstract: The growing performance gap between multi-core CPUs and main memory necessitates hardware-aware software design paradigms. This study provides a comprehensive performance analysis of Data Oriented Design (DOD) versus the traditional Object-Oriented Design (OOD), focusing on cache utilization and efficiency in multi-threaded environments. We developed and compared four distinct versions of the A* search algorithm: single-threaded OOD (ST-OOD), single-threaded DOD (ST-DOD), multi-threaded OOD (MT-OOD), and multi-threaded DOD (MT-DOD). The evaluation was based on metrics including execution time, memory usage, and CPU cache misses. In multi-threaded tests, the DOD implementation demonstrated considerable performance gains, with faster execution times and a lower number of raw system calls and cache misses. While OOD occasionally showed marginal advantages in memory usage or percentage-based cache miss rates, DOD's efficiency in data-intensive operations was more evident. Furthermore, our findings reveal that for a fine-grained task like the A* algorithm, the overhead associated with thread management led to single-threaded versions significantly outperforming their multi-threaded counterparts in both paradigms. We conclude that even when performance differences appear subtle in simple algorithms, the consistent advantages of DOD in critical metrics highlight its foundational architectural superiority, suggesting it is a more effective approach for maximizing hardware efficiency in complex, large-scale AI and parallel computing tasks.

</details>


### [8] [SkipKV: Selective Skipping of KV Generation and Storage for Efficient Inference with Large Reasoning Models](https://arxiv.org/abs/2512.07993)
*Jiayi Tian,Seyedarmin Azizi,Yequan Zhao,Erfan Baghaei Potraghloo,Sean McPherson,Sharath Nittur Sridhar,Zhengyang Wang,Zheng Zhang,Massoud Pedram,Souvik Kundu*

Main category: cs.AI

TL;DR: SkipKV是一种无需训练的KV缓存压缩方法，通过句子级评分和选择性删除来减少推理过程中的KV缓存开销，同时保持推理准确性。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在推理过程中会产生大量KV缓存，随着思维链推理过程的线性增长，导致内存和吞吐量瓶颈。现有KV缓存逐出方法在多批次设置下无法保持准确性，且会生成更长的序列。

Method: 提出SkipKV方法：1）使用句子评分指标识别和删除高度相似的句子；2）动态调整引导向量来更新隐藏激活状态，抑制冗余生成；3）在粗粒度句子级别进行序列删除，保持语义连贯性。

Result: 在多个推理基准测试中，SkipKV在相似压缩预算下相比替代方法保持了高达26.7%的准确性改进，同时生成长度减少1.6倍，吞吐量提高1.7倍。

Conclusion: SkipKV是一种有效的训练免费KV压缩方法，通过句子级操作解决了现有方法在多批次设置下的准确性问题，同时减少了生成长度并提高了推理效率。

Abstract: Large reasoning models (LRMs) often cost significant key-value (KV) cache overhead, due to their linear growth with the verbose chain-of-thought (CoT) reasoning process. This costs both memory and throughput bottleneck limiting their efficient deployment. Towards reducing KV cache size during inference, we first investigate the effectiveness of existing KV cache eviction methods for CoT reasoning. Interestingly, we find that due to unstable token-wise scoring and the reduced effective KV budget caused by padding tokens, state-of-the-art (SoTA) eviction methods fail to maintain accuracy in the multi-batch setting. Additionally, these methods often generate longer sequences than the original model, as semantic-unaware token-wise eviction leads to repeated revalidation during reasoning. To address these issues, we present \textbf{SkipKV}, a \textbf{\textit{training-free}} KV compression method for selective \textit{eviction} and \textit{generation} operating at a coarse-grained sentence-level sequence removal for efficient CoT reasoning. In specific, it introduces a \textit{sentence-scoring metric} to identify and remove highly similar sentences while maintaining semantic coherence. To suppress redundant generation, SkipKV dynamically adjusts a steering vector to update the hidden activation states during inference enforcing the LRM to generate concise response. Extensive evaluations on multiple reasoning benchmarks demonstrate the effectiveness of SkipKV in maintaining up to $\mathbf{26.7}\%$ improved accuracy compared to the alternatives, at a similar compression budget. Additionally, compared to SoTA, SkipKV yields up to $\mathbf{1.6}\times$ fewer generation length while improving throughput up to $\mathbf{1.7}\times$.

</details>


### [9] [Toward an AI Reasoning-Enabled System for Patient-Clinical Trial Matching](https://arxiv.org/abs/2512.08026)
*Caroline N. Leach,Mitchell A. Klusty,Samuel E. Armstrong,Justine C. Pickarski,Kristen L. Hankins,Emily B. Collier,Maya Shah,Aaron D. Mullen,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 开发了一个基于大语言模型的AI增强患者-临床试验匹配系统，能够处理异构电子健康记录数据，生成结构化资格评估和可解释的推理链，支持人工审查，旨在减轻协调员负担并提高匹配效率。


<details>
  <summary>Details</summary>
Motivation: 目前患者临床试验资格筛查仍然是手动、耗时且资源密集的过程，需要解决电子健康记录数据整合、专家审查支持和安全标准维护等关键实施挑战。

Method: 利用开源、具备推理能力的大语言模型，超越二元分类，生成结构化资格评估和可解释的推理链，支持人工在环审查，将资格表示为动态状态而非固定判定。

Result: 开发了一个安全、可扩展的概念验证系统，能够整合异构电子健康记录数据，识别可用匹配，并提供使患者未来符合资格的可操作建议，保证所有AI生成输出的全面可审计性。

Conclusion: 该系统旨在减轻协调员负担，智能扩展为每位患者考虑的试验范围，通过AI增强的患者-试验匹配提高临床试验筛选效率，同时保持严格的安全标准和可解释性。

Abstract: Screening patients for clinical trial eligibility remains a manual, time-consuming, and resource-intensive process. We present a secure, scalable proof-of-concept system for Artificial Intelligence (AI)-augmented patient-trial matching that addresses key implementation challenges: integrating heterogeneous electronic health record (EHR) data, facilitating expert review, and maintaining rigorous security standards. Leveraging open-source, reasoning-enabled large language models (LLMs), the system moves beyond binary classification to generate structured eligibility assessments with interpretable reasoning chains that support human-in-the-loop review. This decision support tool represents eligibility as a dynamic state rather than a fixed determination, identifying matches when available and offering actionable recommendations that could render a patient eligible in the future. The system aims to reduce coordinator burden, intelligently broaden the set of trials considered for each patient and guarantee comprehensive auditability of all AI-generated outputs.

</details>


### [10] [Large Language Models for Education and Research: An Empirical and User Survey-based Analysis](https://arxiv.org/abs/2512.08057)
*Md Mostafizer Rahman,Ariful Islam Shiplu,Md Faizul Ibne Amin,Yutaka Watanobe,Lu Peng*

Main category: cs.AI

TL;DR: 本研究对ChatGPT和DeepSeek两大LLM在教育研究领域进行综合评估，包括技术分析、实证实验和用户调查，比较它们在文本生成、编程和专业问题解决等方面的表现。


<details>
  <summary>Details</summary>
Motivation: 随着预训练大语言模型在教育研究领域的广泛应用，需要系统评估当前领先模型（ChatGPT和DeepSeek）在不同任务中的表现差异，为教育研究应用提供指导。

Method: 采用综合评估方法：1）背景技术分析；2）实证实验（文本生成、编程、专业问题解决）；3）真实用户调查（学生、教育者、研究者）。

Result: ChatGPT在通用语言理解和文本生成方面表现更优，DeepSeek在编程任务中因效率导向设计而表现更佳；两者都能提供准确的医学诊断输出并有效解决复杂数学问题。

Conclusion: 不同LLM在不同任务中存在优势差异，用户调查揭示了这些模型在教育研究中的实际价值和局限性，为领域应用提供了重要参考。

Abstract: Pretrained Large Language Models (LLMs) have achieved remarkable success across diverse domains, with education and research emerging as particularly impactful areas. Among current state-of-the-art LLMs, ChatGPT and DeepSeek exhibit strong capabilities in mathematics, science, medicine, literature, and programming. In this study, we present a comprehensive evaluation of these two LLMs through background technology analysis, empirical experiments, and a real-world user survey. The evaluation explores trade-offs among model accuracy, computational efficiency, and user experience in educational and research affairs. We benchmarked these LLMs performance in text generation, programming, and specialized problem-solving. Experimental results show that ChatGPT excels in general language understanding and text generation, while DeepSeek demonstrates superior performance in programming tasks due to its efficiency- focused design. Moreover, both models deliver medically accurate diagnostic outputs and effectively solve complex mathematical problems. Complementing these quantitative findings, a survey of students, educators, and researchers highlights the practical benefits and limitations of these models, offering deeper insights into their role in advancing education and research.

</details>


### [11] [Scalable Back-End for an AI-Based Diabetes Prediction Application](https://arxiv.org/abs/2512.08147)
*Henry Anand Septian Radityo,Bernardus Willson,Reynard Tanadi,Latifa Dwiyanti,Saiful Akbar*

Main category: cs.AI

TL;DR: 开发了一个用于糖尿病预测移动应用的可扩展后端系统，采用水平扩展、数据库分片和消息队列等技术，83%的功能达到性能目标，支持1万并发用户


<details>
  <summary>Details</summary>
Motivation: 随着糖尿病全球患病率上升，需要早期检测来预防严重并发症。AI预测应用需要响应迅速且可扩展的后端架构来有效服务大量用户

Method: 采用水平扩展、数据库分片和通过消息队列进行异步通信的架构设计，目标是故障率低于5%，平均延迟低于1000毫秒

Result: 83%的系统功能（24个中的20个）达到性能目标，用户档案管理、活动跟踪和读取密集型预测操作等关键功能成功实现目标性能，系统能处理1万并发用户

Conclusion: 使用RabbitMQ的异步通信对于最小化计算密集型预测请求的错误率至关重要，通过排队请求确保系统可靠性，防止高负载下的数据丢失，验证了系统的可扩展性

Abstract: The rising global prevalence of diabetes necessitates early detection to prevent severe complications. While AI-powered prediction applications offer a promising solution, they require a responsive and scalable back-end architecture to serve a large user base effectively. This paper details the development and evaluation of a scalable back-end system designed for a mobile diabetes prediction application. The primary objective was to maintain a failure rate below 5% and an average latency of under 1000 ms. The architecture leverages horizontal scaling, database sharding, and asynchronous communication via a message queue. Performance evaluation showed that 83% of the system's features (20 out of 24) met the specified performance targets. Key functionalities such as user profile management, activity tracking, and read-intensive prediction operations successfully achieved the desired performance. The system demonstrated the ability to handle up to 10,000 concurrent users without issues, validating its scalability. The implementation of asynchronous communication using RabbitMQ proved crucial in minimizing the error rate for computationally intensive prediction requests, ensuring system reliability by queuing requests and preventing data loss under heavy load.

</details>


### [12] [Beyond Traditional Diagnostics: Transforming Patient-Side Information into Predictive Insights with Knowledge Graphs and Prototypes](https://arxiv.org/abs/2512.08261)
*Yibowen Zhao,Yinan Zhang,Zhixiang Su,Lizhen Cui,Chunyan Miao*

Main category: cs.AI

TL;DR: KPI框架通过知识图谱增强、原型感知和可解释性设计，从患者基本信息预测疾病，解决数据不平衡和可解释性不足的问题。


<details>
  <summary>Details</summary>
Motivation: 仅基于患者基本信息（如人口统计和自述症状）预测疾病具有重要价值，但现有方法面临疾病分布不平衡和缺乏可解释性的挑战，导致预测存在偏见或不可靠。

Method: 提出KPI框架：1）将结构化医学知识整合为统一疾病知识图谱；2）构建临床有意义的疾病原型；3）使用对比学习提升预测准确性（尤其对长尾疾病）；4）利用大语言模型生成患者特异性、医学相关的解释。

Result: 在真实数据集上的实验表明，KPI在预测准确性上优于现有最先进方法，并能提供与患者叙述高度一致的临床有效解释。

Conclusion: KPI框架通过整合医学知识、增强长尾疾病预测和提供可解释性，在患者为中心的医疗保健服务中具有实用价值。

Abstract: Predicting diseases solely from patient-side information, such as demographics and self-reported symptoms, has attracted significant research attention due to its potential to enhance patient awareness, facilitate early healthcare engagement, and improve healthcare system efficiency. However, existing approaches encounter critical challenges, including imbalanced disease distributions and a lack of interpretability, resulting in biased or unreliable predictions. To address these issues, we propose the Knowledge graph-enhanced, Prototype-aware, and Interpretable (KPI) framework. KPI systematically integrates structured and trusted medical knowledge into a unified disease knowledge graph, constructs clinically meaningful disease prototypes, and employs contrastive learning to enhance predictive accuracy, which is particularly important for long-tailed diseases. Additionally, KPI utilizes large language models (LLMs) to generate patient-specific, medically relevant explanations, thereby improving interpretability and reliability. Extensive experiments on real-world datasets demonstrate that KPI outperforms state-of-the-art methods in predictive accuracy and provides clinically valid explanations that closely align with patient narratives, highlighting its practical value for patient-centered healthcare delivery.

</details>


### [13] [Reasoning Models Ace the CFA Exams](https://arxiv.org/abs/2512.08270)
*Jaisal Patel,Yunzhe Chen,Kaiwen He,Keyi Wang,David Li,Kairong Xiao,Xiao-Yang Liu*

Main category: cs.AI

TL;DR: 最新推理模型在CFA各级考试中表现出色，多个模型通过所有级别，Gemini 3.0 Pro在Level I创下97.6%最高分


<details>
  <summary>Details</summary>
Motivation: 先前研究表明大型语言模型在CFA考试中表现不佳，但最新推理模型在其他专业考试中表现出色，需要评估这些模型在CFA考试中的实际表现

Method: 使用980道模拟题评估最先进的推理模型，涵盖CFA三个级别的考试（Level I三套、Level II两套、Level III三套），采用与先前研究相同的通过标准

Result: 大多数模型通过了所有三个级别，按总体表现排序为：Gemini 3.0 Pro、Gemini 2.5 Pro、GPT-5、Grok 4、Claude Opus 4.1、DeepSeek-V3.1。Gemini 3.0 Pro在Level I创下97.6%记录，GPT-5在Level II以94.3%领先，Gemini 2.5 Pro在Level III选择题获86.4%，Gemini 3.0 Pro在构建回答题获92.0%

Conclusion: 最新推理模型在CFA考试中表现出色，显著超越了先前大型语言模型的性能，证明了这些模型在专业金融知识评估方面的能力

Abstract: Previous research has reported that large language models (LLMs) demonstrate poor performance on the Chartered Financial Analyst (CFA) exams. However, recent reasoning models have achieved strong results on graduate-level academic and professional examinations across various disciplines. In this paper, we evaluate state-of-the-art reasoning models on a set of mock CFA exams consisting of 980 questions across three Level I exams, two Level II exams, and three Level III exams. Using the same pass/fail criteria from prior studies, we find that most models clear all three levels. The models that pass, ordered by overall performance, are Gemini 3.0 Pro, Gemini 2.5 Pro, GPT-5, Grok 4, Claude Opus 4.1, and DeepSeek-V3.1. Specifically, Gemini 3.0 Pro achieves a record score of 97.6% on Level I. Performance is also strong on Level II, led by GPT-5 at 94.3%. On Level III, Gemini 2.5 Pro attains the highest score with 86.4% on multiple-choice questions while Gemini 3.0 Pro achieves 92.0% on constructed-response questions.

</details>


### [14] [AgentEval: Generative Agents as Reliable Proxies for Human Evaluation of AI-Generated Content](https://arxiv.org/abs/2512.08273)
*Thanh Vu,Richi Nayak,Thiru Balasubramaniam*

Main category: cs.AI

TL;DR: 该研究提出使用生成式智能体来评估AI生成内容的质量，以解决传统人工评估成本高、效率低的问题，帮助企业实现高效、自动化的内容生成与评估。


<details>
  <summary>Details</summary>
Motivation: 现代企业在内容创作和评估方面面临时间和成本的双重挑战：人工创作耗时，人工评估昂贵。虽然大型语言模型在内容生成方面有潜力，但AI生成内容的质量问题仍然存在担忧，需要更高效、自动化的解决方案。

Method: 引入生成式智能体作为评估工具，这些智能体能够模拟人类判断，从连贯性、趣味性、清晰度、公平性和相关性等多个维度对AI生成内容进行快速、低成本的评估。

Result: 通过使用生成式智能体，企业可以简化内容生成流程，确保高质量的内容输出，同时减少对昂贵人工评估的依赖，实现更高效、自动化的内容创作与评估。

Conclusion: 该研究为提升大型语言模型生成符合业务需求的高质量内容提供了重要见解，在自动化内容生成和评估方面取得了显著进展，有助于企业优化内容创作流程并控制成本。

Abstract: Modern businesses are increasingly challenged by the time and expense required to generate and assess high-quality content. Human writers face time constraints, and extrinsic evaluations can be costly. While Large Language Models (LLMs) offer potential in content creation, concerns about the quality of AI-generated content persist. Traditional evaluation methods, like human surveys, further add operational costs, highlighting the need for efficient, automated solutions. This research introduces Generative Agents as a means to tackle these challenges. These agents can rapidly and cost-effectively evaluate AI-generated content, simulating human judgment by rating aspects such as coherence, interestingness, clarity, fairness, and relevance. By incorporating these agents, businesses can streamline content generation and ensure consistent, high-quality output while minimizing reliance on costly human evaluations. The study provides critical insights into enhancing LLMs for producing business-aligned, high-quality content, offering significant advancements in automated content generation and evaluation.

</details>


### [15] [Towards a Science of Scaling Agent Systems](https://arxiv.org/abs/2512.08296)
*Yubin Kim,Ken Gu,Chanwoo Park,Chunjong Park,Samuel Schmidgall,A. Ali Heydari,Yao Yan,Zhihan Zhang,Yuchen Zhuang,Mark Malhotra,Paul Pu Liang,Hae Won Park,Yuzhe Yang,Xuhai Xu,Yilun Du,Shwetak Patel,Tim Althoff,Daniel McDuff,Xin Liu*

Main category: cs.AI

TL;DR: 该研究提出了智能体系统的定量扩展原则，通过四个基准测试和五种架构的180种配置实验，建立了基于协调指标的预测模型，揭示了工具协调权衡、能力饱和和拓扑依赖错误放大三大主导效应。


<details>
  <summary>Details</summary>
Motivation: 尽管基于语言模型的智能体系统在现实AI应用中日益普及，但其性能决定原则仍未被充分探索，导致从业者依赖启发式方法而非原则性设计选择。本研究旨在填补这一空白。

Method: 使用五种典型架构（单一、独立、集中式、分散式、混合）在三个LLM系列上实例化，在四个多样化基准（Finance-Agent、BrowseComp-Plus、PlanCraft、Workbench）上进行控制评估，涵盖180种配置，使用标准化工具和令牌预算。通过经验协调指标（效率、开销、错误放大、冗余）建立预测模型。

Result: 预测模型达到交叉验证R^2=0.513。发现三大主导效应：1）工具协调权衡：固定计算预算下，工具密集型任务受多智能体开销影响更大；2）能力饱和：单智能体基线超过约45%后，协调带来递减或负回报；3）拓扑依赖错误放大：独立智能体将错误放大17.2倍，集中式协调控制在4.4倍。集中式协调在金融推理等并行任务上提升80.9%，分散式协调在动态网页导航上表现更好（+9.2% vs +0.2%），但所有多智能体变体在顺序推理任务上性能下降39-70%。

Conclusion: 该框架能预测87%保留配置的最优协调策略，提供了基于可测量任务特性的智能体扩展预测原则，为智能体系统设计提供了定量指导。

Abstract: Agents, language model (LM)-based systems that are capable of reasoning, planning, and acting are becoming the dominant paradigm for real-world AI applications. Despite this widespread adoption, the principles that determine their performance remain underexplored, leaving practitioners to rely on heuristics rather than principled design choices. We address this gap by deriving quantitative scaling principles for agent systems. We evaluate this across four diverse benchmarks: Finance-Agent, BrowseComp-Plus, PlanCraft, and Workbench. Using five canonical architectures (Single, Independent, Centralized, Decentralized, Hybrid) instantiated across three LLM families, we perform a controlled evaluation spanning 180 configurations with standardized tools and token budgets. We derive a predictive model using empirical coordination metrics, including efficiency, overhead, error amplification, and redundancy, that achieves cross-validated R^2=0.513. We identify three dominant effects: (1) a tool-coordination trade-off: under fixed computational budgets, tool-heavy tasks suffer disproportionately from multi-agent overhead. (2) a capability saturation: coordination yields diminishing or negative returns (beta=-0.408, p<0.001) once single-agent baselines exceed ~45%. (3) topology-dependent error amplification: independent agents amplify errors 17.2x through unchecked propagation, while centralized coordination contains this to 4.4x. Centralized coordination improves performance by 80.9% on parallelizable tasks like financial reasoning, while decentralized coordination excels on dynamic web navigation (+9.2% vs. +0.2%). Yet for sequential reasoning tasks, all multi-agent variants degraded performance by 39-70%. The framework predicts the optimal coordination strategy for 87% of held-out configurations, providing a predictive principle of agentic scaling based on measurable task properties.

</details>


### [16] [rSIM: Incentivizing Reasoning Capabilities of LLMs via Reinforced Strategy Injection](https://arxiv.org/abs/2512.08300)
*Sijia Chen,Baochun Li,Di Niu*

Main category: cs.AI

TL;DR: 该论文提出了一种强化策略注入机制(rSIM)，通过小型规划器指导大语言模型的思维链，将普通LLM转变为推理语言模型(RLM)，显著提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通过强化学习后训练可以进化为推理语言模型，表现出"顿悟"时刻和高级推理策略。为了将这种能力扩展到更多LLM，需要一种机制来注入推理策略。

Method: 提出强化策略注入机制(rSIM)，使用小型规划器作为领导者智能体，LLM作为跟随者智能体，基于领导者-跟随者框架和多智能体强化学习进行联合训练，通过基于规则的奖励实现自适应策略注入。

Result: rSIM使Qwen2.5-0.5B模型转变为RLM，性能显著超过Qwen2.5-14B。规划器具有通用性，只需训练一次即可作为插件提升现有LLM的推理能力，并支持跨任务的持续学习。

Conclusion: rSIM机制成功将普通LLM转变为具有高级推理能力的RLM，规划器的通用性和可扩展性为提升LLM推理能力提供了有效途径。

Abstract: Large language models (LLMs) are post-trained through reinforcement learning (RL) to evolve into Reasoning Language Models (RLMs), where the hallmark of this advanced reasoning is ``aha'' moments when they start to perform strategies, such as self-reflection and deep thinking, within chain of thoughts (CoTs). Motivated by this, this paper proposes a novel reinforced strategy injection mechanism (rSIM), that enables any LLM to become an RLM by employing a small planner to guide the LLM's CoT through the adaptive injection of reasoning strategies. To achieve this, the planner (leader agent) is jointly trained with an LLM (follower agent) using multi-agent RL (MARL), based on a leader-follower framework and straightforward rule-based rewards. Experimental results show that rSIM enables Qwen2.5-0.5B to become an RLM and significantly outperform Qwen2.5-14B. Moreover, the planner is generalizable: it only needs to be trained once and can be applied as a plug-in to substantially improve the reasoning capabilities of existing LLMs. In addition, the planner supports continual learning across various tasks, allowing its planning abilities to gradually improve and generalize to a wider range of problems.

</details>


### [17] [Predicting California Bearing Ratio with Ensemble and Neural Network Models: A Case Study from Türkiye](https://arxiv.org/abs/2512.08340)
*Abdullah Hulusi Kökçam,Uğur Dağdeviren,Talas Fikret Kurnaz,Alparslan Serhat Demir,Caner Erden*

Main category: cs.AI

TL;DR: 该研究开发了一个机器学习框架，使用土耳其382个土壤样本的物理化学特性来预测加州承载比，随机森林回归器表现最佳。


<details>
  <summary>Details</summary>
Motivation: 传统CBR实验室测试耗时、昂贵且不适用于大规模或多样化的土壤剖面，需要更快速、精确的预测方法。

Method: 使用土耳其不同地理气候区域的382个土壤样本数据集，包含与承载能力相关的物理化学特性，测试了12种机器学习算法并进行训练、验证和评估。

Result: 随机森林回归器表现最佳，R2分数分别为：训练0.95、验证0.76、测试0.83，展示了强大的非线性映射能力。

Conclusion: 机器学习模型为预测性岩土工程任务提供了有前景的工具，支持智能数据驱动模型在岩土工程中的集成，为传统方法提供了有效替代方案。

Abstract: The California Bearing Ratio (CBR) is a key geotechnical indicator used to assess the load-bearing capacity of subgrade soils, especially in transportation infrastructure and foundation design. Traditional CBR determination relies on laboratory penetration tests. Despite their accuracy, these tests are often time-consuming, costly, and can be impractical, particularly for large-scale or diverse soil profiles. Recent progress in artificial intelligence, especially machine learning (ML), has enabled data-driven approaches for modeling complex soil behavior with greater speed and precision. This study introduces a comprehensive ML framework for CBR prediction using a dataset of 382 soil samples collected from various geoclimatic regions in Türkiye. The dataset includes physicochemical soil properties relevant to bearing capacity, allowing multidimensional feature representation in a supervised learning context. Twelve ML algorithms were tested, including decision tree, random forest, extra trees, gradient boosting, xgboost, k-nearest neighbors, support vector regression, multi-layer perceptron, adaboost, bagging, voting, and stacking regressors. Each model was trained, validated, and evaluated to assess its generalization and robustness. Among them, the random forest regressor performed the best, achieving strong R2 scores of 0.95 (training), 0.76 (validation), and 0.83 (test). These outcomes highlight the model's powerful nonlinear mapping ability, making it a promising tool for predictive geotechnical tasks. The study supports the integration of intelligent, data-centric models in geotechnical engineering, offering an effective alternative to traditional methods and promoting digital transformation in infrastructure analysis and design.

</details>


### [18] [Soil Compaction Parameters Prediction Based on Automated Machine Learning Approach](https://arxiv.org/abs/2512.08343)
*Caner Erden,Alparslan Serhat Demir,Abdullah Hulusi Kokcam,Talas Fikret Kurnaz,Ugur Dagdeviren*

Main category: cs.AI

TL;DR: 使用AutoML方法预测土壤压实参数（最优含水量和最大干密度），XGBoost算法表现最佳，在异质数据集上实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 传统确定土壤压实参数的方法（实验室实验和回归模型）劳动强度大、适用性有限且精度不足，而现有机器学习模型在异质土壤数据集上的预测精度和泛化能力有限。

Method: 采用自动化机器学习（AutoML）方法，自动选择算法和优化超参数，使用异质土壤数据集预测最优含水量（OMC）和最大干密度（MDD）。

Result: 极端梯度提升（XGBoost）算法表现最佳，在独立数据集上对MDD和OMC的R平方值分别达到80.4%和89.1%，证明了AutoML在不同土壤类型中预测压实参数的有效性。

Conclusion: AutoML方法能有效预测土壤压实参数，异质数据集对提升机器学习模型的泛化能力和性能至关重要，该研究有助于提高施工实践的效率和可靠性。

Abstract: Soil compaction is critical in construction engineering to ensure the stability of structures like road embankments and earth dams. Traditional methods for determining optimum moisture content (OMC) and maximum dry density (MDD) involve labor-intensive laboratory experiments, and empirical regression models have limited applicability and accuracy across diverse soil types. In recent years, artificial intelligence (AI) and machine learning (ML) techniques have emerged as alternatives for predicting these compaction parameters. However, ML models often struggle with prediction accuracy and generalizability, particularly with heterogeneous datasets representing various soil types. This study proposes an automated machine learning (AutoML) approach to predict OMC and MDD. AutoML automates algorithm selection and hyperparameter optimization, potentially improving accuracy and scalability. Through extensive experimentation, the study found that the Extreme Gradient Boosting (XGBoost) algorithm provided the best performance, achieving R-squared values of 80.4% for MDD and 89.1% for OMC on a separate dataset. These results demonstrate the effectiveness of AutoML in predicting compaction parameters across different soil types. The study also highlights the importance of heterogeneous datasets in improving the generalization and performance of ML models. Ultimately, this research contributes to more efficient and reliable construction practices by enhancing the prediction of soil compaction parameters.

</details>


### [19] [Reflecting with Two Voices: A Co-Adaptive Dual-Strategy Framework for LLM-Based Agent Decision Making](https://arxiv.org/abs/2512.08366)
*Wentao Zhang,Qunbo Wang,Tao Zhang,Junsheng Wu,Hongping Gan,Yang Liu,Ling Dai,Shizhuang Deng,Shuntong Sun*

Main category: cs.AI

TL;DR: DuSAR是一个无需演示的LLM智能体框架，通过双策略协调（高层整体规划与上下文局部策略）和轻量级反思机制，在ALFWorld和Mind2Web上实现SOTA性能，同时大幅降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 现有LLM智能体依赖外部演示或检索增强规划，导致脆弱性、泛化能力差和计算开销高。受人类问题解决启发，需要一种无需演示的框架，让单个冻结LLM能够通过互补策略进行协同自适应推理。

Method: 提出DuSAR框架：1）双策略协调 - 高层整体规划和上下文局部策略；2）轻量级反思机制 - 通过策略适应度分数持续评估进展；3）动态调整 - 在卡住时修订全局计划或在有意义进展时细化计划，模拟人类元认知行为。

Result: 在ALFWorld上达到37.1%成功率（Llama3.1-70B），比先前最佳结果（13.0%）提高一倍以上；在Mind2Web上达到4.02%，同样比最强基线提高一倍以上；每步token消耗减少3-9倍；消融研究证实双策略协调的必要性。

Conclusion: DuSAR展示了无需演示的LLM智能体框架的有效性，通过双策略协调和反思机制实现高性能和高效性，同时保持与外部知识集成的灵活性，为LLM智能体设计提供了新方向。

Abstract: Large language model (LLM) agents often rely on external demonstrations or retrieval-augmented planning, leading to brittleness, poor generalization, and high computational overhead. Inspired by human problem-solving, we propose DuSAR (Dual-Strategy Agent with Reflecting) - a demonstration-free framework that enables a single frozen LLM to perform co-adaptive reasoning via two complementary strategies: a high-level holistic plan and a context-grounded local policy. These strategies interact through a lightweight reflection mechanism, where the agent continuously assesses progress via a Strategy Fitness Score and dynamically revises its global plan when stuck or refines it upon meaningful advancement, mimicking human metacognitive behavior. On ALFWorld and Mind2Web, DuSAR achieves state-of-the-art performance with open-source LLMs (7B-70B), reaching 37.1% success on ALFWorld (Llama3.1-70B) - more than doubling the best prior result (13.0%) - and 4.02% on Mind2Web, also more than doubling the strongest baseline. Remarkably, it reduces per-step token consumption by 3-9X while maintaining strong performance. Ablation studies confirm the necessity of dual-strategy coordination. Moreover, optional integration of expert demonstrations further boosts results, highlighting DuSAR's flexibility and compatibility with external knowledge.

</details>


### [20] [DeepFeature: Iterative Context-aware Feature Generation for Wearable Biosignals](https://arxiv.org/abs/2512.08379)
*Kaiwei Liu,Yuting He,Bufang Yang,Mu Yuan,Chun Man Victor Wong,Ho Pong Andrew Sze,Zhenyu Yan,Hongkai Chen*

Main category: cs.AI

TL;DR: DeepFeature是一个基于大语言模型的上下文感知特征生成框架，用于可穿戴生物信号分析，通过多源特征生成、迭代特征精炼和鲁棒代码转换，显著提升了医疗应用中的模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前可穿戴设备生物信号的特征提取方法存在三个主要问题：缺乏任务特定的上下文知识、在高维特征空间中难以找到最优特征提取设置、容易产生代码生成和自动化错误，这限制了医疗应用中机器学习模型的性能。

Method: DeepFeature采用多源特征生成机制，整合专家知识和任务设置；使用基于特征评估反馈的迭代特征精炼过程；采用鲁棒的多层过滤和验证方法进行特征到代码的转换，确保提取函数稳定运行。

Result: 在八个不同任务上的实验评估显示，DeepFeature相比基线方法平均AUROC提升了4.21-9.67%，在五个任务上超越了最先进方法，在其余任务上保持了相当的性能。

Conclusion: DeepFeature作为首个LLM赋能的上下文感知特征生成框架，通过整合专家知识、迭代精炼和鲁棒代码转换，显著提升了可穿戴生物信号特征提取的效能，为医疗应用提供了更优的解决方案。

Abstract: Biosignals collected from wearable devices are widely utilized in healthcare applications. Machine learning models used in these applications often rely on features extracted from biosignals due to their effectiveness, lower data dimensionality, and wide compatibility across various model architectures. However, existing feature extraction methods often lack task-specific contextual knowledge, struggle to identify optimal feature extraction settings in high-dimensional feature space, and are prone to code generation and automation errors. In this paper, we propose DeepFeature, the first LLM-empowered, context-aware feature generation framework for wearable biosignals. DeepFeature introduces a multi-source feature generation mechanism that integrates expert knowledge with task settings. It also employs an iterative feature refinement process that uses feature assessment-based feedback for feature re-selection. Additionally, DeepFeature utilizes a robust multi-layer filtering and verification approach for robust feature-to-code translation to ensure that the extraction functions run without crashing. Experimental evaluation results show that DeepFeature achieves an average AUROC improvement of 4.21-9.67% across eight diverse tasks compared to baseline methods. It outperforms state-of-the-art approaches on five tasks while maintaining comparable performance on the remaining tasks.

</details>


### [21] [Principles2Plan: LLM-Guided System for Operationalising Ethical Principles into Plans](https://arxiv.org/abs/2512.08536)
*Tammy Zhong,Yang Song,Maurice Pagnucco*

Main category: cs.AI

TL;DR: Principles2Plan：一个交互式研究原型，通过人类与大型语言模型协作生成情境敏感的伦理规则，指导自动化规划


<details>
  <summary>Details</summary>
Motivation: 机器人在人类环境中操作需要伦理意识，但现有自动化规划工具对此支持有限。手动指定伦理规则既费力又高度依赖具体情境。

Method: 开发Principles2Plan原型系统，让领域专家提供规划领域、问题细节和相关高级原则（如行善、隐私），系统生成可操作的伦理规则，用户可审查、优先排序并供给规划器生成伦理感知的计划。

Result: 创建了首个支持用户在经典规划情境中生成基于原则的伦理规则的系统，展示了人类-LLM协作在使伦理自动化规划更实用可行方面的潜力。

Conclusion: Principles2Plan展示了人类与大型语言模型协作的潜力，使伦理自动化规划变得更加实用和可行，填补了现有规划工具在伦理支持方面的空白。

Abstract: Ethical awareness is critical for robots operating in human environments, yet existing automated planning tools provide little support. Manually specifying ethical rules is labour-intensive and highly context-specific. We present Principles2Plan, an interactive research prototype demonstrating how a human and a Large Language Model (LLM) can collaborate to produce context-sensitive ethical rules and guide automated planning. A domain expert provides the planning domain, problem details, and relevant high-level principles such as beneficence and privacy. The system generates operationalisable ethical rules consistent with these principles, which the user can review, prioritise, and supply to a planner to produce ethically-informed plans. To our knowledge, no prior system supports users in generating principle-grounded rules for classical planning contexts. Principles2Plan showcases the potential of human-LLM collaboration for making ethical automated planning more practical and feasible.

</details>


### [22] [CogMCTS: A Novel Cognitive-Guided Monte Carlo Tree Search Framework for Iterative Heuristic Evolution with Large Language Models](https://arxiv.org/abs/2512.08609)
*Hui Wang,Yang Liu,Xiaoyu Zhang,Chaoxu Mu*

Main category: cs.AI

TL;DR: 本文提出CogMCTS框架，通过将LLM的认知引导机制与MCTS紧密集成，实现高效的自动启发式设计，在稳定性、效率和解决方案质量方面优于现有LLM-based AHD方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的进化方法依赖种群策略且易陷入局部最优，而LLM与MCTS结合的方法在多轮认知整合和搜索多样性方面存在限制，需要克服这些局限性。

Method: 提出认知引导的MCTS框架(CogMCTS)：1) 多轮认知反馈整合历史经验、节点信息和负面结果；2) 双轨节点扩展结合精英启发式管理平衡探索与利用；3) 策略性变异增强解决方案多样性。

Result: 实验结果表明CogMCTS在稳定性、效率和解决方案质量方面优于现有基于LLM的自动启发式设计方法。

Conclusion: CogMCTS框架通过紧密集成LLM认知引导与MCTS，有效解决了现有方法的局限性，实现了更高效的自动启发式优化。

Abstract: Automatic Heuristic Design (AHD) is an effective1 framework for solving complex optimization prob-2 lems. The development of large language mod-3 els (LLMs) enables the automated generation of4 heuristics. Existing LLM-based evolutionary meth-5 ods rely on population strategies and are prone6 to local optima. Integrating LLMs with Monte7 Carlo Tree Search (MCTS) improves the trade-off8 between exploration and exploitation, but multi-9 round cognitive integration remains limited and10 search diversity is constrained. To overcome these11 limitations, this paper proposes a novel cognitive-12 guided MCTS framework (CogMCTS). CogMCTS13 tightly integrates the cognitive guidance mecha-14 nism of LLMs with MCTS to achieve efficient au-15 tomated heuristic optimization. The framework16 employs multi-round cognitive feedback to incor-17 porate historical experience, node information, and18 negative outcomes, dynamically improving heuris-19 tic generation. Dual-track node expansion com-20 bined with elite heuristic management balances the21 exploration of diverse heuristics and the exploita-22 tion of high-quality experience. In addition, strate-23 gic mutation modifies the heuristic forms and pa-24 rameters to further enhance the diversity of the so-25 lution and the overall optimization performance.26 The experimental results indicate that CogMCTS27 outperforms existing LLM-based AHD methods in28 stability, efficiency, and solution quality.

</details>


### [23] [Protein Secondary Structure Prediction Using Transformers](https://arxiv.org/abs/2512.08613)
*Manzi Kevin Maxime*

Main category: cs.AI

TL;DR: 基于Transformer的蛋白质二级结构预测模型，使用注意力机制处理氨基酸序列，通过滑动窗口数据增强技术提升性能


<details>
  <summary>Details</summary>
Motivation: 从氨基酸序列预测蛋白质二级结构（如α螺旋、β折叠和卷曲）对于理解蛋白质功能至关重要

Method: 提出基于Transformer的模型，应用注意力机制处理蛋白质序列数据；使用滑动窗口数据增强技术在CB513数据集上扩展训练样本

Result: Transformer模型在可变长度序列上表现出强大的泛化能力，能有效捕捉局部和长程残基相互作用

Conclusion: Transformer架构适用于蛋白质二级结构预测任务，能够有效处理序列数据并捕捉复杂的残基相互作用模式

Abstract: Predicting protein secondary structures such as alpha helices, beta sheets, and coils from amino acid sequences is essential for understanding protein function. This work presents a transformer-based model that applies attention mechanisms to protein sequence data to predict structural motifs. A sliding-window data augmentation technique is used on the CB513 dataset to expand the training samples. The transformer shows strong ability to generalize across variable-length sequences while effectively capturing both local and long-range residue interactions.

</details>


### [24] [Multi-Agent Intelligence for Multidisciplinary Decision-Making in Gastrointestinal Oncology](https://arxiv.org/abs/2512.08674)
*Rongzhao Zhang,Junqiao Wang,Shuyun Yang,Mouxiao Bian,Chao Ding,Yuwei Bai,Chihao Zhang,Yuguang Shen,Lei Wang,Lei Zheng,Qiujuan Yan,Yun Zhong,Meiling Liu,Jiwei Yu,Zheng Wang,Jie Xu,Meng Luo*

Main category: cs.AI

TL;DR: 提出分层多智能体框架模拟多学科团队协作，解决多模态临床推理中的上下文稀释和幻觉问题，在胃肠道肿瘤学中显著提升推理逻辑和医学准确性。


<details>
  <summary>Details</summary>
Motivation: 胃肠道肿瘤学的多模态临床推理需要整合内镜图像、放射学数据和生化标志物。现有的多模态大语言模型在处理复杂异质医疗数据时面临上下文稀释和幻觉问题，需要更有效的解决方案。

Method: 提出分层多智能体框架，模拟人类多学科团队的协作工作流程，通过智能体间的协作来处理多模态医疗数据，提高推理的准确性和逻辑性。

Result: 系统获得4.60/5.00的专家评估分数，相比单一基线模型有显著改进。智能体架构在推理逻辑和医学准确性方面提升最为明显。

Conclusion: 模拟多学科团队协作的智能体框架为肿瘤学自动决策支持提供了可扩展、可解释且临床稳健的范式，能够有效处理多模态医疗数据的复杂推理任务。

Abstract: Multimodal clinical reasoning in the field of gastrointestinal (GI) oncology necessitates the integrated interpretation of endoscopic imagery, radiological data, and biochemical markers. Despite the evident potential exhibited by Multimodal Large Language Models (MLLMs), they frequently encounter challenges such as context dilution and hallucination when confronted with intricate, heterogeneous medical histories. In order to address these limitations, a hierarchical Multi-Agent Framework is proposed, which emulates the collaborative workflow of a human Multidisciplinary Team (MDT). The system attained a composite expert evaluation score of 4.60/5.00, thereby demonstrating a substantial improvement over the monolithic baseline. It is noteworthy that the agent-based architecture yielded the most substantial enhancements in reasoning logic and medical accuracy. The findings indicate that mimetic, agent-based collaboration provides a scalable, interpretable, and clinically robust paradigm for automated decision support in oncology.

</details>


### [25] [Towards Foundation Models with Native Multi-Agent Intelligence](https://arxiv.org/abs/2512.08743)
*Shuyue Hu,Haoyang Yan,Yiqun Zhang,Yang Chen,Dongzhan Zhou,Lei Bai*

Main category: cs.AI

TL;DR: 基础模型在单智能体任务上表现出色，但缺乏原生多智能体智能，需要专门的研究来弥补这一差距。


<details>
  <summary>Details</summary>
Motivation: 基础模型正成为AI智能体的"大脑"，虽然已有研究赋予其单智能体能力（如GUI交互、工具使用），但多智能体智能是下一个前沿领域。当前假设认为单智能体能力会自然转化为多智能体智能，但实际并非如此。

Method: 通过41个大语言模型的广泛实证研究，评估基础模型在多智能体环境中的四个核心能力：理解、规划、高效通信和适应能力。

Result: 实证证据表明，强大的单智能体性能并不能自动产生稳健的多智能体智能，两者之间存在显著差距。

Conclusion: 需要从数据集构建、评估方法、训练范式和安全考虑等多个研究方向入手，专门构建具有原生多智能体智能的基础模型。

Abstract: Foundation models (FMs) are increasingly assuming the role of the "brain" of AI agents. While recent efforts have begun to equip FMs with native single-agent abilities -- such as GUI interaction or integrated tool use -- we argue that the next frontier is endowing FMs with native multi-agent intelligence. We identify four core capabilities of FMs in multi-agent contexts: understanding, planning, efficient communication, and adaptation. Contrary to assumptions about the spontaneous emergence of such abilities, we provide extensive empirical evidence across 41 large language models showing that strong single-agent performance alone does not automatically yield robust multi-agent intelligence. To address this gap, we outline key research directions -- spanning dataset construction, evaluation, training paradigms, and safety considerations -- for building FMs with native multi-agent intelligence.

</details>


### [26] [Performance Comparison of Aerial RIS and STAR-RIS in 3D Wireless Environments](https://arxiv.org/abs/2512.08755)
*Dongdong Yang,Bin Li,Jiguang He*

Main category: cs.AI

TL;DR: 该论文对无人机搭载的可重构智能表面(RIS)和同时透射反射智能表面(STAR-RIS)在三维无线环境中的性能进行了全面比较，发现STAR-RIS在低空场景表现更优，而RIS在基站附近的高空场景性能更好。


<details>
  <summary>Details</summary>
Motivation: 尽管无人机搭载的RIS和STAR-RIS在增强无线覆盖和容量方面具有巨大潜力，但两种架构之间的全面性能比较尚未得到深入研究，特别是在三维无线环境中。

Method: 建立了包含方向性辐射模式的精确信道模型，并深入研究了部署高度和方向的影响。针对两种架构分别制定了联合优化问题，提出了基于加权最小均方误差和块坐标下降算法的高效解决方案。

Result: 仿真结果表明：STAR-RIS在低空场景中由于具备全空间覆盖能力而表现优于RIS；而在基站附近的高空场景中，RIS则能提供更好的性能表现。

Conclusion: 研究结果为未来6G通信系统中空中智能表面的部署提供了实用见解，表明应根据部署场景的高度和位置选择合适的智能表面架构。

Abstract: Reconfigurable intelligent surface (RIS) and simultaneously transmitting and reflecting RIS (STAR-RIS) have emerged as key enablers for enhancing wireless coverage and capacity in next-generation networks. When mounted on unmanned aerial vehicles (UAVs), they benefit from flexible deployment and improved line-of-sight conditions. Despite their promising potential, a comprehensive performance comparison between aerial RIS and STAR-RIS architectures has not been thoroughly investigated. This letter presents a detailed performance comparison between aerial RIS and STAR-RIS in three-dimensional wireless environments. Accurate channel models incorporating directional radiation patterns are established, and the influence of deployment altitude and orientation is thoroughly examined. To optimize the system sum-rate, we formulate joint optimization problems for both architectures and propose an efficient solution based on the weighted minimum mean square error and block coordinate descent algorithms. Simulation results reveal that STAR-RIS outperforms RIS in low-altitude scenarios due to its full-space coverage capability, whereas RIS delivers better performance near the base station at higher altitudes. The findings provide practical insights for the deployment of aerial intelligent surfaces in future 6G communication systems.

</details>


### [27] [A Practical Guide for Designing, Developing, and Deploying Production-Grade Agentic AI Workflows](https://arxiv.org/abs/2512.08769)
*Eranga Bandara,Ross Gore,Peter Foytik,Sachin Shetty,Ravi Mukkamala,Abdul Rahman,Xueping Liang,Safdar H. Bouk,Amin Hass,Sachini Rajapakse,Ng Wee Keong,Kasun De Zoysa,Aruna Withanage,Nilaan Loganathan*

Main category: cs.AI

TL;DR: 本文提供了构建生产级Agentic AI工作流的端到端实用指南，涵盖设计、开发和部署全流程，提出了9个核心最佳实践，并通过多模态新闻分析案例进行验证。


<details>
  <summary>Details</summary>
Motivation: 随着Agentic AI在行业和研究中的加速采用，组织面临如何设计、工程化和运营生产级Agentic AI工作流的挑战，需要确保其可靠性、可观察性、可维护性，并符合安全和治理要求。

Method: 提出结构化工程生命周期，包括工作流分解、多智能体设计模式、模型上下文协议(MCP)和工具集成、确定性编排、负责任AI考虑因素、环境感知部署策略，并介绍了9个核心最佳实践。

Result: 通过多模态新闻分析和媒体生成工作流的案例研究，展示了所提原则的实际应用，为构建稳健、可扩展、生产就绪的Agentic AI工作流提供了基础参考。

Conclusion: 本文提供了构建生产级Agentic AI系统的综合指南，结合架构指导、操作模式和实际实现见解，为开发可靠、可扩展的Agentic AI工作流奠定了坚实基础。

Abstract: Agentic AI marks a major shift in how autonomous systems reason, plan, and execute multi-step tasks. Unlike traditional single model prompting, agentic workflows integrate multiple specialized agents with different Large Language Models(LLMs), tool-augmented capabilities, orchestration logic, and external system interactions to form dynamic pipelines capable of autonomous decision-making and action. As adoption accelerates across industry and research, organizations face a central challenge: how to design, engineer, and operate production-grade agentic AI workflows that are reliable, observable, maintainable, and aligned with safety and governance requirements. This paper provides a practical, end-to-end guide for designing, developing, and deploying production-quality agentic AI systems. We introduce a structured engineering lifecycle encompassing workflow decomposition, multi-agent design patterns, Model Context Protocol(MCP), and tool integration, deterministic orchestration, Responsible-AI considerations, and environment-aware deployment strategies. We then present nine core best practices for engineering production-grade agentic AI workflows, including tool-first design over MCP, pure-function invocation, single-tool and single-responsibility agents, externalized prompt management, Responsible-AI-aligned model-consortium design, clean separation between workflow logic and MCP servers, containerized deployment for scalable operations, and adherence to the Keep it Simple, Stupid (KISS) principle to maintain simplicity and robustness. To demonstrate these principles in practice, we present a comprehensive case study: a multimodal news-analysis and media-generation workflow. By combining architectural guidance, operational patterns, and practical implementation insights, this paper offers a foundational reference to build robust, extensible, and production-ready agentic AI workflows.

</details>


### [28] [CARLoS: Retrieval via Concise Assessment Representation of LoRAs at Scale](https://arxiv.org/abs/2512.08826)
*Shahar Sarfaty,Adi Haviv,Uri Hacohen,Niva Elkin-Koren,Roi Livni,Amit H. Bermano*

Main category: cs.AI

TL;DR: CARLoS是一个大规模LoRA表征框架，通过分析650多个LoRA在图像生成中的行为，构建了方向、强度和一致性的三维表示，实现了优于文本基线的语义检索系统。


<details>
  <summary>Details</summary>
Motivation: 生成式组件（如LoRA）的快速扩散形成了一个庞大但无结构的生态系统。现有的发现方法依赖于不可靠的用户描述或有偏见的流行度指标，阻碍了可用性。

Method: 通过分析650多个LoRA在各种提示和种子下的图像生成行为，使用CLIP嵌入及其与基础模型生成的差异，定义了三维表示：方向（定义语义偏移）、强度（量化效果显著性）和一致性（量化效果稳定性）。

Result: 开发了高效的检索框架，能够语义匹配文本查询到相关LoRA，同时过滤过强或不稳定的LoRA，在自动和人工评估中优于文本基线。

Conclusion: CARLoS不仅是一个实用的LoRA检索系统，其表示方法还能支持分析强度、一致性与版权法中实质性、意愿性等法律概念的关联，具有更广泛的LoRA分析意义。

Abstract: The rapid proliferation of generative components, such as LoRAs, has created a vast but unstructured ecosystem. Existing discovery methods depend on unreliable user descriptions or biased popularity metrics, hindering usability. We present CARLoS, a large-scale framework for characterizing LoRAs without requiring additional metadata. Analyzing over 650 LoRAs, we employ them in image generation over a variety of prompts and seeds, as a credible way to assess their behavior. Using CLIP embeddings and their difference to a base-model generation, we concisely define a three-part representation: Directions, defining semantic shift; Strength, quantifying the significance of the effect; and Consistency, quantifying how stable the effect is. Using these representations, we develop an efficient retrieval framework that semantically matches textual queries to relevant LoRAs while filtering overly strong or unstable ones, outperforming textual baselines in automated and human evaluations. While retrieval is our primary focus, the same representation also supports analyses linking Strength and Consistency to legal notions of substantiality and volition, key considerations in copyright, positioning CARLoS as a practical system with broader relevance for LoRA analysis.

</details>


### [29] [Interpolation in Knowledge Representation](https://arxiv.org/abs/2512.08833)
*Jean Christoph Jung,Patrick Koopmann,Matthias Knorr*

Main category: cs.AI

TL;DR: 论文探讨了描述逻辑和逻辑编程中Craig插值和均匀插值的理论结果与计算方法


<details>
  <summary>Details</summary>
Motivation: Craig插值和均匀插值在知识表示中有广泛应用，但许多相关形式化方法通常不具备这些插值特性，且实际计算插值具有挑战性

Method: 深入分析描述逻辑和逻辑编程这两种主要知识表示形式化方法，讨论计算插值的理论结果和实用方法

Result: 论文系统梳理了两种形式化方法中插值问题的理论性质，并提供了实际计算插值的方法

Conclusion: Craig插值和均匀插值在知识表示中具有重要应用价值，需要进一步研究如何在描述逻辑和逻辑编程中有效计算插值

Abstract: Craig interpolation and uniform interpolation have many applications in knowledge representation, including explainability, forgetting, modularization and reuse, and even learning. At the same time, many relevant knowledge representation formalisms do in general not have Craig or uniform interpolation, and computing interpolants in practice is challenging. We have a closer look at two prominent knowledge representation formalisms, description logics and logic programming, and discuss theoretical results and practical methods for computing interpolants.

</details>


### [30] [EcomBench: Towards Holistic Evaluation of Foundation Agents in E-commerce](https://arxiv.org/abs/2512.08868)
*Rui Min,Zile Qiao,Ze Xu,Jiawen Zhai,Wenyu Gao,Xuanzhong Chen,Haozhen Sun,Zhen Zhang,Xinyu Wang,Hong Zhou,Wenbiao Yin,Xuan Zhou,Yong Jiang,Haicheng Liu,Liang Ding,Ling Zou,Yi R.,Fung,Yalong Li,Pengjun Xie*

Main category: cs.AI

TL;DR: EcomBench是一个基于真实电商环境的综合性基准测试，用于评估智能体在实际电商场景中的核心能力，包括深度信息检索、多步推理和跨源知识整合等。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试大多关注学术环境或人工设计场景，忽视了实际应用中的挑战。电商领域涉及大量用户交互、动态市场条件和真实决策过程，需要更贴近实际应用的评估方法。

Method: 从全球领先电商生态系统的真实用户需求中构建基准，通过人工专家精心策划和标注，确保清晰度、准确性和领域相关性。涵盖电商场景中的多个任务类别，并定义了三个难度级别。

Result: EcomBench提供了一个严谨且动态的测试平台，能够评估智能体在深度信息检索、多步推理和跨源知识整合等关键能力上的表现。

Conclusion: 通过将评估建立在真实电商环境中，EcomBench为衡量智能体在现代电商中的实际能力提供了有效的测试基准，弥补了现有基准测试与实际应用之间的差距。

Abstract: Foundation agents have rapidly advanced in their ability to reason and interact with real environments, making the evaluation of their core capabilities increasingly important. While many benchmarks have been developed to assess agent performance, most concentrate on academic settings or artificially designed scenarios while overlooking the challenges that arise in real applications. To address this issue, we focus on a highly practical real-world setting, the e-commerce domain, which involves a large volume of diverse user interactions, dynamic market conditions, and tasks directly tied to real decision-making processes. To this end, we introduce EcomBench, a holistic E-commerce Benchmark designed to evaluate agent performance in realistic e-commerce environments. EcomBench is built from genuine user demands embedded in leading global e-commerce ecosystems and is carefully curated and annotated through human experts to ensure clarity, accuracy, and domain relevance. It covers multiple task categories within e-commerce scenarios and defines three difficulty levels that evaluate agents on key capabilities such as deep information retrieval, multi-step reasoning, and cross-source knowledge integration. By grounding evaluation in real e-commerce contexts, EcomBench provides a rigorous and dynamic testbed for measuring the practical capabilities of agents in modern e-commerce.

</details>


### [31] [Same Content, Different Answers: Cross-Modal Inconsistency in MLLMs](https://arxiv.org/abs/2512.08923)
*Angela van Sprang,Laurens Samson,Ana Lucic,Erman Acar,Sennay Ghebreab,Yuki M. Asano*

Main category: cs.AI

TL;DR: 论文提出了REST和REST+两个新基准，用于系统评估多模态大语言模型中的跨模态不一致性问题，发现现有模型在不同模态（图像、文本、混合）中无法保持一致的推理能力。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型被训练在同一个嵌入空间中表示视觉和语言信息，但它们无法在两种模态中执行相同的任务。现有研究缺乏系统评估跨模态不一致性的基准，因此需要创建专门的测试工具来揭示这一重要问题。

Method: 创建了REST和REST+两个基准，包含相同语义信息在三种模态（纯图像、纯文本、混合模态）中的样本。评估了15个最先进的多模态大语言模型，分析了文本识别（OCR）问题、视觉特征（文本颜色、分辨率、字体）以及视觉token数量对模型性能的影响。

Result: 研究发现：1）不同模型的模态不一致程度差异很大；2）将文本渲染为图像或将图像渲染为文本都无法解决不一致性问题；3）即使OCR正确，视觉特征（文本颜色和分辨率）和视觉token数量仍会影响模型性能；4）一致性得分与文本和图像之间的模态差距相关。

Conclusion: 多模态大语言模型存在显著的跨模态不一致性问题，这种不一致性与模态差距有机制性关联。研究强调了需要开发更一致的多模态模型，并为评估模型一致性提供了有效的基准工具。

Abstract: We introduce two new benchmarks REST and REST+(Render-Equivalence Stress Tests) to enable systematic evaluation of cross-modal inconsistency in multimodal large language models (MLLMs). MLLMs are trained to represent vision and language in the same embedding space, yet they cannot perform the same tasks in both modalities. Our benchmarks contain samples with the same semantic information in three modalities (image, text, mixed) and we show that state-of-the-art MLLMs cannot consistently reason over these different modalities. We evaluate 15 MLLMs and find that the degree of modality inconsistency varies substantially, even when accounting for problems with text recognition (OCR). Neither rendering text as image nor rendering an image as text solves the inconsistency. Even if OCR is correct, we find that visual characteristics (text colour and resolution, but not font) and the number of vision tokens have an impact on model performance. Finally, we find that our consistency score correlates with the modality gap between text and images, highlighting a mechanistic interpretation of cross-modal inconsistent MLLMs.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [32] [ThreadWeaver: Adaptive Threading for Efficient Parallel Reasoning in Language Models](https://arxiv.org/abs/2512.07843)
*Long Lian,Sida Wang,Felix Juefei-Xu,Tsu-Jui Fu,Xiuyu Li,Adam Yala,Trevor Darrell,Alane Suhr,Yuandong Tian,Xi Victoria Lin*

Main category: cs.LG

TL;DR: ThreadWeaver是一个自适应并行推理框架，在保持与顺序推理模型相当准确率的同时，显著降低推理延迟，在数学推理任务上实现1.53倍加速


<details>
  <summary>Details</summary>
Motivation: 现有LLM的顺序解码导致高延迟，而现有的并行推理方法要么局限于监督行为克隆，要么相比顺序CoT基线准确率显著下降，且需要定制化推理引擎，部署复杂

Method: 1) 两阶段并行轨迹生成器产生大规模高质量带并行标注的CoT数据用于监督微调；2) 基于trie的训练-推理协同设计，可在任何现成的自回归推理引擎上实现并行推理；3) 并行感知的强化学习框架，教导模型平衡准确率和并行效率

Result: 在六个数学推理基准测试中，基于Qwen3-8B的ThreadWeaver达到与先进顺序推理模型相当的准确率（平均71.9%，AIME24上79.9%），同时实现最高1.53倍的平均token延迟加速

Conclusion: ThreadWeaver在准确率和效率之间建立了新的帕累托前沿，为LLM推理提供了高效且易于部署的并行解决方案

Abstract: Scaling inference-time computation has enabled Large Language Models (LLMs) to achieve strong reasoning performance, but inherently sequential decoding leads to substantial latency, especially on complex tasks. Recent work on adaptive parallel reasoning aims to improve inference efficiency by decomposing the problem-solving process into concurrent reasoning threads when beneficial. However, existing methods on realistic tasks are either limited to supervised behavior cloning or exhibit significant accuracy drops compared to widely-used sequential long chain-of-thought (CoT) baselines. Moreover, many require customized inference engines, complicating deployment. We introduce ThreadWeaver, a framework for adaptive parallel reasoning that achieves accuracy on par with popular sequential reasoning models of comparable size while significantly reducing inference latency. ThreadWeaver's performance stems from three key innovations: 1) a two-stage parallel trajectory generator that produces large-scale, high-quality CoT data with parallel annotations for supervised fine-tuning; 2) a trie-based training-inference co-design that enables parallel reasoning on any off-the-shelf autoregressive inference engine without modifying position embeddings or KV caches; and 3) a parallelization-aware reinforcement learning framework that teaches the model to balance accuracy with effective parallelization. Across six challenging mathematical reasoning benchmarks, ThreadWeaver trained atop Qwen3-8B achieves accuracy comparable to cutting-edge sequential reasoning models (71.9% on average and 79.9% on AIME24) while delivering up to 1.53x average speedup in token latency, establishing a new Pareto frontier between accuracy and efficiency.

</details>


### [33] [RaX-Crash: A Resource Efficient and Explainable Small Model Pipeline with an Application to City Scale Injury Severity Prediction](https://arxiv.org/abs/2512.07848)
*Di Zhu,Chen Xie,Ziwei Wang,Haoyun Zhang*

Main category: cs.LG

TL;DR: RaX-Crash是一个资源高效、可解释的小模型管道，用于预测纽约市机动车碰撞事故的伤害严重程度，基于结构化数据训练树模型，优于小型语言模型。


<details>
  <summary>Details</summary>
Motivation: 纽约市每年发生超过十万起机动车碰撞事故，造成严重的伤害和公共卫生负担，需要高效、可解释的预测模型来支持城市规模的事故分析。

Method: 整合三个关联表（数千万条记录），构建统一特征模式，训练紧凑的树模型集成（随机森林和XGBoost），并与基于文本摘要的小型语言模型进行比较。

Result: XGBoost和随机森林在时间保留测试集上分别达到0.7828和0.7828的准确率，明显优于小型语言模型（0.594和0.496）；类别不平衡分析显示简单类别加权可改善致命事故召回率。

Conclusion: 可解释的小模型集成仍然是城市规模伤害分析的强基线，而将表格预测器与SLM生成的叙述相结合的混合管道可以在不牺牲可扩展性的情况下改善沟通效果。

Abstract: New York City reports over one hundred thousand motor vehicle collisions each year, creating substantial injury and public health burden. We present RaX-Crash, a resource efficient and explainable small model pipeline for structured injury severity prediction on the official NYC Motor Vehicle Collisions dataset. RaX-Crash integrates three linked tables with tens of millions of records, builds a unified feature schema in partitioned storage, and trains compact tree based ensembles (Random Forest and XGBoost) on engineered tabular features, which are compared against locally deployed small language models (SLMs) prompted with textual summaries. On a temporally held out test set, XGBoost and Random Forest achieve accuracies of 0.7828 and 0.7794, clearly outperforming SLMs (0.594 and 0.496); class imbalance analysis shows that simple class weighting improves fatal recall with modest accuracy trade offs, and SHAP attribution highlights human vulnerability factors, timing, and location as dominant drivers of predicted severity. Overall, RaX-Crash indicates that interpretable small model ensembles remain strong baselines for city scale injury analytics, while hybrid pipelines that pair tabular predictors with SLM generated narratives improve communication without sacrificing scalability.

</details>


### [34] [SABER: Small Actions, Big Errors - Safeguarding Mutating Steps in LLM Agents](https://arxiv.org/abs/2512.07850)
*Alejandro Cuadron,Pengfei Yu,Yang Liu,Arpit Gupta*

Main category: cs.LG

TL;DR: 该论文研究了LLM智能体在长时程工具使用任务中的脆弱性，发现突变性操作对失败影响最大，提出了包含突变门控验证、针对性反思和块级上下文清理的CM安全机制，并发布了τ-Bench Verified基准以解决原有基准的局限性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM智能体发展迅速，但在长时程工具使用任务上的性能仍然脆弱。为了理解这种脆弱性，研究者提出了一个简单问题：所有操作对失败的影响是否相同？他们希望识别导致任务失败的关键因素，并开发相应的安全机制。

Method: 1. 分析τ-Bench（航空/零售）和SWE-Bench Verified的执行轨迹，将轨迹分解为突变性（改变环境）和非突变性步骤；2. 形式化"决定性偏离"概念，即最早导致成功转为失败的操作级偏离；3. 使用逻辑回归分析突变性和非突变性操作对成功率的影响；4. 提出CM安全机制，包括突变门控验证、针对性反思和块级上下文清理；5. 发布τ-Bench Verified基准，通过针对性修订恢复基准的评估空间。

Result: 1. 每个额外的突变性操作偏离使成功率降低：航空任务最多92%，零售任务最多96%（SoTA模型）；2. 非突变性操作的偏离几乎没有影响；3. 错误随上下文长度增加而增长，智能体会偏离角色并基于过时约束行动；4. CM机制带来一致性能提升：Qwen3-Thinking在航空任务上相对提升28%，零售任务11%，SWE-Bench Verified 7%；Claude模型提升9%/7%；5. 发现τ-Bench存在天花板效应，标注错误和任务定义不明确人为限制了模型性能。

Conclusion: 该研究强调需要进行操作级分析、针对性安全机制和可靠评估，这些是构建鲁棒多轮智能体的先决条件。突变性操作是导致失败的关键因素，而CM机制能有效提升性能。同时需要更可靠的基准来准确评估智能体能力。

Abstract: Despite rapid progress in LLM agents, performance on long-horizon, tool-using tasks remains fragile. To better understand this fragility, we ask a simple question: \emph{do all actions contribute equally to failure?} Analyzing execution traces on $τ$-Bench (Airline/Retail) and SWE-Bench Verified, we decompose trajectories into \emph{mutating} (environment-changing) vs.\ non-mutating steps and formalize \emph{decisive deviations}, earliest action, level divergences that flip success to failure. A logistic regression reveals that each additional deviation in a mutating action reduces the odds of success by upto $92\%$ on Airline and upto $96\%$ on Retail for SoTA models. In contrast, deviations in non-mutating actions have little to no effect. Errors also grow with context length as agents drift from role and act on stale constraints. Motivated by these observations, we introduce \cm{}, a model-agnostic, gradient-free, test-time safeguard that (i) adds mutation-gated verification, (ii) injects \emph{Targeted Reflection} before mutating steps, and (iii) performs block-based context cleaning. \cm{} delivers consistent gains, e.g., Qwen3-Thinking: +28\% \emph{relative} on Airline, +11\% on Retail, and +7\% on SWE-Bench Verified; Claude: +9\%/+7\%. We further identify ceiling effects in $τ$-Bench, where annotation errors and underspecified tasks artificially cap model performance. To address this, we release $τ$-Bench Verified, which restores benchmark headroom through targeted revisions. Our results argue for action-level analysis, targeted safeguards, and reliable evaluations as prerequisites for robust multi-turn agents.

</details>


### [35] [GPU Memory Prediction for Multimodal Model Training](https://arxiv.org/abs/2512.07853)
*Jinwoo Jeong,Minchul Kang,Younghun Go,Changyong Shin,Hyunho Lee,Junho Yoon,Gyeongsik Yang,Chuck Yoo*

Main category: cs.LG

TL;DR: 提出一个预测多模态模型GPU峰值内存使用的框架，通过分解模型架构和分析训练行为来准确预测内存需求


<details>
  <summary>Details</summary>
Motivation: 随着智能体AI系统中深度学习模型的规模和复杂性增加，GPU内存需求经常超过可用容量，导致内存溢出错误。现有研究仅关注单模态架构，无法推广到多模态模型，而多模态模型在智能体AI系统中很常见。

Method: 提出一个框架，通过将多模态模型分解为组成层并应用因子化来估计每层的内存使用，分析模型架构和训练行为来预测GPU峰值内存使用。

Result: 评估显示该框架实现了约8.7%的平均MAPE（平均绝对百分比误差）预测准确率。

Conclusion: 该框架能够准确预测多模态模型的GPU内存使用，有助于防止内存溢出错误，提高训练效率和资源利用率。

Abstract: As deep learning models in agentic AI systems grow in scale and complexity, GPU memory requirements increase and often exceed the available GPU memory capacity, so that out-of-memory (OoM) errors occur. It is well known that OoM interrupts the whole training itself and wastes substantial computational resources. Therefore, to prevent OoM, accurate prediction of GPU memory usage is essential. However, previous studies focus only on unimodal architectures and fail to generalize to multimodal models, even though the multimodal models are a common choice in agentic AI systems. To address this limitation, we propose a framework that predicts the peak GPU memory usage by analyzing the model architecture and training behavior of multimodal models. Specifically, the framework decomposes the multimodal model into its constituent layers and applies factorization to estimate the memory usage of each layer. Our evaluation shows that our framework achieves high prediction accuracy of ~8.7% average MAPE.

</details>


### [36] [LAPA: Log-Domain Prediction-Driven Dynamic Sparsity Accelerator for Transformer Model](https://arxiv.org/abs/2512.07855)
*Huizheng Wang,Hongbin Wang,Shaojun Wei,Yang Hu,Shouyi Yin*

Main category: cs.LG

TL;DR: LAPA提出了一种基于对数域注意力预测的算法-架构协同设计，通过消除昂贵乘法运算和减少累加开销，实现跨阶段稀疏Transformer加速，能效比SOTA方法提升2.79-3.52倍。


<details>
  <summary>Details</summary>
Motivation: Transformer模型在NLP和CV任务中表现出色，但随着输入序列变化，计算瓶颈在不同阶段呈现动态特性，需要跨阶段稀疏加速策略。现有稀疏Transformer方法多为单阶段设计，其稀疏预测机制在跨阶段应用时会产生显著功耗开销。

Method: 提出LAPA对数域注意力预测算法-架构协同设计：1）设计非对称前导一计算(ALOC)方案消除昂贵乘法；2）提出混合精度多轮移位累加(MRSA)机制减少累加开销；3）设计数据特征依赖滤波器(DDF)与MRSA协同工作；4）设计专用加速器将理论增强转化为实际硬件改进。

Result: 实验结果显示，LAPA相比SOTA方法Spatten、Sanger和FACT分别实现了3.52倍、3.24倍和2.79倍的能效提升。

Conclusion: LAPA通过算法-架构协同设计有效解决了跨阶段稀疏Transformer的功耗问题，显著提升了能效，为动态计算瓶颈的Transformer加速提供了有效解决方案。

Abstract: Attention-based Transformers have revolutionized natural language processing (NLP) and shown strong performance in computer vision (CV) tasks. However, as the input sequence varies, the computational bottlenecks in Transformer models exhibit dynamic behavior across stages, which calls for a cross-stage sparse acceleration strategy. Unfortunately, most existing sparse Transformer approaches are single-stage based, and their sparsity prediction mechanisms lead to significant power overhead when applied across multiple stages. To this end, this paper proposes a log-domain attention prediction algorithm-architecture co-design, named LAPA. First, an asymmetric leading one computing (ALOC) scheme is designed to eliminate expensive multiplications. Next, a mixed-precision multi-round shifting accumulation (MRSA) mechanism is further proposed to mitigate the accumulation overhead. A data-feature dependent filter (DDF) strategy is designed to work in concert with the MRSA process. Finally, an elaborate accelerator is designed to translate the theoretical enhancement into practical hardware improvement. Experimental results show that LAPA achieves 3.52x, 3.24x and 2.79x higher energy efficiency than the state-of-the-art (SOTA) works Spatten, Sanger and FACT, respectively.

</details>


### [37] [Medical Test-free Disease Detection Based on Big Data](https://arxiv.org/abs/2512.07856)
*Haokun Zhao,Yingzhe Bai,Qingyang Xu,Lixin Zhou,Jianxin Chen,Jicong Fan*

Main category: cs.LG

TL;DR: CLDD是一种基于图的深度学习模型，通过利用疾病间关联和患者间相似性进行协作学习，实现无需医学检测的大规模疾病检测。


<details>
  <summary>Details</summary>
Motivation: 传统疾病检测需要进行大量医学测试且成本高昂，无法对所有患者进行数百甚至数千种疾病的全面检测。需要一种能够减少诊断成本、提高可及性的解决方案。

Method: 提出协作学习疾病检测模型(CLDD)，将疾病检测构建为协作学习任务，自适应地利用疾病间关联和患者间相似性。整合患者-疾病交互和电子健康记录中的人口统计学特征，实现对数百至数千种疾病的检测，基本不依赖相应医学测试。

Result: 在包含61,191名患者和2,000种疾病的MIMIC-IV数据集上，CLDD在多个指标上持续优于代表性基线模型，召回率提升6.33%，精确率提升7.63%。病例研究表明CLDD能够在其排名靠前的预测中成功恢复被掩盖的疾病。

Conclusion: CLDD通过降低诊断成本和提高可及性，为大规模疾病筛查和社会健康保障提供了有前景的解决方案，同时展示了疾病预测的可解释性和可靠性。

Abstract: Accurate disease detection is of paramount importance for effective medical treatment and patient care. However, the process of disease detection is often associated with extensive medical testing and considerable costs, making it impractical to perform all possible medical tests on a patient to diagnose or predict hundreds or thousands of diseases. In this work, we propose Collaborative Learning for Disease Detection (CLDD), a novel graph-based deep learning model that formulates disease detection as a collaborative learning task by exploiting associations among diseases and similarities among patients adaptively. CLDD integrates patient-disease interactions and demographic features from electronic health records to detect hundreds or thousands of diseases for every patient, with little to no reliance on the corresponding medical tests. Extensive experiments on a processed version of the MIMIC-IV dataset comprising 61,191 patients and 2,000 diseases demonstrate that CLDD consistently outperforms representative baselines across multiple metrics, achieving a 6.33\% improvement in recall and 7.63\% improvement in precision. Furthermore, case studies on individual patients illustrate that CLDD can successfully recover masked diseases within its top-ranked predictions, demonstrating both interpretability and reliability in disease prediction. By reducing diagnostic costs and improving accessibility, CLDD holds promise for large-scale disease screening and social health security.

</details>


### [38] [SA^2GFM: Enhancing Robust Graph Foundation Models with Structure-Aware Semantic Augmentation](https://arxiv.org/abs/2512.07857)
*Junhua Shi,Qingyun Sun,Haonan Yuan,Xingcheng Fu*

Main category: cs.LG

TL;DR: SA^2GFM是一个鲁棒的图基础模型框架，通过结构感知语义增强来改进领域自适应表示，在节点和图分类任务中优于9个最先进的基线方法。


<details>
  <summary>Details</summary>
Motivation: 当前图基础模型在各种任务中取得了显著进展，但其对领域噪声、结构扰动和对抗攻击的鲁棒性尚未充分探索。关键限制在于对层次结构语义的建模不足，而这些语义对于泛化至关重要。

Method: 1. 通过将基于熵的编码树转换为结构感知文本提示来进行特征增强；2. 使用自监督信息瓶颈机制通过结构引导压缩来提取鲁棒、可迁移的表示；3. 引入专家自适应路由机制，结合混合专家架构和空专家设计来解决跨领域适应中的负迁移问题；4. 提出微调模块，通过联合社区内和社区间结构学习来优化层次结构。

Result: 大量实验表明，SA^2GFM在节点和图分类任务中，在对抗随机噪声和对抗扰动的有效性和鲁棒性方面优于9个最先进的基线方法。

Conclusion: SA^2GFM通过结构感知语义增强和自适应路由机制，显著提高了图基础模型在噪声和对抗环境下的鲁棒性和领域自适应能力。

Abstract: We present Graph Foundation Models (GFMs) which have made significant progress in various tasks, but their robustness against domain noise, structural perturbations, and adversarial attacks remains underexplored. A key limitation is the insufficient modeling of hierarchical structural semantics, which are crucial for generalization. In this paper, we propose SA^2GFM, a robust GFM framework that improves domain-adaptive representations through Structure-Aware Semantic Augmentation. First, we encode hierarchical structural priors by transforming entropy-based encoding trees into structure-aware textual prompts for feature augmentation. The enhanced inputs are processed by a self-supervised Information Bottleneck mechanism that distills robust, transferable representations via structure-guided compression. To address negative transfer in cross-domain adaptation, we introduce an expert adaptive routing mechanism, combining a mixture-of-experts architecture with a null expert design. For efficient downstream adaptation, we propose a fine-tuning module that optimizes hierarchical structures through joint intra- and inter-community structure learning. Extensive experiments demonstrate that SA^2GFM outperforms 9 state-of-the-art baselines in terms of effectiveness and robustness against random noise and adversarial perturbations for node and graph classification.

</details>


### [39] [FAIM: Frequency-Aware Interactive Mamba for Time Series Classification](https://arxiv.org/abs/2512.07858)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Yanhan Zhang,Junyu Gao,Feiping Nie,Xuelong Li*

Main category: cs.LG

TL;DR: FAIM是一个轻量级的频率感知交互式Mamba模型，用于时间序列分类任务，通过自适应滤波块和交互式Mamba块实现高效的多粒度信息交互，在多个基准测试中优于现有SOTA方法。


<details>
  <summary>Details</summary>
Motivation: 时间序列分类在环境监测、医疗诊断等应用中很重要，但现有深度学习模型存在计算成本高、对噪声敏感、在小数据集上容易过拟合等问题，需要更轻量且鲁棒的解决方案。

Method: 提出FAIM模型，包含自适应滤波块（AFB）利用傅里叶变换提取频域特征，采用可学习自适应阈值动态抑制噪声；设计交互式Mamba块（IMB）实现多粒度信息交互；引入自监督预训练机制增强对复杂时序模式的理解。

Result: 在多个基准测试上的广泛实验表明，FAIM始终优于现有的最先进方法，在准确性和效率之间实现了优越的平衡，并在各种领域和高噪声场景中表现出色。

Conclusion: FAIM通过频域特征提取、自适应噪声抑制和多粒度信息交互，为时间序列分类提供了一个轻量级、鲁棒且高效的解决方案，在准确性和效率方面都表现出色。

Abstract: Time series classification (TSC) is crucial in numerous real-world applications, such as environmental monitoring, medical diagnosis, and posture recognition. TSC tasks require models to effectively capture discriminative information for accurate class identification. Although deep learning architectures excel at capturing temporal dependencies, they often suffer from high computational cost, sensitivity to noise perturbations, and susceptibility to overfitting on small-scale datasets. To address these challenges, we propose FAIM, a lightweight Frequency-Aware Interactive Mamba model. Specifically, we introduce an Adaptive Filtering Block (AFB) that leverages Fourier Transform to extract frequency-domain features from time series data. The AFB incorporates learnable adaptive thresholds to dynamically suppress noise and employs element-wise coupling of global and local semantic adaptive filtering, enabling in-depth modeling of the synergy among different frequency components. Furthermore, we design an Interactive Mamba Block (IMB) to facilitate efficient multi-granularity information interaction, balancing the extraction of fine-grained discriminative features and comprehensive global contextual information, thereby endowing FAIM with powerful and expressive representations for TSC tasks. Additionally, we incorporate a self-supervised pre-training mechanism to enhance FAIM's understanding of complex temporal patterns and improve its robustness across various domains and high-noise scenarios. Extensive experiments on multiple benchmarks demonstrate that FAIM consistently outperforms existing state-of-the-art (SOTA) methods, achieving a superior trade-off between accuracy and efficiency and exhibits outstanding performance.

</details>


### [40] [SetAD: Semi-Supervised Anomaly Learning in Contextual Sets](https://arxiv.org/abs/2512.07863)
*Jianling Gao,Chongyang Tao,Xuelian Lin,Junfeng Liu,Shuai Ma*

Main category: cs.LG

TL;DR: SetAD将半监督异常检测重构为集合级任务，通过注意力集合编码器和分级学习目标，直接建模定义异常的复杂群体级交互，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有半监督异常检测方法通常围绕单个点或简单对进行评分，这种点对中心视角不仅忽略了异常的情境性本质（异常是通过与集体群体的偏离来定义的），而且未能利用集合组合可以生成的丰富监督信号。因此，这些模型难以利用数据中的高阶交互，而这些交互对于学习判别性表征至关重要。

Method: 提出SetAD框架，将半监督异常检测重构为集合级异常检测任务。采用基于注意力的集合编码器，通过分级学习目标进行训练，模型学习量化整个集合的异常程度。此外，提出上下文校准的异常评分机制，通过聚合点在多个不同上下文集合中相对于同伴行为的归一化偏差来评估点的异常分数。

Result: 在10个真实世界数据集上的广泛实验表明，SetAD显著优于最先进模型。特别值得注意的是，模型性能随着集合大小的增加而持续提升，为基于集合的异常检测公式提供了强有力的实证支持。

Conclusion: SetAD通过将异常检测重构为集合级任务，直接建模定义异常的复杂群体级交互，解决了现有点对中心方法的局限性。上下文校准评分机制增强了鲁棒性和分数校准，实验结果表明该方法在真实数据集上具有优越性能。

Abstract: Semi-supervised anomaly detection (AD) has shown great promise by effectively leveraging limited labeled data. However, existing methods are typically structured around scoring individual points or simple pairs. Such {point- or pair-centric} view not only overlooks the contextual nature of anomalies, which are defined by their deviation from a collective group, but also fails to exploit the rich supervisory signals that can be generated from the combinatorial composition of sets. Consequently, such models struggle to exploit the high-order interactions within the data, which are critical for learning discriminative representations. To address these limitations, we propose SetAD, a novel framework that reframes semi-supervised AD as a Set-level Anomaly Detection task. SetAD employs an attention-based set encoder trained via a graded learning objective, where the model learns to quantify the degree of anomalousness within an entire set. This approach directly models the complex group-level interactions that define anomalies. Furthermore, to enhance robustness and score calibration, we propose a context-calibrated anomaly scoring mechanism, which assesses a point's anomaly score by aggregating its normalized deviations from peer behavior across multiple, diverse contextual sets. Extensive experiments on 10 real-world datasets demonstrate that SetAD significantly outperforms state-of-the-art models. Notably, we show that our model's performance consistently improves with increasing set size, providing strong empirical support for the set-based formulation of anomaly detection.

</details>


### [41] [Pattern Recognition of Ozone-Depleting Substance Exports in Global Trade Data](https://arxiv.org/abs/2512.07864)
*Muhammad Sukri Bin Ramli*

Main category: cs.LG

TL;DR: 使用无监督机器学习框架分析海关数据，检测可疑贸易模式，为环境条约（如蒙特利尔议定书）监管提供优先级审查建议


<details>
  <summary>Details</summary>
Motivation: 需要新方法来监控环境条约（如蒙特利尔议定书），通过审查大型复杂海关数据集来发现可疑贸易活动

Method: 结合多种无监督机器学习技术：K-Means聚类发现贸易原型；隔离森林和IQR进行异常检测识别"超大贸易"和异常单价；启发式标记检测模糊描述等策略；综合成优先级评分系统

Result: 成功识别1,351个价格异常值和1,288个高优先级货物供海关审查；发现高优先级商品的价值重量比与普通商品不同且更高；SHAP解释性AI确认模糊描述和高价值是最重要的风险预测因子；模型检测到2021年初"超大贸易"激增，与美国AIM法案监管影响直接相关

Conclusion: 提出了可重复的无监督学习流程，将原始贸易数据转化为监管机构可用的优先级情报，为环境条约监控提供了有效工具

Abstract: New methods are needed to monitor environmental treaties, like the Montreal Protocol, by reviewing large, complex customs datasets. This paper introduces a framework using unsupervised machine learning to systematically detect suspicious trade patterns and highlight activities for review. Our methodology, applied to 100,000 trade records, combines several ML techniques. Unsupervised Clustering (K-Means) discovers natural trade archetypes based on shipment value and weight. Anomaly Detection (Isolation Forest and IQR) identifies rare "mega-trades" and shipments with commercially unusual price-per-kilogram values. This is supplemented by Heuristic Flagging to find tactics like vague shipment descriptions. These layers are combined into a priority score, which successfully identified 1,351 price outliers and 1,288 high-priority shipments for customs review. A key finding is that high-priority commodities show a different and more valuable value-to-weight ratio than general goods. This was validated using Explainable AI (SHAP), which confirmed vague descriptions and high value as the most significant risk predictors. The model's sensitivity was validated by its detection of a massive spike in "mega-trades" in early 2021, correlating directly with the real-world regulatory impact of the US AIM Act. This work presents a repeatable unsupervised learning pipeline to turn raw trade data into prioritized, usable intelligence for regulatory groups.

</details>


### [42] [Using Text-Based Life Trajectories from Swedish Register Data to Predict Residential Mobility with Pretrained Transformers](https://arxiv.org/abs/2512.07865)
*Philipp Stark,Alexandros Sopasakis,Ola Hall,Markus Grillitsch*

Main category: cs.LG

TL;DR: 将瑞典大规模登记数据转化为文本化生命轨迹，解决分类变量高基数和编码不一致问题，用于预测居民迁移行为


<details>
  <summary>Details</summary>
Motivation: 解决数据分析中长期存在的两个挑战：分类变量的高基数和随时间变化的编码方案不一致性，同时利用瑞典独特全面的人口登记数据进行纵向预测研究

Method: 将690万个体（2001-2013年）的登记数据转化为语义丰富的文本化生命轨迹，结合人口统计信息和年度居住、工作、教育、收入、家庭状况变化，使用多种NLP架构（LSTM、DistilBERT、BERT、Qwen）预测2013-2017年的居民迁移

Result: 序列化和基于transformer的模型比基线模型更有效地捕捉时间和语义结构，文本化登记数据保留了关于个体路径的有意义信息，支持复杂、可扩展的建模

Conclusion: 语义丰富的登记数据与现代语言模型相结合，可以显著推进社会科学中的纵向分析，为开发和评估新的序列建模方法提供了严格的测试平台

Abstract: We transform large-scale Swedish register data into textual life trajectories to address two long-standing challenges in data analysis: high cardinality of categorical variables and inconsistencies in coding schemes over time. Leveraging this uniquely comprehensive population register, we convert register data from 6.9 million individuals (2001-2013) into semantically rich texts and predict individuals' residential mobility in later years (2013-2017). These life trajectories combine demographic information with annual changes in residence, work, education, income, and family circumstances, allowing us to assess how effectively such sequences support longitudinal prediction. We compare multiple NLP architectures (including LSTM, DistilBERT, BERT, and Qwen) and find that sequential and transformer-based models capture temporal and semantic structure more effectively than baseline models. The results show that textualized register data preserves meaningful information about individual pathways and supports complex, scalable modeling. Because few countries maintain longitudinal microdata with comparable coverage and precision, this dataset enables analyses and methodological tests that would be difficult or impossible elsewhere, offering a rigorous testbed for developing and evaluating new sequence-modeling approaches. Overall, our findings demonstrate that combining semantically rich register data with modern language models can substantially advance longitudinal analysis in social sciences.

</details>


### [43] [Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion](https://arxiv.org/abs/2512.07873)
*Ci Zhang,Huayu Li,Changdi Yang,Jiangnan Xia,Yanzhi Wang,Xiaolong Ma,Jin Lu,Geng Yuan*

Main category: cs.LG

TL;DR: 提出基于混合专家（MoE）的噪声估计器，用于医学时间序列信号重建，通过RFAMoE模块自适应选择感受野，Fusion MoE模块并行生成多个噪声信号并融合，单次推理完成重建，显著提升性能并降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 医学时间序列信号具有多变量、高时间变异性、高噪声和易受伪影影响等独特特性，使得基于深度学习的插补等任务仍然具有挑战性。扩散模型在时间序列重建中显示出潜力，但在医学领域尚未充分探索。

Method: 提出基于分数扩散框架的混合专家（MoE）噪声估计器：1）RFAMoE模块使每个通道在扩散过程中自适应选择所需感受野；2）Fusion MoE模块并行生成K个噪声信号，通过路由机制融合，单次推理完成信号重建。

Result: 实验结果表明，所提框架在不同任务和数据集上始终优于基于扩散的SOTA方法，不仅提高了性能，还消除了多次推理过程带来的显著计算成本和延迟。

Conclusion: 提出的MoE-based噪声估计器框架有效解决了医学时间序列信号重建的挑战，通过自适应感受野选择和并行噪声生成融合机制，在保持高性能的同时显著降低了计算复杂度。

Abstract: Recent studies show that using diffusion models for time series signal reconstruc- tion holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adap- tively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency over- head. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.

</details>


### [44] [Softly Symbolifying Kolmogorov-Arnold Networks](https://arxiv.org/abs/2512.07875)
*James Bagrow,Josh Bongard*

Main category: cs.LG

TL;DR: S2KAN通过将符号基元集成到KAN训练中，使用可学习的门控机制稀疏化表示，在保持准确性的同时提高可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统KAN虽然提供了可解释性潜力，但训练后的激活函数往往缺乏符号保真度，学习到的分解没有有意义的对应关系，需要改进以获得真正的可解释性。

Method: 提出Softly Symbolified KANs (S2KAN)，将符号基元直接集成到训练中，每个激活函数从符号项和密集项的字典中提取，使用可学习的门控机制稀疏化表示，通过最小描述长度目标指导稀疏化过程。

Result: 在符号基准测试、动态系统预测和真实世界预测任务中，S2KAN实现了竞争性或更优的准确性，同时模型规模显著减小，并观察到即使没有正则化压力也会出现自稀疏化现象。

Conclusion: S2KAN在保持KAN准确性的同时显著提高了可解释性，当符号项足够时能发现可解释形式，不足时则优雅地退化为密集样条，为可解释机器学习提供了有前景的路径。

Abstract: Kolmogorov-Arnold Networks (KANs) offer a promising path toward interpretable machine learning: their learnable activations can be studied individually, while collectively fitting complex data accurately. In practice, however, trained activations often lack symbolic fidelity, learning pathological decompositions with no meaningful correspondence to interpretable forms. We propose Softly Symbolified Kolmogorov-Arnold Networks (S2KAN), which integrate symbolic primitives directly into training. Each activation draws from a dictionary of symbolic and dense terms, with learnable gates that sparsify the representation. Crucially, this sparsification is differentiable, enabling end-to-end optimization, and is guided by a principled Minimum Description Length objective. When symbolic terms suffice, S2KAN discovers interpretable forms; when they do not, it gracefully degrades to dense splines. We demonstrate competitive or superior accuracy with substantially smaller models across symbolic benchmarks, dynamical systems forecasting, and real-world prediction tasks, and observe evidence of emergent self-sparsification even without regularization pressure.

</details>


### [45] [Graph Contrastive Learning via Spectral Graph Alignment](https://arxiv.org/abs/2512.07878)
*Manh Nguyen,Joshua Cape*

Main category: cs.LG

TL;DR: SpecMatch-CL是一种新的对比学习损失函数，通过最小化视图特定图-图的归一化拉普拉斯矩阵差异来对齐图嵌入，在无监督和半监督图学习任务中取得SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有对比学习方法（如InfoNCE）只优化跨视图的成对图嵌入对齐，但无法控制从这些嵌入构建的视图特定图-图的全局结构。需要一种能够对齐图-图全局结构的方法。

Method: 提出SpecMatch-CL损失函数，通过最小化视图特定图-图的归一化拉普拉斯矩阵差异来对齐图嵌入。理论上证明归一化拉普拉斯矩阵差异为理想完美对齐对比损失与当前损失之间的差异提供了上界。

Result: 在8个TU基准测试的无监督学习和低标签率半监督学习中建立新的SOTA，在PPI-306K和ZINC 2M数据集的迁移学习中获得一致性能提升。

Conclusion: SpecMatch-CL通过对齐图-图的全局结构，显著提升了图对比学习的性能，为图表示学习提供了有效的理论框架和实用方法。

Abstract: Given augmented views of each input graph, contrastive learning methods (e.g., InfoNCE) optimize pairwise alignment of graph embeddings across views while providing no mechanism to control the global structure of the view specific graph-of-graphs built from these embeddings. We introduce SpecMatch-CL, a novel loss function that aligns the view specific graph-of-graphs by minimizing the difference between their normalized Laplacians. Theoretically, we show that under certain assumptions, the difference between normalized Laplacians provides an upper bound not only for the difference between the ideal Perfect Alignment contrastive loss and the current loss, but also for the Uniformly loss. Empirically, SpecMatch-CL establishes new state of the art on eight TU benchmarks under unsupervised learning and semi-supervised learning at low label rates, and yields consistent gains in transfer learning on PPI-306K and ZINC 2M datasets.

</details>


### [46] [Nonnegative Matrix Factorization through Cone Collapse](https://arxiv.org/abs/2512.07879)
*Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 论文提出Cone Collapse算法，从几何角度重新审视非负矩阵分解(NMF)，通过收缩非负象限到数据生成的最小锥体来恢复极端射线，并在此基础上构建锥体感知的正交NMF模型(CC-NMF)，在聚类任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有NMF算法主要从优化角度出发，未能充分利用NMF诱导的锥体几何结构。数据点位于凸锥中，其极端射线编码了基本方向或"主题"。从几何视角重新审视NMF，可以更好地理解数据的内在结构。

Method: 提出Cone Collapse算法：从完整的非负象限开始，迭代收缩到数据生成的最小锥体。在温和的数据假设下，该算法在有限步内终止并恢复X^⊤的最小生成锥体。基于此，通过将单正交NMF应用于恢复的极端射线，推导出锥体感知的正交NMF模型(CC-NMF)。

Result: 在16个基准基因表达、文本和图像数据集上，CC-NMF在聚类纯度方面始终匹配或优于强NMF基线方法，包括乘法更新、ANLS、投影NMF、ONMF和稀疏NMF。这些结果表明显式恢复数据锥体可以产生既有理论依据又具有强大实证性能的NMF聚类方法。

Conclusion: 从几何角度重新审视NMF，通过Cone Collapse算法显式恢复数据锥体，可以开发出理论上有依据且实证表现优异的NMF聚类方法。锥体感知方法为NMF提供了新的视角和更强大的工具。

Abstract: Nonnegative matrix factorization (NMF) is a widely used tool for learning parts-based, low-dimensional representations of nonnegative data, with applications in vision, text, and bioinformatics. In clustering applications, orthogonal NMF (ONMF) variants further impose (approximate) orthogonality on the representation matrix so that its rows behave like soft cluster indicators. Existing algorithms, however, are typically derived from optimization viewpoints and do not explicitly exploit the conic geometry induced by NMF: data points lie in a convex cone whose extreme rays encode fundamental directions or "topics". In this work we revisit NMF from this geometric perspective and propose Cone Collapse, an algorithm that starts from the full nonnegative orthant and iteratively shrinks it toward the minimal cone generated by the data. We prove that, under mild assumptions on the data, Cone Collapse terminates in finitely many steps and recovers the minimal generating cone of $\mathbf{X}^\top$ . Building on this basis, we then derive a cone-aware orthogonal NMF model (CC-NMF) by applying uni-orthogonal NMF to the recovered extreme rays. Across 16 benchmark gene-expression, text, and image datasets, CC-NMF consistently matches or outperforms strong NMF baselines-including multiplicative updates, ANLS, projective NMF, ONMF, and sparse NMF-in terms of clustering purity. These results demonstrate that explicitly recovering the data cone can yield both theoretically grounded and empirically strong NMF-based clustering methods.

</details>


### [47] [Semi-Supervised Contrastive Learning with Orthonormal Prototypes](https://arxiv.org/abs/2512.07880)
*Huanran Li,Manh Nguyen,Daniel Pimentel-Alarcón*

Main category: cs.LG

TL;DR: 论文提出CLOP损失函数，通过促进类别嵌入的正交线性子空间形成来防止对比学习中的维度塌缩问题，在图像分类和目标检测任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 对比学习在深度学习中表现出色，但维度塌缩问题（嵌入收敛到低维空间）在半监督和自监督设置中构成重大挑战，特别是在高学习率下会导致塌缩解。

Method: 首先识别了导致维度塌缩的关键学习率阈值，然后提出CLOP损失函数，通过促进类别嵌入的正交线性子空间形成来防止维度塌缩。

Result: 在真实和合成数据集上的广泛实验表明，CLOP在图像分类和目标检测任务中提高了性能，同时在不同学习率和批量大小下表现出更好的稳定性。

Conclusion: CLOP是一种有效的半监督损失函数，能够防止对比学习中的维度塌缩问题，提高模型性能并增强训练稳定性。

Abstract: Contrastive learning has emerged as a powerful method in deep learning, excelling at learning effective representations through contrasting samples from different distributions. However, dimensional collapse, where embeddings converge into a lower-dimensional space, poses a significant challenge, especially in semi-supervised and self-supervised setups. In this paper, we first identify a critical learning-rate threshold, beyond which standard contrastive losses converge to collapsed solutions. Building on these insights, we propose CLOP, a novel semi-supervised loss function designed to prevent dimensional collapse by promoting the formation of orthogonal linear subspaces among class embeddings. Through extensive experiments on real and synthetic datasets, we demonstrate that CLOP improves performance in image classification and object detection tasks while also exhibiting greater stability across different learning rates and batch sizes.

</details>


### [48] [GSPN-2: Efficient Parallel Sequence Modeling](https://arxiv.org/abs/2512.07884)
*Hongjun Wang,Yitong Jiang,Collin McCarthy,David Wehr,Hanrong Ye,Xinhao Li,Ka Chun Cheung,Wonmin Byeon,Jinwei Gu,Ke Chen,Kai Han,Hongxu Yin,Pavlo Molchanov,Jan Kautz,Sifei Liu*

Main category: cs.LG

TL;DR: GSPN-2是对GSPN的改进，通过算法-系统联合重设计，解决了原实现中的GPU内核重复启动、全局内存数据传输和通道间冗余计算问题，在保持精度的同时显著降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 虽然GSPN通过线扫描传播方案将自注意力计算成本降低到接近线性，但其实现仍存在GPU内核重复启动、全局内存数据传输和通道间冗余计算等问题，限制了实际应用效率。

Method: 1. 系统优化：将数千个微启动合并为单个2D内核，为每个通道片固定一个warp，将前一列激活暂存到共享内存；2. 模型优化：引入紧凑通道传播策略，替代逐通道矩阵，减少参数，并与transformer注意力中的亲和力图自然对齐。

Result: GSPN-2在图像分类和文本到图像合成任务中表现出色，能够匹配transformer级别的准确度，同时显著降低计算成本，为视觉应用中的全局空间上下文建模建立了新的效率边界。

Conclusion: GSPN-2通过独特的结构化矩阵变换和GPU优化实现，为高分辨率图像和长视频相关应用提供了高效的全局空间上下文建模解决方案，在保持精度的同时大幅提升了计算效率。

Abstract: Efficient vision transformer remains a bottleneck for high-resolution images and long-video related real-world applications. Generalized Spatial Propagation Network (GSPN) addresses this by replacing quadratic self-attention with a line-scan propagation scheme, bringing the cost close to linear in the number of rows or columns, while retaining accuracy. Despite this advancement, the existing GSPN implementation still suffers from (i) heavy overhead due to repeatedly launching GPU kernels, (ii) excessive data transfers from global GPU memory, and (iii) redundant computations caused by maintaining separate propagation weights for each channel. We introduce GSPN-2, a joint algorithm-system redesign. In particular, we eliminate thousands of micro-launches from the previous implementation into one single 2D kernel, explicitly pin one warp to each channel slice, and stage the previous column's activations in shared memory. On the model side, we introduce a compact channel propagation strategy that replaces per-channel matrices, trimming parameters, and align naturally with the affinity map used in transformer attention. Experiments demonstrate GSPN-2's effectiveness across image classification and text-to-image synthesis tasks, matching transformer-level accuracy with significantly lower computational cost. GSPN-2 establishes a new efficiency frontier for modeling global spatial context in vision applications through its unique combination of structured matrix transformations and GPU-optimized implementation. Project page: https://whj363636.github.io/GSPN2/

</details>


### [49] [Towards symbolic regression for interpretable clinical decision scores](https://arxiv.org/abs/2512.07961)
*Guilherme Seidyo Imai Aldeia,Joseph D. Romano,Fabricio Olivetti de Franca,Daniel S. Herman,William G. La Cava*

Main category: cs.LG

TL;DR: Brush是一种将决策树分割算法与非线性常数优化相结合的符号回归方法，能够将基于规则的逻辑无缝集成到符号回归和分类模型中，在医疗决策中表现出色。


<details>
  <summary>Details</summary>
Motivation: 医疗决策经常使用结合风险方程和规则的算法，但传统符号回归难以建模这种决策过程。然而，由于其能够推导数据驱动的可解释模型，符号回归在开发数据驱动的临床风险评分方面具有潜力。

Method: 提出Brush算法，将决策树式的分割算法与非线性常数优化相结合，允许将基于规则的逻辑无缝集成到符号回归和分类模型中。

Result: Brush在SRBench上实现了帕累托最优性能，成功复现了两个广泛使用的临床评分系统，实现了高准确性和可解释模型。与决策树、随机森林和其他符号回归方法相比，Brush实现了相当或更优的预测性能，同时产生更简单的模型。

Conclusion: Brush算法能够有效建模医疗决策过程，结合了规则逻辑和符号回归的优势，在保持高预测性能的同时提供更简单、可解释的模型，有望用于开发数据驱动的临床风险评分系统。

Abstract: Medical decision-making makes frequent use of algorithms that combine risk equations with rules, providing clear and standardized treatment pathways. Symbolic regression (SR) traditionally limits its search space to continuous function forms and their parameters, making it difficult to model this decision-making. However, due to its ability to derive data-driven, interpretable models, SR holds promise for developing data-driven clinical risk scores. To that end we introduce Brush, an SR algorithm that combines decision-tree-like splitting algorithms with non-linear constant optimization, allowing for seamless integration of rule-based logic into symbolic regression and classification models. Brush achieves Pareto-optimal performance on SRBench, and was applied to recapitulate two widely used clinical scoring systems, achieving high accuracy and interpretable models. Compared to decision trees, random forests, and other SR methods, Brush achieves comparable or superior predictive performance while producing simpler models.

</details>


### [50] [CIP-Net: Continual Interpretable Prototype-based Network](https://arxiv.org/abs/2512.07981)
*Federico Di Valerio,Michela Proietti,Alessio Ragno,Roberto Capobianco*

Main category: cs.LG

TL;DR: CIP-Net是一种无需存储历史样本的自解释原型模型，用于持续学习，在任务和类别增量设置中均取得SOTA性能，同时显著降低内存开销。


<details>
  <summary>Details</summary>
Motivation: 持续学习面临灾难性遗忘的挑战，现有可解释方法大多使用事后解释或需要为每个新任务存储额外内存，导致可扩展性有限。

Method: 提出CIP-Net，一种无需示例的自解释原型模型，不存储过去样本，保持简单架构，同时提供有用解释和强大性能。

Result: CIP-Net在任务和类别增量设置中，相比先前无需示例和自解释方法实现了最先进的性能，同时显著降低了内存相关开销。

Conclusion: CIP-Net为持续学习提供了一个实用且可解释的解决方案，平衡了性能、内存效率和可解释性。

Abstract: Continual learning constrains models to learn new tasks over time without forgetting what they have already learned. A key challenge in this setting is catastrophic forgetting, where learning new information causes the model to lose its performance on previous tasks. Recently, explainable AI has been proposed as a promising way to better understand and reduce forgetting. In particular, self-explainable models are useful because they generate explanations during prediction, which can help preserve knowledge. However, most existing explainable approaches use post-hoc explanations or require additional memory for each new task, resulting in limited scalability. In this work, we introduce CIP-Net, an exemplar-free self-explainable prototype-based model designed for continual learning. CIP-Net avoids storing past examples and maintains a simple architecture, while still providing useful explanations and strong performance. We demonstrate that CIPNet achieves state-of-the-art performances compared to previous exemplar-free and self-explainable methods in both task- and class-incremental settings, while bearing significantly lower memory-related overhead. This makes it a practical and interpretable solution for continual learning.

</details>


### [51] [Benchmarking Offline Multi-Objective Reinforcement Learning in Critical Care](https://arxiv.org/abs/2512.08012)
*Aryaman Bansal,Divya Sharma*

Main category: cs.LG

TL;DR: 本文在重症监护环境中比较了三种离线多目标强化学习算法与三种单目标基线方法，发现PEDA DT算法在灵活性方面表现最优，为个性化医疗决策提供了有前景的框架。


<details>
  <summary>Details</summary>
Motivation: 重症监护环境中临床医生面临平衡患者生存率和资源利用率的复杂挑战，传统单目标强化学习方法采用固定标量化奖励函数，导致策略僵化无法适应变化的临床优先级。

Method: 在MIMIC-IV数据集上对三种离线多目标强化学习算法（CPQL、Adaptive CPQL、PEDA DT）与三种单目标标量化基线方法（BC、CQL、DDQN）进行基准测试，使用离线策略评估指标进行性能比较。

Result: PEDA DT算法在灵活性方面优于静态标量化基线方法，序列建模架构在多目标条件下生成时仍保持鲁棒性和有效性，验证了先前关于单目标决策变换器在医疗领域的研究发现。

Conclusion: 离线多目标强化学习是重症监护中实现个性化、可调整决策的有前景框架，无需重新训练即可适应不同的临床优先级。

Abstract: In critical care settings such as the Intensive Care Unit, clinicians face the complex challenge of balancing conflicting objectives, primarily maximizing patient survival while minimizing resource utilization (e.g., length of stay). Single-objective Reinforcement Learning approaches typically address this by optimizing a fixed scalarized reward function, resulting in rigid policies that fail to adapt to varying clinical priorities. Multi-objective Reinforcement Learning (MORL) offers a solution by learning a set of optimal policies along the Pareto Frontier, allowing for dynamic preference selection at test time. However, applying MORL in healthcare necessitates strict offline learning from historical data.
  In this paper, we benchmark three offline MORL algorithms, Conditioned Conservative Pareto Q-Learning (CPQL), Adaptive CPQL, and a modified Pareto Efficient Decision Agent (PEDA) Decision Transformer (PEDA DT), against three scalarized single-objective baselines (BC, CQL, and DDQN) on the MIMIC-IV dataset. Using Off-Policy Evaluation (OPE) metrics, we demonstrate that PEDA DT algorithm offers superior flexibility compared to static scalarized baselines. Notably, our results extend previous findings on single-objective Decision Transformers in healthcare, confirming that sequence modeling architectures remain robust and effective when scaled to multi-objective conditioned generation. These findings suggest that offline MORL is a promising framework for enabling personalized, adjustable decision-making in critical care without the need for retraining.

</details>


### [52] [CLARITY: Medical World Model for Guiding Treatment Decisions by Modeling Context-Aware Disease Trajectories in Latent Space](https://arxiv.org/abs/2512.08029)
*Tianxingjian Ding,Yuanhao Zou,Chen Chen,Mubarak Shah,Yu Tian*

Main category: cs.LG

TL;DR: CLARITY是一种医学世界模型，通过在结构化潜在空间中预测疾病演化，整合时间间隔和患者特异性数据，生成个体化治疗计划，并在胶质瘤数据集上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前肿瘤临床决策需要预测动态疾病演化，但现有静态AI预测器无法完成此任务。现有医学世界模型方法通常忽略患者特异性时空和临床背景，缺乏将预测与治疗决策连接的反馈机制。

Method: CLARITY是一种医学世界模型，在结构化潜在空间中直接预测疾病演化。它明确整合时间间隔（时间背景）和患者特异性数据（临床背景），将治疗条件进展建模为平滑可解释的轨迹，从而生成生理上可信的个体化治疗计划。还引入了新颖的预测到决策框架，将潜在推演转化为透明可操作的建议。

Result: 在MU-Glioma-Post数据集上，CLARITY方法比最近的MeWM模型性能提升12%，并显著超越所有其他医学专用大语言模型，在治疗规划方面展示了最先进的性能。

Conclusion: CLARITY通过结构化潜在空间中的疾病演化预测，整合时空和临床背景，提供了一种能够生成个体化治疗计划并转化为可操作建议的医学世界模型框架。

Abstract: Clinical decision-making in oncology requires predicting dynamic disease evolution, a task current static AI predictors cannot perform. While world models (WMs) offer a paradigm for generative prediction, existing medical applications remain limited. Existing methods often rely on stochastic diffusion models, focusing on visual reconstruction rather than causal, physiological transitions. Furthermore, in medical domain, models like MeWM typically ignore patient-specific temporal and clinical contexts and lack a feedback mechanism to link predictions to treatment decisions. To address these gaps, we introduce CLARITY, a medical world model that forecasts disease evolution directly within a structured latent space. It explicitly integrates time intervals (temporal context) and patient-specific data (clinical context) to model treatment-conditioned progression as a smooth, interpretable trajectory, and thus generate physiologically faithful, individualized treatment plans. Finally, CLARITY introduces a novel prediction-to-decision framework, translating latent rollouts into transparent, actionable recommendations. CLARITY demonstrates state-of-the-art performance in treatment planning. On the MU-Glioma-Post dataset, our approach outperforms recent MeWM by 12\%, and significantly surpasses all other medical-specific large language models.

</details>


### [53] [LUNA: Linear Universal Neural Attention with Generalization Guarantees](https://arxiv.org/abs/2512.08061)
*Ashkan Shahbazi,Ping He,Ali Abbasi,Yikun Bai,Xinran Liu,Elaheh Akbari,Darian Salehi,Navid NaderiAlizadeh,Soheil Kolouri*

Main category: cs.LG

TL;DR: LUNA是一种可学习的核化线性注意力机制，在保持线性计算成本的同时，匹配并超越了传统二次方注意力机制的准确性，解决了线性注意力中固定特征映射导致的精度-效率权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统softmax注意力存在O(n²)的二次计算成本，限制了其在长序列领域的应用。现有的线性注意力机制虽然将成本降至O(n)，但依赖固定的随机特征映射，导致模型精度与计算效率之间存在根本性权衡。

Method: LUNA的核心创新在于将核特征映射本身设计为可学习的参数，而非固定的先验函数。通过参数化核函数，LUNA学习针对特定数据和任务的特征基，克服了固定特征方法的表达能力限制。该方法实现了一个可学习的特征映射，该映射诱导出正定核，并允许流式处理，从而在序列长度上实现线性时间和内存缩放。

Result: 在Long Range Arena（LRA）基准测试中，LUNA在计算对等条件下（相同参数量、训练步数和近似FLOPs）取得了高效Transformer中的最先进平均准确率。此外，在BERT和ViT-B/16微调检查点中替换softmax并进行简短微调后，LUNA能够恢复大部分原始性能，显著优于固定的线性化方法。

Conclusion: LUNA成功消除了线性注意力中精度与效率之间的权衡，通过可学习的核特征映射实现了线性计算成本下与二次方注意力相当甚至更优的性能，为长序列处理提供了有效的解决方案。

Abstract: Scaling attention faces a critical bottleneck: the $\mathcal{O}(n^2)$ quadratic computational cost of softmax attention, which limits its application in long-sequence domains. While linear attention mechanisms reduce this cost to $\mathcal{O}(n)$, they typically rely on fixed random feature maps, such as random Fourier features or hand-crafted functions. This reliance on static, data-agnostic kernels creates a fundamental trade-off, forcing practitioners to sacrifice significant model accuracy for computational efficiency. We introduce \textsc{LUNA}, a kernelized linear attention mechanism that eliminates this trade-off, retaining linear cost while matching and surpassing the accuracy of quadratic attention. \textsc{LUNA} is built on the key insight that the kernel feature map itself should be learned rather than fixed a priori. By parameterizing the kernel, \textsc{LUNA} learns a feature basis tailored to the specific data and task, overcoming the expressive limitations of fixed-feature methods. \textsc{Luna} implements this with a learnable feature map that induces a positive-definite kernel and admits a streaming form, yielding linear time and memory scaling in the sequence length. Empirical evaluations validate our approach across diverse settings. On the Long Range Arena (LRA), \textsc{Luna} achieves state-of-the-art average accuracy among efficient Transformers under compute parity, using the same parameter count, training steps, and approximate FLOPs. \textsc{Luna} also excels at post-hoc conversion: replacing softmax in fine-tuned BERT and ViT-B/16 checkpoints and briefly fine-tuning recovers most of the original performance, substantially outperforming fixed linearizations.

</details>


### [54] [Deep Kernel Aalen-Johansen Estimator: An Interpretable and Flexible Neural Net Framework for Competing Risks](https://arxiv.org/abs/2512.08063)
*Xiaobin Shen,George H. Chen*

Main category: cs.LG

TL;DR: 提出了一种可解释的深度竞争风险模型DKAJ，它通过自动学习的核函数将数据点表示为簇的加权组合，在保持Aalen-Johansen估计器优点的同时提供可视化解释。


<details>
  <summary>Details</summary>
Motivation: 现有竞争风险模型在预测准确性和可解释性之间存在权衡。经典的Aalen-Johansen估计器虽然可解释但缺乏灵活性，而现代深度学习方法虽然准确但难以解释。需要一种既能保持预测性能又能提供解释性的方法。

Method: 提出Deep Kernel Aalen-Johansen (DKAJ)估计器，将每个数据点表示为多个簇的加权组合。通过自动学习的核函数衡量数据点之间的相似性，生成权重。当数据点仅对一个簇有非零权重时，其预测CIFs对应于该簇的经典Aalen-Johansen估计。

Result: 在四个标准竞争风险数据集上，DKAJ与最先进的基线方法竞争性相当，同时能够提供可视化辅助模型解释，实现了预测准确性和可解释性的良好平衡。

Conclusion: DKAJ成功地将经典Aalen-Johansen估计器推广到深度学习方法中，在保持竞争性预测性能的同时提供了模型可解释性，为竞争风险分析提供了一种新的可解释深度学习框架。

Abstract: We propose an interpretable deep competing risks model called the Deep Kernel Aalen-Johansen (DKAJ) estimator, which generalizes the classical Aalen-Johansen nonparametric estimate of cumulative incidence functions (CIFs). Each data point (e.g., patient) is represented as a weighted combination of clusters. If a data point has nonzero weight only for one cluster, then its predicted CIFs correspond to those of the classical Aalen-Johansen estimator restricted to data points from that cluster. These weights come from an automatically learned kernel function that measures how similar any two data points are. On four standard competing risks datasets, we show that DKAJ is competitive with state-of-the-art baselines while being able to provide visualizations to assist model interpretation.

</details>


### [55] [CAMO: Causality-Guided Adversarial Multimodal Domain Generalization for Crisis Classification](https://arxiv.org/abs/2512.08071)
*Pingchuan Ma,Chengshuai Zhao,Bohan Jiang,Saketh Vishnubhatla,Ujun Jeong,Alimohammad Beigi,Adrienne Raglin,Huan Liu*

Main category: cs.LG

TL;DR: 提出因果引导的多模态域泛化框架，通过对抗解缠和统一表示学习提升社交媒体危机分类在未见灾难类型上的泛化能力


<details>
  <summary>Details</summary>
Motivation: 现有社交媒体危机分类方法主要依赖深度学习融合文本和视觉线索，在域内设置下取得数值上合理的结果，但在未见危机类型上泛化能力差，原因包括：1. 未能解缠虚假特征和因果特征，导致域偏移下性能下降；2. 未能对齐异构模态表示，阻碍单模态域泛化技术直接应用于多模态设置

Method: 提出因果引导的多模态域泛化框架，结合对抗解缠和统一表示学习。对抗目标鼓励模型解缠并关注域不变的因果特征，统一表示将不同模态特征对齐到共享潜在空间，使单模态域泛化策略能无缝扩展到多模态学习

Result: 在不同数据集上的实验表明，该方法在未见灾难场景中实现了最佳性能

Conclusion: 通过因果引导的多模态域泛化框架，能够有效提升社交媒体危机分类在未见灾难类型上的泛化能力，为增强态势感知和促进及时应急响应提供支持

Abstract: Crisis classification in social media aims to extract actionable disaster-related information from multimodal posts, which is a crucial task for enhancing situational awareness and facilitating timely emergency responses. However, the wide variation in crisis types makes achieving generalizable performance across unseen disasters a persistent challenge. Existing approaches primarily leverage deep learning to fuse textual and visual cues for crisis classification, achieving numerically plausible results under in-domain settings. However, they exhibit poor generalization across unseen crisis types because they 1. do not disentangle spurious and causal features, resulting in performance degradation under domain shift, and 2. fail to align heterogeneous modality representations within a shared space, which hinders the direct adaptation of established single-modality domain generalization (DG) techniques to the multimodal setting. To address these issues, we introduce a causality-guided multimodal domain generalization (MMDG) framework that combines adversarial disentanglement with unified representation learning for crisis classification. The adversarial objective encourages the model to disentangle and focus on domain-invariant causal features, leading to more generalizable classifications grounded in stable causal mechanisms. The unified representation aligns features from different modalities within a shared latent space, enabling single-modality DG strategies to be seamlessly extended to multimodal learning. Experiments on the different datasets demonstrate that our approach achieves the best performance in unseen disaster scenarios.

</details>


### [56] [Scalable Offline Model-Based RL with Action Chunks](https://arxiv.org/abs/2512.08108)
*Kwanyoung Park,Seohong Park,Youngwoon Lee,Sergey Levine*

Main category: cs.LG

TL;DR: MAC提出了一种基于模型的离线强化学习方法，通过动作块模型减少长期预测误差，并使用拒绝采样防止模型利用，在复杂长时域任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 研究基于模型的强化学习（特别是基于模型的价值扩展）是否能为离线RL中的复杂长时域任务提供可扩展的解决方案。传统方法中，更大的n值虽然减少了价值引导的偏差，但会放大长期累积的模型误差。

Method: 1. 提出动作块模型：预测未来状态时使用动作序列（动作块）而非单个动作，减少复合误差；2. 采用拒绝采样：从表达性强的行为动作块策略中进行拒绝采样，防止模型利用分布外动作；3. 整体方法称为基于模型的强化学习与动作块（MAC）。

Result: 在包含高达1亿个转移的大规模数据集上的高度挑战性任务实验中，MAC在离线基于模型的RL算法中取得了最佳性能，特别是在挑战性的长时域任务上。

Conclusion: MAC通过动作块模型减少长期预测误差，结合拒绝采样防止模型利用，为离线RL中的复杂长时域任务提供了一种有效的基于模型解决方案。

Abstract: In this paper, we study whether model-based reinforcement learning (RL), in particular model-based value expansion, can provide a scalable recipe for tackling complex, long-horizon tasks in offline RL. Model-based value expansion fits an on-policy value function using length-n imaginary rollouts generated by the current policy and a learned dynamics model. While larger n reduces bias in value bootstrapping, it amplifies accumulated model errors over long horizons, degrading future predictions. We address this trade-off with an \emph{action-chunk} model that predicts a future state from a sequence of actions (an "action chunk") instead of a single action, which reduces compounding errors. In addition, instead of directly training a policy to maximize rewards, we employ rejection sampling from an expressive behavioral action-chunk policy, which prevents model exploitation from out-of-distribution actions. We call this recipe \textbf{Model-Based RL with Action Chunks (MAC)}. Through experiments on highly challenging tasks with large-scale datasets of up to 100M transitions, we show that MAC achieves the best performance among offline model-based RL algorithms, especially on challenging long-horizon tasks.

</details>


### [57] [Balanced Accuracy: The Right Metric for Evaluating LLM Judges - Explained through Youden's J statistic](https://arxiv.org/abs/2512.08121)
*Stephane Collot,Colin Fraser,Justin Zhao,William F. Shen,Timon Willi,Ilias Leontiadis*

Main category: cs.LG

TL;DR: 论文提出使用Youden's J统计量和平衡准确率来选择评估大语言模型的分类器，相比传统指标能更好地比较模型表现


<details>
  <summary>Details</summary>
Motivation: 当前评估大语言模型时，常用的分类器评估指标（如准确率、精确率、F1分数）对类别不平衡和正类选择敏感，可能导致选择那些扭曲流行率估计的分类器，影响模型比较的可信度

Method: 提出使用Youden's J统计量作为选择最佳分类器的理论依据，证明平衡准确率是J的线性变换，并通过理论分析和实证模拟验证这一方法的有效性

Result: Youden's J统计量在理论上与选择最佳分类器来比较模型表现相一致，使用平衡准确率选择分类器能获得更好、更稳健的分类器选择结果

Conclusion: 在评估大语言模型时，应该使用Youden's J统计量或平衡准确率来选择分类器，而不是传统的准确率、精确率或F1分数，这样可以获得更可靠、更稳健的模型比较结果

Abstract: Rigorous evaluation of large language models (LLMs) relies on comparing models by the prevalence of desirable or undesirable behaviors, such as task pass rates or policy violations. These prevalence estimates are produced by a classifier, either an LLM-as-a-judge or human annotators, making the choice of classifier central to trustworthy evaluation. Common metrics used for this choice, such as Accuracy, Precision, and F1, are sensitive to class imbalance and to arbitrary choices of positive class, and can favor judges that distort prevalence estimates. We show that Youden's $J$ statistic is theoretically aligned with choosing the best judge to compare models, and that Balanced Accuracy is an equivalent linear transformation of $J$. Through both analytical arguments and empirical examples and simulations, we demonstrate how selecting judges using Balanced Accuracy leads to better, more robust classifier selection.

</details>


### [58] [Improving the Sensitivity of Backdoor Detectors via Class Subspace Orthogonalization](https://arxiv.org/abs/2512.08129)
*Guangmingmei Yang,David J. Miller,George Kesidis*

Main category: cs.LG

TL;DR: 提出了一种名为CSO的后门检测方法，通过抑制内在特征来增强对隐蔽后门的检测能力，解决了现有方法在易区分类别和弱后门特征情况下的失效问题。


<details>
  <summary>Details</summary>
Motivation: 现有后门检测方法依赖目标类表现出极端异常检测统计量，但在两种情况下会失效：1）某些非目标类本身容易与其他类区分，自然获得极端统计量；2）后门特征相对于内在类别区分特征较弱时。关键观察是后门目标类的检测统计量来自后门触发器和内在特征两方面，而非目标类仅来自内在特征。

Method: 提出类子空间正交化（CSO）方法，通过抑制内在特征来优化检测统计量。具体使用少量干净样本，在优化检测统计量的同时，将其与类的内在特征正交化。这形成了一个约束优化问题，对于非目标类，这种抑制会大幅降低可达到的统计量，而对于目标类，来自后门触发器的显著贡献仍然保留。

Result: CSO方法能够更敏感地检测后门攻击，特别是在具有挑战性的混合标签攻击和自适应攻击场景下表现出色。该方法解决了现有检测方法在易区分类别和弱后门特征情况下的局限性。

Conclusion: CSO是一种即插即用的后门检测方法，通过正交化抑制内在特征，能够更有效地检测隐蔽的后门攻击，特别是在传统方法容易失效的复杂攻击场景下具有优势。

Abstract: Most post-training backdoor detection methods rely on attacked models exhibiting extreme outlier detection statistics for the target class of an attack, compared to non-target classes. However, these approaches may fail: (1) when some (non-target) classes are easily discriminable from all others, in which case they may naturally achieve extreme detection statistics (e.g., decision confidence); and (2) when the backdoor is subtle, i.e., with its features weak relative to intrinsic class-discriminative features. A key observation is that the backdoor target class has contributions to its detection statistic from both the backdoor trigger and from its intrinsic features, whereas non-target classes only have contributions from their intrinsic features. To achieve more sensitive detectors, we thus propose to suppress intrinsic features while optimizing the detection statistic for a given class. For non-target classes, such suppression will drastically reduce the achievable statistic, whereas for the target class the (significant) contribution from the backdoor trigger remains. In practice, we formulate a constrained optimization problem, leveraging a small set of clean examples from a given class, and optimizing the detection statistic while orthogonalizing with respect to the class's intrinsic features. We dub this plug-and-play approach Class Subspace Orthogonalization (CSO) and assess it against challenging mixed-label and adaptive attacks.

</details>


### [59] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models I: The Task-Query Architecture](https://arxiv.org/abs/2512.08130)
*Gary Ackerman,Brandon Behlendorf,Zachary Kallenborn,Sheriff Almakki,Doug Clifford,Jenna LaTourette,Hayley Peterson,Noah Sheinbaum,Olivia Shoemaker,Anna Wetzel*

Main category: cs.LG

TL;DR: 论文提出首个生物威胁基准生成框架（BBG），专门用于评估AI模型（特别是大语言模型）在细菌生物威胁方面的安全风险，考虑了不同攻击者能力和操作风险因素。


<details>
  <summary>Details</summary>
Motivation: 随着前沿AI模型（特别是大语言模型）的快速发展，模型开发者和政策制定者需要量化并减轻这些模型可能被用于生物恐怖主义或获取生物武器的风险。现有基准测试往往忽略了威胁本身的关键方面，如不同攻击者能力水平和操作风险因素。

Method: 开发了生物威胁基准生成框架（BBG），采用分层结构的生物威胁类别、要素和任务，并以此为基础开发任务对齐的查询。作为试点，首先针对细菌生物威胁开发了"细菌生物威胁模式"。

Result: 提出了细菌生物威胁模式作为BBG框架的第一个组成部分，为评估AI模型在细菌生物风险方面提供了可重复使用的结构框架，能够捕捉生物对手的完整技术操作需求，并考虑广泛的生物对手能力谱系。

Conclusion: BBG框架（包括细菌生物威胁模式）旨在为评估大语言模型在细菌生物风险方面提供稳健、可重复使用的结构，未来研究将把查询转化为模型提示，并实施基准测试进行模型评估。

Abstract: Both model developers and policymakers seek to quantify and mitigate the risk of rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons. An important element of such efforts is the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper describes the first component of a novel Biothreat Benchmark Generation (BBG) Framework. The BBG approach is designed to help model developers and evaluators reliably measure and assess the biosecurity risk uplift and general harm potential of existing and future AI models, while accounting for key aspects of the threat itself that are often overlooked in other benchmarking efforts, including different actor capability levels, and operational (in addition to purely technical) risk factors. As a pilot, the BBG is first being developed to address bacterial biological threats only. The BBG is built upon a hierarchical structure of biothreat categories, elements and tasks, which then serves as the basis for the development of task-aligned queries. This paper outlines the development of this biothreat task-query architecture, which we have named the Bacterial Biothreat Schema, while future papers will describe follow-on efforts to turn queries into model prompts, as well as how the resulting benchmarks can be implemented for model evaluation. Overall, the BBG Framework, including the Bacterial Biothreat Schema, seeks to offer a robust, re-usable structure for evaluating bacterial biological risks arising from LLMs across multiple levels of aggregation, which captures the full scope of technical and operational requirements for biological adversaries, and which accounts for a wide spectrum of biological adversary capabilities.

</details>


### [60] [Robust Agents in Open-Ended Worlds](https://arxiv.org/abs/2512.08139)
*Mikayel Samvelyan*

Main category: cs.LG

TL;DR: 该论文通过开放性和多智能体学习方法，训练和评估能够在新颖环境、分布外输入以及与其他智能体交互中实现泛化的鲁棒AI智能体，涵盖MiniHack框架、Maestro对抗课程生成、足球游戏中的多智能体鲁棒性分析，以及LLM对抗提示的鲁棒性增强。


<details>
  <summary>Details</summary>
Motivation: 随着AI在各种应用中的日益普及，需要能够成功导航和适应不断变化的开放世界的智能体。关键挑战是确保这些AI智能体不仅能在训练期间熟悉的设置中表现出色，还能有效地泛化到先前未见过的多样化场景。

Method: 1. 引入MiniHack：基于NetHack游戏的沙盒框架，通过程序内容生成创建多样化环境；2. 提出Maestro：生成对抗性课程的新方法，逐步增强RL智能体在双人零和游戏中的鲁棒性和泛化能力；3. 利用质量-多样性方法在足球游戏领域系统识别预训练RL策略的漏洞；4. 使用进化搜索生成多样化有效输入，诊断和增强LLM对抗对抗提示的鲁棒性。

Result: 开发了能够创建多样化任务的MiniHack框架；提出了增强RL智能体鲁棒性的Maestro对抗课程生成方法；在复杂足球游戏中识别了最先进RL策略的漏洞；建立了评估和增强LLM对抗对抗提示鲁棒性的方法。

Conclusion: 这项工作为AI鲁棒性的未来发展铺平了道路，使智能体不仅能够适应不断变化的世界，还能在面对不可预见的挑战和交互时蓬勃发展，通过开放性和多智能体学习方法实现了对新颖环境、分布外输入和多智能体交互的鲁棒泛化。

Abstract: The growing prevalence of artificial intelligence (AI) in various applications underscores the need for agents that can successfully navigate and adapt to an ever-changing, open-ended world. A key challenge is ensuring these AI agents are robust, excelling not only in familiar settings observed during training but also effectively generalising to previously unseen and varied scenarios. In this thesis, we harness methodologies from open-endedness and multi-agent learning to train and evaluate robust AI agents capable of generalising to novel environments, out-of-distribution inputs, and interactions with other co-player agents. We begin by introducing MiniHack, a sandbox framework for creating diverse environments through procedural content generation. Based on the game of NetHack, MiniHack enables the construction of new tasks for reinforcement learning (RL) agents with a focus on generalisation. We then present Maestro, a novel approach for generating adversarial curricula that progressively enhance the robustness and generality of RL agents in two-player zero-sum games. We further probe robustness in multi-agent domains, utilising quality-diversity methods to systematically identify vulnerabilities in state-of-the-art, pre-trained RL policies within the complex video game football domain, characterised by intertwined cooperative and competitive dynamics. Finally, we extend our exploration of robustness to the domain of LLMs. Here, our focus is on diagnosing and enhancing the robustness of LLMs against adversarial prompts, employing evolutionary search to generate a diverse range of effective inputs that aim to elicit undesirable outputs from an LLM. This work collectively paves the way for future advancements in AI robustness, enabling the development of agents that not only adapt to an ever-evolving world but also thrive in the face of unforeseen challenges and interactions.

</details>


### [61] [PolyLingua: Margin-based Inter-class Transformer for Robust Cross-domain Language Detection](https://arxiv.org/abs/2512.08143)
*Ali Lotfi Rezaabad,Bikram Khanal,Shashwat Chaurasia,Lu Zeng,Dezhi Hong,Hossein Beshashati,Thomas Butler,Megan Ganji*

Main category: cs.LG

TL;DR: PolyLingua是一个轻量级Transformer模型，用于领域内语言检测和细粒度语言分类，在计算和延迟受限的环境中表现优异


<details>
  <summary>Details</summary>
Motivation: 语言识别是多语言系统的关键第一步，现有工具在特定场景（如音乐请求）中表现不佳，开源工具准确率低，大语言模型成本高，需要轻量高效的解决方案

Method: 采用两层对比学习框架，结合实例级分离和类级对齐，使用自适应边界，生成紧凑且分离良好的嵌入表示

Result: 在Amazon Massive数据集上达到99.25% F1，在Song数据集上达到98.15% F1，超越Sonnet 3.5，参数量减少10倍

Conclusion: PolyLingua在保持高准确率的同时显著降低计算成本，适合计算和延迟受限的环境，解决了现有语言识别工具的关键局限性

Abstract: Language identification is a crucial first step in multilingual systems such as chatbots and virtual assistants, enabling linguistically and culturally accurate user experiences. Errors at this stage can cascade into downstream failures, setting a high bar for accuracy. Yet, existing language identification tools struggle with key cases--such as music requests where the song title and user language differ. Open-source tools like LangDetect, FastText are fast but less accurate, while large language models, though effective, are often too costly for low-latency or low-resource settings. We introduce PolyLingua, a lightweight Transformer-based model for in-domain language detection and fine-grained language classification. It employs a two-level contrastive learning framework combining instance-level separation and class-level alignment with adaptive margins, yielding compact and well-separated embeddings even for closely related languages. Evaluated on two challenging datasets--Amazon Massive (multilingual digital assistant utterances) and a Song dataset (music requests with frequent code-switching)--PolyLingua achieves 99.25% F1 and 98.15% F1, respectively, surpassing Sonnet 3.5 while using 10x fewer parameters, making it ideal for compute- and latency-constrained environments.

</details>


### [62] [TreeGRPO: Tree-Advantage GRPO for Online RL Post-Training of Diffusion Models](https://arxiv.org/abs/2512.08153)
*Zheng Ding,Weirui Ye*

Main category: cs.LG

TL;DR: TreeGRPO是一种新颖的强化学习框架，通过将去噪过程重构为搜索树，显著提高了生成模型与人类偏好对齐的训练效率，实现2.4倍加速训练。


<details>
  <summary>Details</summary>
Motivation: 强化学习后训练对于将生成模型与人类偏好对齐至关重要，但其高昂的计算成本阻碍了广泛采用。现有方法在样本效率和信用分配方面存在局限性。

Method: 将去噪过程重构为搜索树，从共享的初始噪声样本出发，策略性地分支生成多个候选轨迹，同时高效复用它们的共同前缀。采用树结构方法实现高样本效率、细粒度信用分配（通过奖励反向传播计算步骤特定优势）和摊销计算（多子分支实现每次前向传递多次策略更新）。

Result: 在扩散模型和流模型上的广泛实验表明，TreeGRPO实现了2.4倍更快的训练速度，并在效率-奖励权衡空间中建立了更优的帕累托前沿。该方法在多个基准测试和奖励模型上始终优于GRPO基线。

Conclusion: TreeGRPO为基于强化学习的视觉生成模型对齐提供了一条可扩展且有效的途径，显著提高了训练效率同时保持或提升性能。

Abstract: Reinforcement learning (RL) post-training is crucial for aligning generative models with human preferences, but its prohibitive computational cost remains a major barrier to widespread adoption. We introduce \textbf{TreeGRPO}, a novel RL framework that dramatically improves training efficiency by recasting the denoising process as a search tree. From shared initial noise samples, TreeGRPO strategically branches to generate multiple candidate trajectories while efficiently reusing their common prefixes. This tree-structured approach delivers three key advantages: (1) \emph{High sample efficiency}, achieving better performance under same training samples (2) \emph{Fine-grained credit assignment} via reward backpropagation that computes step-specific advantages, overcoming the uniform credit assignment limitation of trajectory-based methods, and (3) \emph{Amortized computation} where multi-child branching enables multiple policy updates per forward pass. Extensive experiments on both diffusion and flow-based models demonstrate that TreeGRPO achieves \textbf{2.4$\times$ faster training} while establishing a superior Pareto frontier in the efficiency-reward trade-off space. Our method consistently outperforms GRPO baselines across multiple benchmarks and reward models, providing a scalable and effective pathway for RL-based visual generative model alignment. The project website is available at treegrpo.github.io.

</details>


### [63] [MobileFineTuner: A Unified End-to-End Framework for Fine-Tuning LLMs on Mobile Phones](https://arxiv.org/abs/2512.08211)
*Jiaxiang Geng,Lunyu Zhao,Yiyi Lu,Bing Luo*

Main category: cs.LG

TL;DR: MobileFineTuner是一个开源框架，支持在普通手机上直接进行端到端的LLM微调，解决了移动设备内存和能耗限制问题。


<details>
  <summary>Details</summary>
Motivation: 随着高质量公共数据接近枯竭，设备端微调可以利用私有用户数据同时保护隐私。现有方法主要是模拟或依赖IoT设备和PC，普通手机领域尚未充分探索，缺乏开源框架支持。

Method: 提出MobileFineTuner统一开源框架，支持全参数微调和参数高效微调。引入参数分片、梯度累积和能量感知计算调度等系统级优化，解决手机内存和能耗限制。

Result: 在真实手机上成功微调了GPT-2、Gemma 3和Qwen 2.5模型。大量实验和消融研究验证了优化效果，证明MobileFineTuner是设备端LLM训练研究的可行基础。

Conclusion: MobileFineTuner填补了移动设备LLM微调的空白，为未来设备端LLM训练研究提供了实用框架，平衡了效率、可扩展性和可用性。

Abstract: Mobile phones are the most ubiquitous end devices, generating vast amounts of human-authored data and serving as the primary platform for end-side applications. As high-quality public data for large language models (LLMs) approaches exhaustion, on-device fine-tuning provides an opportunity to leverage private user data while preserving privacy. However, existing approaches are predominantly simulation-based or rely on IoT devices and PCs, leaving commodity mobile phones largely unexplored. A key gap is the absence of an open-source framework that enables practical LLM fine-tuning on mobile phones. We present MobileFineTuner, a unified open-source framework that enables end-to-end LLM fine-tuning directly on commodity mobile phones. MobileFineTuner is designed for efficiency, scalability, and usability, supporting full-parameters fine-tuning (Full-FT) and parameter-efficient fine-tuning (PEFT). To address the memory and energy limitations inherent to mobile phones, we introduce system-level optimizations including parameter sharding, gradient accumulation, and energy-aware computation scheduling. We demonstrate the practicality of MobileFineTuner by fine-tuning GPT-2, Gemma 3, and Qwen 2.5 on real mobile phones. Extensive experiments and ablation studies validate the effectiveness of the proposed optimizations and establish MobileFineTuner as a viable foundation for future research on on-device LLM training.

</details>


### [64] [Correction of Decoupled Weight Decay](https://arxiv.org/abs/2512.08217)
*Jason Chuan-Chih Chou*

Main category: cs.LG

TL;DR: 该论文挑战了传统AdamW优化器中权重衰减与学习率γ成正比的假设，提出权重衰减应与γ²成正比，基于稳态下权重范数稳定的理论推导，并验证了这种设置能改善训练动态和模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统AdamW优化器中解耦权重衰减被设定为与学习率γ成正比，但作者质疑这一长期未受挑战的假设。一些研究者基于稳态正交性论证提出权重衰减应与γ²成正比，作者旨在深入探讨这一问题的理论基础并提供更合理的权重衰减设置方案。

Method: 作者首先分析了消除更新垂直分量对权重范数贡献的影响，发现对训练动态改变不大。然后基于"稳态时更新与权重无关"的简单假设，推导出解耦权重衰减应与γ²成正比才能实现稳定的权重范数。基于同一假设，推导了Scion优化器中每个小批量的总更新贡献(TUC)更好地由动量相关的有效学习率表征，并进行了实证验证。

Result: 研究发现：1) 解耦权重衰减∝γ²能实现稳定的权重和梯度范数；2) 这种设置能更好地控制训练动态；3) 能提高模型性能；4) 最优有效学习率值具有可迁移性。

Conclusion: 论文得出结论：解耦权重衰减应与学习率的平方(γ²)而非学习率本身(γ)成正比，这一设置基于稳态下权重范数稳定的理论推导，能改善优化器的训练动态并提升模型性能，为AdamW等优化器的超参数设置提供了新的理论基础。

Abstract: Decoupled weight decay, solely responsible for the performance advantage of AdamW over Adam, has long been set to proportional to learning rate $γ$ without questioning. Some researchers have recently challenged such assumption and argued that decoupled weight decay should be set $\propto γ^2$ instead based on orthogonality arguments at steady state. To the contrary, we find that eliminating the contribution of the perpendicular component of the update to the weight norm leads to little change to the training dynamics. Instead, we derive that decoupled weight decay $\propto γ^2$ results in stable weight norm based on the simple assumption that updates become independent of the weights at steady state, regardless of the nature of the optimizer. Based on the same assumption, we derive and empirically verify that the Total Update Contribution (TUC) of a minibatch under the Scion optimizer is better characterized by the momentum-dependent effective learning rate whose optimal value transfers and we show that decoupled weight decay $\propto γ^2$ leads to stable weight and gradient norms and allows us to better control the training dynamics and improve the model performance.

</details>


### [65] [SPROCKET: Extending ROCKET to Distance-Based Time-Series Transformations With Prototypes](https://arxiv.org/abs/2512.08246)
*Nicholas Harner*

Main category: cs.LG

TL;DR: SPROCKET是一种基于原型的时间序列分类特征工程方法，通过原型选择实现随机卷积核变换，在UCR和UEA数据集上性能与现有卷积算法相当，其MR-HY-SP集成模型超越了之前最好的卷积集成HYDRA-MR。


<details>
  <summary>Details</summary>
Motivation: 传统时间序列分类算法主要依赖特征工程策略，其中ROCKET通过随机核特征取得了良好性能。研究者希望探索基于原型的特征工程策略，以进一步提升时间序列分类的准确性和鲁棒性。

Method: 提出SPROCKET（Selected Prototype Random Convolutional Kernel Transform）方法，采用基于原型的特征工程策略，通过原型选择实现随机卷积核变换。同时构建了MR-HY-SP（MultiROCKET-HYDRA-SPROCKET）集成模型。

Result: 在大多数UCR和UEA时间序列分类数据集上，SPROCKET达到了与现有卷积算法相当的性能。MR-HY-SP集成模型的平均准确率排名超过了之前最好的卷积集成HYDRA-MR。

Conclusion: 基于原型的特征变换能够增强时间序列分类的准确性和鲁棒性，SPROCKET方法为时间序列分类提供了一种有效的特征工程新策略。

Abstract: Classical Time Series Classification algorithms are dominated by feature engineering strategies. One of the most prominent of these transforms is ROCKET, which achieves strong performance through random kernel features. We introduce SPROCKET (Selected Prototype Random Convolutional Kernel Transform), which implements a new feature engineering strategy based on prototypes. On a majority of the UCR and UEA Time Series Classification archives, SPROCKET achieves performance comparable to existing convolutional algorithms and the new MR-HY-SP ( MultiROCKET-HYDRA-SPROCKET) ensemble's average accuracy ranking exceeds HYDRA-MR, the previous best convolutional ensemble's performance. These experimental results demonstrate that prototype-based feature transformation can enhance both accuracy and robustness in time series classification.

</details>


### [66] [Geometric-Stochastic Multimodal Deep Learning for Predictive Modeling of SUDEP and Stroke Vulnerability](https://arxiv.org/abs/2512.08257)
*Preksha Girish,Rachana Mysore,Mahanthesha U,Shrey Kumar,Misbah Fatimah Annigeri,Tanish Jain*

Main category: cs.LG

TL;DR: 提出一个统一的几何-随机多模态深度学习框架，整合多种生理信号来建模癫痫猝死和急性缺血性中风的风险，通过流形嵌入、分数随机动力学等方法提高预测准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 癫痫猝死和急性缺血性中风是危及生命的疾病，涉及皮层、脑干和自主神经系统的复杂相互作用。现有方法缺乏统一的数学框架来整合多模态生理信号进行风险建模和早期检测。

Method: 提出统一的几何-随机多模态深度学习框架，整合EEG、ECG、呼吸、SpO2、EMG和fMRI信号。结合黎曼流形嵌入、李群不变特征表示、分数随机动力学、哈密顿能量流建模和跨模态注意力机制。使用分数流行病扩散在结构脑图上建模中风传播。

Result: 在MULTI-CLARID数据集上的实验表明，该方法提高了预测准确性，并获得了可解释的生物标志物，包括流形曲率、分数记忆指数、注意力熵和扩散中心性等。

Conclusion: 该框架为神经自主神经疾病的早期检测、风险分层和可解释多模态建模提供了数学原理基础，有助于改善癫痫猝死和中风的预测和管理。

Abstract: Sudden Unexpected Death in Epilepsy (SUDEP) and acute ischemic stroke are life-threatening conditions involving complex interactions across cortical, brainstem, and autonomic systems. We present a unified geometric-stochastic multimodal deep learning framework that integrates EEG, ECG, respiration, SpO2, EMG, and fMRI signals to model SUDEP and stroke vulnerability. The approach combines Riemannian manifold embeddings, Lie-group invariant feature representations, fractional stochastic dynamics, Hamiltonian energy-flow modeling, and cross-modal attention mechanisms. Stroke propagation is modeled using fractional epidemic diffusion over structural brain graphs. Experiments on the MULTI-CLARID dataset demonstrate improved predictive accuracy and interpretable biomarkers derived from manifold curvature, fractional memory indices, attention entropy, and diffusion centrality. The proposed framework provides a mathematically principled foundation for early detection, risk stratification, and interpretable multimodal modeling in neural-autonomic disorders.

</details>


### [67] [Biothreat Benchmark Generation Framework for Evaluating Frontier AI Models III: Implementing the Bacterial Biothreat Benchmark (B3) Dataset](https://arxiv.org/abs/2512.08459)
*Gary Ackerman,Theodore Wilson,Zachary Kallenborn,Olivia Shoemaker,Anna Wetzel,Hayley Peterson,Abigail Danfora,Jenna LaTourette,Brandon Behlendorf,Douglas Clifford*

Main category: cs.LG

TL;DR: B3数据集为评估大语言模型的生物安全风险提供了一个可行的基准测试方法，通过试点实施验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 前沿AI模型（特别是大语言模型）可能被用于生物恐怖主义或获取生物武器，这引发了政策、学术和公众的广泛担忧。需要开发能够量化这些风险的基准测试方法。

Method: 实施B3（细菌生物威胁基准）数据集的试点测试，包括：1）在样本前沿AI模型上运行基准测试；2）人工评估模型响应；3）从多个维度进行应用风险分析。

Result: 试点测试表明B3数据集能够有效、细致地快速评估LLM的生物安全风险，识别风险的关键来源，并为优先缓解领域提供指导。

Conclusion: B3数据集提供了一个可行的、细致的方法来评估大语言模型的生物安全风险，有助于量化风险并指导缓解措施的优先顺序。

Abstract: The potential for rapidly-evolving frontier artificial intelligence (AI) models, especially large language models (LLMs), to facilitate bioterrorism or access to biological weapons has generated significant policy, academic, and public concern. Both model developers and policymakers seek to quantify and mitigate any risk, with an important element of such efforts being the development of model benchmarks that can assess the biosecurity risk posed by a particular model. This paper discusses the pilot implementation of the Bacterial Biothreat Benchmark (B3) dataset. It is the third in a series of three papers describing an overall Biothreat Benchmark Generation (BBG) framework, with previous papers detailing the development of the B3 dataset. The pilot involved running the benchmarks through a sample frontier AI model, followed by human evaluation of model responses, and an applied risk analysis of the results along several dimensions. Overall, the pilot demonstrated that the B3 dataset offers a viable, nuanced method for rapidly assessing the biosecurity risk posed by a LLM, identifying the key sources of that risk and providing guidance for priority areas of mitigation priority.

</details>


### [68] [Jacobian Aligned Random Forests](https://arxiv.org/abs/2512.08306)
*Sarwesh Rauniyar*

Main category: cs.LG

TL;DR: JARF是一种通过计算预测梯度来构建全局线性预条件器的方法，将特征空间旋转后使用标准轴对齐随机森林，在保持简单性的同时提升对旋转决策边界的处理能力。


<details>
  <summary>Details</summary>
Motivation: 轴对齐决策树在处理具有旋转或交互依赖决策边界的数据集时表现不佳，而倾斜森林虽然能解决这个问题但计算成本高且实现复杂。需要一种既能处理复杂决策边界又保持简单性的方法。

Method: 首先拟合轴对齐森林来估计类别概率或回归输出，计算预测相对于每个特征的有限差分梯度，将这些梯度聚合成期望雅可比外积（扩展了期望梯度外积），并将其用作所有输入的单一全局线性预条件器。这个监督预条件器应用单一全局特征空间旋转，然后将转换后的数据交给标准轴对齐森林处理。

Result: 在表格分类和回归基准测试中，这种预条件处理一致地改进了轴对齐森林的性能，通常匹配或超过倾斜基线方法，同时改善了训练时间。实验结果表明监督预条件能够恢复倾斜森林的大部分准确性，同时保持轴对齐树的简单性和鲁棒性。

Conclusion: JARF通过监督预条件提供了一种简单有效的替代方案，能够在保持轴对齐决策树快速稳定特性的同时，处理需要特征组合的复杂决策边界问题，实现了性能与复杂性的良好平衡。

Abstract: Axis-aligned decision trees are fast and stable but struggle on datasets with rotated or interaction-dependent decision boundaries, where informative splits require linear combinations of features rather than single-feature thresholds. Oblique forests address this with per-node hyperplane splits, but at added computational cost and implementation complexity. We propose a simple alternative: JARF, Jacobian-Aligned Random Forests. Concretely, we first fit an axis-aligned forest to estimate class probabilities or regression outputs, compute finite-difference gradients of these predictions with respect to each feature, aggregate them into an expected Jacobian outer product that generalizes the expected gradient outer product (EGOP), and use it as a single global linear preconditioner for all inputs. This supervised preconditioner applies a single global rotation of the feature space, then hands the transformed data back to a standard axis-aligned forest, preserving off-the-shelf training pipelines while capturing oblique boundaries and feature interactions that would otherwise require many axis-aligned splits to approximate. The same construction applies to any model that provides gradients, though we focus on random forests and gradient-boosted trees in this work. On tabular classification and regression benchmarks, this preconditioning consistently improves axis-aligned forests and often matches or surpasses oblique baselines while improving training time. Our experimental results and theoretical analysis together indicate that supervised preconditioning can recover much of the accuracy of oblique forests while retaining the simplicity and robustness of axis-aligned trees.

</details>


### [69] [Minimizing Layerwise Activation Norm Improves Generalization in Federated Learning](https://arxiv.org/abs/2512.08314)
*M Yashwanth,Gaurav Kumar Nayak,Harsh Rangwani,Arya Singh,R. Venkatesh Babu,Anirban Chakraborty*

Main category: cs.LG

TL;DR: 提出一种名为MAN的联邦学习正则化技术，通过最小化客户端模型每层激活范数来约束优化问题的平坦性，从而提高联邦学习模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习训练可能导致全局模型收敛到"尖锐最小值"，从而影响模型的泛化能力。需要改进联邦学习框架下的模型泛化性能。

Method: 引入平坦性约束的联邦学习优化问题，通过最小化客户端模型每层激活范数(MAN)的正则化技术，理论上证明这能降低Hessian矩阵的最大特征值，确保收敛到平坦最小值。

Result: 将提出的平坦性约束优化应用于现有联邦学习技术，获得了显著改进，建立了新的最先进性能。

Conclusion: MAN正则化技术通过约束平坦性有效提高了联邦学习模型的泛化能力，为联邦学习优化提供了新的有效方法。

Abstract: Federated Learning (FL) is an emerging machine learning framework that enables multiple clients (coordinated by a server) to collaboratively train a global model by aggregating the locally trained models without sharing any client's training data. It has been observed in recent works that learning in a federated manner may lead the aggregated global model to converge to a 'sharp minimum' thereby adversely affecting the generalizability of this FL-trained model. Therefore, in this work, we aim to improve the generalization performance of models trained in a federated setup by introducing a 'flatness' constrained FL optimization problem. This flatness constraint is imposed on the top eigenvalue of the Hessian computed from the training loss. As each client trains a model on its local data, we further re-formulate this complex problem utilizing the client loss functions and propose a new computationally efficient regularization technique, dubbed 'MAN,' which Minimizes Activation's Norm of each layer on client-side models. We also theoretically show that minimizing the activation norm reduces the top eigenvalue of the layer-wise Hessian of the client's loss, which in turn decreases the overall Hessian's top eigenvalue, ensuring convergence to a flat minimum. We apply our proposed flatness-constrained optimization to the existing FL techniques and obtain significant improvements, thereby establishing new state-of-the-art.

</details>


### [70] [Fed-SE: Federated Self-Evolution for Privacy-Constrained Multi-Environment LLM Agents](https://arxiv.org/abs/2512.08870)
*Xiang Chen,Yuling Shi,Qizhen Lan,Yuchao Qiu,Xiaodong Gu*

Main category: cs.LG

TL;DR: Fed-SE：联邦自进化框架，解决LLM智能体在隐私约束下的跨环境知识迁移问题，通过局部进化-全局聚合范式，在异构任务中提升约18%的任务成功率


<details>
  <summary>Details</summary>
Motivation: LLM智能体在复杂交互任务中广泛部署，但隐私约束限制了集中式优化和跨动态环境的协同进化。传统联邦学习在静态数据集上有效，但在开放式的智能体自进化场景中应用不足，直接应用标准联邦学习会因异构任务和稀疏的轨迹级奖励导致梯度冲突，破坏全局优化过程。

Method: 提出Fed-SE联邦自进化框架：1）局部进化：智能体在过滤后的高回报轨迹上进行参数高效微调，实现稳定梯度更新；2）全局聚合：在低秩子空间内聚合更新，解耦环境特定动态，有效减少客户端间的负迁移。

Result: 在五个异构环境中的实验表明，Fed-SE相比联邦学习基线平均任务成功率提升约18%，验证了其在隐私约束部署中实现鲁棒跨环境知识迁移的有效性。

Conclusion: Fed-SE成功解决了LLM智能体在隐私约束下的联邦自进化问题，通过局部进化-全局聚合范式有效处理了异构任务和稀疏奖励带来的梯度冲突，实现了跨环境的鲁棒知识迁移。

Abstract: LLM agents are widely deployed in complex interactive tasks, yet privacy constraints often preclude centralized optimization and co-evolution across dynamic environments. While Federated Learning (FL) has proven effective on static datasets, its extension to the open-ended self-evolution of agents remains underexplored. Directly applying standard FL is challenging: heterogeneous tasks and sparse, trajectory-level rewards introduce severe gradient conflicts, destabilizing the global optimization process. To bridge this gap, we propose Fed-SE, a Federated Self-Evolution framework for LLM agents. Fed-SE establishes a local evolution-global aggregation paradigm. Locally, agents employ parameter-efficient fine-tuning on filtered, high-return trajectories to achieve stable gradient updates. Globally, Fed-SE aggregates updates within a low-rank subspace that disentangles environment-specific dynamics, effectively reducing negative transfer across clients. Experiments across five heterogeneous environments demonstrate that Fed-SE improves average task success rates by approximately 18% over federated baselines, validating its effectiveness in robust cross-environment knowledge transfer in privacy-constrained deployments.

</details>


### [71] [A Multivariate Bernoulli-Based Sampling Method for Multi-Label Data with Application to Meta-Research](https://arxiv.org/abs/2512.08371)
*Simon Chung,Colby J. Vorland,Donna L. Maney,Andrew W. Brown*

Main category: cs.LG

TL;DR: 提出一种考虑标签依赖关系的多标签数据加权采样算法，用于处理标签频率差异大且非互斥的数据集，通过估计多元伯努利分布参数计算权重，生成更平衡的子样本。


<details>
  <summary>Details</summary>
Motivation: 多标签数据集中标签通常非互斥且频率差异巨大，要获得包含足够稀缺标签样本以进行推断，同时偏离总体频率已知方式，存在挑战。需要一种能考虑标签依赖关系的采样方法。

Method: 使用多元伯努利分布作为多标签问题的底层分布，利用观测到的标签频率估计分布参数，为每个标签组合计算权重。通过加权采样确保获得目标分布特性，同时考虑标签间的依赖关系。

Result: 将方法应用于Web of Science的64个生物医学主题类别研究文章样本，成功保留了类别频率顺序，减少了最常⻅和最不常⻅类别之间的频率差异，并考虑了类别依赖关系，生成了更平衡的子样本。

Conclusion: 提出的加权采样算法能有效处理多标签数据中的不平衡问题，通过考虑标签依赖关系，增强了少数类别的代表性，为多标签数据集采样提供了新方法。

Abstract: Datasets may contain observations with multiple labels. If the labels are not mutually exclusive, and if the labels vary greatly in frequency, obtaining a sample that includes sufficient observations with scarcer labels to make inferences about those labels, and which deviates from the population frequencies in a known manner, creates challenges. In this paper, we consider a multivariate Bernoulli distribution as our underlying distribution of a multi-label problem. We present a novel sampling algorithm that takes label dependencies into account. It uses observed label frequencies to estimate multivariate Bernoulli distribution parameters and calculate weights for each label combination. This approach ensures the weighted sampling acquires target distribution characteristics while accounting for label dependencies. We applied this approach to a sample of research articles from Web of Science labeled with 64 biomedical topic categories. We aimed to preserve category frequency order, reduce frequency differences between most and least common categories, and account for category dependencies. This approach produced a more balanced sub-sample, enhancing the representation of minority categories.

</details>


### [72] [When Tables Leak: Attacking String Memorization in LLM-Based Tabular Data Generation](https://arxiv.org/abs/2512.08875)
*Joshua Ward,Bochao Gu,Chi-Hua Wang,Guang Cheng*

Main category: cs.LG

TL;DR: LLM生成的表格合成数据存在隐私泄露风险，数字序列容易被记忆和复制，研究者提出了LevAtt攻击方法和防御策略


<details>
  <summary>Details</summary>
Motivation: LLM在生成高质量表格合成数据方面表现出色，但现有方法（微调小模型或提示大模型）存在隐私风险，可能泄露训练数据中的数字模式

Method: 提出LevAtt黑盒成员推理攻击，仅访问生成的合成数据，针对数字序列进行分析；同时提出两种防御方法，包括在生成过程中策略性扰动数字的采样策略

Result: LevAtt攻击在多种模型和数据集上暴露了显著的隐私泄露，在某些情况下甚至成为完美的成员分类器；提出的防御方法能在最小化保真度和效用损失的情况下有效抵御攻击

Conclusion: LLM基于的合成数据生成存在独特的隐私漏洞，需要有效的防御措施；提出的数字扰动策略是解决这一问题的可行方法

Abstract: Large Language Models (LLMs) have recently demonstrated remarkable performance in generating high-quality tabular synthetic data. In practice, two primary approaches have emerged for adapting LLMs to tabular data generation: (i) fine-tuning smaller models directly on tabular datasets, and (ii) prompting larger models with examples provided in context. In this work, we show that popular implementations from both regimes exhibit a tendency to compromise privacy by reproducing memorized patterns of numeric digits from their training data. To systematically analyze this risk, we introduce a simple No-box Membership Inference Attack (MIA) called LevAtt that assumes adversarial access to only the generated synthetic data and targets the string sequences of numeric digits in synthetic observations. Using this approach, our attack exposes substantial privacy leakage across a wide range of models and datasets, and in some cases, is even a perfect membership classifier on state-of-the-art models. Our findings highlight a unique privacy vulnerability of LLM-based synthetic data generation and the need for effective defenses. To this end, we propose two methods, including a novel sampling strategy that strategically perturbs digits during generation. Our evaluation demonstrates that this approach can defeat these attacks with minimal loss of fidelity and utility of the synthetic data.

</details>


### [73] [DAO-GP Drift Aware Online Non-Linear Regression Gaussian-Process](https://arxiv.org/abs/2512.08879)
*Mohammad Abu-Shaira,Ajita Rattani,Weishi Shi*

Main category: cs.LG

TL;DR: DAO-GP是一种新型的漂移感知在线高斯过程模型，能够自适应处理数据分布随时间变化的问题，无需手动调整超参数，具有漂移检测和适应机制。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集常呈现随时间变化的数据分布（概念漂移），忽视这一问题会显著降低模型预测准确性。传统在线高斯过程方法存在多个关键限制：缺乏漂移感知能力、依赖固定超参数、易受数据窥探影响、缺乏原则性衰减机制以及内存效率低下。

Method: 提出DAO-GP（漂移感知在线高斯过程），这是一种完全自适应、无超参数、衰减和稀疏的非线性回归模型。该模型内置漂移检测和适应机制，能够根据漂移严重程度动态调整模型行为。

Result: 广泛的实证评估证实了DAO-GP在平稳条件、多种漂移类型（突然、增量、渐进）以及不同数据特征下的鲁棒性。分析展示了其动态适应能力、高效的内存和衰减管理以及演化诱导点。与最先进的参数和非参数模型相比，DAO-GP始终实现优越或竞争性性能。

Conclusion: DAO-GP被确立为一种漂移弹性的在线非线性回归解决方案，能够有效处理概念漂移问题，无需手动超参数调整，在各种动态数据环境中表现优异。

Abstract: Real-world datasets often exhibit temporal dynamics characterized by evolving data distributions. Disregarding this phenomenon, commonly referred to as concept drift, can significantly diminish a model's predictive accuracy. Furthermore, the presence of hyperparameters in online models exacerbates this issue. These parameters are typically fixed and cannot be dynamically adjusted by the user in response to the evolving data distribution. Gaussian Process (GP) models offer powerful non-parametric regression capabilities with uncertainty quantification, making them ideal for modeling complex data relationships in an online setting. However, conventional online GP methods face several critical limitations, including a lack of drift-awareness, reliance on fixed hyperparameters, vulnerability to data snooping, absence of a principled decay mechanism, and memory inefficiencies. In response, we propose DAO-GP (Drift-Aware Online Gaussian Process), a novel, fully adaptive, hyperparameter-free, decayed, and sparse non-linear regression model. DAO-GP features a built-in drift detection and adaptation mechanism that dynamically adjusts model behavior based on the severity of drift. Extensive empirical evaluations confirm DAO-GP's robustness across stationary conditions, diverse drift types (abrupt, incremental, gradual), and varied data characteristics. Analyses demonstrate its dynamic adaptation, efficient in-memory and decay-based management, and evolving inducing points. Compared with state-of-the-art parametric and non-parametric models, DAO-GP consistently achieves superior or competitive performance, establishing it as a drift-resilient solution for online non-linear regression.

</details>


### [74] [Transformers for Multimodal Brain State Decoding: Integrating Functional Magnetic Resonance Imaging Data and Medical Metadata](https://arxiv.org/abs/2512.08462)
*Danial Jafarzadeh Jazi,Maryam Hajiesmaeili*

Main category: cs.LG

TL;DR: 提出了一种结合fMRI数据和DICOM元数据的Transformer多模态框架，用于解码大脑状态，提高准确性、可解释性和鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习和深度学习方法在处理fMRI数据时未能充分利用DICOM元数据提供的上下文信息，限制了大脑状态解码的准确性和应用潜力。

Method: 采用基于Transformer的架构，整合多模态输入（fMRI数据和DICOM元数据），利用注意力机制捕捉复杂的时空模式和上下文关系。

Result: 提出的框架增强了模型的准确性、可解释性和鲁棒性，在临床诊断、认知神经科学和个性化医疗等领域具有应用潜力。

Conclusion: 该多模态Transformer框架有效利用了DICOM元数据的上下文信息，提升了fMRI大脑状态解码能力，同时讨论了元数据变异性、计算需求等局限性，并提出了未来优化可扩展性和泛化性的方向。

Abstract: Decoding brain states from functional magnetic resonance imaging (fMRI) data is vital for advancing neuroscience and clinical applications. While traditional machine learning and deep learning approaches have made strides in leveraging the high-dimensional and complex nature of fMRI data, they often fail to utilize the contextual richness provided by Digital Imaging and Communications in Medicine (DICOM) metadata. This paper presents a novel framework integrating transformer-based architectures with multimodal inputs, including fMRI data and DICOM metadata. By employing attention mechanisms, the proposed method captures intricate spatial-temporal patterns and contextual relationships, enhancing model accuracy, interpretability, and robustness. The potential of this framework spans applications in clinical diagnostics, cognitive neuroscience, and personalized medicine. Limitations, such as metadata variability and computational demands, are addressed, and future directions for optimizing scalability and generalizability are discussed.

</details>


### [75] [Optimal Perturbation Budget Allocation for Data Poisoning in Offline Reinforcement Learning](https://arxiv.org/abs/2512.08485)
*Junnan Qiu,Jie Li*

Main category: cs.LG

TL;DR: 提出一种针对离线强化学习的全局预算分配攻击策略，通过TD误差敏感度分配扰动预算，相比传统均匀扰动方法更高效且隐蔽


<details>
  <summary>Details</summary>
Motivation: 现有离线RL数据投毒攻击采用局部均匀扰动，对所有样本不加区分，效率低下且缺乏隐蔽性，容易因统计偏差被检测

Method: 提出全局预算分配攻击策略，基于TD误差与价值函数收敛影响成正比的洞察，将攻击建模为全局资源分配问题，推导出在L2约束下扰动幅度与TD误差敏感度成正比的闭式解

Result: 在D4RL基准测试中显著优于基线策略，仅用最小扰动即可实现高达80%的性能下降，并能逃避最先进的统计和频谱防御检测

Conclusion: 全局预算分配攻击策略通过智能分配扰动预算到高影响样本，实现了更高效和隐蔽的离线RL数据投毒攻击

Abstract: Offline Reinforcement Learning (RL) enables policy optimization from static datasets but is inherently vulnerable to data poisoning attacks. Existing attack strategies typically rely on locally uniform perturbations, which treat all samples indiscriminately. This approach is inefficient, as it wastes the perturbation budget on low-impact samples, and lacks stealthiness due to significant statistical deviations. In this paper, we propose a novel Global Budget Allocation attack strategy. Leveraging the theoretical insight that a sample's influence on value function convergence is proportional to its Temporal Difference (TD) error, we formulate the attack as a global resource allocation problem. We derive a closed-form solution where perturbation magnitudes are assigned proportional to the TD-error sensitivity under a global L2 constraint. Empirical results on D4RL benchmarks demonstrate that our method significantly outperforms baseline strategies, achieving up to 80% performance degradation with minimal perturbations that evade detection by state-of-the-art statistical and spectral defenses.

</details>


### [76] [Long-Sequence LSTM Modeling for NBA Game Outcome Prediction Using a Novel Multi-Season Dataset](https://arxiv.org/abs/2512.08591)
*Charles Rios,Longzhen Han,Almas Baimagambetov,Nikolaos Polatidis*

Main category: cs.LG

TL;DR: 该研究构建了覆盖2004-2024赛季的纵向NBA数据集，并开发了基于LSTM的深度学习框架，通过长达9840场比赛（相当于8个完整赛季）的序列长度来捕捉球队长期动态变化，在NBA比赛结果预测中取得了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 预测NBA比赛结果对于教练策略、球迷参与和体育博彩都很重要，但现有模型存在概念漂移、时间上下文有限和跨赛季不稳定性等问题，需要更好的长期趋势建模方法。

Method: 构建了覆盖2004-05至2024-25赛季的纵向NBA数据集，提出了基于LSTM的深度学习框架，使用9840场比赛的扩展序列长度（相当于8个完整NBA赛季）来建模长期性能趋势和赛季间依赖关系。

Result: LSTM模型在所有指标上表现最佳：准确率72.35%，精确率73.15%，AUC-ROC 76.13%。相比逻辑回归、随机森林、MLP和CNN等传统机器学习与深度学习基线模型都有显著提升。

Conclusion: 研究证明了长序列时间建模在篮球结果预测中的重要性，展示了新构建的多赛季数据集对于开发稳健、可泛化的NBA预测系统的价值，LSTM架构能有效捕捉球队动态演变和赛季间依赖关系。

Abstract: Predicting the outcomes of professional basketball games, particularly in the National Basketball Association (NBA), has become increasingly important for coaching strategy, fan engagement, and sports betting. However, many existing prediction models struggle with concept drift, limited temporal context, and instability across seasons. To advance forecasting in this domain, we introduce a newly constructed longitudinal NBA dataset covering the 2004-05 to 2024-25 seasons and present a deep learning framework designed to model long-term performance trends. Our primary contribution is a Long Short-Term Memory (LSTM) architecture that leverages an extended sequence length of 9,840 games equivalent to eight full NBA seasons to capture evolving team dynamics and season-over-season dependencies. We compare this model against several traditional Machine Learning (ML) and Deep Learning (DL) baselines, including Logistic Regression, Random Forest, Multi-Layer Perceptron (MLP), and Convolutional Neural Network (CNN). The LSTM achieves the best performance across all metrics, with 72.35 accuracy, 73.15 precision and 76.13 AUC-ROC. These results demonstrate the importance of long-sequence temporal modeling in basketball outcome prediction and highlight the value of our new multi-season dataset for developing robust, generalizable NBA forecasting systems.

</details>


### [77] [DS FedProxGrad: Asymptotic Stationarity Without Noise Floor in Fair Federated Learning](https://arxiv.org/abs/2512.08671)
*Huzaifa Arif*

Main category: cs.LG

TL;DR: 本文改进了FedProxGrad算法的收敛性分析，提出了DS FedProxGrad框架，证明了在Robbins-Monro步长调度和局部不精确性衰减条件下，算法能够渐近收敛到平稳点，消除了方差引起的噪声下限依赖。


<details>
  <summary>Details</summary>
Motivation: 现有FedProxGrad算法在非凸复合优化问题的群公平联邦学习中，其收敛分析仅能证明收敛到噪声主导的平稳邻域，且明确依赖于方差引起的噪声下限。这限制了算法的理论保证和实际性能。

Method: 提出了DS FedProxGrad（衰减步长FedProxGrad）分析框架，该框架包含不精确的局部近端解和显式公平正则化。采用Robbins-Monro步长调度策略，并要求局部不精确性满足温和的衰减条件。

Result: 证明了liminf_{r→∞} E[‖∇F(x^r)‖^2] = 0，即算法渐近收敛到平稳点，收敛速率不再依赖于方差引起的噪声下限，实现了真正的渐近平稳性。

Conclusion: DS FedProxGrad框架在理论上显著改进了FedProxGrad的收敛性分析，消除了噪声下限依赖，为群公平联邦学习中的非凸复合优化问题提供了更强的收敛保证。

Abstract: Recent work \cite{arifgroup} introduced Federated Proximal Gradient \textbf{(\texttt{FedProxGrad})} for solving non-convex composite optimization problems in group fair federated learning. However, the original analysis established convergence only to a \textit{noise-dominated neighborhood of stationarity}, with explicit dependence on a variance-induced noise floor. In this work, we provide an improved asymptotic convergence analysis for a generalized \texttt{FedProxGrad}-type analytical framework with inexact local proximal solutions and explicit fairness regularization. We call this extended analytical framework \textbf{DS \texttt{FedProxGrad}} (Decay Step Size \texttt{FedProxGrad}). Under a Robbins-Monro step-size schedule \cite{robbins1951stochastic} and a mild decay condition on local inexactness, we prove that $\liminf_{r\to\infty} \mathbb{E}[\|\nabla F(\mathbf{x}^r)\|^2] = 0$, i.e., the algorithm is asymptotically stationary and the convergence rate does not depend on a variance-induced noise floor.

</details>


### [78] [Exposing Hidden Biases in Text-to-Image Models via Automated Prompt Search](https://arxiv.org/abs/2512.08724)
*Manos Plitsis,Giorgos Bouritsas,Vassilis Katsouros,Yannis Panagakis*

Main category: cs.LG

TL;DR: BGPS框架自动生成能最大化文本到图像模型中偏见的提示词，通过LLM生成属性中性提示，并用属性分类器引导LLM解码过程，发现了稳定扩散模型中未记录的微妙偏见。


<details>
  <summary>Details</summary>
Motivation: 现有文本到图像扩散模型存在社会偏见，现有去偏见方法依赖人工或LLM构建的提示数据集，成本高且可能遗漏未预见的偏见触发提示，需要自动发现偏见的框架。

Method: 提出Bias-Guided Prompt Search (BGPS)框架：1) 使用LLM生成属性中性提示；2) 利用属性分类器作用于TTI内部表示，引导LLM解码过程向放大目标图像属性的提示空间区域发展。

Result: 在Stable Diffusion 1.5和先进去偏见模型上发现大量微妙且未记录的偏见，严重降低公平性指标，发现的提示具有可解释性，相比硬提示优化方法改进了困惑度指标。

Conclusion: BGPS揭示了TTI模型的脆弱性，扩展了偏见搜索空间，可作为新的偏见缓解评估工具，发现的提示具有实际用户可能输入的现实意义。

Abstract: Text-to-image (TTI) diffusion models have achieved remarkable visual quality, yet they have been repeatedly shown to exhibit social biases across sensitive attributes such as gender, race and age. To mitigate these biases, existing approaches frequently depend on curated prompt datasets - either manually constructed or generated with large language models (LLMs) - as part of their training and/or evaluation procedures. Beside the curation cost, this also risks overlooking unanticipated, less obvious prompts that trigger biased generation, even in models that have undergone debiasing. In this work, we introduce Bias-Guided Prompt Search (BGPS), a framework that automatically generates prompts that aim to maximize the presence of biases in the resulting images. BGPS comprises two components: (1) an LLM instructed to produce attribute-neutral prompts and (2) attribute classifiers acting on the TTI's internal representations that steer the decoding process of the LLM toward regions of the prompt space that amplify the image attributes of interest. We conduct extensive experiments on Stable Diffusion 1.5 and a state-of-the-art debiased model and discover an array of subtle and previously undocumented biases that severely deteriorate fairness metrics. Crucially, the discovered prompts are interpretable, i.e they may be entered by a typical user, quantitatively improving the perplexity metric compared to a prominent hard prompt optimization counterpart. Our findings uncover TTI vulnerabilities, while BGPS expands the bias search space and can act as a new evaluation tool for bias mitigation.

</details>


### [79] [Neural Ordinary Differential Equations for Simulating Metabolic Pathway Dynamics from Time-Series Multiomics Data](https://arxiv.org/abs/2512.08732)
*Udesh Habaraduwa,Andrei Lixandru*

Main category: cs.LG

TL;DR: 该研究引入神经常微分方程（NODEs）作为学习蛋白质组与代谢组复杂相互作用的动态框架，应用于工程化大肠杆菌的时间序列数据，相比传统机器学习方法在预测精度和推理速度上均有显著提升。


<details>
  <summary>Details</summary>
Motivation: 人类健康寿命和生物工程的进步严重依赖于预测复杂生物系统的行为。虽然高通量多组学数据日益丰富，但将这些数据转化为可操作的预测模型仍然是一个瓶颈。需要高容量、数据驱动的模拟系统来直接从观测数据中推断潜在相互作用，以支持个性化医疗和合成生物学中的干预效果预测。

Method: 引入神经常微分方程（NODEs）作为动态框架，用于学习蛋白质组与代谢组之间的复杂相互作用。将该框架应用于工程化大肠杆菌菌株的时间序列数据，对代谢途径的连续动态进行建模。

Result: NODE架构在捕捉系统动态方面表现出优于传统机器学习管道的性能。在柠檬烯和异戊烯醇途径数据集上，均方根误差相比基线改善了90%以上（柠檬烯最高94.38%，异戊烯醇最高97.65%）。此外，NODE模型的推理时间加速了1000倍。

Conclusion: 神经常微分方程模型被确立为可扩展、高保真的工具，适用于下一代代谢工程和生物发现，能够高效预测生物系统的动态行为。

Abstract: The advancement of human healthspan and bioengineering relies heavily on predicting the behavior of complex biological systems. While high-throughput multiomics data is becoming increasingly abundant, converting this data into actionable predictive models remains a bottleneck. High-capacity, datadriven simulation systems are critical in this landscape; unlike classical mechanistic models restricted by prior knowledge, these architectures can infer latent interactions directly from observational data, allowing for the simulation of temporal trajectories and the anticipation of downstream intervention effects in personalized medicine and synthetic biology. To address this challenge, we introduce Neural Ordinary Differential Equations (NODEs) as a dynamic framework for learning the complex interplay between the proteome and metabolome. We applied this framework to time-series data derived from engineered Escherichia coli strains, modeling the continuous dynamics of metabolic pathways. The proposed NODE architecture demonstrates superior performance in capturing system dynamics compared to traditional machine learning pipelines. Our results show a greater than 90% improvement in root mean squared error over baselines across both Limonene (up to 94.38% improvement) and Isopentenol (up to 97.65% improvement) pathway datasets. Furthermore, the NODE models demonstrated a 1000x acceleration in inference time, establishing them as a scalable, high-fidelity tool for the next generation of metabolic engineering and biological discovery.

</details>


### [80] [Learning and Editing Universal Graph Prompt Tuning via Reinforcement Learning](https://arxiv.org/abs/2512.08763)
*Jinfeng Xu,Zheyu Chen,Shuo Yang,Jinze Li,Hewei Wang,Yijie Li,Edith C. H. Ngai*

Main category: cs.LG

TL;DR: 本文提出LEAP模型，通过强化学习选择节点并编辑提示，在保持通用图提示调优理论基础上追求更理想的提示效果。


<details>
  <summary>Details</summary>
Motivation: 现有选择性节点图提示调优方法会损害通用图提示调优的理论基础，需要一种既能保持理论基础又能获得更理想提示的新方法。

Method: 提出LEAP模型：首先构建基本通用图提示以保持理论基础，然后使用actor-critic强化学习选择节点并编辑提示。

Result: 在图级和节点级任务的各种预训练策略下，无论是全样本还是少样本场景，LEAP都持续优于微调和其他基于提示的方法。

Conclusion: 通过引入更严格的约束条件，证明了向所有节点添加提示是实现图提示通用性的必要条件，LEAP模型在保持理论基础的同时实现了更好的性能。

Abstract: Early graph prompt tuning approaches relied on task-specific designs for Graph Neural Networks (GNNs), limiting their adaptability across diverse pre-training strategies. In contrast, another promising line of research has investigated universal graph prompt tuning, which operates directly in the input graph's feature space and builds a theoretical foundation that universal graph prompt tuning can theoretically achieve an equivalent effect of any prompting function, eliminating dependence on specific pre-training strategies. Recent works propose selective node-based graph prompt tuning to pursue more ideal prompts. However, we argue that selective node-based graph prompt tuning inevitably compromises the theoretical foundation of universal graph prompt tuning. In this paper, we strengthen the theoretical foundation of universal graph prompt tuning by introducing stricter constraints, demonstrating that adding prompts to all nodes is a necessary condition for achieving the universality of graph prompts. To this end, we propose a novel model and paradigm, Learning and Editing Universal GrAph Prompt Tuning (LEAP), which preserves the theoretical foundation of universal graph prompt tuning while pursuing more ideal prompts. Specifically, we first build the basic universal graph prompts to preserve the theoretical foundation and then employ actor-critic reinforcement learning to select nodes and edit prompts. Extensive experiments on graph- and node-level tasks across various pre-training strategies in both full-shot and few-shot scenarios show that LEAP consistently outperforms fine-tuning and other prompt-based approaches.

</details>


### [81] [Identifying counterfactual probabilities using bivariate distributions and uplift modeling](https://arxiv.org/abs/2512.08805)
*Théo Verhelst,Gianluca Bontempi*

Main category: cs.LG

TL;DR: 论文提出了一种利用提升模型进行反事实估计的方法，通过拟合双变量beta分布到预测的提升分数，获得反事实结果的后验分布。


<details>
  <summary>Details</summary>
Motivation: 反事实识别旨在恢复潜在结果的联合分布（如"如果给客户提供营销优惠，他们是否仍会流失？"），这比提升建模提供更丰富的信息，但更难估计。然而，这两种方法是协同的：提升模型可以用于反事实估计。

Method: 提出了一种反事实估计器，将双变量beta分布拟合到预测的提升分数上，从而获得反事实结果的后验分布。该方法除了提升建模所需的因果假设外，不需要额外的因果假设。

Result: 模拟实验显示了该方法的有效性，可以应用于电信客户流失等问题，揭示了标准机器学习或单独使用提升模型无法获得的洞察。

Conclusion: 该方法成功地将提升建模与反事实估计相结合，通过利用提升模型进行反事实识别，为因果推断提供了更丰富的分布信息。

Abstract: Uplift modeling estimates the causal effect of an intervention as the difference between potential outcomes under treatment and control, whereas counterfactual identification aims to recover the joint distribution of these potential outcomes (e.g., "Would this customer still have churned had we given them a marketing offer?"). This joint counterfactual distribution provides richer information than the uplift but is harder to estimate. However, the two approaches are synergistic: uplift models can be leveraged for counterfactual estimation. We propose a counterfactual estimator that fits a bivariate beta distribution to predicted uplift scores, yielding posterior distributions over counterfactual outcomes. Our approach requires no causal assumptions beyond those of uplift modeling. Simulations show the efficacy of the approach, which can be applied, for example, to the problem of customer churn in telecom, where it reveals insights unavailable to standard ML or uplift models alone.

</details>


### [82] [Forecasting Fails: Unveiling Evasion Attacks in Weather Prediction Models](https://arxiv.org/abs/2512.08832)
*Huzaifa Arif,Pin-Yu Chen,Alex Gittens,James Diffenderfer,Bhavya Kailkhura*

Main category: cs.LG

TL;DR: WAAPO框架生成针对AI天气预报模型的对抗性扰动，通过通道稀疏性、空间定位和平滑性约束确保扰动有效且隐蔽，揭示了AI天气预报系统的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着AI模型在天气预报中的广泛应用，需要评估其对对抗性扰动的脆弱性，以保护天气预报系统免受恶意攻击。

Method: 提出Weather Adaptive Adversarial Perturbation Optimization (WAAPO)框架，通过通道稀疏性、空间定位和平滑性约束生成对抗性扰动，确保扰动物理真实且难以检测。

Result: 使用ERA5数据集和FourCastNet模型，WAAPO能够生成与预定目标紧密对齐的对抗性轨迹，即使是在约束条件下，初始条件的微小扰动也能导致预测天气模式的显著偏差。

Conclusion: AI驱动的天气预报模型存在关键脆弱性，需要建立强大的防护措施来防止对抗性攻击在业务预报系统中的利用。

Abstract: With the increasing reliance on AI models for weather forecasting, it is imperative to evaluate their vulnerability to adversarial perturbations. This work introduces Weather Adaptive Adversarial Perturbation Optimization (WAAPO), a novel framework for generating targeted adversarial perturbations that are both effective in manipulating forecasts and stealthy to avoid detection. WAAPO achieves this by incorporating constraints for channel sparsity, spatial localization, and smoothness, ensuring that perturbations remain physically realistic and imperceptible. Using the ERA5 dataset and FourCastNet (Pathak et al. 2022), we demonstrate WAAPO's ability to generate adversarial trajectories that align closely with predefined targets, even under constrained conditions. Our experiments highlight critical vulnerabilities in AI-driven forecasting models, where small perturbations to initial conditions can result in significant deviations in predicted weather patterns. These findings underscore the need for robust safeguards to protect against adversarial exploitation in operational forecasting systems.

</details>


### [83] [Reinforcement Learning From State and Temporal Differences](https://arxiv.org/abs/2512.08855)
*Lex Weaver,Jonathan Baxter*

Main category: cs.LG

TL;DR: 论文提出STD(λ)算法改进TD(λ)，通过关注状态相对值而非绝对值来避免策略退化，在简单系统和Acrobot问题上验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 传统TD(λ)算法使用函数逼近时最小化状态值的平方误差，但对于策略而言，状态间的相对排序比绝对值更重要。作者发现TD(λ)即使从最优策略开始，也可能收敛到次优策略，这在实际问题如西洋双陆棋中也有体现。

Method: 提出STD(λ)算法，在二元决策问题中训练函数逼近器时关注状态的相对值而非绝对值。该方法基于相对状态值进行训练，并提供了理论分析，包括在两状态系统中单调策略改进的证明。

Result: 在两状态系统和Acrobot问题的变体上成功演示了STD(λ)的有效性。与Bertsekas的差分训练方法进行了比较，STD(λ)能够避免TD(λ)中出现的策略退化问题。

Conclusion: STD(λ)通过关注状态相对值而非绝对值，解决了TD(λ)在函数逼近中可能导致策略退化的缺陷，为强化学习中的值函数逼近提供了更稳健的方法。

Abstract: TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas' differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.

</details>


### [84] [Refining Diffusion Models for Motion Synthesis with an Acceleration Loss to Generate Realistic IMU Data](https://arxiv.org/abs/2512.08859)
*Lars Ole Häusler,Lena Uhlenberg,Göran Köber,Diyora Salimova,Oliver Amft*

Main category: cs.LG

TL;DR: 提出一种文本到IMU运动合成框架，通过使用基于加速度的二阶损失(L_acc)微调预训练扩散模型，生成更真实的IMU数据


<details>
  <summary>Details</summary>
Motivation: 现有文本到运动模型生成的IMU数据与真实IMU记录存在差异，特别是在加速度模式方面，需要专门针对IMU传感器特性优化运动生成

Method: 1. 提出加速度感知的二阶损失函数L_acc，强制生成运动在离散二阶时间差分上的一致性；2. 将L_acc集成到现有扩散模型的训练目标中，微调得到IMU特定的运动先验；3. 结合表面建模和虚拟传感器模拟的现有文本到IMU框架进行评估

Result: 1. L_acc损失相比原始模型降低了12.7%；2. 高动态活动（跑步、跳跃）改进显著大于低动态活动（坐、站）；3. 合成IMU数据在低维嵌入中更接近真实IMU记录分布；4. 仅使用合成IMU数据训练的人体活动识别性能提升8.7%

Conclusion: 加速度感知的扩散模型细化提供了一种有效方法，能够对齐运动生成和IMU合成，展示了深度学习管道在将通用文本到运动先验专门化到传感器特定任务方面的灵活性

Abstract: We propose a text-to-IMU (inertial measurement unit) motion-synthesis framework to obtain realistic IMU data by fine-tuning a pretrained diffusion model with an acceleration-based second-order loss (L_acc). L_acc enforces consistency in the discrete second-order temporal differences of the generated motion, thereby aligning the diffusion prior with IMU-specific acceleration patterns. We integrate L_acc into the training objective of an existing diffusion model, finetune the model to obtain an IMU-specific motion prior, and evaluate the model with an existing text-to-IMU framework that comprises surface modelling and virtual sensor simulation. We analysed acceleration signal fidelity and differences between synthetic motion representation and actual IMU recordings. As a downstream application, we evaluated Human Activity Recognition (HAR) and compared the classification performance using data of our method with the earlier diffusion model and two additional diffusion model baselines. When we augmented the earlier diffusion model objective with L_acc and continued training, L_acc decreased by 12.7% relative to the original model. The improvements were considerably larger in high-dynamic activities (i.e., running, jumping) compared to low-dynamic activities~(i.e., sitting, standing). In a low-dimensional embedding, the synthetic IMU data produced by our refined model shifts closer to the distribution of real IMU recordings. HAR classification trained exclusively on our refined synthetic IMU data improved performance by 8.7% compared to the earlier diffusion model and by 7.6% over the best-performing comparison diffusion model. We conclude that acceleration-aware diffusion refinement provides an effective approach to align motion generation and IMU synthesis and highlights how flexible deep learning pipelines are for specialising generic text-to-motion priors to sensor-specific tasks.

</details>


### [85] [Unsupervised Learning of Density Estimates with Topological Optimization](https://arxiv.org/abs/2512.08895)
*Suina Tanweer,Firas A. Khasawneh*

Main category: cs.LG

TL;DR: 提出一种基于拓扑的损失函数，用于无监督自动选择核密度估计的最优带宽，并在不同维度下与传统方法进行对比验证


<details>
  <summary>Details</summary>
Motivation: 核密度估计在机器学习、贝叶斯推断等领域广泛应用，但其关键超参数——核带宽的选择至关重要，直接影响偏差-方差权衡和拓扑特征的平滑程度。传统方法需要人工调参，而拓扑数据分析能够量化高维数据中的拓扑特征，为解决带宽选择问题提供了新思路

Method: 提出一种无监督学习方法，使用基于拓扑的损失函数来自动选择最优带宽。该方法利用拓扑数据分析技术量化密度估计的拓扑特征（如连通分量、环、空洞等），通过优化拓扑损失函数实现带宽的自动化选择

Result: 该方法在不同维度下进行了基准测试，与传统技术进行了对比，展示了其在自动选择最优带宽方面的潜力

Conclusion: 基于拓扑的损失函数为核密度估计的带宽选择提供了一种有效的无监督自动化方法，能够在不同维度下实现优于传统方法的性能

Abstract: Kernel density estimation is a key component of a wide variety of algorithms in machine learning, Bayesian inference, stochastic dynamics and signal processing. However, the unsupervised density estimation technique requires tuning a crucial hyperparameter: the kernel bandwidth. The choice of bandwidth is critical as it controls the bias-variance trade-off by over- or under-smoothing the topological features. Topological data analysis provides methods to mathematically quantify topological characteristics, such as connected components, loops, voids et cetera, even in high dimensions where visualization of density estimates is impossible. In this paper, we propose an unsupervised learning approach using a topology-based loss function for the automated and unsupervised selection of the optimal bandwidth and benchmark it against classical techniques -- demonstrating its potential across different dimensions.

</details>


### [86] [Open Polymer Challenge: Post-Competition Report](https://arxiv.org/abs/2512.08896)
*Gang Liu,Sobin Alosious,Subhamoy Mahajan,Eric Inae,Yihan Zhu,Yuhan Liu,Renzheng Zhang,Jiaxin Xu,Addison Howard,Ying Li,Tengfei Luo,Meng Jiang*

Main category: cs.LG

TL;DR: Open Polymer Challenge发布首个聚合物信息学社区基准数据集，包含1万种聚合物和5种性质，通过多任务预测竞赛推动可持续聚合物材料的机器学习发现。


<details>
  <summary>Details</summary>
Motivation: 机器学习在发现可持续聚合物材料方面具有巨大潜力，但进展受到缺乏大规模、高质量、开放可访问的聚合物数据集的限制。需要建立社区基准来推动该领域发展。

Method: 发布包含10K聚合物和5种性质（热导率、回转半径、密度、自由体积分数、玻璃化转变温度）的数据集。组织多任务聚合物性质预测竞赛，参与者在数据量小、标签不平衡、模拟源异质等现实约束下开发模型，使用特征增强、迁移学习、自监督预训练和针对性集成策略等技术。

Result: 竞赛揭示了数据准备、分布偏移和跨组模拟一致性等方面的重要经验教训，为未来大规模聚合物数据集的最佳实践提供了指导。生成的模型、分析和发布数据为聚合物科学中的分子AI建立了新基础。

Conclusion: Open Polymer Challenge通过发布基准数据集和组织预测竞赛，为可持续和节能材料的开发加速提供了新基础，同时公开了测试数据集和可模拟25种以上性质的数据生成管道。

Abstract: Machine learning (ML) offers a powerful path toward discovering sustainable polymer materials, but progress has been limited by the lack of large, high-quality, and openly accessible polymer datasets. The Open Polymer Challenge (OPC) addresses this gap by releasing the first community-developed benchmark for polymer informatics, featuring a dataset with 10K polymers and 5 properties: thermal conductivity, radius of gyration, density, fractional free volume, and glass transition temperature. The challenge centers on multi-task polymer property prediction, a core step in virtual screening pipelines for materials discovery. Participants developed models under realistic constraints that include small data, label imbalance, and heterogeneous simulation sources, using techniques such as feature-based augmentation, transfer learning, self-supervised pretraining, and targeted ensemble strategies. The competition also revealed important lessons about data preparation, distribution shifts, and cross-group simulation consistency, informing best practices for future large-scale polymer datasets. The resulting models, analysis, and released data create a new foundation for molecular AI in polymer science and are expected to accelerate the development of sustainable and energy-efficient materials. Along with the competition, we release the test dataset at https://www.kaggle.com/datasets/alexliu99/neurips-open-polymer-prediction-2025-test-data. We also release the data generation pipeline at https://github.com/sobinalosious/ADEPT, which simulates more than 25 properties, including thermal conductivity, radius of gyration, and density.

</details>
