<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 32]
- [cs.AI](#cs.AI) [Total: 14]
- [cs.HC](#cs.HC) [Total: 12]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [TECM*: A Data-Driven Assessment to Reinforcement Learning Methods and Application to Heparin Treatment Strategy for Surgical Sepsis](https://arxiv.org/abs/2512.10973)
*Jiang Liu,Yujie Li,Chan Zhou,Yihao Xie,Qilong Sun,Xin Shu,Peiwei Li,Chunyong Yang,Yiziting Zhu,Jiaqi Zhu,Yuwen Chen,Bo An,Hao Wu,Bin Yi*

Main category: cs.LG

TL;DR: 该研究提出了一种基于强化学习的个性化肝素治疗优化框架，通过连续cxSOFA评分和TECM评估矩阵，在手术脓毒症患者中实现治疗策略优化，显著降低死亡率和住院时间。


<details>
  <summary>Details</summary>
Motivation: 脓毒症是危及生命的严重感染导致急性器官功能障碍的疾病。传统SOFA评分是离散的，限制了治疗优化的精细度。需要数据驱动的指标和连续奖励函数来优化手术脓毒症患者的个性化肝素治疗。

Method: 使用MIMIC-IV v1.0和eICU v2.0数据库数据。提出新的RL框架：1)将离散SOFA评分转换为连续cxSOFA用于状态和奖励函数；2)基于cxSOFA逐步定义"好"或"坏"治疗策略；3)提出治疗效应比较矩阵(TECM)评估治疗策略。应用Q-Learning、DQN、DDQN、BCQ和CQL等RL算法优化治疗。

Result: 在AI衍生策略中，cxSOFA-CQL模型表现最佳，将死亡率从1.83%降至0.74%，平均住院时间从11.11天降至9.42天。TECM在不同模型中显示一致结果，证明框架稳健性。

Conclusion: 提出的RL框架能够实现手术脓毒症肝素治疗的可解释和稳健优化。连续cxSOFA评分和TECM评估提供了精细的治疗评估，有望改善临床结果和决策支持可靠性。

Abstract: Objective: Sepsis is a life-threatening condition caused by severe infection leading to acute organ dysfunction. This study proposes a data-driven metric and a continuous reward function to optimize personalized heparin therapy in surgical sepsis patients. Methods: Data from the MIMIC-IV v1.0 and eICU v2.0 databases were used for model development and evaluation. The training cohort consisted of abdominal surgery patients receiving unfractionated heparin (UFH) after postoperative sepsis onset. We introduce a new RL-based framework: converting the discrete SOFA score to a continuous cxSOFA for more nuanced state and reward functions; Second, defining "good" or "bad" strategies based on cxSOFA by a stepwise manner; Third, proposing a Treatment Effect Comparison Matrix (TECM), analogous to a confusion matrix for classification tasks, to evaluate the treatment strategies. We applied different RL algorithms, Q-Learning, DQN, DDQN, BCQ and CQL to optimize the treatment and comprehensively evaluated the framework. Results: Among the AI-derived strategies, the cxSOFA-CQL model achieved the best performance, reducing mortality from 1.83% to 0.74% with the average hospital stay from 11.11 to 9.42 days. TECM demonstrated consistent outcomes across models, highlighting robustness. Conclusion: The proposed RL framework enables interpretable and robust optimization of heparin therapy in surgical sepsis. Continuous cxSOFA scoring and TECM-based evaluation provide nuanced treatment assessment, showing promise for improving clinical outcomes and decision-support reliability.

</details>


### [2] [Agent-Based Modular Learning for Multimodal Emotion Recognition in Human-Agent Systems](https://arxiv.org/abs/2512.10975)
*Matvey Nepomnyaschiy,Oleg Pereziabov,Anvar Tliamov,Stanislav Mikhailov,Ilya Afanasyev*

Main category: cs.LG

TL;DR: 提出一种多智能体框架用于训练多模态情感识别系统，通过模块化架构实现模态灵活集成和计算效率提升


<details>
  <summary>Details</summary>
Motivation: 传统多模态深度学习模型在情感识别方面虽然准确率高，但训练和维护计算密集，且对模态变化的适应性不足，需要更灵活、可扩展的解决方案

Method: 采用多智能体框架，每个模态编码器和融合分类器作为自主智能体，由中央监督器协调，支持模块化集成新模态（如emotion2vec音频特征）和组件替换

Result: 通过支持视觉、音频和文本模态的概念验证实现，展示了框架的可行性，分类器作为共享决策智能体，提高了训练效率

Conclusion: 该框架不仅提升了训练效率，还为HAI场景中的具身和虚拟智能体设计了更灵活、可扩展和可维护的感知模块

Abstract: Effective human-agent interaction (HAI) relies on accurate and adaptive perception of human emotional states. While multimodal deep learning models - leveraging facial expressions, speech, and textual cues - offer high accuracy in emotion recognition, their training and maintenance are often computationally intensive and inflexible to modality changes. In this work, we propose a novel multi-agent framework for training multimodal emotion recognition systems, where each modality encoder and the fusion classifier operate as autonomous agents coordinated by a central supervisor. This architecture enables modular integration of new modalities (e.g., audio features via emotion2vec), seamless replacement of outdated components, and reduced computational overhead during training. We demonstrate the feasibility of our approach through a proof-of-concept implementation supporting vision, audio, and text modalities, with the classifier serving as a shared decision-making agent. Our framework not only improves training efficiency but also contributes to the design of more flexible, scalable, and maintainable perception modules for embodied and virtual agents in HAI scenarios.

</details>


### [3] [MolSculpt: Sculpting 3D Molecular Geometries from Chemical Syntax](https://arxiv.org/abs/2512.10991)
*Zhanpeng Chen,Weihao Gao,Shunyu Wang,Yanan Zhu,Hong Meng,Yuexian Zou*

Main category: cs.LG

TL;DR: MolSculpt是一个新颖的3D分子生成框架，通过从冻结的1D分子基础模型中提取化学知识，利用扩散模型指导3D几何结构生成，实现了1D化学语法与3D几何结构的深度集成。


<details>
  <summary>Details</summary>
Motivation: 现有方法使用1D表示（如SELFIES）确保分子有效性，但未能充分利用1D模型中丰富的化学知识，导致1D语法生成与3D几何实现之间存在脱节。需要弥合这一差距以实现更精确的3D分子几何生成。

Method: MolSculpt基于冻结的1D分子基础模型和3D分子扩散模型构建。引入可学习查询从基础模型中提取固有化学知识，通过可训练投影器将跨模态信息注入扩散模型的条件空间，以端到端优化方式深度集成1D潜在化学知识到3D生成过程。

Result: 实验表明，MolSculpt在从头3D分子生成和条件3D分子生成方面实现了最先进的性能，在GEOM-DRUGS和QM9数据集上显示出优越的3D保真度和稳定性。

Conclusion: MolSculpt成功地将1D化学知识与3D几何生成深度集成，为药物发现和材料科学提供了更精确的3D分子生成解决方案。

Abstract: Generating precise 3D molecular geometries is crucial for drug discovery and material science. While prior efforts leverage 1D representations like SELFIES to ensure molecular validity, they fail to fully exploit the rich chemical knowledge entangled within 1D models, leading to a disconnect between 1D syntactic generation and 3D geometric realization. To bridge this gap, we propose MolSculpt, a novel framework that "sculpts" 3D molecular geometries from chemical syntax. MolSculpt is built upon a frozen 1D molecular foundation model and a 3D molecular diffusion model. We introduce a set of learnable queries to extract inherent chemical knowledge from the foundation model, and a trainable projector then injects this cross-modal information into the conditioning space of the diffusion model to guide the 3D geometry generation. In this way, our model deeply integrates 1D latent chemical knowledge into the 3D generation process through end-to-end optimization. Experiments demonstrate that MolSculpt achieves state-of-the-art (SOTA) performance in \textit{de novo} 3D molecule generation and conditional 3D molecule generation, showing superior 3D fidelity and stability on both the GEOM-DRUGS and QM9 datasets. Code is available at https://github.com/SakuraTroyChen/MolSculpt.

</details>


### [4] [Memoryless Policy Iteration for Episodic POMDPs](https://arxiv.org/abs/2512.11082)
*Roy van Zuijlen,Duarte Antunes*

Main category: cs.LG

TL;DR: 提出一种新的策略迭代算法家族，用于求解部分可观测马尔可夫决策过程（POMDPs），通过交替执行单阶段输出策略改进和周期性策略评估，实现单调改进，并开发了无模型变体。


<details>
  <summary>Details</summary>
Motivation: 无记忆和有限记忆策略为求解POMDPs提供了实用替代方案，因为它们直接在输出空间而非高维信念空间中操作。然而，将经典方法（如策略迭代）扩展到这一设置仍然困难，因为输出过程是非马尔可夫的，使得策略改进步骤在不同阶段相互依赖。

Method: 引入一种新的单调改进策略迭代算法家族，交替执行单阶段输出策略改进和按照预定周期性模式进行策略评估。该家族包含最大化计算效率指标的最优模式，并识别出具有最小周期的最简单模式。基于此结构，进一步开发了从数据估计值并直接学习无记忆策略的无模型变体。

Result: 在多个POMDPs示例中，该方法在基于模型和无模型设置下，相比策略梯度基线和最近的专业算法，实现了显著的计算加速。

Conclusion: 提出的周期性策略迭代算法家族为求解POMDPs提供了一种高效方法，通过交替执行单阶段改进和评估，克服了输出过程非马尔可夫性带来的挑战，并在实际应用中表现出优越的计算性能。

Abstract: Memoryless and finite-memory policies offer a practical alternative for solving partially observable Markov decision processes (POMDPs), as they operate directly in the output space rather than in the high-dimensional belief space. However, extending classical methods such as policy iteration to this setting remains difficult; the output process is non-Markovian, making policy-improvement steps interdependent across stages. We introduce a new family of monotonically improving policy-iteration algorithms that alternate between single-stage output-based policy improvements and policy evaluations according to a prescribed periodic pattern. We show that this family admits optimal patterns that maximize a natural computational-efficiency index, and we identify the simplest pattern with minimal period. Building on this structure, we further develop a model-free variant that estimates values from data and learns memoryless policies directly. Across several POMDPs examples, our method achieves significant computational speedups over policy-gradient baselines and recent specialized algorithms in both model-based and model-free settings.

</details>


### [5] [Investigating ECG Diagnosis with Ambiguous Labels using Partial Label Learning](https://arxiv.org/abs/2512.11095)
*Sana Rahmani,Javad Hashemi,Ali Etemad*

Main category: cs.LG

TL;DR: 该研究首次系统性地将部分标签学习（PLL）方法应用于心电图（ECG）诊断，评估了9种PLL算法在不同临床相关标签模糊性下的表现，揭示了现有方法在临床环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现实世界ECG诊断中存在固有的标签模糊性问题（如疾病重叠、诊断分歧），但现有ECG模型都假设标签是干净无歧义的，这限制了模型在真实临床环境中的发展和评估。尽管部分标签学习（PLL）框架旨在处理模糊标签，但其在医疗时间序列领域（特别是ECG）的有效性尚未得到充分探索。

Method: 将9种PLL算法适配到多标签ECG诊断任务中，使用多种临床驱动的模糊性生成策略进行评估，包括非结构化（如随机）和结构化模糊性（如心脏病专家定义的相似性、治疗关系、诊断分类学）。在PTB-XL和Chapman数据集上进行实验。

Result: 实验表明，不同PLL方法对各种类型和程度的模糊性表现出显著不同的鲁棒性。通过深入分析，识别了当前PLL方法在临床环境中的关键局限性。

Conclusion: 该研究为ECG诊断中模糊感知学习框架的开发提供了基础，指出了未来研究方向，以开发更鲁棒且符合临床需求的ECG诊断模型。

Abstract: Label ambiguity is an inherent problem in real-world electrocardiogram (ECG) diagnosis, arising from overlapping conditions and diagnostic disagreement. However, current ECG models are trained under the assumption of clean and non-ambiguous annotations, which limits both the development and the meaningful evaluation of models under real-world conditions. Although Partial Label Learning (PLL) frameworks are designed to learn from ambiguous labels, their effectiveness in medical time-series domains, ECG in particular, remains largely unexplored. In this work, we present the first systematic study of PLL methods for ECG diagnosis. We adapt nine PLL algorithms to multi-label ECG diagnosis and evaluate them using a diverse set of clinically motivated ambiguity generation strategies, capturing both unstructured (e.g., random) and structured ambiguities (e.g., cardiologist-derived similarities, treatment relationships, and diagnostic taxonomies). Our experiments on the PTB-XL and Chapman datasets demonstrate that PLL methods vary substantially in their robustness to different types and degrees of ambiguity. Through extensive analysis, we identify key limitations of current PLL approaches in clinical settings and outline future directions for developing robust and clinically aligned ambiguity-aware learning frameworks for ECG diagnosis.

</details>


### [6] [Limits and Gains of Test-Time Scaling in Vision-Language Reasoning](https://arxiv.org/abs/2512.11109)
*Mohammadjavad Ahmadpour,Amirmahdi Meighani,Payam Taebi,Omid Ghahroodi,Amirmohammad Izadi,Mahdieh Soleymani Baghshah*

Main category: cs.LG

TL;DR: 测试时扩展(TTS)在提升LLMs推理能力方面有效，但在多模态VLM中的应用研究不足。研究发现闭源模型能从结构化推理和迭代自优化中获益，而开源VLM表现不一致，外部验证效果最好，迭代优化反而降低性能。TTS效果依赖数据集，在多步推理任务中改进明显，但在感知任务中增益有限。


<details>
  <summary>Details</summary>
Motivation: 测试时扩展(TTS)范式在提升大型语言模型推理能力方面显示出强大效果，但该技术在多模态系统（如视觉语言模型VLMs）中的应用尚未得到充分探索。研究者希望系统性地研究推理时间方法在不同开源和闭源VLM上的应用效果。

Method: 对开源和闭源视觉语言模型进行系统性的实证研究，应用不同的推理时间方法，包括结构化推理、迭代自优化和外部验证等，并在不同基准测试上进行评估。

Result: 闭源模型始终能从结构化推理和迭代自优化中获益；开源VLM表现不一致：外部验证提供最可靠的性能提升，而迭代优化通常会降低性能。TTS效果具有数据集依赖性，在多步推理任务中能带来明显改进，但在感知为主的基准测试中增益有限。

Conclusion: TTS不是通用解决方案，必须根据模型能力和任务特性进行定制。这为未来自适应TTS策略和多模态奖励模型的研究提供了动机。

Abstract: Test-time scaling (TTS) has emerged as a powerful paradigm for improving the reasoning ability of Large Language Models (LLMs) by allocating additional computation at inference, yet its application to multimodal systems such as Vision-Language Models (VLMs) remains underexplored. In this work, we present a systematic empirical study of inference time reasoning methods applied across both open-source and closed-source VLMs on different benchmarks. Our results reveal that while closed-source models consistently benefit from structured reasoning and iterative Self-Refinement, open-source VLMs show inconsistent behavior: external verification provides the most reliable gains, whereas iterative refinement often degrades performance. We further find that the effectiveness of TTS is dataset-dependent, yielding clear improvements on multi-step reasoning tasks but offering only limited gains on perception-focused benchmarks. These findings demonstrate that TTS is not a universal solution and must be tailored to both model capabilities and task characteristics, motivating future work on adaptive TTS strategies and multimodal reward models.

</details>


### [7] [Fairness-Regularized Online Optimization with Switching Costs](https://arxiv.org/abs/2512.11131)
*Pengfei Li,Yuelin Han,Adam Wierman,Shaolei Ren*

Main category: cs.LG

TL;DR: 本文提出FairOBD算法，首次同时解决在线凸优化中的公平性和动作平滑性问题，通过引入辅助变量将长期公平成本分解为在线成本，并在动态计算资源分配场景中验证了有效性。


<details>
  <summary>Details</summary>
Motivation: 公平性和动作平滑性是在线优化中的两个关键考量，但现有研究尚未能同时解决这两个问题。本文旨在研究带有切换成本的公平性正则化平滑在线凸优化这一新挑战性场景。

Method: 提出FairOBD（公平性正则化在线平衡下降）算法：1）将长期公平成本通过引入辅助变量分解为一系列在线成本；2）利用辅助变量正则化在线动作以实现公平结果；3）采用新方法处理切换成本。

Result: 理论证明：1）即使没有切换成本，任何在线算法也无法实现相对于离线最优算法的次线性遗憾或有限竞争比；2）FairOBD在考虑T→∞时，相对于参数化约束的最优离线算法提供最坏情况渐进竞争比。实验验证：在动态计算资源分配场景中，FairOBD能有效降低总公平性正则化成本，比现有基线方案更好地促进公平结果。

Conclusion: FairOBD算法成功调和了命中成本、切换成本和公平成本之间的张力，首次实现了公平性和动作平滑性的同时优化，为公平性正则化平滑在线凸优化问题提供了有效解决方案。

Abstract: Fairness and action smoothness are two crucial considerations in many online optimization problems, but they have yet to be addressed simultaneously. In this paper, we study a new and challenging setting of fairness-regularized smoothed online convex optimization with switching costs. First, to highlight the fundamental challenges introduced by the long-term fairness regularizer evaluated based on the entire sequence of actions, we prove that even without switching costs, no online algorithms can possibly achieve a sublinear regret or finite competitive ratio compared to the offline optimal algorithm as the problem episode length $T$ increases. Then, we propose FairOBD (Fairness-regularized Online Balanced Descent), which reconciles the tension between minimizing the hitting cost, switching cost, and fairness cost. Concretely, FairOBD decomposes the long-term fairness cost into a sequence of online costs by introducing an auxiliary variable and then leverages the auxiliary variable to regularize the online actions for fair outcomes. Based on a new approach to account for switching costs, we prove that FairOBD offers a worst-case asymptotic competitive ratio against a novel benchmark -- the optimal offline algorithm with parameterized constraints -- by considering $T\to\infty$. Finally, we run trace-driven experiments of dynamic computing resource provisioning for socially responsible AI inference to empirically evaluate FairOBD, showing that FairOBD can effectively reduce the total fairness-regularized cost and better promote fair outcomes compared to existing baseline solutions.

</details>


### [8] [The Vekua Layer: Exact Physical Priors for Implicit Neural Representations via Generalized Analytic Functions](https://arxiv.org/abs/2512.11138)
*Vladimer Khasia*

Main category: cs.LG

TL;DR: Vekua Layer (VL) 是一种基于广义解析函数理论的微分谱方法，通过将假设空间限制在控制微分算子的核中，将隐式神经表示的学习任务从非凸优化转化为严格凸的最小二乘问题，在椭圆PDE上实现了机器精度重建和优越的噪声稳定性。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）在参数化物理场方面表现出色，但存在谱偏差和非凸优化的计算开销问题。需要一种更高效、更稳定的方法来学习物理场表示。

Method: 提出Vekua Layer（VL），一种基于广义解析函数理论的微分谱方法。通过将假设空间限制在控制微分算子的核中（使用调和基和傅里叶-贝塞尔基），将学习任务从迭代梯度下降转化为严格凸的最小二乘问题，通过线性投影求解。

Result: 在齐次椭圆偏微分方程上，VL相比SIRENs实现了机器精度（MSE ≈ 10^{-33}）的精确重建，在非相干传感器噪声下表现出优越的稳定性（MSE ≈ 0.03），并能够通过解析延拓从部分边界数据实现全局场的"全息"外推。

Conclusion: VL提供了一种基于物理信息的谱滤波方法，将隐式神经表示的学习转化为凸优化问题，实现了更高的精度、稳定性和外推能力，为物理场参数化提供了新的有效工具。

Abstract: Implicit Neural Representations (INRs) have emerged as a powerful paradigm for parameterizing physical fields, yet they often suffer from spectral bias and the computational expense of non-convex optimization. We introduce the Vekua Layer (VL), a differentiable spectral method grounded in the classical theory of Generalized Analytic Functions. By restricting the hypothesis space to the kernel of the governing differential operator -- specifically utilizing Harmonic and Fourier-Bessel bases -- the VL transforms the learning task from iterative gradient descent to a strictly convex least-squares problem solved via linear projection. We evaluate the VL against Sinusoidal Representation Networks (SIRENs) on homogeneous elliptic Partial Differential Equations (PDEs). Our results demonstrate that the VL achieves machine precision ($\text{MSE} \approx 10^{-33}$) on exact reconstruction tasks and exhibits superior stability in the presence of incoherent sensor noise ($\text{MSE} \approx 0.03$), effectively acting as a physics-informed spectral filter. Furthermore, we show that the VL enables "holographic" extrapolation of global fields from partial boundary data via analytic continuation, a capability absent in standard coordinate-based approximations.

</details>


### [9] [Autoencoder-based Semi-Supervised Dimensionality Reduction and Clustering for Scientific Ensembles](https://arxiv.org/abs/2512.11145)
*Lennard Manuel,Hamid Gadirov,Steffen Frey*

Main category: cs.LG

TL;DR: 提出一种结合聚类损失和对比损失的增强自编码器框架，用于高维复杂科学集成数据集的可视化和特征提取，在土壤通道结构和液滴冲击动力学数据集上表现优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 高维复杂的科学集成数据集在分析和可视化方面面临重大挑战，传统降维技术和自编码器难以有效处理此类数据，需要改进方法来提升可视化和可解释性。

Method: 1. 使用EfficientNetV2为无标签的科学集成数据集生成伪标签；2. 提出增强自编码器框架，结合基于软轮廓分数的聚类损失和对比损失；3. 联合优化重构、聚类和对比目标，使相似数据点在潜在空间中聚集，不同簇分离；4. 对潜在表示应用UMAP生成2D投影；5. 使用轮廓分数评估性能，比较多种自编码器。

Result: 在两个科学集成数据集（基于马尔可夫链蒙特卡洛的土壤通道结构、液滴冲击动力学）上的实验表明，结合聚类或对比损失的模型在特征提取能力上略优于基线方法。

Conclusion: 提出的增强自编码器框架通过整合聚类和对比损失，能够有效改善高维复杂科学集成数据集的可视化和可解释性，为科学数据分析提供了有价值的工具。

Abstract: Analyzing and visualizing scientific ensemble datasets with high dimensionality and complexity poses significant challenges. Dimensionality reduction techniques and autoencoders are powerful tools for extracting features, but they often struggle with such high-dimensional data. This paper presents an enhanced autoencoder framework that incorporates a clustering loss, based on the soft silhouette score, alongside a contrastive loss to improve the visualization and interpretability of ensemble datasets. First, EfficientNetV2 is used to generate pseudo-labels for the unlabeled portions of the scientific ensemble datasets. By jointly optimizing the reconstruction, clustering, and contrastive objectives, our method encourages similar data points to group together while separating distinct clusters in the latent space. UMAP is subsequently applied to this latent representation to produce 2D projections, which are evaluated using the silhouette score. Multiple types of autoencoders are evaluated and compared based on their ability to extract meaningful features. Experiments on two scientific ensemble datasets - channel structures in soil derived from Markov chain Monte Carlo, and droplet-on-film impact dynamics - show that models incorporating clustering or contrastive loss marginally outperform the baseline approaches.

</details>


### [10] [Harnessing Rich Multi-Modal Data for Spatial-Temporal Homophily-Embedded Graph Learning Across Domains and Localities](https://arxiv.org/abs/2512.11178)
*Takuya Kurihana,Xiaojian Zhang,Wing Yee Au,Hon Yung Wong*

Main category: cs.LG

TL;DR: 提出一个异构数据管道，用于融合城市中随时间、空间变化的多元数据集，解决跨域城市问题，并在多个城市数据集上验证了其通用性和灵活性。


<details>
  <summary>Details</summary>
Motivation: 现代城市依赖数据驱动决策，但城市数据存在异构格式、由不同机构独立收集、标准不一的问题。国家级数据集虽然广泛可用但具有显著异质性和多模态特性，需要统一处理框架。

Method: 提出异构数据管道，执行跨域数据融合，处理随时间变化、空间变化和时空序列数据集。数据学习模块将空间变化数据集中的同质性整合到图学习中，将不同地区的信息嵌入模型。利用50多个数据源。

Result: 在五个真实世界观测中使用多种公开可访问数据集（如共享乘车、交通事故、犯罪报告）进行验证。结果显示框架具有强大的预测性能，在转移到新地区或领域时只需最小重新配置。

Conclusion: 该研究推进了以可扩展方式构建数据驱动的城市系统的目标，解决了智慧城市分析中最紧迫的挑战之一，为跨域城市问题提供了通用解决方案。

Abstract: Modern cities are increasingly reliant on data-driven insights to support decision making in areas such as transportation, public safety and environmental impact. However, city-level data often exists in heterogeneous formats, collected independently by local agencies with diverse objectives and standards. Despite their numerous, wide-ranging, and uniformly consumable nature, national-level datasets exhibit significant heterogeneity and multi-modality. This research proposes a heterogeneous data pipeline that performs cross-domain data fusion over time-varying, spatial-varying and spatial-varying time-series datasets. We aim to address complex urban problems across multiple domains and localities by harnessing the rich information over 50 data sources. Specifically, our data-learning module integrates homophily from spatial-varying dataset into graph-learning, embedding information of various localities into models. We demonstrate the generalizability and flexibility of the framework through five real-world observations using a variety of publicly accessible datasets (e.g., ride-share, traffic crash, and crime reports) collected from multiple cities. The results show that our proposed framework demonstrates strong predictive performance while requiring minimal reconfiguration when transferred to new localities or domains. This research advances the goal of building data-informed urban systems in a scalable way, addressing one of the most pressing challenges in smart city analytics.

</details>


### [11] [Progress over Points: Reframing LM Benchmarks Around Scientific Objectives](https://arxiv.org/abs/2512.11183)
*Alwin Jin,Sean M. Hendryx,Vaskar Nath*

Main category: cs.LG

TL;DR: 论文提出"进展导向基准测试"新范式，以NanoGPT速度跑为实例，将基准测试从静态问题排行榜转变为可测量的开放式科学研究工具，通过改进语言建模堆栈推动科学进步。


<details>
  <summary>Details</summary>
Motivation: 当前基于静态已解决问题（如数学应用题）的基准测试虽然能展示基本能力获取，但限制了可测量的进步类型。需要一种能直接推动科学进步的基准测试范式。

Method: 提出进展导向基准测试框架，以NanoGPT速度跑为实例环境，标准化数据集切片、参考模型、训练工具和丰富遥测数据，包含运行时验证和反作弊检查。评估聚焦于实现的科学增量：最佳损失值和效率前沿。

Result: 在该环境中实现了新的最先进训练时间，比先前记录快3秒，并定性观察到新颖算法思想的涌现。模型和智能体之间的比较仍然可能，但只是手段而非目的。

Conclusion: 基准测试应成为科学进步的工具，而非仅仅是模型比较的平台。通过这种新范式，基准测试上的进步就是科学上的进步，从而将"基准测试"重新定义为科学进步的载体。

Abstract: Current benchmarks that test LLMs on static, already-solved problems (e.g., math word problems) effectively demonstrated basic capability acquisition. The natural progression has been toward larger, more comprehensive and challenging collections of static problems, an approach that inadvertently constrains the kinds of advances we can measure and incentivize. To address this limitation, we argue for progress-oriented benchmarks, problem environments whose objectives are themselves the core targets of scientific progress, so that achieving state of the art on the benchmark advances the field. As a introductory step, we instantiate an environment based on the NanoGPT speedrun. The environment standardizes a dataset slice, a reference model and training harness, and rich telemetry, with run-time verification and anti-gaming checks. Evaluation centers on the scientific delta achieved: best-attained loss and the efficiency frontier. Using this environment, we achieve a new state-of-the-art training time, improving upon the previous record by 3 seconds, and qualitatively observe the emergence of novel algorithmic ideas. Moreover, comparisons between models and agents remain possible, but they are a means, not the end; the benchmark's purpose is to catalyze reusable improvements to the language modeling stack. With this release, the overarching goal is to seed a community shift from static problem leaderboards to test-time research on open-ended yet measurable scientific problems. In this new paradigm, progress on the benchmark is progress on the science, thus reframing "benchmarking" as a vehicle for scientific advancement.

</details>


### [12] [Beyond Memorization: Gradient Projection Enables Selective Learning in Diffusion Models](https://arxiv.org/abs/2512.11194)
*Divya Kothandaraman,Jaclyn Pytlarz*

Main category: cs.LG

TL;DR: 提出梯度投影框架，通过将梯度更新投影到敏感特征嵌入空间的正交补空间，实现概念级别的选择性遗忘，解决扩散模型中的记忆化问题


<details>
  <summary>Details</summary>
Motivation: 大规模文本到图像扩散模型中的记忆化带来安全和知识产权风险，传统方法只能限制对特定训练样本的过拟合，无法系统防止禁止概念级特征的内部化，丢弃所有包含敏感特征的图像会浪费宝贵训练数据

Method: 引入梯度投影框架，在反向传播期间识别并剔除与禁止属性嵌入对齐的训练信号，将每个梯度更新投影到敏感特征嵌入空间的正交补空间，从而消除其对模型权重的影响

Result: 在广泛实验中证明该框架能大幅减少记忆化，同时严格保持生成质量和语义保真度，可无缝集成到标准扩散模型训练流程中，并补充现有防御措施

Conclusion: 通过将记忆化控制重新定义为选择性学习，该方法为知识产权安全和隐私保护的生成式AI建立了新范式

Abstract: Memorization in large-scale text-to-image diffusion models poses significant security and intellectual property risks, enabling adversarial attribute extraction and the unauthorized reproduction of sensitive or proprietary features. While conventional dememorization techniques, such as regularization and data filtering, limit overfitting to specific training examples, they fail to systematically prevent the internalization of prohibited concept-level features. Simply discarding all images containing a sensitive feature wastes invaluable training data, necessitating a method for selective unlearning at the concept level.
  To address this, we introduce a Gradient Projection Framework designed to enforce a stringent requirement of concept-level feature exclusion. Our defense operates during backpropagation by systematically identifying and excising training signals aligned with embeddings of prohibited attributes. Specifically, we project each gradient update onto the orthogonal complement of the sensitive feature's embedding space, thereby zeroing out its influence on the model's weights. Our method integrates seamlessly into standard diffusion model training pipelines and complements existing defenses. We analyze our method against an adversary aiming for feature extraction. In extensive experiments, we demonstrate that our framework drastically reduces memorization while rigorously preserving generation quality and semantic fidelity. By reframing memorization control as selective learning, our approach establishes a new paradigm for IP-safe and privacy-preserving generative AI.

</details>


### [13] [Fast EXP3 Algorithms](https://arxiv.org/abs/2512.11201)
*Ryoma Sato,Shinji Ito*

Main category: cs.LG

TL;DR: EXP3算法可在每轮实现常数时间运行，作者提出了更实用的算法，并分析了这些算法的遗憾界与时间复杂度的权衡关系。


<details>
  <summary>Details</summary>
Motivation: EXP3算法作为多臂赌博机问题的经典算法，在实际应用中可能存在时间复杂度过高的问题。作者旨在改进EXP3算法的时间效率，使其更适合实际部署，同时研究算法性能与计算复杂度之间的权衡。

Method: 作者首先指出标准EXP3算法可以在每轮实现常数时间运行，然后提出了更实用的算法变体。这些方法可能包括优化权重更新机制、减少计算开销的技术，以及平衡计算效率与理论性能保证的设计。

Result: 论文展示了如何在保持EXP3算法理论保证的同时显著降低其时间复杂度。提出的算法在计算效率上有明显提升，同时提供了不同算法变体在遗憾界与时间复杂度之间的具体权衡分析。

Conclusion: 通过优化EXP3算法的实现，可以在每轮获得常数时间复杂度的算法，这大大提高了算法的实用性。论文为多臂赌博机问题提供了更高效的解决方案，并系统分析了计算效率与理论性能之间的权衡关系。

Abstract: We point out that EXP3 can be implemented in constant time per round, propose more practical algorithms, and analyze the trade-offs between the regret bounds and time complexities of these algorithms.

</details>


### [14] [Latent Variable Causal Discovery under Selection Bias](https://arxiv.org/abs/2512.11219)
*Haoyue Dai,Yiwen Qiu,Ignavier Ng,Xinshuai Dong,Peter Spirtes,Kun Zhang*

Main category: cs.LG

TL;DR: 该论文研究了在存在选择偏差的情况下进行潜变量因果发现，提出利用协方差子矩阵的秩约束作为条件独立性约束的推广，以处理选择偏差问题。


<details>
  <summary>Details</summary>
Motivation: 处理潜变量因果发现中的选择偏差是一个重要但尚未充分探索的问题，主要原因是缺乏合适的统计工具。现有的处理潜变量的工具都没有被适配用于处理选择偏差。

Method: 研究秩约束作为条件独立性约束的推广，在线性高斯模型中利用协方差子矩阵的秩。提供了这种秩约束的图论特征化，并证明单因子模型在选择偏差下可以被识别。

Result: 研究发现，尽管选择会显著复杂化联合分布，但偏差协方差矩阵中的秩仍然保留了关于因果结构和选择机制的有意义信息。模拟和真实世界实验证实了使用秩约束的有效性。

Conclusion: 秩约束为处理潜变量因果发现中的选择偏差问题提供了一个有效的统计工具，能够识别因果结构和选择机制，为这一重要但未充分探索的问题提供了解决方案。

Abstract: Addressing selection bias in latent variable causal discovery is important yet underexplored, largely due to a lack of suitable statistical tools: While various tools beyond basic conditional independencies have been developed to handle latent variables, none have been adapted for selection bias. We make an attempt by studying rank constraints, which, as a generalization to conditional independence constraints, exploits the ranks of covariance submatrices in linear Gaussian models. We show that although selection can significantly complicate the joint distribution, interestingly, the ranks in the biased covariance matrices still preserve meaningful information about both causal structures and selection mechanisms. We provide a graph-theoretic characterization of such rank constraints. Using this tool, we demonstrate that the one-factor model, a classical latent variable model, can be identified under selection bias. Simulations and real-world experiments confirm the effectiveness of using our rank constraints.

</details>


### [15] [Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery: Sublinear Memory Growth for Efficient LLM Inference](https://arxiv.org/abs/2512.11221)
*Adilet Metinov,Gulida M. Kudakeeva,Bolotbek uulu Nursultan,Gulnara D. Kabaeva*

Main category: cs.LG

TL;DR: 提出ASR-KF-EGR框架，通过可逆软冻结机制在推理时动态管理KV缓存，识别低重要性token并暂时冻结其KV更新，实现55-67%的KV缓存减少而不影响生成质量。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在长上下文生成时的内存瓶颈问题，特别是KV缓存的内存消耗，为内存受限环境提供实用的部署方案。

Method: 采用自适应软滚动KV冻结与熵引导恢复机制：1）在滑动注意力窗口内识别低重要性token；2）可逆软冻结机制暂时暂停这些token的KV更新；3）所有token保存在GPU外存储，按需恢复；4）次线性冻结调度，冻结时长随重复检测次数次线性增长。

Result: 在LLaMA-3 8B上的初步实验显示：1）主动KV缓存大小减少55-67%；2）保持生成质量；3）通过"大海捞针"检索测试；4）无需微调，架构无关。

Conclusion: ASR-KF-EGR为内存受限的长上下文LLM部署提供了有效的训练无关推理时解决方案，在显著减少KV缓存的同时保持了模型性能。

Abstract: We present Adaptive Soft Rolling KV Freeze with Entropy-Guided Recovery (ASR-KF-EGR), a training-free inference-time framework for efficient large language model generation. Our method introduces a reversible soft-freeze mechanism that temporarily suspends key-value (KV) updates for low-importance tokens identified within a sliding attention window. Unlike eviction-based approaches that permanently discard context, ASR-KF-EGR preserves all tokens in off-GPU storage and restores them on demand. We extend the framework with sublinear freeze scheduling, where freeze duration grows sublinearly with repeated low-importance detections, preventing over-aggressive compression. Preliminary experiments on LLaMA-3 8B demonstrate 55-67% reduction in active KV cache size while maintaining generation quality and passing needle-in-haystack retrieval tests. The method is architecture-agnostic, requires no fine-tuning, and provides a practical solution for memory-constrained deployment of long-context LLMs.

</details>


### [16] [Features Emerge as Discrete States: The First Application of SAEs to 3D Representations](https://arxiv.org/abs/2512.11263)
*Albert Miao,Chenliang Zhou,Jiawei Zhou,Cengiz Oztireli*

Main category: cs.LG

TL;DR: 首次将稀疏自编码器应用于3D领域，分析3D重建VAE的特征，发现模型编码离散特征而非连续特征，揭示了相变驱动的离散状态空间近似机制。


<details>
  <summary>Details</summary>
Motivation: 稀疏自编码器在文本领域表现优异，但很少应用于3D领域，限制了特征分解的理论探索。本研究旨在将SAE技术扩展到3D领域，分析3D重建模型的特征表示机制。

Method: 将稀疏自编码器应用于3D领域，分析一个先进的3D重建变分自编码器在Objaverse数据集（53k个3D模型）上的特征。通过特征激活的相变框架分析模型行为。

Result: 发现模型编码离散特征而非连续特征，模型近似于离散状态空间，由特征激活的相变驱动。解释了三个反直觉现象：模型偏好位置编码表示、特征消融的重建损失呈Sigmoid行为、相变点分布的双峰性。

Conclusion: 3D重建模型通过相变机制近似离散状态空间，重新分配叠加干扰以优先处理不同特征的显著性。该工作不仅解释了特征分解中的意外现象，还提供了理解模型特征学习动态的框架。

Abstract: Sparse Autoencoders (SAEs) are a powerful dictionary learning technique for decomposing neural network activations, translating the hidden state into human ideas with high semantic value despite no external intervention or guidance. However, this technique has rarely been applied outside of the textual domain, limiting theoretical explorations of feature decomposition. We present the \textbf{first application of SAEs to the 3D domain}, analyzing the features used by a state-of-the-art 3D reconstruction VAE applied to 53k 3D models from the Objaverse dataset. We observe that the network encodes discrete rather than continuous features, leading to our key finding: \textbf{such models approximate a discrete state space, driven by phase-like transitions from feature activations}. Through this state transition framework, we address three otherwise unintuitive behaviors -- the inclination of the reconstruction model towards positional encoding representations, the sigmoidal behavior of reconstruction loss from feature ablation, and the bimodality in the distribution of phase transition points. This final observation suggests the model \textbf{redistributes the interference caused by superposition to prioritize the saliency of different features}. Our work not only compiles and explains unexpected phenomena regarding feature decomposition, but also provides a framework to explain the model's feature learning dynamics. The code and dataset of encoded 3D objects will be available on release.

</details>


### [17] [QGEC : Quantum Golay Code Error Correction](https://arxiv.org/abs/2512.11307)
*Hideo Mukai,Hoshitaro Ohnishi*

Main category: cs.LG

TL;DR: 提出基于经典Golay码的量子纠错方法QGEC，使用Transformer解码器，在多种噪声模型下评估性能，发现Golay码比toric码在更少数据量子位下获得更高解码精度。


<details>
  <summary>Details</summary>
Motivation: 量子计算机在特定问题上相比经典计算机有计算负载优势，但量子比特易受外部噪声影响。量子纠错对处理量子比特至关重要，需要从稳定子生成元的综合征测量结果预测实际错误，而不是直接测量数据量子比特。

Method: 提出量子Golay码纠错方法QGEC，使用经典信息论中的高效编码方法Golay码。采用Transformer进行解码计算，在三种不同权重集合的生成多项式定义的码空间和三种不同比特翻转错误与相位翻转错误相关性的噪声模型下评估解码器准确性。

Result: 较小相关性的噪声模型给出更好的准确性，而生成多项式的权重对解码器准确性影响很小。Golay码（需要23个数据量子位，码距为7）比toric码（需要50个数据量子位，码距为5）实现了更高的解码准确性。

Conclusion: 使用Transformer实现量子纠错可能使Golay码更高效地实现容错量子计算，因为Golay码在更少量子位资源下获得了更好的纠错性能。

Abstract: Quantum computers have the possibility of a much reduced calculation load compared with classical computers in specific problems. Quantum error correction (QEC) is vital for handling qubits, which are vulnerable to external noise. In QEC, actual errors are predicted from the results of syndrome measurements by stabilizer generators, in place of making direct measurements of the data qubits. Here, we propose Quantum Golay code Error Correction (QGEC), a QEC method using Golay code, which is an efficient coding method in classical information theory. We investigated our method's ability in decoding calculations with the Transformer. We evaluated the accuracy of the decoder in a code space defined by the generative polynomials with three different weights sets and three noise models with different correlations of bit-flip error and phase-flip error. Furthermore, under a noise model following a discrete uniform distribution, we compared the decoding performance of Transformer decoders with identical architectures trained respectively on Golay and toric codes. The results showed that the noise model with the smaller correlation gave better accuracy, while the weights of the generative polynomials had little effect on the accuracy of the decoder. In addition, they showed that Golay code requiring 23 data qubits and having a code distance of 7 achieved higher decoding accuracy than toric code which requiring 50 data qubits and having a code distance of 5. This suggests that implementing quantum error correction using a Transformer may enable the Golay code to realize fault-tolerant quantum computation more efficiently.

</details>


### [18] [Benchmarking the Generality of Vision-Language-Action Models](https://arxiv.org/abs/2512.11315)
*Pranav Guruprasad,Sudipta Chowdhury,Harsh Sikka,Mridul Sharma,Helen Lu,Sean Rivera,Aryan Khurana,Hangliang Ren,Yangyue Wang*

Main category: cs.LG

TL;DR: MultiNet v1.0是一个统一基准测试，用于评估视觉语言模型和视觉语言动作模型在六个核心能力领域的跨领域泛化能力，发现当前模型在未见领域存在显著性能下降。


<details>
  <summary>Details</summary>
Motivation: 当前多模态智能体评估方法分散在不同基准测试中，难以评估基础模型是否真正超越了训练分布实现泛化。需要统一的评估框架来衡量模型的跨领域通用性。

Method: 开发MultiNet v1.0基准测试，涵盖六个基础能力领域：视觉定位、空间推理、工具使用、物理常识、多智能体协调和连续机器人控制。使用该基准评估GPT-5、Pi0和Magma等模型。

Result: 所有测试模型都未表现出一致的泛化能力。尽管在训练分布内表现良好，但在未见领域、不熟悉模态或跨领域任务转换时都出现显著性能下降。失败表现为模态错位、输出格式不稳定和领域转移下的灾难性知识退化。

Conclusion: 当前基础模型的实际能力与通用智能的期望之间存在持续差距。MultiNet v1.0为诊断这些差距和指导未来通用智能体的发展提供了标准化评估基础。

Abstract: Generalist multimodal agents are expected to unify perception, language, and control - operating robustly across diverse real world domains. However, current evaluation practices remain fragmented across isolated benchmarks, making it difficult to assess whether today's foundation models truly generalize beyond their training distributions. We introduce MultiNet v1.0, a unified benchmark for measuring the cross domain generality of vision language models (VLMs) and vision language action models (VLAs) across six foundational capability regimes. Visual grounding, spatial reasoning, tool use, physical commonsense, multi agent coordination, and continuous robot control. Evaluating GPT 5, Pi0, and Magma, we find that no model demonstrates consistent generality. All exhibit substantial degradation on unseen domains, unfamiliar modalities, or cross domain task shifts despite strong performance within their training distributions.These failures manifest as modality misalignment, output format instability, and catastrophic knowledge degradation under domain transfer.Our findings reveal a persistent gap between the aspiration of generalist intelligence and the actual capabilities of current foundation models.MultiNet v1.0 provides a standardized evaluation substrate for diagnosing these gaps and guiding the development of future generalist agents.Code, data, and leaderboards are publicly available.

</details>


### [19] [Contrastive Time Series Forecasting with Anomalies](https://arxiv.org/abs/2512.11526)
*Joel Ekstrand,Zahra Taghiyarrenani,Slawomir Nowaczyk*

Main category: cs.LG

TL;DR: Co-TSFA是一个对比学习框架，通过区分短期异常和持久性分布偏移来改进时间序列预测，在异常条件下提升性能的同时保持正常数据的准确性。


<details>
  <summary>Details</summary>
Motivation: 现实世界的时间序列预测中，有些异常事件具有持久影响需要响应，而有些短期异常应该被忽略。传统预测模型无法区分这两类异常，要么对噪声过度反应，要么错过持久性分布偏移。

Method: 提出Co-TSFA对比学习正则化框架：1）生成仅输入和输入输出两种数据增强来分别建模预测无关和预测相关的异常；2）引入潜在输出对齐损失，将表示变化与预测变化联系起来；3）鼓励对无关扰动保持不变性，同时保持对有意义分布偏移的敏感性。

Result: 在Traffic和Electricity基准数据集以及真实现金需求数据集上的实验表明，Co-TSFA在异常条件下提高了预测性能，同时在正常数据上保持了准确性。

Conclusion: Co-TSFA通过对比学习框架有效区分了短期异常和持久性分布偏移，为时间序列预测中的异常处理提供了新思路，在现实应用中具有实用价值。

Abstract: Time series forecasting predicts future values from past data. In real-world settings, some anomalous events have lasting effects and influence the forecast, while others are short-lived and should be ignored. Standard forecasting models fail to make this distinction, often either overreacting to noise or missing persistent shifts. We propose Co-TSFA (Contrastive Time Series Forecasting with Anomalies), a regularization framework that learns when to ignore anomalies and when to respond. Co-TSFA generates input-only and input-output augmentations to model forecast-irrelevant and forecast-relevant anomalies, and introduces a latent-output alignment loss that ties representation changes to forecast changes. This encourages invariance to irrelevant perturbations while preserving sensitivity to meaningful distributional shifts. Experiments on the Traffic and Electricity benchmarks, as well as on a real-world cash-demand dataset, demonstrate that Co-TSFA improves performance under anomalous conditions while maintaining accuracy on normal data. An anonymized GitHub repository with the implementation of Co-TSFA is provided and will be made public upon acceptance.

</details>


### [20] [Atomic Action Slicing: Planner-Aligned Options for Generalist VLA Agents](https://arxiv.org/abs/2512.11584)
*Stefan Tabakov,Asen Popov,Dimitar Dimitrov,S. Ensiye Kiyamousavi,Vladimir Hristov,Boris Kraychev*

Main category: cs.LG

TL;DR: 该论文提出了原子动作切片(AAS)方法，将长时程演示分解为短时、类型化的原子动作，以提升视觉-语言-动作模型的泛化能力，特别是在需要新技能或对象组合的任务中。


<details>
  <summary>Details</summary>
Motivation: 当前视觉-语言-动作模型泛化能力差，特别是在需要新技能或对象组合的任务中表现不佳。需要一种能够更好地分解复杂任务的方法来提升模型的泛化性能。

Method: 提出了原子动作切片方法，将长时程演示分解为短时、类型化的原子动作。使用LIBERO演示创建了包含2,124个原子片段的验证数据集，标注了动作类型、时间跨度和置信度。使用Gemini 2.5 Pro作为更强的分割器，并与规划器定义的计划对齐。

Result: 在LIBERO-Goal数据集上，任务成功率从94.2%提升到95.3%；在LIBERO-Long数据集上，从83.8%提升到88.8%。更强的分割器(Gemini 2.5 Pro)能更好地匹配规划器定义的计划，并在关键帧抖动下保持鲁棒性。

Conclusion: 原子动作切片方法能有效提升视觉-语言-动作模型的泛化能力，特别是在需要新技能组合的任务中。公开发布了GATE-VLAP数据集，为相关研究提供了有价值的资源。

Abstract: Current vision-language-action (VLA) models generalize poorly, particularly when tasks require new compositions of skills or objects. We introduce Atomic Action Slicing (AAS), a planner-aligned approach that decomposes long-horizon demonstrations into short, typed atomic actions that are easier for planners to use and policies to learn. Using LIBERO demonstrations, AAS produces a validated dataset of 2,124 atomic segments labeled with action type, temporal span, and confidence. A stronger segmenter (Gemini 2.5 Pro) closely matches planner-defined plans and remains robust under keyframe jitter, while smaller models perform worse on multi-object tasks. Fine-tuning CLIP-RT+ on our atomic dataset improves task success from 94.2% to 95.3% on LIBERO-Goal and 83.8% to 88.8% on LIBERO-Long. We publicly release the GATE-VLAP dataset on HuggingFace(https://huggingface.co/datasets/gate-institute/GATE-VLAP-datasets)

</details>


### [21] [Symmetry-Aware Steering of Equivariant Diffusion Policies: Benefits and Limits](https://arxiv.org/abs/2512.11345)
*Minwoo Park,Junwoo Chang,Jongeun Choi,Roberto Horowitz*

Main category: cs.LG

TL;DR: 论文提出了一种基于对称性的扩散策略引导框架，通过理论证明和实验验证，利用对称性提升强化学习在引导等变扩散策略时的样本效率和稳定性。


<details>
  <summary>Details</summary>
Motivation: 等变扩散策略结合了扩散模型的生成能力和几何对称性的泛化优势，但使用标准（非等变）强化学习进行引导时存在样本效率低和不稳定的问题，因为它忽略了EDPs设计时利用的对称性。

Method: 1. 理论证明EDPs的扩散过程具有等变性，从而诱导出适合等变扩散引导的群不变潜在噪声MDP；2. 提出基于对称性的引导框架；3. 通过实验比较标准、等变和近似等变强化学习策略在不同对称性程度任务上的表现。

Result: 1. 识别了严格等变性在对称性破坏下的实际边界；2. 证明在引导过程中利用对称性能显著提升样本效率；3. 防止价值发散；4. 即使在极有限演示数据训练的EDPs上也能实现强大的策略改进。

Conclusion: 利用对称性进行扩散策略引导能有效提升强化学习的样本效率和稳定性，即使在对称性不完全或数据有限的情况下也能实现显著改进，为对称性感知的强化学习提供了理论依据和实践框架。

Abstract: Equivariant diffusion policies (EDPs) combine the generative expressivity of diffusion models with the strong generalization and sample efficiency afforded by geometric symmetries. While steering these policies with reinforcement learning (RL) offers a promising mechanism for fine-tuning beyond demonstration data, directly applying standard (non-equivariant) RL can be sample-inefficient and unstable, as it ignores the symmetries that EDPs are designed to exploit. In this paper, we theoretically establish that the diffusion process of an EDP is equivariant, which in turn induces a group-invariant latent-noise MDP that is well-suited for equivariant diffusion steering. Building on this theory, we introduce a principled symmetry-aware steering framework and compare standard, equivariant, and approximately equivariant RL strategies through comprehensive experiments across tasks with varying degrees of symmetry. While we identify the practical boundaries of strict equivariance under symmetry breaking, we show that exploiting symmetry during the steering process yields substantial benefits-enhancing sample efficiency, preventing value divergence, and achieving strong policy improvements even when EDPs are trained from extremely limited demonstrations.

</details>


### [22] [Mitigating the Safety Alignment Tax with Null-Space Constrained Policy Optimization](https://arxiv.org/abs/2512.11391)
*Yifan Niu,Han Xiao,Dongyi Liu,Nuo Chen,Jia Li*

Main category: cs.LG

TL;DR: NSPO是一种新颖的RL框架，通过将安全策略梯度投影到通用任务零空间来解决LLM安全对齐中的能力遗忘问题，在保持核心能力的同时实现有效安全对齐。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在现实应用中需要确保其行为符合人类价值观、社会规范和伦理原则，但现有的强化学习安全对齐方法往往会导致模型遗忘已学习的通用能力（对齐税问题）。

Method: 提出零空间约束策略优化（NSPO）框架，将安全策略梯度几何投影到通用任务的零空间中，从而减轻安全对齐税。理论证明该方法能保持模型原始核心能力，同时保证安全对齐的有效下降方向。

Result: NSPO在实验中大幅优于现有方法，在数学、代码和指令跟随等通用任务上保持准确性的同时，实现了最先进的安全性能。仅需PKU-SafeRLHF中40%的公开人工标注安全数据即可获得良好安全性能，无需现有对齐方法所需的大量混合通用任务数据。

Conclusion: NSPO是一种数据高效的安全对齐方法，能有效解决LLM安全对齐中的能力遗忘问题，在保持核心能力的同时实现高质量的安全对齐。

Abstract: As Large Language Models (LLMs) are increasingly deployed in real-world applications, it is important to ensure their behaviors align with human values, societal norms, and ethical principles. However, safety alignment under Reinforcement Learning (RL) often suffers from forgetting learned general abilities, which is also known as the alignment tax. To address this issue, we introduce Null-Space constrained Policy Optimization (NSPO), a novel RL framework for LLM safety alignment while preserving their core abilities. The safety policy gradients are geometrically projected into the null space of general tasks, thereby mitigating the safety alignment tax. In addition, we theoretically prove that NSPO preserves the model's original core capabilities, while still guaranteeing a descent direction for effective safety alignment. Extensive experiments demonstrate that NSPO outperforms existing methods by a large margin, achieving state-of-the-art safety performance without sacrificing accuracy on general tasks, including math, code, and instruction-following tasks. Notably, NSPO is data-efficient and only requires 40% of public human-annotated safety data from PKU-SafeRLHF to achieve promising safety performance, without a large amount of mixed general tasks data in existing alignment methods.

</details>


### [23] [Sliced ReLU attention: Quasi-linear contextual expressivity via sorting](https://arxiv.org/abs/2512.11411)
*Siwan Boufadène,François-Xavier Vialard*

Main category: cs.LG

TL;DR: 提出切片ReLU注意力机制，通过一维投影和排序实现O(n log n)复杂度，适合长上下文处理，同时保持理论表达能力。


<details>
  <summary>Details</summary>
Motivation: 现有注意力机制（如softmax和ReLU-based）在计算复杂度或表达能力方面存在限制，需要一种既能处理长上下文又保持理论表达能力的新注意力机制。

Method: 提出切片ReLU注意力机制，不直接对点积对应用非线性，而是对键-查询差异的一维投影进行操作，利用排序获得准线性复杂度，形成可微分、非对称的核。

Result: 该机制计算复杂度为O(n log n)，适合长上下文处理；理论证明其具有序列到序列解缠任务能力和上下文通用逼近性质；小规模实验展示了实际应用潜力。

Conclusion: 切片ReLU注意力机制在计算效率和理论表达能力之间取得了良好平衡，为长上下文处理提供了有前景的替代方案。

Abstract: We introduce sliced ReLU attention, a new attention mechanism that departs structurally from both softmax and ReLU-based alternatives. Instead of applying a nonlinearity to pairwise dot products, we operate on one-dimensional projections of key--query differences and leverage sorting to obtain quasi-linear complexity. This construction yields a differentiable, non-symmetric kernel that can be computed in O(n log(n)) through a sorting procedure, making it suitable for very long contexts. Beyond computational benefits, the model retains strong theoretical expressive power: we establish two in-context expressivity results, previously known for softmax attention, showing that sliced ReLU attention preserves the ability to perform nontrivial sequence-to-sequence disentangling tasks and satisfies a contextual universal approximation property. Finally, we illustrate the potential practical interest of this kernel in small-scale experiments.

</details>


### [24] [Hyperbolic Gaussian Blurring Mean Shift: A Statistical Mode-Seeking Framework for Clustering in Curved Spaces](https://arxiv.org/abs/2512.11448)
*Arghya Pratihar,Arnab Seal,Swagatam Das,Inesh Chattopadhyay*

Main category: cs.LG

TL;DR: HypeGBMS是高斯模糊均值漂移在双曲空间的扩展，用于处理具有层次结构的数据集，在非欧几里得设置中显著优于传统均值漂移方法。


<details>
  <summary>Details</summary>
Motivation: 传统高斯模糊均值漂移在欧几里得空间中有效识别任意形状的簇，但对于具有层次或树状结构的数据集表现不佳。需要将均值漂移扩展到双曲空间以更好地捕捉数据的潜在层次结构。

Method: HypeGBMS将欧几里得计算替换为双曲距离，并采用Möbius加权均值，确保所有更新与空间几何保持一致。该方法保留了GBMS的密度寻求行为，同时适应双曲空间的曲率。

Result: 在11个真实世界数据集上的实验评估表明，HypeGBMS在非欧几里得设置中显著优于传统均值漂移聚类方法，证明了其鲁棒性和有效性。理论分析提供了收敛性和计算复杂度的见解。

Conclusion: HypeGBMS将经典均值漂移聚类与双曲表示学习相结合，为弯曲空间中的基于密度的聚类提供了原则性方法，能够有效捕捉数据的潜在层次结构。

Abstract: Clustering is a fundamental unsupervised learning task for uncovering patterns in data. While Gaussian Blurring Mean Shift (GBMS) has proven effective for identifying arbitrarily shaped clusters in Euclidean space, it struggles with datasets exhibiting hierarchical or tree-like structures. In this work, we introduce HypeGBMS, a novel extension of GBMS to hyperbolic space. Our method replaces Euclidean computations with hyperbolic distances and employs Möbius-weighted means to ensure that all updates remain consistent with the geometry of the space. HypeGBMS effectively captures latent hierarchies while retaining the density-seeking behavior of GBMS. We provide theoretical insights into convergence and computational complexity, along with empirical results that demonstrate improved clustering quality in hierarchical datasets. This work bridges classical mean-shift clustering and hyperbolic representation learning, offering a principled approach to density-based clustering in curved spaces. Extensive experimental evaluations on $11$ real-world datasets demonstrate that HypeGBMS significantly outperforms conventional mean-shift clustering methods in non-Euclidean settings, underscoring its robustness and effectiveness.

</details>


### [25] [Rethinking Expert Trajectory Utilization in LLM Post-training](https://arxiv.org/abs/2512.11470)
*Bowen Ding,Yuhan Chen,Jiayang Lv,Jiyao Yuan,Qi Zhu,Shuangshuang Tian,Dantong Zhu,Futing Wang,Heyuan Deng,Fei Mi,Lifeng Shang,Tao Lin*

Main category: cs.LG

TL;DR: 本文提出塑性-天花板框架，理论分析专家轨迹在监督微调(SFT)和强化学习(RL)后训练中的最优使用机制，确立SFT-then-RL为最佳流程，并提供具体扩展指南。


<details>
  <summary>Details</summary>
Motivation: 当前有效的后训练通常结合监督微调(SFT)和强化学习(RL)，但如何最优利用专家轨迹的问题尚未解决。本文旨在理论分析这一领域，为最大化专家轨迹价值提供指导。

Method: 提出塑性-天花板框架，将性能分解为基础SFT性能和后续RL塑性。通过广泛基准测试，比较不同训练流程，并推导精确的扩展指南。

Result: 1. 确立SFT-then-RL顺序流程为最优标准；2. 在SFT稳定或轻度过拟合阶段转向RL可最大化最终性能；3. 数据规模决定主要后训练潜力，轨迹难度作为性能乘数；4. 最小SFT验证损失可作为选择专家轨迹的可靠指标。

Conclusion: 研究提供了最大化专家轨迹价值的可操作指南，包括训练流程选择、转向RL的最佳时机、数据规模和轨迹难度的作用，以及专家轨迹选择指标。

Abstract: While effective post-training integrates Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), the optimal mechanism for utilizing expert trajectories remains unresolved. We propose the Plasticity-Ceiling Framework to theoretically ground this landscape, decomposing performance into foundational SFT performance and the subsequent RL plasticity. Through extensive benchmarking, we establish the Sequential SFT-then-RL pipeline as the superior standard, overcoming the stability deficits of synchronized approaches. Furthermore, we derive precise scaling guidelines: (1) Transitioning to RL at the SFT Stable or Mild Overfitting Sub-phase maximizes the final ceiling by securing foundational SFT performance without compromising RL plasticity; (2) Refuting ``Less is More'' in the context of SFT-then-RL scaling, we demonstrate that Data Scale determines the primary post-training potential, while Trajectory Difficulty acts as a performance multiplier; and (3) Identifying that the Minimum SFT Validation Loss serves as a robust indicator for selecting the expert trajectories that maximize the final performance ceiling. Our findings provide actionable guidelines for maximizing the value extracted from expert trajectories.

</details>


### [26] [Parametric Numerical Integration with (Differential) Machine Learning](https://arxiv.org/abs/2512.11530)
*Álvaro Leitao,Jonatan Ráfales*

Main category: cs.LG

TL;DR: 提出一种结合导数信息的微分机器学习框架，用于求解参数化积分问题，在统计泛函、切比雪夫展开和微分方程积分三类问题上均优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习方法在求解参数化积分问题时存在精度和效率限制，需要一种能够有效利用导数信息来提高性能的新方法。

Method: 提出微分机器学习框架，在训练过程中融入导数信息，应用于三类代表性积分问题：统计泛函（矩和累积分布函数）、切比雪夫展开函数逼近、以及微分方程直接产生的积分。

Result: 微分机器学习方法在所有测试案例中均优于标准架构，实现了更低的均方误差、更好的可扩展性和更高的样本效率。

Conclusion: 微分机器学习框架为参数化积分求解提供了一种有效方法，通过利用导数信息显著提升了性能，适用于从光滑闭式基准到挑战性数值积分的广泛问题。

Abstract: In this work, we introduce a machine/deep learning methodology to solve parametric integrals. Besides classical machine learning approaches, we consider a differential learning framework that incorporates derivative information during training, emphasizing its advantageous properties. Our study covers three representative problem classes: statistical functionals (including moments and cumulative distribution functions), approximation of functions via Chebyshev expansions, and integrals arising directly from differential equations. These examples range from smooth closed-form benchmarks to challenging numerical integrals. Across all cases, the differential machine learning-based approach consistently outperforms standard architectures, achieving lower mean squared error, enhanced scalability, and improved sample efficiency.

</details>


### [27] [Fully Inductive Node Representation Learning via Graph View Transformation](https://arxiv.org/abs/2512.11561)
*Dooho Lee,Myeong Kong,Minho Jeong,Jaemin Yoo*

Main category: cs.LG

TL;DR: 论文提出了一种名为"视图空间"的新表示轴，通过Graph View Transformation (GVT)实现完全归纳的图节点表示学习，在27个基准测试中显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 预训练模型在未见数据集上的泛化是基础模型的关键，但在图结构化数据中，由于特征空间在维度和语义上的巨大差异，实现跨数据集的完全归纳推理非常困难。特征空间的任何变换都可能破坏对未见数据集的归纳适用性，严格限制了图模型的设计空间。

Method: 提出了视图空间作为新的表示轴，任意图都可以在这个空间中统一编码。然后提出了Graph View Transformation (GVT)，这是一种在视图空间中节点和特征置换等变的映射。GVT作为构建块，形成了Recurrent GVT，一个用于节点表示学习的完全归纳模型。

Result: 在OGBN-Arxiv上预训练并在27个节点分类基准上评估，Recurrent GVT比之前的完全归纳图模型GraphAny提升了+8.93%，并且比12个单独调优的GNN至少提升了+3.30%。

Conclusion: 视图空间为完全归纳节点表示学习提供了一个原则性和有效的基础，证明了该方法在图模型泛化方面的优越性。

Abstract: Generalizing a pretrained model to unseen datasets without retraining is an essential step toward a foundation model. However, achieving such cross-dataset, fully inductive inference is difficult in graph-structured data where feature spaces vary widely in both dimensionality and semantics. Any transformation in the feature space can easily violate the inductive applicability to unseen datasets, strictly limiting the design space of a graph model. In this work, we introduce the view space, a novel representational axis in which arbitrary graphs can be naturally encoded in a unified manner. We then propose Graph View Transformation (GVT), a node- and feature-permutation-equivariant mapping in the view space. GVT serves as the building block for Recurrent GVT, a fully inductive model for node representation learning. Pretrained on OGBN-Arxiv and evaluated on 27 node-classification benchmarks, Recurrent GVT outperforms GraphAny, the prior fully inductive graph model, by +8.93% and surpasses 12 individually tuned GNNs by at least +3.30%. These results establish the view space as a principled and effective ground for fully inductive node representation learning.

</details>


### [28] [A Fast Interpretable Fuzzy Tree Learner](https://arxiv.org/abs/2512.11616)
*Javier Fumanal-Idocin,Raquel Fernandez-Peralta,Javier Andreu-Perez*

Main category: cs.LG

TL;DR: 提出一种将经典树分裂算法从清晰规则扩展到模糊树的适应方法，结合贪婪算法的计算效率和模糊逻辑的可解释性优势，在保持竞争力的预测性能的同时显著降低计算成本。


<details>
  <summary>Details</summary>
Motivation: 模糊规则系统因其可解释性而被广泛用于决策制定，但现有模糊规则挖掘算法不能保证合理的语言划分和小规则库规模。进化方法计算成本过高，而神经方法如ANFIS难以保持语言解释性。

Method: 将经典的基于树的分裂算法从清晰规则扩展到模糊树，结合贪婪算法的计算效率和模糊逻辑的可解释性优势，实现可解释的语言划分。

Result: 在表格分类基准测试中，该方法达到了与最先进模糊分类器相当的准确率，同时显著降低了计算成本，并产生具有约束复杂度的更可解释规则库。

Conclusion: 该方法成功地将经典树分裂算法适应到模糊树中，在保持竞争力的预测性能的同时，实现了计算效率的大幅提升和更好的可解释性。

Abstract: Fuzzy rule-based systems have been mostly used in interpretable decision-making because of their interpretable linguistic rules. However, interpretability requires both sensible linguistic partitions and small rule-base sizes, which are not guaranteed by many existing fuzzy rule-mining algorithms. Evolutionary approaches can produce high-quality models but suffer from prohibitive computational costs, while neural-based methods like ANFIS have problems retaining linguistic interpretations. In this work, we propose an adaptation of classical tree-based splitting algorithms from crisp rules to fuzzy trees, combining the computational efficiency of greedy algoritms with the interpretability advantages of fuzzy logic. This approach achieves interpretable linguistic partitions and substantially improves running time compared to evolutionary-based approaches while maintaining competitive predictive performance. Our experiments on tabular classification benchmarks proof that our method achieves comparable accuracy to state-of-the-art fuzzy classifiers with significantly lower computational cost and produces more interpretable rule bases with constrained complexity. Code is available in: https://github.com/Fuminides/fuzzy_greedy_tree_public

</details>


### [29] [Bridging Streaming Continual Learning via In-Context Large Tabular Models](https://arxiv.org/abs/2512.11668)
*Afonso Lourenço,João Gama,Eric P. Xing,Goreti Marreiros*

Main category: cs.LG

TL;DR: 论文提出使用大型上下文表格模型作为流式持续学习的桥梁，通过将无界数据流压缩为紧凑摘要，同时满足流学习的数据压缩需求和持续学习的经验回放需求。


<details>
  <summary>Details</summary>
Motivation: 当前持续学习和流学习研究社区各自独立，缺乏统一框架。持续学习关注长期记忆但缺乏实时约束，流学习强调快速适应但忽视遗忘问题。需要一种方法同时处理概念漂移和灾难性遗忘。

Method: 提出使用大型上下文表格模型作为流式持续学习的桥梁，将无界数据流实时压缩为紧凑摘要。采用分而治之策略平衡可塑性（适应当前分布）和稳定性（保留过去知识），通过分布匹配和分布压缩两个核心原则进行数据选择。

Result: 论文提出了一个理论框架，将流学习和持续学习统一在流式持续学习范式下，通过大型上下文表格模型实现数据流的实时压缩和知识保留。

Conclusion: 大型上下文表格模型为流式持续学习提供了自然桥梁，通过分布匹配和分布压缩原则，能够同时满足流学习的数据压缩需求和持续学习的经验回放需求，有效平衡可塑性、稳定性、多样性和检索需求。

Abstract: In streaming scenarios, models must learn continuously, adapting to concept drifts without erasing previously acquired knowledge. However, existing research communities address these challenges in isolation. Continual Learning (CL) focuses on long-term retention and mitigating catastrophic forgetting, often without strict real-time constraints. Stream Learning (SL) emphasizes rapid, efficient adaptation to high-frequency data streams, but typically neglects forgetting. Recent efforts have tried to combine these paradigms, yet no clear algorithmic overlap exists. We argue that large in-context tabular models (LTMs) provide a natural bridge for Streaming Continual Learning (SCL). In our view, unbounded streams should be summarized on-the-fly into compact sketches that can be consumed by LTMs. This recovers the classical SL motivation of compressing massive streams with fixed-size guarantees, while simultaneously aligning with the experience-replay desiderata of CL. To clarify this bridge, we show how the SL and CL communities implicitly adopt a divide-to-conquer strategy to manage the tension between plasticity (performing well on the current distribution) and stability (retaining past knowledge), while also imposing a minimal complexity constraint that motivates diversification (avoiding redundancy in what is stored) and retrieval (re-prioritizing past information when needed). Within this perspective, we propose structuring SCL with LTMs around two core principles of data selection for in-context learning: (1) distribution matching, which balances plasticity and stability, and (2) distribution compression, which controls memory size through diversification and retrieval mechanisms.

</details>


### [30] [SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning](https://arxiv.org/abs/2512.11760)
*Aditya Tripathi,Karan Sharma,Rahul Mishra,Tapas Kumar Maiti*

Main category: cs.LG

TL;DR: SpectralKrum是一种新的联邦学习防御方法，通过谱子空间估计和几何邻居选择相结合来对抗拜占庭攻击，在非IID数据分布下表现优于传统方法，但对某些攻击类型效果有限。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在客户端数据分布异构（非IID）且攻击者能够观察或近似防御机制的情况下，现有的鲁棒聚合方法（如Krum、Bulyan等）的有效性会大幅降低，需要新的防御机制来应对这些挑战。

Method: SpectralKrum融合了谱子空间估计和基于几何邻居的选择方法。核心思想是：尽管存在客户端异构性，良性优化轨迹会集中在可以通过历史聚合估计的低维流形附近。该方法将传入的更新投影到学习到的子空间中，在压缩坐标中应用Krum选择，并过滤那些正交残差能量超过数据驱动阈值的候选更新。

Result: 在CIFAR-10数据集上使用Dirichlet分布的非IID分区（alpha=0.1）对8个鲁棒基线方法进行了评估，覆盖7种攻击场景和超过56,000个训练轮次。SpectralKrum在方向性和子空间感知攻击（adaptive-steer、buffer-drift）方面具有竞争力，但在标签翻转和最小-最大攻击下优势有限，因为恶意更新在谱域上与良性更新难以区分。

Conclusion: SpectralKrum提供了一种无需辅助数据、完全在模型更新上操作且保持联邦学习隐私特性的防御方法，在非IID数据分布下对某些攻击类型有效，但对恶意更新在谱域上与良性更新难以区分的攻击类型效果有限。

Abstract: Federated Learning (FL) distributes model training across clients who retain their data locally, but this architecture exposes a fundamental vulnerability: Byzantine clients can inject arbitrarily corrupted updates that degrade or subvert the global model. While robust aggregation methods (including Krum, Bulyan, and coordinate-wise defenses) offer theoretical guarantees under idealized assumptions, their effectiveness erodes substantially when client data distributions are heterogeneous (non-IID) and adversaries can observe or approximate the defense mechanism.
  This paper introduces SpectralKrum, a defense that fuses spectral subspace estimation with geometric neighbor-based selection. The core insight is that benign optimization trajectories, despite per-client heterogeneity, concentrate near a low-dimensional manifold that can be estimated from historical aggregates. SpectralKrum projects incoming updates into this learned subspace, applies Krum selection in compressed coordinates, and filters candidates whose orthogonal residual energy exceeds a data-driven threshold. The method requires no auxiliary data, operates entirely on model updates, and preserves FL privacy properties.
  We evaluate SpectralKrum against eight robust baselines across seven attack scenarios on CIFAR-10 with Dirichlet-distributed non-IID partitions (alpha = 0.1). Experiments spanning over 56,000 training rounds show that SpectralKrum is competitive against directional and subspace-aware attacks (adaptive-steer, buffer-drift), but offers limited advantage under label-flip and min-max attacks where malicious updates remain spectrally indistinguishable from benign ones.

</details>


### [31] [Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective](https://arxiv.org/abs/2512.11784)
*Etienne Boursier,Claire Boyer*

Main category: cs.LG

TL;DR: 该论文建立了一个统一的测度框架分析softmax注意力，证明在长提示下softmax注意力收敛到线性算子，从而可以将线性注意力的优化分析直接迁移到softmax注意力。


<details>
  <summary>Details</summary>
Motivation: softmax注意力是transformer架构的核心组件，但其非线性结构给理论分析带来重大挑战。需要开发一个统一的框架来研究softmax注意力在有限和无限提示下的行为。

Method: 建立基于测度的统一框架，利用softmax算子在无限提示极限下收敛到作用于底层输入标记测度的线性算子这一事实。对于i.i.d.高斯输入，建立输出和梯度的非渐近集中界，量化有限提示模型接近无限提示对应物的速度。

Result: 证明了softmax注意力在足够长的提示下继承其线性对应物的分析结构，浓度界在整个训练轨迹上保持稳定，在线性回归的上下文学习中可以使用可处理的无限提示动力学分析有限提示长度的训练。

Conclusion: 当提示足够长时，为线性注意力开发的优化分析可以直接迁移到softmax注意力，这为研究大提示机制下softmax注意力层的训练动态和统计行为提供了原则性和广泛适用的工具包。

Abstract: Softmax attention is a central component of transformer architectures, yet its nonlinear structure poses significant challenges for theoretical analysis. We develop a unified, measure-based framework for studying single-layer softmax attention under both finite and infinite prompts. For i.i.d. Gaussian inputs, we lean on the fact that the softmax operator converges in the infinite-prompt limit to a linear operator acting on the underlying input-token measure. Building on this insight, we establish non-asymptotic concentration bounds for the output and gradient of softmax attention, quantifying how rapidly the finite-prompt model approaches its infinite-prompt counterpart, and prove that this concentration remains stable along the entire training trajectory in general in-context learning settings with sub-Gaussian tokens. In the case of in-context linear regression, we use the tractable infinite-prompt dynamics to analyze training at finite prompt length. Our results allow optimization analyses developed for linear attention to transfer directly to softmax attention when prompts are sufficiently long, showing that large-prompt softmax attention inherits the analytical structure of its linear counterpart. This, in turn, provides a principled and broadly applicable toolkit for studying the training dynamics and statistical behavior of softmax attention layers in large prompt regimes.

</details>


### [32] [A General Algorithm for Detecting Higher-Order Interactions via Random Sequential Additions](https://arxiv.org/abs/2512.11793)
*Ahmad Shamail,Claire McWhite*

Main category: cs.LG

TL;DR: 提出一种基于随机顺序添加元素并绘制贡献图的几何方法，通过L形模式发现元素间的相互作用、独立性和冗余性


<details>
  <summary>Details</summary>
Motivation: 许多系统组件之间存在复杂的相互作用：有些特征或行为相互增强，有些提供冗余信息，有些则独立贡献。需要一种统一的方法来发现和量化这些交互结构。

Method: 通过随机顺序依次添加元素，在多次试验中绘制每个元素的贡献图。观察特征性的L形模式：冗余对形成L形（只有先添加的元素贡献），协同对形成反L形（只有元素共同作用时贡献），独立元素显示顺序不变分布。引入L分数作为连续度量。

Result: 该方法能够量化每个元素对先前添加元素的依赖程度，区分相互作用、独立性和冗余性。L分数范围从-1（完美协同）到0（独立）到+1（完美冗余）。通过两两测量自然涌现出高阶相互作用。

Conclusion: 提出了一种度量无关的几何方法，适用于任何可以按非重复元素序列增量评估性能的领域，为揭示交互结构提供了统一的几何框架。

Abstract: Many systems exhibit complex interactions between their components: some features or actions amplify each other's effects, others provide redundant information, and some contribute independently. We present a simple geometric method for discovering interactions and redundancies: when elements are added in random sequential orders and their contributions plotted over many trials, characteristic L-shaped patterns emerge that directly reflect interaction structure. The approach quantifies how the contribution of each element depends on those added before it, revealing patterns that distinguish interaction, independence, and redundancy on a unified scale. When pairwise contributions are visualized as two--dimensional point clouds, redundant pairs form L--shaped patterns where only the first-added element contributes, while synergistic pairs form L--shaped patterns where only elements contribute together. Independent elements show order--invariant distributions. We formalize this with the L--score, a continuous measure ranging from $-1$ (perfect synergy, e.g. $Y=X_1X_2$) to $0$ (independence) to $+1$ (perfect redundancy, $X_1 \approx X_2$). The relative scaling of the L--shaped arms reveals feature dominance in which element consistently provides more information. Although computed only from pairwise measurements, higher--order interactions among three or more elements emerge naturally through consistent cross--pair relationships (e.g. AB, AC, BC). The method is metric--agnostic and broadly applicable to any domain where performance can be evaluated incrementally over non-repeating element sequences, providing a unified geometric approach to uncovering interaction structure.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [33] [CORL: Reinforcement Learning of MILP Policies Solved via Branch and Bound](https://arxiv.org/abs/2512.11169)
*Akhil S Anand,Elias Aarekol,Martin Mziray Dalseg,Magnus Stalhane,Sebastien Gros*

Main category: cs.AI

TL;DR: CORL框架使用强化学习端到端微调MILP方案，将分支定界求解的MILP建模为可微分的随机策略，以最大化实际操作性能而非精确建模现实问题。


<details>
  <summary>Details</summary>
Motivation: 传统MILP建模难以准确表示随机现实问题，导致实际性能不佳。现有机器学习方法依赖监督学习、假设可获得最优决策真值，并使用MILP梯度的替代方法，存在局限性。

Method: 提出CORL概念验证框架，将分支定界算法求解的MILP建模为可微分的随机策略，使其与强化学习兼容，使用现实数据端到端微调MILP方案以最大化操作性能。

Result: 在简单的组合顺序决策示例中验证了CORL方法的有效性，展示了该框架的可行性。

Conclusion: CORL框架通过强化学习直接优化MILP在实际操作中的性能，避免了传统方法对精确建模的依赖和监督学习的限制，为组合顺序决策问题提供了新的解决方案。

Abstract: Combinatorial sequential decision making problems are typically modeled as mixed integer linear programs (MILPs) and solved via branch and bound (B&B) algorithms. The inherent difficulty of modeling MILPs that accurately represent stochastic real world problems leads to suboptimal performance in the real world. Recently, machine learning methods have been applied to build MILP models for decision quality rather than how accurately they model the real world problem. However, these approaches typically rely on supervised learning, assume access to true optimal decisions, and use surrogates for the MILP gradients. In this work, we introduce a proof of concept CORL framework that end to end fine tunes an MILP scheme using reinforcement learning (RL) on real world data to maximize its operational performance. We enable this by casting an MILP solved by B&B as a differentiable stochastic policy compatible with RL. We validate the CORL method in a simple illustrative combinatorial sequential decision making example.

</details>


### [34] [FutureWeaver: Planning Test-Time Compute for Multi-Agent Systems with Modularized Collaboration](https://arxiv.org/abs/2512.11213)
*Dongwon Jung,Peng Shi,Yi Zhang*

Main category: cs.AI

TL;DR: FutureWeaver：一个在固定预算下规划和优化多智能体系统中测试时计算分配的框架，通过模块化协作和双级规划架构提升多智能体协作性能


<details>
  <summary>Details</summary>
Motivation: 现有测试时计算扩展技术（如重复采样、自我验证、自我反思）能提升单智能体性能，但难以应用于多智能体系统。缺乏原则性机制来分配计算以促进智能体间协作，将测试时计算扩展到协作交互，或在明确预算约束下跨智能体分配计算。

Method: 提出FutureWeaver框架：1）引入模块化协作，将可复用的多智能体工作流封装为可调用函数；2）通过自我反思从过往轨迹中抽象出重复交互模式自动推导模块；3）采用双级规划架构，在推理当前任务状态的同时推测未来步骤，优化计算分配。

Result: 在复杂智能体基准测试上的实验表明，FutureWeaver在不同预算设置下始终优于基线方法，验证了其在推理时优化中多智能体协作的有效性。

Conclusion: FutureWeaver填补了多智能体系统中测试时计算分配的空白，通过模块化协作和前瞻性规划，在固定预算约束下显著提升了多智能体协作性能，为推理时优化提供了新范式。

Abstract: Scaling test-time computation improves large language model performance without additional training. Recent work demonstrates that techniques such as repeated sampling, self-verification, and self-reflection can significantly enhance task success by allocating more inference-time compute. However, applying these techniques across multiple agents in a multi-agent system is difficult: there does not exist principled mechanisms to allocate compute to foster collaboration among agents, to extend test-time scaling to collaborative interactions, or to distribute compute across agents under explicit budget constraints. To address this gap, we propose FutureWeaver, a framework for planning and optimizing test-time compute allocation in multi-agent systems under fixed budgets. FutureWeaver introduces modularized collaboration, formalized as callable functions that encapsulate reusable multi-agent workflows. These modules are automatically derived through self-play reflection by abstracting recurring interaction patterns from past trajectories. Building on these modules, FutureWeaver employs a dual-level planning architecture that optimizes compute allocation by reasoning over the current task state while also speculating on future steps. Experiments on complex agent benchmarks demonstrate that FutureWeaver consistently outperforms baselines across diverse budget settings, validating its effectiveness for multi-agent collaboration in inference-time optimization.

</details>


### [35] [TriFlow: A Progressive Multi-Agent Framework for Intelligent Trip Planning](https://arxiv.org/abs/2512.11271)
*Yuxing Chen,Basem Suleiman,Qifan Chen*

Main category: cs.AI

TL;DR: TriFlow是一个渐进式多智能体框架，通过检索、规划和治理三阶段流水线，将结构化推理与语言灵活性相结合，解决现实世界行程规划中的约束满足问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界的行程规划需要将开放式用户请求转化为可执行的行程，同时满足严格的空间、时间和预算约束，并与用户偏好保持一致。现有的基于LLM的智能体在约束满足、工具协调和效率方面存在困难，经常产生不可行或成本过高的计划。

Method: TriFlow采用渐进式多智能体框架，通过三阶段流水线：1) 检索阶段缩小搜索空间；2) 规划阶段通过规则-LLM协作组装约束一致的行程；3) 治理阶段执行有界迭代细化以确保全局可行性和个性化。

Result: 在TravelPlanner和TripTailor基准测试中取得了最先进的结果，分别达到91.1%和97%的最终通过率，与当前SOTA相比实现了超过10倍的运行时效率提升。

Conclusion: TriFlow通过统一结构化推理和语言灵活性，有效解决了行程规划中的约束满足问题，显著提高了规划质量和效率。

Abstract: Real-world trip planning requires transforming open-ended user requests into executable itineraries under strict spatial, temporal, and budgetary constraints while aligning with user preferences. Existing LLM-based agents struggle with constraint satisfaction, tool coordination, and efficiency, often producing infeasible or costly plans. To address these limitations, we present TriFlow, a progressive multi-agent framework that unifies structured reasoning and language-based flexibility through a three-stage pipeline of retrieval, planning, and governance. By this design, TriFlow progressively narrows the search space, assembles constraint-consistent itineraries via rule-LLM collaboration, and performs bounded iterative refinement to ensure global feasibility and personalisation. Evaluations on TravelPlanner and TripTailor benchmarks demonstrated state-of-the-art results, achieving 91.1% and 97% final pass rates, respectively, with over 10x runtime efficiency improvement compared to current SOTA.

</details>


### [36] [CAPTURE: A Benchmark and Evaluation for LVLMs in CAPTCHA Resolving](https://arxiv.org/abs/2512.11323)
*Jianyi Zhang,Ziyin Zhou,Xu Ji,Shizhao Liu,Zhangchi Zhao*

Main category: cs.AI

TL;DR: 本文首次为大型视觉语言模型（LVLMs）引入了一个名为CAPTURE的CAPTCHA基准测试，涵盖4种主要类型和25种子类型，来自31个供应商，用于全面评估LVLMs解决验证码的能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉验证码的基准测试存在局限性，无法全面覆盖所有验证码类型，且缺乏专门针对LVLMs的基准测试。为了解决这个问题，需要创建一个专门为LVLMs设计的全面验证码基准。

Method: 创建了CAPTURE（CAPTCHA for Testing Under Real-world Experiments）基准，包含4种主要验证码类型和25种子类型，来自31个不同供应商。该基准具有广泛的类别多样性、大规模数据和专门为LVLMs定制的标签。

Result: 使用该基准测试评估当前LVLMs时，发现它们在解决验证码方面表现不佳，表明现有模型在验证码识别能力上仍有很大提升空间。

Conclusion: CAPTURE基准填补了先前研究在数据全面性和标签针对性方面的空白，为LVLMs提供了一个多维度、全面的验证码性能评估工具，有助于推动LVLMs在验证码解决能力方面的研究和发展。

Abstract: Benefiting from strong and efficient multi-modal alignment strategies, Large Visual Language Models (LVLMs) are able to simulate human visual and reasoning capabilities, such as solving CAPTCHAs. However, existing benchmarks based on visual CAPTCHAs still face limitations. Previous studies, when designing benchmarks and datasets, customized them according to their research objectives. Consequently, these benchmarks cannot comprehensively cover all CAPTCHA types. Notably, there is a dearth of dedicated benchmarks for LVLMs. To address this problem, we introduce a novel CAPTCHA benchmark for the first time, named CAPTURE CAPTCHA for Testing Under Real-world Experiments, specifically for LVLMs. Our benchmark encompasses 4 main CAPTCHA types and 25 sub-types from 31 vendors. The diversity enables a multi-dimensional and thorough evaluation of LVLM performance. CAPTURE features extensive class variety, large-scale data, and unique LVLM-tailored labels, filling the gaps in previous research in terms of data comprehensiveness and labeling pertinence. When evaluated by this benchmark, current LVLMs demonstrate poor performance in solving CAPTCHAs.

</details>


### [37] [Towards Trustworthy Multi-Turn LLM Agents via Behavioral Guidance](https://arxiv.org/abs/2512.11421)
*Gonca Gürsun*

Main category: cs.AI

TL;DR: 提出了一个任务完成框架，使基于LLM的智能体能在强化学习形式化环境中遵循明确的行为指导，通过任务分析器、推理模块和生成模块的协同进化实现可信行为。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在推理和生成方面表现出强大能力，但在多轮任务中的行为往往缺乏可靠性和可验证性。需要一种框架让LLM智能体在具有明确定义的环境中以可验证的方式行动。

Method: 框架包含三个组件：1) 轻量级任务分析器选择推理和生成策略；2) 推理模块学习可验证的观察-动作映射；3) 生成模块通过验证或确定性合成确保约束合规的输出。这些组件在智能体与环境交互过程中协同进化。

Result: 该框架使LLM智能体能够在强化学习形式化环境中产生可信赖的行为，通过组件协同进化提高任务完成的可靠性和可验证性。

Conclusion: 提出的任务完成框架通过整合任务分析、可验证推理和约束合规生成，实现了LLM智能体在结构化环境中的可信行为，为解决多轮任务中的可靠性问题提供了系统化方案。

Abstract: Large Language Models demonstrate strong reasoning and generation abilities, yet their behavior in multi-turn tasks often lacks reliability and verifiability. We present a task completion framework that enables LLM-based agents to act under explicit behavioral guidance in environments described by reinforcement learning formalisms with defined observation, action, and reward signals.
  The framework integrates three components: a lightweight task profiler that selects reasoning and generation strategies, a reasoning module that learns verifiable observation - action mappings, and a generation module that enforces constraint-compliant outputs through validation or deterministic synthesis. We show that as the agent interacts with the environment, these components co-evolve, yielding trustworthy behavior.

</details>


### [38] [AgentBalance: Backbone-then-Topology Design for Cost-Effective Multi-Agent Systems under Budget Constraints](https://arxiv.org/abs/2512.11426)
*Shuowei Cai,Yansong Ning,Hao Liu*

Main category: cs.AI

TL;DR: AgentBalance是一个在明确token成本和延迟预算下构建成本效益多智能体系统的框架，采用"先骨干后拓扑"设计，相比现有方法在相同预算下性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的多智能体系统研究很少在明确的token成本和延迟预算下进行建模和优化，导致在预算约束下采用"拓扑优先"设计，成本效益不理想。实际部署中成本效益是规模化部署的主要约束。

Method: 采用"先骨干后拓扑"设计：1)骨干导向的智能体生成：通过LLM池构建、池选择和角色-骨干匹配构建异构骨干智能体；2)自适应MAS拓扑生成：通过智能体表示学习、门控和延迟感知拓扑合成指导智能体间通信。

Result: 在包含14个候选LLM骨干的基准测试中，AgentBalance在匹配的token成本预算下实现高达10%的性能提升，在延迟预算下实现高达22%的性能提升，并在性能-预算曲线上表现出强大的AUC。

Conclusion: AgentBalance能够作为现有MAS的插件，在相同token成本和延迟约束下提升性能，并能很好地泛化到未见过的LLM，实现实用的预算感知部署。

Abstract: Large Language Model (LLM)-based multi-agent systems (MAS) are becoming indispensable building blocks for web-scale applications such as web search, social network analytics, and online customer support, where cost-effectiveness is increasingly the primary constraint for large-scale deployment. While recent work improves MAS cost-effectiveness by shaping inter-agent communication topologies and selecting agent backbones, it rarely models and optimizes under explicit token-cost and latency budgets that reflect deployment constraints. This often leads to topology-first designs and suboptimal cost-effectiveness when budgets are binding. We present AgentBalance, a framework for constructing cost-effective MAS under explicit token-cost and latency budgets via a backbone-then-topology design. AgentBalance first performs backbone-oriented agent generation, constructing agents with heterogeneous backbones through LLM pool construction, pool selection, and role-backbone matching. It then performs adaptive MAS topology generation, guiding inter-agent communication via agent representation learning, gating, and latency-aware topology synthesis. Experiments on benchmarks with 14 candidate LLM backbones show that AgentBalance achieves up to 10% and 22% performance gains under matched token-cost and latency budgets, respectively, and yields strong AUC on performance-versus-budget curves across benchmarks. AgentBalance also functions as a plug-in for existing MAS, improving performance under the same token-cost and latency constraints, and it generalizes well to unseen LLMs for practical, budget-aware deployment. Code: https://github.com/usail-hkust/AgentBalance

</details>


### [39] [Back to the Baseline: Examining Baseline Effects on Explainability Metrics](https://arxiv.org/abs/2512.11433)
*Agustin Martin Picard,Thibaut Boissin,Varshini Subhash,Rémi Cadène,Thomas Fel*

Main category: cs.AI

TL;DR: 该论文指出当前XAI中基于基线函数的保真度评估指标存在问题，不同基线的选择会偏向不同的归因方法，甚至线性模型在不同基线下的最优方法都相互矛盾。作者提出了基线应满足的两个理想属性，并开发了一种新的模型依赖基线来改善现有基线的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 当前可解释人工智能中广泛使用的归因方法评估指标（如插入和删除指标）存在严重问题：不同基线函数的选择会系统性地偏向某些归因方法，导致评估结果不一致。更令人担忧的是，即使是简单的线性模型，在不同基线下的最优归因方法也会相互矛盾，这引发了"应该使用哪个基线"的根本问题。

Method: 作者首先分析了基线应满足的两个理想属性：1) 能够移除信息；2) 不会产生过度分布外图像。通过实验验证现有基线都无法同时满足这两个标准。然后，作者利用特征可视化技术，开发了一种新的模型依赖基线，该基线能够移除信息同时避免产生过度分布外图像，从而改善了现有基线的权衡问题。

Result: 实验表明，现有基线都存在权衡：要么能移除信息但会产生分布外图像序列，要么不会产生分布外图像但移除信息效果不佳。作者提出的新基线在移除信息和避免过度分布外图像之间取得了更好的平衡，相比现有基线改善了这种权衡关系。

Conclusion: 归因方法评估中基线函数的选择对结果有重大影响，当前评估指标存在系统性偏差。作者提出的模型依赖基线为改善这一权衡提供了新思路，强调了在XAI评估中需要更严谨地考虑基线选择问题。

Abstract: Attribution methods are among the most prevalent techniques in Explainable Artificial Intelligence (XAI) and are usually evaluated and compared using Fidelity metrics, with Insertion and Deletion being the most popular. These metrics rely on a baseline function to alter the pixels of the input image that the attribution map deems most important. In this work, we highlight a critical problem with these metrics: the choice of a given baseline will inevitably favour certain attribution methods over others. More concerningly, even a simple linear model with commonly used baselines contradicts itself by designating different optimal methods. A question then arises: which baseline should we use? We propose to study this problem through two desirable properties of a baseline: (i) that it removes information and (ii) that it does not produce overly out-of-distribution (OOD) images. We first show that none of the tested baselines satisfy both criteria, and there appears to be a trade-off among current baselines: either they remove information or they produce a sequence of OOD images. Finally, we introduce a novel baseline by leveraging recent work in feature visualisation to artificially produce a model-dependent baseline that removes information without being overly OOD, thus improving on the trade-off when compared to other existing baselines. Our code is available at https://github.com/deel-ai-papers/Back-to-the-Baseline

</details>


### [40] [Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes](https://arxiv.org/abs/2512.11463)
*Junghwan Lim,Sungmin Lee,Dongseok Kim,Taehyun Kim,Eunhwan Park,Jeesoo Lee,Jeongdoo Lee,Junhyeok Lee,Wai Ting Cheung,Dahye Choi,Minsu Ha,Jaeheui Her,Jaeyeon Huh,Hanbin Jung,Changjin Kang,Beomgyu Kim,Minjae Kim,Taewhan Kim,Youngrok Kim,Hyukjin Kweon,Haesol Lee,Kungyu Lee,Dongpin Oh,Yeongjae Park,Bokki Ryu,Dongjoo Weon*

Main category: cs.AI

TL;DR: Motif-2-12.7B-Reasoning是一个12.7B参数的语言模型，旨在缩小开源模型与前沿专有模型在复杂推理和长上下文理解方面的差距，通过创新的训练方法在有限计算资源下实现高性能。


<details>
  <summary>Details</summary>
Motivation: 解决开源模型在复杂推理和长上下文理解方面与前沿专有模型之间的性能差距，同时克服推理适应过程中常见的模型崩溃和训练不稳定问题。

Method: 采用综合训练方案：1）使用混合并行和内核级优化的内存高效基础设施支持64K标记长上下文；2）两阶段监督微调课程，通过验证对齐的合成数据缓解分布不匹配；3）强化学习微调管道，通过难度感知数据过滤和混合策略轨迹重用来稳定训练。

Result: Motif-2-12.7B-Reasoning在数学、编码和智能体基准测试中取得了与参数数量显著更大的模型相当的性能，为社区提供了具有竞争力的开源模型。

Conclusion: 该研究提供了一个实用的蓝图，展示了如何在现实计算约束下扩展推理能力，为开源社区提供了高性能的推理模型和可复现的训练方法。

Abstract: We introduce Motif-2-12.7B-Reasoning, a 12.7B parameter language model designed to bridge the gap between open-weight systems and proprietary frontier models in complex reasoning and long-context understanding. Addressing the common challenges of model collapse and training instability in reasoning adaptation, we propose a comprehensive, reproducible training recipe spanning system, data, and algorithmic optimizations. Our approach combines memory-efficient infrastructure for 64K-token contexts using hybrid parallelism and kernel-level optimizations with a two-stage Supervised Fine-Tuning (SFT) curriculum that mitigates distribution mismatch through verified, aligned synthetic data. Furthermore, we detail a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline that stabilizes training via difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical results demonstrate that Motif-2-12.7B-Reasoning achieves performance comparable to models with significantly larger parameter counts across mathematics, coding, and agentic benchmarks, offering the community a competitive open model and a practical blueprint for scaling reasoning capabilities under realistic compute constraints.

</details>


### [41] [Three methods, one problem: Classical and AI approaches to no-three-in-line](https://arxiv.org/abs/2512.11469)
*Pranav Ramanathan,Thomas Prellberg,Matthew Lewis,Prathamesh Dinesh Joshi,Raj Abhijit Dandekar,Rajat Dandekar,Sreedath Panat*

Main category: cs.AI

TL;DR: 该研究首次系统比较了经典优化方法（整数线性规划ILP）与AI方法（PatternBoost变换器学习和PPO强化学习）在No-Three-In-Line问题上的表现，发现ILP在19×19网格内能获得最优解，而AI方法在小规模问题上具有竞争力，混合方法可能是未来方向。


<details>
  <summary>Details</summary>
Motivation: No-Three-In-Line是组合几何中的经典问题，传统ILP方法虽然能保证最优解但面临指数级扩展问题，而机器学习方法为模式近似提供了有前景的替代方案。本研究旨在首次系统比较经典优化与AI方法在该问题上的性能。

Method: 研究采用三种方法：1）整数线性规划（ILP）作为经典优化基准；2）首次应用PatternBoost变换器学习；3）首次应用PPO强化学习。在n×n网格上评估这些方法，并与传统算法进行性能比较。

Result: ILP在19×19网格内获得可证明的最优解；PatternBoost在14×14网格内匹配最优性能，测试损失减少96%；PPO在10×10网格上获得完美解，但在11×11网格上因约束违反而失败。

Conclusion: 经典优化方法对于精确解仍然至关重要，而AI方法在小规模实例上具有竞争力。混合方法为扩展到更大问题规模提供了最有前景的方向。

Abstract: The No-Three-In-Line problem asks for the maximum number of points that can be placed on an n by n grid with no three collinear, representing a famous problem in combinatorial geometry. While classical methods like Integer Linear Programming (ILP) guarantee optimal solutions, they face exponential scaling with grid size, and recent advances in machine learning offer promising alternatives for pattern-based approximation. This paper presents the first systematic comparison of classical optimization and AI approaches to this problem, evaluating their performance against traditional algorithms. We apply PatternBoost transformer learning and reinforcement learning (PPO) to this problem for the first time, comparing them against ILP. ILP achieves provably optimal solutions up to 19 by 19 grids, while PatternBoost matches optimal performance up to 14 by 14 grids with 96% test loss reduction. PPO achieves perfect solutions on 10 by 10 grids but fails at 11 by 11 grids, where constraint violations prevent valid configurations. These results demonstrate that classical optimization remains essential for exact solutions while AI methods offer competitive performance on smaller instances, with hybrid approaches presenting the most promising direction for scaling to larger problem sizes.

</details>


### [42] [BAID: A Benchmark for Bias Assessment of AI Detectors](https://arxiv.org/abs/2512.11505)
*Priyam Basu,Yunfeng Zhang,Vipul Raheja*

Main category: cs.AI

TL;DR: BAID框架系统评估AI文本检测器在人口统计、年龄、教育水平、方言、正式程度、政治倾向和主题等7个维度的偏见，发现检测器对少数群体文本存在系统性偏差，特别是召回率低。


<details>
  <summary>Details</summary>
Motivation: 现有AI文本检测器在教育和工作场景中被广泛采用，但先前研究仅发现针对英语学习者的孤立偏见案例，缺乏跨更广泛社会语言学因素的系统性评估。

Method: 提出BAID评估框架，收集超过20万样本覆盖7个主要类别：人口统计、年龄、教育水平、方言、正式程度、政治倾向和主题。为每个样本生成保留原始内容但反映特定亚组写作风格的合成版本，并用此框架评估4个开源先进AI文本检测器。

Result: 评估发现AI文本检测器存在一致的性能差异，特别是对来自代表性不足群体的文本召回率较低，表明检测器存在系统性偏见。

Conclusion: BAID提供了一个可扩展、透明的AI检测器审计方法，强调在部署这些工具供公众使用前需要进行偏见感知评估。

Abstract: AI-generated text detectors have recently gained adoption in educational and professional contexts. Prior research has uncovered isolated cases of bias, particularly against English Language Learners (ELLs) however, there is a lack of systematic evaluation of such systems across broader sociolinguistic factors. In this work, we propose BAID, a comprehensive evaluation framework for AI detectors across various types of biases. As a part of the framework, we introduce over 200k samples spanning 7 major categories: demographics, age, educational grade level, dialect, formality, political leaning, and topic. We also generated synthetic versions of each sample with carefully crafted prompts to preserve the original content while reflecting subgroup-specific writing styles. Using this, we evaluate four open-source state-of-the-art AI text detectors and find consistent disparities in detection performance, particularly low recall rates for texts from underrepresented groups. Our contributions provide a scalable, transparent approach for auditing AI detectors and emphasize the need for bias-aware evaluation before these tools are deployed for public use.

</details>


### [43] [AI-MASLD Metabolic Dysfunction and Information Steatosis of Large Language Models in Unstructured Clinical Narratives](https://arxiv.org/abs/2512.11544)
*Yuan Shen,Xiaojun Wu,Linghua Yu*

Main category: cs.AI

TL;DR: 该研究首次实证发现大语言模型在处理临床信息时表现出类似代谢功能障碍的特征，提出了"AI-代谢功能障碍相关脂肪性肝病(AI-MASLD)"的创新概念，警告当前LLMs在医疗应用中存在功能缺陷和安全隐患。


<details>
  <summary>Details</summary>
Motivation: 模拟真实临床场景，系统评估大语言模型从充满噪声和冗余的患者主诉中提取核心医学信息的能力，验证其是否表现出类似代谢功能障碍相关脂肪性肝病(MASLD)的功能下降。

Method: 采用基于标准化医学探针的横断面分析设计，选取GPT-4o、Gemini 2.5、DeepSeek 3.1和Qwen3-Max四款主流LLMs作为研究对象。使用包含5个核心维度、20个医学探针的评估系统模拟真实临床沟通环境，所有探针均有临床专家定义的金标准答案，由两名独立临床医生采用双盲、反向评分量表进行评估。

Result: 所有测试模型均表现出不同程度的功能缺陷，Qwen3-Max整体表现最佳，Gemini 2.5最差。在极端噪声条件下，大多数模型出现功能崩溃。GPT-4o在深静脉血栓(DVT)继发肺栓塞(PE)风险评估中做出严重误判。

Conclusion: 首次实证确认LLMs在处理临床信息时表现出类似代谢功能障碍的特征，提出"AI-MASLD"创新概念。研究为人工智能在医疗领域的应用提供了重要的安全警示，强调当前LLMs必须在人类专家监督下作为辅助工具使用，其理论知识与实际临床应用之间仍存在显著差距。

Abstract: This study aims to simulate real-world clinical scenarios to systematically evaluate the ability of Large Language Models (LLMs) to extract core medical information from patient chief complaints laden with noise and redundancy, and to verify whether they exhibit a functional decline analogous to Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD). We employed a cross-sectional analysis design based on standardized medical probes, selecting four mainstream LLMs as research subjects: GPT-4o, Gemini 2.5, DeepSeek 3.1, and Qwen3-Max. An evaluation system comprising twenty medical probes across five core dimensions was used to simulate a genuine clinical communication environment. All probes had gold-standard answers defined by clinical experts and were assessed via a double-blind, inverse rating scale by two independent clinicians. The results show that all tested models exhibited functional defects to varying degrees, with Qwen3-Max demonstrating the best overall performance and Gemini 2.5 the worst. Under conditions of extreme noise, most models experienced a functional collapse. Notably, GPT-4o made a severe misjudgment in the risk assessment for pulmonary embolism (PE) secondary to deep vein thrombosis (DVT). This research is the first to empirically confirm that LLMs exhibit features resembling metabolic dysfunction when processing clinical information, proposing the innovative concept of "AI-Metabolic Dysfunction-Associated Steatotic Liver Disease (AI-MASLD)". These findings offer a crucial safety warning for the application of Artificial Intelligence (AI) in healthcare, emphasizing that current LLMs must be used as auxiliary tools under human expert supervision, as there remains a significant gap between their theoretical knowledge and practical clinical application.

</details>


### [44] [AI Benchmark Democratization and Carpentry](https://arxiv.org/abs/2512.11588)
*Gregor von Laszewski,Wesley Brewer,Jeyan Thiyagalingam,Juri Papay,Armstrong Foundjem,Piotr Luszczek,Murali Emani,Shirley V. Moore,Vijay Janapa Reddi,Matthew D. Sinclair,Sebastian Lobentanzer,Sujata Goswami,Benjamin Hawks,Marco Colombo,Nhan Tran,Christine R. Kirkpatrick,Abdulkareem Alsudais,Gregg Barrett,Tianhao Li,Kirsten Morehouse,Shivaram Venkataraman,Rutwik Jain,Kartik Mathur,Victor Lu,Tejinder Singh,Khojasteh Z. Mirza,Kongtao Chen,Sasidhar Kunapuli,Gavin Farrell,Renato Umeton,Geoffrey C. Fox*

Main category: cs.AI

TL;DR: 论文指出当前AI基准测试存在静态化、资源门槛高、与实际应用脱节等问题，提出需要建立动态自适应基准测试框架和AI基准测试技能教育体系。


<details>
  <summary>Details</summary>
Motivation: 当前AI基准测试面临多重挑战：模型架构快速演进、规模扩大、数据集更新和部署环境多样化，使得评估成为移动目标。静态基准容易被大语言模型记忆，导致基准结果与实际性能脱节。此外，资源需求高、硬件访问受限、基准设计专业知识缺乏等问题阻碍了基准测试的民主化。

Method: 提出建立动态自适应基准测试框架，包含持续演进的模型、更新的数据和异构平台。同时倡导"AI基准测试技能教育"（AI Benchmark Carpentry），通过技术革新和系统性教育培养基准设计和使用方面的专业能力。强调社区合作，如MLCommons、DOE万亿参数联盟等项目的经验。

Result: 识别了当前基准测试的关键障碍：高资源需求、专业硬件访问限制、基准设计专业知识缺乏、结果与应用领域关联的不确定性。指出当前基准过分强调顶级硬件上的峰值性能，对多样化实际场景指导有限。

Conclusion: 基准测试必须向动态化转变，保持透明性、可复现性和可解释性。需要技术革新和系统教育相结合，建立支持应用相关比较的基准体系，确保评估跟上AI发展步伐，支持负责任、可复现、可访问的AI部署。社区努力可为AI基准测试技能教育提供基础。

Abstract: Benchmarks are a cornerstone of modern machine learning, enabling reproducibility, comparison, and scientific progress. However, AI benchmarks are increasingly complex, requiring dynamic, AI-focused workflows. Rapid evolution in model architectures, scale, datasets, and deployment contexts makes evaluation a moving target. Large language models often memorize static benchmarks, causing a gap between benchmark results and real-world performance.
  Beyond traditional static benchmarks, continuous adaptive benchmarking frameworks are needed to align scientific assessment with deployment risks. This calls for skills and education in AI Benchmark Carpentry. From our experience with MLCommons, educational initiatives, and programs like the DOE's Trillion Parameter Consortium, key barriers include high resource demands, limited access to specialized hardware, lack of benchmark design expertise, and uncertainty in relating results to application domains. Current benchmarks often emphasize peak performance on top-tier hardware, offering limited guidance for diverse, real-world scenarios.
  Benchmarking must become dynamic, incorporating evolving models, updated data, and heterogeneous platforms while maintaining transparency, reproducibility, and interpretability. Democratization requires both technical innovation and systematic education across levels, building sustained expertise in benchmark design and use. Benchmarks should support application-relevant comparisons, enabling informed, context-sensitive decisions. Dynamic, inclusive benchmarking will ensure evaluation keeps pace with AI evolution and supports responsible, reproducible, and accessible AI deployment. Community efforts can provide a foundation for AI Benchmark Carpentry.

</details>


### [45] [Causal Inference in Energy Demand Prediction](https://arxiv.org/abs/2512.11653)
*Chutian Ma,Grigorii Pomazkin,Giacinto Paolo Saggese,Paul Smith*

Main category: cs.AI

TL;DR: 该论文提出了一种基于结构因果模型的能源需求预测方法，通过分析天气因素和日历信息的因果关系，构建贝叶斯模型实现高精度预测。


<details>
  <summary>Details</summary>
Motivation: 能源需求预测对电网运营商、工业能源消费者和服务提供商至关重要。能源需求受多种因素影响，包括天气条件和日历信息，这些因素之间存在复杂的因果关系，传统的基于简单相关性的学习方法无法充分处理这种复杂性。

Method: 提出结构因果模型来解释变量间的因果关系，通过完整分析验证因果信念。基于获得的因果洞察作为先验知识，构建贝叶斯模型进行预测。

Result: 模型在测试集上实现了3.84%的平均绝对百分比误差（MAPE），达到最先进性能。在两年数据的交叉验证中，平均MAPE为3.88%，表现出强大的鲁棒性。因果分析揭示了能源需求对温度波动的响应具有季节依赖性敏感性，冬季能源需求方差较低。

Conclusion: 通过结合结构因果模型和贝叶斯方法，能够有效捕捉能源需求预测中的复杂因果关系，实现高精度和鲁棒的预测性能，为能源管理提供重要洞察。

Abstract: Energy demand prediction is critical for grid operators, industrial energy
  consumers, and service providers. Energy demand is influenced by multiple
  factors, including weather conditions (e.g. temperature, humidity, wind
  speed, solar radiation), and calendar information (e.g. hour of day and
  month of year), which further affect daily work and life schedules. These
  factors are causally interdependent, making the problem more complex than
  simple correlation-based learning techniques satisfactorily allow for. We
  propose a structural causal model that explains the causal relationship
  between these variables. A full analysis is performed to validate our causal
  beliefs, also revealing important insights consistent with prior studies.
  For example, our causal model reveals that energy demand responds to
  temperature fluctuations with season-dependent sensitivity. Additionally, we
  find that energy demand exhibits lower variance in winter due to the
  decoupling effect between temperature changes and daily activity patterns.
  We then build a Bayesian model, which takes advantage of the causal insights
  we learned as prior knowledge. The model is trained and tested on unseen
  data and yields state-of-the-art performance in the form of a 3.84 percent MAPE on
  the test set. The model also demonstrates strong robustness, as the
  cross-validation across two years of data yields an average MAPE of 3.88 percent.

</details>


### [46] [MedAI: Evaluating TxAgent's Therapeutic Agentic Reasoning in the NeurIPS CURE-Bench Competition](https://arxiv.org/abs/2512.11682)
*Tim Cofala,Christian Kalfar,Jingge Xiao,Johanna Schrader,Michelle Tang,Wolfgang Nejdl*

Main category: cs.AI

TL;DR: TxAgent：基于检索增强生成和工具调用的医疗决策AI系统，在CURE-Bench挑战中获卓越科学奖


<details>
  <summary>Details</summary>
Motivation: 临床医疗决策是高风险领域，需要AI系统进行稳健的多步推理，基于可靠的生物医学知识。医疗应用有严格的安全约束，要求推理轨迹和工具调用序列的准确性。

Method: TxAgent采用微调的Llama-3.1-8B模型，通过迭代检索增强生成动态生成和执行函数调用，连接到统一的生物医学工具套件ToolUniverse（整合FDA Drug API、OpenTargets和Monarch资源）。

Result: 在CURE-Bench NeurIPS 2025挑战中获奖，分析工具检索质量对整体性能的影响，通过改进工具检索策略实现性能提升。

Conclusion: 医疗决策AI需要将令牌级推理和工具使用行为作为显式监督信号进行评估，改进工具检索策略能显著提升系统性能。

Abstract: Therapeutic decision-making in clinical medicine constitutes a high-stakes domain in which AI guidance interacts with complex interactions among patient characteristics, disease processes, and pharmacological agents. Tasks such as drug recommendation, treatment planning, and adverse-effect prediction demand robust, multi-step reasoning grounded in reliable biomedical knowledge. Agentic AI methods, exemplified by TxAgent, address these challenges through iterative retrieval-augmented generation (RAG). TxAgent employs a fine-tuned Llama-3.1-8B model that dynamically generates and executes function calls to a unified biomedical tool suite (ToolUniverse), integrating FDA Drug API, OpenTargets, and Monarch resources to ensure access to current therapeutic information. In contrast to general-purpose RAG systems, medical applications impose stringent safety constraints, rendering the accuracy of both the reasoning trace and the sequence of tool invocations critical. These considerations motivate evaluation protocols treating token-level reasoning and tool-usage behaviors as explicit supervision signals. This work presents insights derived from our participation in the CURE-Bench NeurIPS 2025 Challenge, which benchmarks therapeutic-reasoning systems using metrics that assess correctness, tool utilization, and reasoning quality. We analyze how retrieval quality for function (tool) calls influences overall model performance and demonstrate performance gains achieved through improved tool-retrieval strategies. Our work was awarded the Excellence Award in Open Science. Complete information can be found at https://curebench.ai/.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [47] [Measuring skill-based uplift from AI in a real biological laboratory](https://arxiv.org/abs/2512.10960)
*Ethan Obie Romero-Severson,Tara Harvey,Nick Generous,Phillip M. Mach*

Main category: cs.HC

TL;DR: 该研究通过实验测量了AI推理模型对无湿实验室经验人员在生物实验中的技能提升效果，比较了AI访问组与仅互联网访问组的差异。


<details>
  <summary>Details</summary>
Motivation: 理解AI系统在真实情境中的使用方式对于预测AI系统的风险和益处至关重要，特别是在生物应用中，技能而非知识通常是未经培训人员的主要障碍。这类研究难以执行且需要长时间规划。

Method: 采用试点研究设计，从洛斯阿拉莫斯国家实验室招募无湿实验室经验的员工，分为两组：一组可访问AI推理模型，另一组仅能访问互联网。参与者需完成将大肠杆菌转化、诱导报告肽表达并通过质谱确认的实验任务。记录定量结果（如实验段成功完成）和定性观察（参与者与AI系统、互联网、实验室设备及彼此的互动方式）。

Result: 研究展示了AI访问组与仅互联网访问组在技能提升方面的差异结果，并记录了参与者与AI系统的互动模式。同时总结了设计和执行此类研究的经验教训。

Conclusion: 该研究为未来探索AI与全球生物安全关系的研究提供了实证基础和方法论参考，强调了在生物安全背景下评估AI技能提升效果的重要性。

Abstract: Understanding how AI systems are used by people in real situations that mirror aspects of both legitimate and illegitimate use is key to predicting the risks and benefits of AI systems. This is especially true in biological applications, where skill rather than knowledge is often the primary barrier for an untrained person. The challenge is that these studies are difficult to execute well and can take months to plan and run.
  Here we report the results of a pilot study that attempted to empirically measure the magnitude of \emph{skills-based uplift} caused by access to an AI reasoning model, compared with a control group that had only internet access. Participants -- drawn from a diverse pool of Los Alamos National Laboratory employees with no prior wet-lab experience -- were asked to transform \ecoli{} with a provided expression construct, induce expression of a reporter peptide, and have expression confirmed by mass spectrometry.
  We recorded quantitative outcomes (e.g., successful completion of experimental segments) and qualitative observations about how participants interacted with the AI system, the internet, laboratory equipment, and one another. We present the results of the study and lessons learned in designing and executing this type of study, and we discuss these results in the context of future studies of the evolving relationship between AI and global biosecurity.

</details>


### [48] [AI as Cognitive Amplifier: Rethinking Human Judgment in the Age of Generative AI](https://arxiv.org/abs/2512.10961)
*Tao An*

Main category: cs.HC

TL;DR: 本文提出AI作为认知放大器而非人类智能替代品的观点，认为AI工具的效果取决于使用者的专业知识和判断力，并提出了三级AI参与模型。


<details>
  <summary>Details</summary>
Motivation: 作者观察到自GPT-3时代以来，同样的AI工具在不同使用者手中产生截然不同的效果。针对将AI视为人类智能替代品或认知衰退警告的两种观点，本文提出第三种基于实践观察的视角：AI作为认知放大器，放大而非替代人类能力。

Method: 结合人机交互、认知增强理论和教育技术研究，以及写作、软件开发和数据分析等领域的公司培训实地观察，提出将AI工具定位为智能放大系统的框架。通过专家-新手差异的实证研究分析和专业培训情境的系统观察，展示领域知识、质量判断和迭代精炼能力如何造成用户间的显著性能差距。

Result: 提出了三级AI参与模型：从被动接受到迭代协作再到认知指导。研究表明，层级间的过渡需要的不是技术培训，而是领域专业知识和元认知技能的发展。AI工具的输出质量根本上取决于用户的专业知识和判断力。

Conclusion: 这对劳动力发展和AI系统设计具有重要启示。不应仅仅关注AI素养或技术提示工程，而应采用综合方法加强领域专业知识、评估判断和反思实践。AI应被视为放大而非替代人类能力的认知工具。

Abstract: Through extensive experience training professionals and individual users in AI tool adoption since the GPT-3 era, I have observed a consistent pattern: the same AI tool produces dramatically different results depending on who uses it. While some frame AI as a replacement for human intelligence, and others warn of cognitive decline, this position paper argues for a third perspective grounded in practical observation: AI as a cognitive amplifier that magnifies existing human capabilities rather than substituting for them. Drawing on research in human-computer interaction, cognitive augmentation theory, and educational technology, alongside field observations from corporate training across writing, software development, and data analysis domains, I present a framework positioning AI tools as intelligence amplification systems where output quality depends fundamentally on user expertise and judgment. Through analysis of empirical studies on expert-novice differences and systematic observations from professional training contexts, I demonstrate that domain knowledge, quality judgment, and iterative refinement capabilities create substantial performance gaps between users. I propose a three-level model of AI engagement -- from passive acceptance through iterative collaboration to cognitive direction -- and argue that the transition between levels requires not technical training but development of domain expertise and metacognitive skills. This position has critical implications for workforce development and AI system design. Rather than focusing solely on AI literacy or technical prompt engineering, I advocate for integrated approaches that strengthen domain expertise, evaluative judgment, and reflective practice.

</details>


### [49] [Immutable Explainability: Towards Verifiable and Auditable Affective AI](https://arxiv.org/abs/2512.11065)
*Marcelo Fransoy,Alejandro Hossian,Hernán Merlino*

Main category: cs.HC

TL;DR: 本文提出"不可变可解释性"架构，将模糊逻辑推理引擎与区块链锚定机制结合，为情感AI系统提供透明决策追踪和防篡改审计日志。


<details>
  <summary>Details</summary>
Motivation: 当前情感AI系统存在两个关键问题：1) 系统决策过程不透明，如同"黑箱"；2) 审计日志缺乏可靠性，运营方可能篡改记录。这些问题在敏感应用中尤为突出。

Method: 提出不可变可解释性架构，结合可解释推理引擎（使用模糊逻辑生成透明决策追踪）和加密锚定机制（将追踪记录在区块链上）。实现了一个启发式管道，整合词汇和韵律分析，采用Mamdani型多模态融合引擎。每个推理生成可审计记录，锚定在Sepolia测试网公链上。

Result: 使用西班牙MEACorpus 2023数据集评估，包括原始转录和Whisper生成转录。结果显示模糊融合方法优于基线方法（线性和单模态融合）。

Conclusion: 该研究为情感AI系统建立了基础，提供透明解释、可信审计追踪和更强的用户数据控制。主要目标是创建既透明又可验证的情感AI系统。

Abstract: Affective artificial intelligence has made substantial advances in recent years; yet two critical issues persist, particularly in sensitive applications. First, these systems frequently operate as 'black boxes', leaving their decision-making processes opaque. Second, audit logs often lack reliability, as the entity operating the system may alter them. In this work, we introduce the concept of Immutable Explainability, an architecture designed to address both challenges simultaneously. Our approach combines an interpretable inference engine - implemented through fuzzy logic to produce a transparent trace of each decision - with a cryptographic anchoring mechanism that records this trace on a blockchain, ensuring that it is tamper-evident and independently verifiable. To validate the approach, we implemented a heuristic pipeline integrating lexical and prosodic analysis within an explicit Mamdani-type multimodal fusion engine. Each inference generates an auditable record that is subsequently anchored on a public blockchain (Sepolia Testnet). We evaluated the system using the Spanish MEACorpus 2023, employing both the original corpus transcriptions and those generated by Whisper. The results show that our fuzzy-fusion approach outperforms baseline methods (linear and unimodal fusion). Beyond these quantitative outcomes, our primary objective is to establish a foundation for affective AI systems that offer transparent explanations, trustworthy audit trails, and greater user control over personal data.

</details>


### [50] [Your plan may succeed, but what about failure? Investigating how people use ChatGPT for long-term life task planning](https://arxiv.org/abs/2512.11096)
*Ben Wang,Jiqun Liu*

Main category: cs.HC

TL;DR: 研究探讨人们如何使用ChatGPT进行长期生活任务规划，发现AI能帮助分解复杂目标、生成想法和维持动力，但输出通常过于通用理想化，缺乏个性化和情境适应性，需要用户主动调整验证。


<details>
  <summary>Details</summary>
Motivation: 长期生活任务规划具有复杂性和不确定性，但人们对新兴AI系统如何支持这一过程了解甚少。本研究旨在探索人们如何使用ChatGPT进行长期规划，关注用户实践、不确定性和对AI辅助的认知。

Method: 对14名使用ChatGPT进行长期规划活动的参与者进行访谈研究，结合分析他们的提示词和访谈回答。任务涵盖个人健康、活动规划和专业学习等多个领域，包括启动、细化和情境化计划的提示。

Result: ChatGPT帮助将复杂目标分解为可管理的步骤、生成想法并维持动力，充当反思伙伴。但其输出通常过于通用或理想化，缺乏个性化、情境现实性和适应性，需要用户主动调整和验证结果。参与者期望AI系统能提供适应性强的可信指导，同时承认长期规划中的不确定性和潜在失败。

Conclusion: 研究发现AI如何在不断变化的不确定性中支持长期生活任务规划，并强调设计适应性、不确定性感知、能够支持长期规划作为不断演进的人机协作系统的设计意义。

Abstract: Long-term life task planning is inherently complex and uncertain, yet little is known about how emerging AI systems support this process. This study investigates how people use ChatGPT for such planning tasks, focusing on user practices, uncertainties, and perceptions of AI assistance. We conducted an interview study with 14 participants who engaged in long-term planning activities using ChatGPT, combining analysis of their prompts and interview responses. The task topics across diverse domains, including personal well-being, event planning, and professional learning, along with prompts to initiate, refine, and contextualize plans. ChatGPT helped structure complex goals into manageable steps, generate ideas, and sustain motivation, serving as a reflective partner. Yet its outputs were often generic or idealized, lacking personalization, contextual realism, and adaptability, requiring users to actively adapt and verify results. Participants expressed a need for AI systems that provide adaptive and trustworthy guidance while acknowledging uncertainty and potential failure in long-term planning. Our findings show how AI supports long-term life task planning under evolving uncertainty and highlight design implications for systems that are adaptive, uncertainty-aware, and capable of supporting long-term planning as an evolving human-AI collaboration.

</details>


### [51] [Supporting Medicinal Chemists in Iterative Hypothesis Generation for Drug Target Identification](https://arxiv.org/abs/2512.11105)
*Youngseung Jeon,Christopher Hwang,Ziwen Li,Taylor Le Lievre,Jesus J. Campagna,Cohn Whitaker,Varghese John,Eunice Jun,Xiang Anthony Chen*

Main category: cs.HC

TL;DR: HAPPIER是一个AI驱动的药物发现工具，通过集成多标准支持帮助药物化学家识别靶蛋白，促进假设生成过程中的发散和收敛思维循环。


<details>
  <summary>Details</summary>
Motivation: 药物发现过程效率低下，药物化学家需要在庞大的蛋白质空间中识别同时满足物理功能相互作用、治疗影响和对接潜力三个标准的靶蛋白。现有方法对每个标准提供零散支持，限制了湿实验室实验的有前景假设的生成。

Method: 开发HAPPIER工具，提供集成多标准支持的靶蛋白识别功能：1）在单一集成图组件中高效探索和验证蛋白质的多标准满足情况；2）利用领域知识验证AI建议。这些功能支持假设生成所需的发散和收敛思维迭代循环。

Result: 对10名药物化学家的评估显示，HAPPIER增加了高置信度假设的数量，支持了迭代循环过程，并证明了参与这种循环与输出置信度之间的关系。

Conclusion: HAPPIER通过集成多标准支持和促进发散-收敛思维循环，有效提升了药物发现中靶蛋白识别的假设生成质量和效率。

Abstract: While drug discovery is vital for human health, the process remains inefficient. Medicinal chemists must navigate a vast protein space to identify target proteins that meet three criteria: physical and functional interactions, therapeutic impact, and docking potential. Prior approaches have provided fragmented support for each criterion, limiting the generation of promising hypotheses for wet-lab experiments. We present HAPPIER, an AI-powered tool that supports hypothesis generation with integrated multi-criteria support for target identification. HAPPIER enables medicinal chemists to 1) efficiently explore and verify proteins in a single integrated graph component showing multi-criteria satisfaction and 2) validate AI suggestions with domain knowledge. These capabilities facilitate iterative cycles of divergent and convergent thinking, essential for hypothesis generation. We evaluated HAPPIER with ten medicinal chemists, finding that it increased the number of high-confidence hypotheses and support for the iterative cycle, and further demonstrated the relationship between engaging in such cycles and confidence in outputs.

</details>


### [52] [Breast-Rehab: A Postoperative Breast Cancer Rehabilitation Training Assessment System Based on Human Action Recognition](https://arxiv.org/abs/2512.11245)
*Zikang Chen,Tan Xie,Qinchuan Wang,Heming Zheng,Xudong Lu*

Main category: cs.HC

TL;DR: 开发了Breast-Rehab系统，通过融合视觉和3D骨骼数据的动作识别算法与RAG框架，为乳腺癌术后患者提供低成本的家庭上肢康复管理，减少计算开销并降低模型幻觉。


<details>
  <summary>Details</summary>
Motivation: 乳腺癌幸存者术后上肢功能障碍普遍存在，但家庭康复锻炼依从性低，且传统VR移动健康方案硬件成本高，限制了临床广泛应用。

Method: 开发了Breast-Rehab系统，集成定制化人体动作识别算法与检索增强生成(RAG)框架，融合视觉和3D骨骼数据准确分割家庭环境中的锻炼视频，结合领域知识库指导多模态大语言模型生成临床评估报告。

Result: 系统在非受控家庭环境中准确分割锻炼视频，性能优于标准模型；初步临床研究验证了系统可行性和用户接受度，患者两周内平均锻炼频率达0.59次/天。

Conclusion: 该工作提供了一个完整、经过验证的AI驱动家庭康复监测管道，为乳腺癌术后患者提供低成本、有效的家庭康复管理方案。

Abstract: Postoperative upper limb dysfunction is prevalent among breast cancer survivors, yet their adherence to at-home rehabilitation exercises is low amidst limited nursing resources. The hardware overhead of commonly adopted VR-based mHealth solutions further hinders their widespread clinical application. Therefore, we developed Breast-Rehab, a novel, low-cost mHealth system to provide patients with out-of-hospital upper limb rehabilitation management. Breast-Rehab integrates a bespoke human action recognition algorithm with a retrieval-augmented generation (RAG) framework. By fusing visual and 3D skeletal data, our model accurately segments exercise videos recorded in uncontrolled home environments, outperforming standard models. These segmented clips, combined with a domain-specific knowledge base, guide a multi-modal large language model to generate clinically relevant assessment reports. This approach significantly reduces computational overhead and mitigates model hallucinations. We implemented the system as a WeChat Mini Program and a nurse-facing dashboard. A preliminary clinical study validated the system's feasibility and user acceptance, with patients achieving an average exercise frequency of 0.59 sessions/day over a two-week period. This work thus presents a complete, validated pipeline for AI-driven, at-home rehabilitation monitoring.

</details>


### [53] [AI Autonomy or Human Dependency? Defining the Boundary in Responsible AI with the $α$-Coefficient](https://arxiv.org/abs/2512.11295)
*Nattaya Mairittha,Gabriel Phorncharoenmusikul,Sorawit Worapradidth*

Main category: cs.HC

TL;DR: 论文提出HISOAI概念，揭示AI系统过度依赖人工而非真正自主的问题，并提出AFHE范式，通过AI自主系数α和部署算法确保AI达到最低功能独立性，将人类角色重新定位为高价值任务执行者。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统存在一个关键设计缺陷：滥用"人在回路"模型来掩盖本质上依赖人工的系统。这种结构依赖被称为"人替代AI"系统，代表了伦理失败和不可持续的经济依赖，人类工作者成为隐藏的操作后备而非战略合作者。

Method: 提出AI优先、人类赋能的范式，要求AI组件在部署前必须达到可量化的功能独立性最低标准。通过AI自主系数α来衡量AI无需人工替代成功处理任务的比例，并引入AFHE部署算法作为算法门控，要求系统在离线和影子测试中达到指定的α阈值。

Result: AFHE框架通过强制结构性分离，重新定义人类角色，使其专注于高价值任务，包括伦理监督、边界拓展和战略模型调优，确保真正的系统透明度和操作独立性。

Conclusion: 这项工作倡导向度量驱动、结构健全的AI架构进行关键转变，推动行业超越欺骗性的人工依赖，实现可验证的自主性。

Abstract: The integrity of contemporary AI systems is undermined by a critical design flaw: the misappropriation of Human-in-the-Loop (HITL) models to mask systems that are fundamentally reliant on human labor. We term this structural reliance Human-Instead-of-AI (HISOAI). HISOAI systems represent an ethical failure and an unsustainable economic dependency, where human workers function as hidden operational fallbacks rather than strategic collaborators. To rectify this, we propose the AI-First, Human-Empowered (AFHE) paradigm. AFHE mandates a technological design where the AI component must achieve a minimum, quantifiable level of functional independence prior to deployment. This standard is formalized through the AI Autonomy Coefficient (alpha), a metric that determines the proportion of tasks that the AI successfully processes without mandatory human substitution. We introduce the AFHE Deployment Algorithm, an algorithmic gate that requires the system to meet a specified alpha threshold across both offline and shadow testing. By enforcing this structural separation, the AFHE framework redefines the human's role to focus exclusively on high-value tasks, including ethical oversight, boundary pushing, and strategic model tuning, thereby ensuring true system transparency and operational independence. This work advocates for a critical shift toward metric-driven, structurally sound AI architecture, moving the industry beyond deceptive human dependency toward verifiable autonomy.

</details>


### [54] [Mirror Skin: In Situ Visualization of Robot Touch Intent on Robotic Skin](https://arxiv.org/abs/2512.11472)
*David Wagmann,Matti Krüger,Chao Wang,Jürgen Steimle*

Main category: cs.HC

TL;DR: Mirror Skin是一种受头足类动物启发的机器人皮肤概念，通过高分辨率镜面视觉反馈来传达机器人触摸意图，显著提高了意图理解的准确性和响应速度。


<details>
  <summary>Details</summary>
Motivation: 现有机器人触摸意图传达方法缺乏空间特异性和语义深度，无法有效传达"谁、哪里、何时"进行触摸，这在物理人机交互中对于安全和可预测性至关重要。

Method: 提出Mirror Skin概念，在机器人皮肤上实现高分辨率镜面视觉反馈，将人体部位的视觉表示映射到对应的机器人触摸区域。通过虚拟现实专家进行结构化设计探索，迭代优化六个关键维度，并进行受控用户研究验证效果。

Result: 用户研究表明，Mirror Skin显著提高了触摸意图解释的准确性，并减少了响应时间，有效传达了谁将发起触摸、触摸将在哪里发生以及何时即将发生的信息。

Conclusion: 机器人皮肤上的视觉反馈具有传达人机触摸交互意图的潜力，Mirror Skin概念为解决现有意图传达方法的局限性提供了创新解决方案。

Abstract: Effective communication of robotic touch intent is a key factor in promoting safe and predictable physical human-robot interaction (pHRI). While intent communication has been widely studied, existing approaches lack the spatial specificity and semantic depth necessary to convey robot touch actions. We present Mirror Skin, a cephalopod-inspired concept that utilizes high-resolution, mirror-like visual feedback on robotic skin. By mapping in-situ visual representations of a human's body parts onto the corresponding robot's touch region, Mirror Skin communicates who shall initiate touch, where it will occur, and when it is imminent. To inform the design of Mirror Skin, we conducted a structured design exploration with experts in virtual reality (VR), iteratively refining six key dimensions. A subsequent controlled user study demonstrated that Mirror Skin significantly enhances accuracy and reduces response times for interpreting touch intent. These findings highlight the potential of visual feedback on robotic skin to communicate human-robot touch interactions.

</details>


### [55] [Say it or AI it: Evaluating Hands-Free Text Correction in Virtual Reality](https://arxiv.org/abs/2512.11564)
*Ziming Li,Joffrey Guilmet,Suzanne Sorli,Hai-Ning Liang,Diego Monteiro*

Main category: cs.HC

TL;DR: 研究VR中免手文本校正，比较AI校正与语音输入在可用性和效率上的表现


<details>
  <summary>Details</summary>
Motivation: VR中的文本输入具有挑战性，现有方法主要关注控制器输入，而免手文本校正研究较少，但实际场景中很重要。大型语言模型虽可用于校正，但预测不总是准确，影响可用性。

Method: 研究VR中免手文本校正，比较AI校正方法与语音输入方法在可用性和效率上的差异

Result: 观察到AI文本校正相比语音输入具有更好的可用性

Conclusion: AI在VR免手文本校正中展现出比语音输入更好的可用性潜力

Abstract: Text entry in Virtual Reality (VR) is challenging, even when accounting for the use of controllers. Prior work has tackled this challenge head-on, improving the efficiency of input methods. These techniques have the advantage of allowing for relatively straightforward text correction. However, text correction without the use of controllers is a topic that has not received the same amount of attention, even though it can be desirable in several scenarios, and can even be the source of frustration. Large language models have been adopted and evaluated as a corrective methodology, given their high power for predictions. Nevertheless, their predictions are not always correct, which can lead to lower usability. In this paper, we investigate whether, for text correction in VR that is hands-free, the use of AI could surpass in terms of usability and efficiency. We observed better usability for AI text correction when compared to voice input.

</details>


### [56] [From Verification Burden to Trusted Collaboration: Design Goals for LLM-Assisted Literature Reviews](https://arxiv.org/abs/2512.11661)
*Brenda Nogueira,Werner Geyer,Andrew Anderson,Toby Jia-Jun Li,Dongwhi Kim,Nuno Moniz,Nitesh V. Chawla*

Main category: cs.HC

TL;DR: 本文研究了研究人员在文献综述过程中使用大语言模型的实际应用、局限性和设计挑战，提出了解决信任问题、验证负担和多工具需求的框架。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究探讨研究人员如何将大语言模型用于科学写作，但在文献综述过程中这些工具的具体实施、局限性和设计挑战仍未得到充分探索。

Method: 通过对多个学科的研究人员进行用户研究，描述当前使用大语言模型进行相关文献调研的实践、益处和痛点，识别出三个主要差距。

Result: 识别出三个反复出现的差距：1）对输出缺乏信任；2）持续的验证负担；3）需要多个工具。基于此提出了六个设计目标和高级框架。

Conclusion: 通过将工作建立在研究人员的实际日常需求基础上，设计了一个解决这些限制的框架，通过可验证的行动推进信任，促进研究人员与AI系统之间的实际协作。

Abstract: Large Language Models (LLMs) are increasingly embedded in academic writing practices. Although numerous studies have explored how researchers employ these tools for scientific writing, their concrete implementation, limitations, and design challenges within the literature review process remain underexplored. In this paper, we report a user study with researchers across multiple disciplines to characterize current practices, benefits, and \textit{pain points} in using LLMs to investigate related work. We identified three recurring gaps: (i) lack of trust in outputs, (ii) persistent verification burden, and (iii) requiring multiple tools. This motivates our proposal of six design goals and a high-level framework that operationalizes them through improved related papers visualization, verification at every step, and human-feedback alignment with generation-guided explanations. Overall, by grounding our work in the practical, day-to-day needs of researchers, we designed a framework that addresses these limitations and models real-world LLM-assisted writing, advancing trust through verifiable actions and fostering practical collaboration between researchers and AI systems.

</details>


### [57] [Natural Language Interaction for Editing Visual Knowledge Graphs](https://arxiv.org/abs/2512.11674)
*Reza Shahriari,Eric D. Ragan,Jaime Ruiz*

Main category: cs.HC

TL;DR: 该研究比较了传统GUI编辑与两种自然语言方法在知识图谱编辑中的效果，发现自然语言方法显著更有效


<details>
  <summary>Details</summary>
Motivation: 知识图谱通常使用节点-链接图可视化，用户需要编辑图谱以确保数据准确性或提供更新。传统GUI编辑需要逐个选择和修改大量项目，过程繁琐。研究探索自然语言输入作为替代编辑方法

Method: 通过用户研究比较GUI图谱编辑与两种自然语言替代方法，收集不同交互方法权衡的实证数据

Result: 研究结果显示自然语言方法比传统GUI交互显著更有效

Conclusion: 自然语言输入可以作为知识图谱编辑的有效替代方法，显著提高编辑效率

Abstract: Knowledge graphs are often visualized using node-link diagrams that reveal relationships and structure. In many applications using graphs, it is desirable to allow users to edit graphs to ensure data accuracy or provides updates. Commonly in graph visualization, users can interact directly with the visual elements by clicking and typing updates to specific items through traditional interaction methods in the graphical user interface. However, it can become tedious to make many updates due to the need to individually select and change numerous items in a graph. Our research investigates natural language input as an alternative method for editing network graphs. We present a user study comparing GUI graph editing with two natural language alternatives to contribute novel empirical data of the trade-offs of the different interaction methods. The findings show natural language methods to be significantly more effective than traditional GUI interaction.

</details>


### [58] [From Signal to Turn: Interactional Friction in Modular Speech-to-Speech Pipelines](https://arxiv.org/abs/2512.11724)
*Titaya Mairittha,Tanakon Sawanglok,Panuwit Raden,Jirapast Buntub,Thanapat Warunee,Napat Asawachaisuvikrom,Thanaphum Saiwongin*

Main category: cs.HC

TL;DR: 该论文分析了模块化语音到语音检索增强生成（S2S-RAG）系统中的交互摩擦，识别了三种对话崩溃模式，并指出这些不是缺陷而是模块化设计的结构性后果。


<details>
  <summary>Details</summary>
Motivation: 尽管语音AI系统在生成能力上取得了显著进展，但其交互体验常常感觉对话不连贯。论文旨在深入分析模块化S2S-RAG系统中出现的交互摩擦问题，超越简单的延迟指标，理解对话崩溃的根本原因。

Method: 通过分析一个代表性的生产系统，识别出三种反复出现的对话崩溃模式：时间错位、表达扁平化和修复刚性。进行系统级分析，探讨这些摩擦点如何源于模块化设计结构。

Result: 研究发现这些交互摩擦不应被理解为缺陷或失败，而是模块化设计优先考虑控制而非流畅性的结构性后果。揭示了时间错位、表达扁平化和修复刚性三种具体模式如何破坏对话的自然性。

Conclusion: 构建自然的语音AI是一个基础设施设计挑战，需要从优化孤立组件转向精心编排组件之间的连接。未来的设计应更注重组件间的协调，而不仅仅是单个组件的性能。

Abstract: While voice-based AI systems have achieved remarkable generative capabilities, their interactions often feel conversationally broken. This paper examines the interactional friction that emerges in modular Speech-to-Speech Retrieval-Augmented Generation (S2S-RAG) pipelines. By analyzing a representative production system, we move beyond simple latency metrics to identify three recurring patterns of conversational breakdown: (1) Temporal Misalignment, where system delays violate user expectations of conversational rhythm; (2) Expressive Flattening, where the loss of paralinguistic cues leads to literal, inappropriate responses; and (3) Repair Rigidity, where architectural gating prevents users from correcting errors in real-time. Through system-level analysis, we demonstrate that these friction points should not be understood as defects or failures, but as structural consequences of a modular design that prioritizes control over fluidity. We conclude that building natural spoken AI is an infrastructure design challenge, requiring a shift from optimizing isolated components to carefully choreographing the seams between them.

</details>
