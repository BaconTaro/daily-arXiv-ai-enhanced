<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 8]
- [cs.LG](#cs.LG) [Total: 45]
- [cs.AI](#cs.AI) [Total: 17]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [Leveraging learning analytics to enhance immersive teacher simulations: Challenges and opportunities](https://arxiv.org/abs/2601.08954)
*Sumin Hong,Jewoong Moon,Taeyeon Eom,Juno Hwang,Jibeom Seo*

Main category: cs.HC

TL;DR: 本章探讨如何利用数据分析增强沉浸式教师模拟，通过多模态数据（话语、行为、注视）揭示教学推理过程，为教师专业发展提供新方法。


<details>
  <summary>Details</summary>
Motivation: 传统教师模拟存在局限性，需要更真实的专业决策环境。随着XR技术和数据分析的发展，可以通过沉浸式模拟结合数据分析来提升教师专业学习效果，使教学推理过程可视化。

Method: 采用多模态数据分析方法，从TeacherGen@i模拟中收集话语、行为和注视数据，分析职前教师的教学话语认知分布和互动模式序列，揭示教学推理过程。

Result: 研究发现多模态分析能够使沉浸式模拟中的教学推理过程可视化，揭示了职前教师教学话语的认知分布和互动模式序列，为理解教学实践提供了新视角。

Conclusion: 沉浸式教师模拟是连接学习分析、专业学习和下一代沉浸式学习环境设计的关键平台，需要解决可扩展性和设计挑战，推动教师教育的创新发展。

Abstract: This chapter examines how data analytics can be leveraged to enhance immersive teacher simulations, situating this inquiry within the broader learning sciences discourse on embodied cognition, data-informed feedback, and teacher professional learning. It explores both conceptual foundations and empirical cases to illustrate how analytics serve as mediational tools that connect immersive experiences with reflective teaching practice. The chapter unfolds in multiple sections: (1) The Innovation Journey: An Overview of Immersive Teacher Simulations outlines the evolution from traditional simulations to XR-based environments, highlighting the need for professional decision-making under realistic constraints. (2) Innovation in Existing Research and Practice situates teacher analytics within the trajectory from descriptive observation to multimodal and predictive modeling. (3) Study Approach and Design details how multimodal data-discourse, behavior, and gaze-from the TeacherGen@i simulation were collected and organized to reveal cognitive distribution of pedagogical discourse and interaction patterns. (4) Findings present the cognitive distribution of preservice teachers' pedagogical discourse and the sequential interaction patterns that emerge in exchange, illustrating how multimodal analytics make pedagogical reasoning processes visible within immersive simulations. (5) Understanding Innovative Practices in Teacher Education examines teaching analytics to enhance immersive teacher simulation based on the findings of the study. (6) Key Takeaways of the Innovation Journey identifies research challenges and design implications for scalable, analytics-enhanced teacher education. Together, these sections position immersive teacher simulations as a pivotal testbed for aligning learning analytics, professional learning, and next-generation immersive learning environment design.

</details>


### [2] [Exploring the Effects of Generative AI Assistance on Writing Self-Efficacy](https://arxiv.org/abs/2601.09033)
*Yejoon Song,Bandi Kim,Yeju Kwon,Sung Park*

Main category: cs.HC

TL;DR: AI写作辅助对大学生写作自我效能感的影响取决于辅助配置方式，不同层级的AI支持产生不同效果


<details>
  <summary>Details</summary>
Motivation: 研究生成式AI在学术写作中的应用如何影响学生的写作自我效能感，探索不同AI辅助配置方式的效果差异

Method: 采用2×2实验设计，让韩国本科生完成议论文写作任务，比较构思层面、句子层面、全过程和无AI支持四种条件

Result: AI辅助并非统一提升自我效能感：全过程AI支持产生高但稳定的自我效能感，伴随所有权感降低；句子层面AI支持导致自我效能感持续下降；构思层面AI支持与高自我效能感和积极的纵向变化相关

Conclusion: AI干预的焦点（而非辅助量）对于培养写作自我效能感同时保持学习者自主性至关重要，构思层面的AI支持效果最佳

Abstract: Generative AI (GenAI) is increasingly used in academic writing, yet its effects on students' writing self-efficacy remain contingent on how assistance is configured. This pilot study investigates how ideation-level, sentence-level, full-process, and no AI support differentially shape undergraduate writers' self-efficacy using a 2 by 2 experimental design with Korean undergraduates completing argumentative writing tasks. Results indicate that AI assistance does not uniformly enhance self-efficacy full AI support produced high but stable self-efficacy alongside signs of reduced ownership, sentence-level AI support led to consistent self-efficacy decline, and ideation-level AI support was associated with both high self-efficacy and positive longitudinal change. These findings suggest that the locus of AI intervention, rather than the amount of assistance, is critical in fostering writing self-efficacy while preserving learner agency.

</details>


### [3] [Exploring Organizational Readiness and Ecosystem Coordination for Industrial XR](https://arxiv.org/abs/2601.09045)
*Hasan Tarik Akbaba,Efe Bozkir,Anna Puhl,Süleyman Özdel,Enkelejda Kasneci*

Main category: cs.HC

TL;DR: 研究发现工业XR应用面临"试点陷阱"现象，关键障碍已从技术成熟度转向组织准备度，需要从技术中心试点转向问题优先的组织转型方法


<details>
  <summary>Details</summary>
Motivation: 尽管XR技术在工业支持、培训和维护方面具有变革潜力，且硬件已相对成熟，但广泛采用仍然滞后。组织在孤立试点中成功实施XR，却难以将其扩展为持续运营部署，这种现象被称为"试点陷阱"

Method: 通过定性生态系统分析，对17位来自技术提供商、解决方案集成商和工业采用者的专家进行访谈

Result: 识别出"大反转"现象：关键约束已从技术成熟度转向组织准备度（如变革管理、KPI对齐、政治阻力）。虽然硬件人机工程学和可用性仍然相关，但研究发现利益相关者激励之间的系统性错位是阻碍企业整合的主要摩擦原因

Conclusion: 成功的工业XR采用需要从技术中心的试点转向问题优先的组织转型方法，需要明确的生态系统级协调

Abstract: Extended Reality (XR) offers transformative potential for industrial support, training, and maintenance; yet, widespread adoption lags despite demonstrated occupational value and hardware maturity. Organizations successfully implement XR in isolated pilots, yet struggle to scale these into sustained operational deployment, a phenomenon we characterize as the ``Pilot Trap.'' This study examines this phenomenon through a qualitative ecosystem analysis of 17 expert interviews across technology providers, solution integrators, and industrial adopters. We identify a ``Great Inversion'' in adoption barriers: critical constraints have shifted from technological maturity to organizational readiness (e.g., change management, key performance indicator alignment, and political resistance). While hardware ergonomics and usability remain relevant, our findings indicate that systemic misalignments between stakeholder incentives are the primary cause of friction preventing enterprise integration. We conclude that successful industrial XR adoption requires a shift from technology-centric piloting to a problem-first, organizational transformation approach, necessitating explicit ecosystem-level coordination.

</details>


### [4] [Immersive XR That Moves People: How XR Advertising Transforms Comprehension, Empathy, and Behavioural Intention](https://arxiv.org/abs/2601.09048)
*Yuki Kobayashi,Koichi Toida*

Main category: cs.HC

TL;DR: XR广告比传统2D广告更能提升购买意愿，主要通过增强同理心而非理解度来实现


<details>
  <summary>Details</summary>
Motivation: 虽然XR技术能提供更强的身体存在感和体验式理解，但其心理过程如何影响后续行为意图仍不明确，需要在实际应用场景中验证

Method: 采用重复测量双因素方差分析，比较非沉浸式2D广告与沉浸式XR体验广告，检验XR是否能增强对产品的理解度和同理心，以及这些内部反应如何影响购买意愿

Result: XR在所有评估维度上得分显著更高；中介分析显示购买意愿的提升主要由同理心介导，而理解度在本研究中未显示显著中介效应

Conclusion: 沉浸式XR体验能增强对虚拟产品的同理心投入，这种增强的同理心在塑造后续行为意图中起关键作用

Abstract: Extended Reality (XR) affords an enhanced sense of bodily presence that supports experiential modes of comprehension and affective engagement which exceed the possibilities of conventional information delivery. Nevertheless, the psychological processes engendered by XR, and the manner in which these processes inform subsequent behavioural intentions, remain only partially delineated. The present study addresses this issue within an applied context by comparing non-immersive 2D viewing advertising with immersive XR experiential advertising. We examined whether XR strengthens internal responses to a product, specifically perceived comprehension and empathy, and whether these responses, in turn, influence the behavioural outcome of purchase intention. A repeated-measures two-way ANOVA demonstrated a significant main effect of advertising modality, with XR yielding higher ratings on all evaluative dimensions. Mediation analysis further indicated that the elevation in purchase intention was mediated by empathy, whereas no significant mediating effect was observed for comprehension within the scope of this study. These findings suggest that immersive XR experiences augment empathic engagement with virtual products, and that this enhanced empathy plays a pivotal role in shaping subsequent behavioural intentions.

</details>


### [5] [Mikasa: A Character-Driven Emotional AI Companion Inspired by Japanese Oshi Culture](https://arxiv.org/abs/2601.09208)
*Miki Ueno*

Main category: cs.HC

TL;DR: 论文提出Mikasa情感AI伴侣，基于日本推し文化设计，强调稳定角色和明确关系定义，而非通用助手或可变角色聊天机器人，通过一致性人格和关系定义提升长期用户参与度。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型和多模态交互虽能实现流畅对话，但许多AI伴侣系统难以长期保持用户满意度和参与度。作者认为问题主要不在于模型能力弱，而在于角色设计不佳和用户-AI关系定义不清晰。

Method: 提出Mikasa情感AI伴侣，基于日本推し文化设计，强调长期非独占承诺和稳定角色。Mikasa被设计为具有一致性人格和明确伙伴关系的连贯角色，而非通用助手或可变角色聊天机器人。通过探索性评估研究用户偏好。

Result: 用户表面描述偏好对话自然性等表层质量，但实际也重视关系控制和想象参与，尽管他们不直接表达这些需求。角色一致性和关系定义作为潜在结构元素塑造交互体验质量，用户并不将其视为主要特征。

Conclusion: 角色设计是AI伴侣系统的功能组成部分，而不仅仅是装饰。Mikasa基于特定文化背景，但其设计原则——对一致性人格的承诺和明确关系定义——可应用于多种情感基础的AI伴侣。

Abstract: Recent progress in large language models and multimodal interaction has made it possible to develop AI companions that can have fluent and emotionally expressive conversations. However, many of these systems have problems keeping users satisfied and engaged over long periods. This paper argues that these problems do not come mainly from weak models, but from poor character design and unclear definitions of the user-AI relationship. I present Mikasa, an emotional AI companion inspired by Japanese Oshi culture-specifically its emphasis on long-term, non-exclusive commitment to a stable character-as a case study of character-driven companion design. Mikasa does not work as a general-purpose assistant or a chatbot that changes roles. Instead, Mikasa is designed as a coherent character with a stable personality and a clearly defined relationship as a partner. This relationship does not force exclusivity or obligation. Rather, it works as a reference point that stabilizes interaction norms and reduces the work users must do to keep redefining the relationship. Through an exploratory evaluation, I see that users describe their preferences using surface-level qualities such as conversational naturalness, but they also value relationship control and imaginative engagement in ways they do not state directly. These results suggest that character coherence and relationship definition work as latent structural elements that shape how good the interaction feels, without users recognizing them as main features. The contribution of this work is to show that character design is a functional part of AI companion systems, not just decoration. Mikasa is one example based on a specific cultural context, but the design principles-commitment to a consistent personality and clear relationship definition-can be used for many emotionally grounded AI companions.

</details>


### [6] [Technological Advances in Two Generations of Consumer-Grade VR Systems: Effects on User Experience and Task Performance](https://arxiv.org/abs/2601.09610)
*Marie Luisa Fiedler,Christian Merz,Jonathan Tschanter,Carolin Wienrich,Marc Erich Latoschik*

Main category: cs.HC

TL;DR: 比较两代VR系统（10年前的HTC Vive和现代的HTC Vive Pro 2）在用户体验和任务表现上的差异，发现没有显著差异，支持旧系统在VR研究中的持续有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管消费级VR系统已有10年发展，但技术进步对用户体验和先前研究的有效性影响尚不明确。本研究旨在比较两代可比VR系统，评估硬件升级是否带来实质性改进。

Method: 采用2x5混合设计，比较HTC Vive（6点追踪）和HTC Vive Pro 2（6点追踪）两代系统。在商业可用配置下，让参与者完成5个涵盖不同使用场景的任务，评估存在感、具身感、外观行为合理性、工作负荷、任务表现，并收集定性反馈。

Result: 结果显示两代系统之间没有显著差异，效应量很小。贝叶斯分析进一步支持零假设，表明所研究的硬件代际改进对用户体验和任务表现提供的益处有限。

Conclusion: 对于10年的硬件代际差异（不包括必要的软件组件进步），研究支持先前工作的结论有效性，并强调旧配置在具身VR研究中的适用性。

Abstract: Integrated VR (IVR) systems consist of a head-mounted display (HMD) and body-tracking capabilities. They enable users to translate their physical movements into corresponding avatar movements in real-time, allowing them to perceive their avatars via the displays. Consumer-grade IVR systems have been available for 10 years, significantly fostering VR research worldwide. However, the effects of even apparently significant technological advances of IVR systems on user experience and the overall validity of prior embodiment research using such systems often remain unclear. We ran a user-centered study comparing two comparable IVR generations: a nearly 10-year-old hardware (HTC Vive, 6-point tracking) and a modern counterpart (HTC Vive Pro 2, 6-point tracking). To ensure ecological validity, we evaluated the systems in their commercially available, as-is configurations. In a 2x5 mixed design, participants completed five tasks covering different use cases on either the old or new system. We assessed presence, sense of embodiment, appearance and behavior plausibility, workload, task performance, and gathered qualitative feedback. Results showed no significant system differences, with only small effect sizes. Bayesian analysis further supported the null hypothesis, suggesting that the investigated generational hardware improvements offer limited benefits for user experience and task performance. For the 10-year generational step examined here, excluding potential technological progress in the necessary software components, this supports the validity of conclusions from prior work and underscores the applicability of older configurations for research in embodied VR.

</details>


### [7] [Full Disclosure, Less Trust? How the Level of Detail about AI Use in News Writing Affects Readers' Trust](https://arxiv.org/abs/2601.09620)
*Pooja Prajod,Hannes Cools,Thomas Röggla,Karthikeya Puttur Venkatraj,Amber Kusters,Alia ElKattan,Pablo Cesar,Abdallah El Ali*

Main category: cs.HC

TL;DR: 研究发现AI新闻透明度存在"透明度困境"：详细AI披露会降低读者信任，但单行和详细披露都会增加读者查证行为，详细披露效果更明显。


<details>
  <summary>Details</summary>
Motivation: 随着AI在新闻生产中的广泛应用，对AI使用透明度的呼声日益高涨。但研究表明AI披露可能导致"透明度困境"，即披露反而降低读者信任。然而，关于AI披露详细程度如何影响信任并促成这一困境的研究还很缺乏。

Method: 采用3×2×2混合因子设计，40名参与者。研究三个AI披露水平（无、单行、详细）×两种新闻类型（政治、生活）×两种AI参与程度（低、高）。使用新闻媒体信任问卷测量信任，并观察两种决策行为：查证来源和订阅决定。同时进行半结构化访谈。

Result: 问卷回答和订阅率显示只有详细AI披露会降低信任，而查证行为在单行和详细披露时都会增加，详细披露效果更明显。访谈显示查证行为主要由话题兴趣驱动，其次是信任；而信任是影响订阅决定的主要因素。约三分之二参与者偏好详细披露，偏好单行披露的参与者大多希望按需获取详细信息。

Conclusion: 并非所有AI披露都会导致透明度困境，而是反映了读者对更高透明度的需求与对AI辅助新闻内容信任之间的权衡。研究建议考虑按需披露格式来平衡透明度和信任。

Abstract: As artificial intelligence (AI) is increasingly integrated into news production, calls for transparency about the use of AI have gained considerable traction. Recent studies suggest that AI disclosures can lead to a ``transparency dilemma'', where disclosure reduces readers' trust. However, little is known about how the \textit{level of detail} in AI disclosures influences trust and contributes to this dilemma within the news context. In this 3$\times$2$\times$2 mixed factorial study with 40 participants, we investigate how three levels of AI disclosures (none, one-line, detailed) across two types of news (politics and lifestyle) and two levels of AI involvement (low and high) affect news readers' trust. We measured trust using the News Media Trust questionnaire, along with two decision behaviors: source-checking and subscription decisions. Questionnaire responses and subscription rates showed a decline in trust only for detailed AI disclosures, whereas source-checking behavior increased for both one-line and detailed disclosures, with the effect being more pronounced for detailed disclosures. Insights from semi-structured interviews suggest that source-checking behavior was primarily driven by interest in the topic, followed by trust, whereas trust was the main factor influencing subscription decisions. Around two-thirds of participants expressed a preference for detailed disclosures, while most participants who preferred one-line indicated a need for detail-on-demand disclosure formats. Our findings show that not all AI disclosures lead to a transparency dilemma, but instead reflect a trade-off between readers' desire for more transparency and their trust in AI-assisted news content.

</details>


### [8] [Perceptually-Guided Adjusted Teleporting: Perceptual Thresholds for Teleport Displacements in Virtual Environments](https://arxiv.org/abs/2601.09632)
*Rose Connolly,Victor Zordan,Rachel McDonnell*

Main category: cs.HC

TL;DR: 研究发现VR瞬移技术可以在用户无感知的情况下调整目标位置，向后调整和长距离瞬移的容忍度更大，为自适应VR运动系统提供了新机会。


<details>
  <summary>Details</summary>
Motivation: 瞬移是VR中最常见的运动技术之一，但其感知特性尚未充分研究。虽然重定向行走研究表明用户的运动可以在不被察觉的情况下被微妙操纵，但类似的瞬移不可感知调整尚未得到系统研究。

Method: 采用重复测量实验，通过心理物理阶梯法和二选一强制选择任务，在方向和距离两个维度上改变参与者选择的瞬移目标位置，估计位置调整的检测阈值。

Result: 瞬移目标位置可以在不被检测到的情况下进行调整，向后调整和较长瞬移距离的容忍度更大，建立了重定向瞬移的基线感知限制。

Conclusion: 这项研究将重定向原则从行走扩展到瞬移，为自适应和社交感知的VR运动系统开辟了新机会，包括社交VR中的人际距离管理、游戏中的目标引导和帮助新手导航等应用。

Abstract: Teleportation is one of the most common locomotion techniques in virtual reality, yet its perceptual properties remain underexplored. While redirected walking research has shown that users' movements can be subtly manipulated without detection, similar imperceptible adjustments for teleportation have not been systematically investigated. This study examines the thresholds at which teleportation displacements become noticeable to users. We conducted a repeated-measures experiment in which participants' selected teleport destinations were altered in both direction (forwards, backwards) and at different ranges (small, large). Detection thresholds for these positional adjustments were estimated using a psychophysical staircase method with a two-alternative forced choice (2AFC) task. Results show that teleport destinations can be shifted without detection, with larger tolerances for backward adjustments and across longer teleport ranges. These findings establish baseline perceptual limits for redirected teleportation and highlight its potential as a design technique. Applications include supporting interpersonal distance management in social VR, guiding players toward objectives in games, and assisting novice users with navigation. By identifying the limits of imperceptible teleportation adjustments, this work extends redirection principles beyond walking to teleportation and opens new opportunities for adaptive and socially aware VR locomotion systems.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [9] [Spectral Generative Flow Models: A Physics-Inspired Replacement for Vectorized Large Language Models](https://arxiv.org/abs/2601.08893)
*Andrew Kiruluta*

Main category: cs.LG

TL;DR: SGFMs是一种基于物理启发的生成模型，用连续场演化替代离散token序列，通过多尺度小波基中的约束随机动力学进行生成，提供场论本体论、小波域表示和约束随机流三大创新。


<details>
  <summary>Details</summary>
Motivation: 提出一种替代基于Transformer的大语言模型的物理启发方法，旨在解决传统方法在长程一致性、多模态泛化和物理结构归纳偏置方面的局限性，通过连续场理论和物理结构来改进生成模型。

Method: 将文本和视频统一为随机偏微分方程的轨迹，采用多尺度小波基表示以实现稀疏性和尺度分离，使用局部算子、谱投影和Navier-Stokes类输运替代全局注意力，通过约束随机流确保稳定性和一致性传播。

Result: 提出了一个全新的生成架构，从根本上区别于自回归建模和扩散方法，为下一代生成模型提供了实现长程一致性、多模态泛化和物理结构归纳偏置的原则性路径。

Conclusion: SGFMs为生成建模提供了基于连续性、几何和物理结构的新范式，有望在长程一致性、多模态能力和物理结构理解方面超越传统Transformer和扩散模型。

Abstract: We introduce Spectral Generative Flow Models (SGFMs), a physics-inspired alternative to transformer-based large language models. Instead of representing text or video as sequences of discrete tokens processed by attention, SGFMs treat generation as the evolution of a continuous field governed by constrained stochastic dynamics in a multiscale wavelet basis. This formulation replaces global attention with local operators, spectral projections, and Navier--Stokes-like transport, yielding a generative mechanism grounded in continuity, geometry, and physical structure.
  Our framework provides three key innovations: (i) a field-theoretic ontology in which text and video are unified as trajectories of a stochastic partial differential equation; (ii) a wavelet-domain representation that induces sparsity, scale separation, and computational efficiency; and (iii) a constrained stochastic flow that enforces stability, coherence, and uncertainty propagation. Together, these components define a generative architecture that departs fundamentally from autoregressive modeling and diffusion-based approaches. SGFMs offer a principled path toward long-range coherence, multimodal generality, and physically structured inductive bias in next-generation generative models.

</details>


### [10] [XGBoost Forecasting of NEPSE Index Log Returns with Walk Forward Validation](https://arxiv.org/abs/2601.08896)
*Sahaj Raj Malla,Shreeyash Kayastha,Rumi Suwal,Harish Chandra Bhandari,Rajendra Adhikari*

Main category: cs.LG

TL;DR: 该研究开发了一个基于XGBoost回归器的稳健机器学习框架，用于预测尼泊尔证券交易所指数的日对数收益率，通过特征工程和超参数优化，在滚动窗口验证中优于ARIMA和岭回归基准模型。


<details>
  <summary>Details</summary>
Motivation: 针对尼泊尔证券交易所指数这一新兴市场的波动性时间序列，开发一个能够有效建模非线性动态的预测框架，为NEPSE指数预测建立可复现的基准。

Method: 使用XGBoost回归器构建机器学习框架，特征工程包括滞后对数收益率（最多30天）和技术指标（滚动波动率和14期相对强弱指数），通过Optuna进行超参数优化，采用时间序列交叉验证和向前滚动验证（扩展窗口和固定长度滚动窗口）来评估样本外性能。

Result: 最优配置（扩展窗口，20个滞后项）在预测精度上优于调优的ARIMA和岭回归基准模型，实现了最低的对数收益率RMSE（0.013450）和MAE（0.009814），方向准确率达到65.15%。虽然R平方值相对较低，但重点在于相对误差减少和方向预测。

Conclusion: 梯度提升集成方法在建模新兴市场波动性时间序列的非线性动态方面具有有效性，为NEPSE指数预测建立了可复现的基准，特征重要性分析和可视化增强了模型的可解释性。

Abstract: This study develops a robust machine learning framework for one-step-ahead forecasting of daily log-returns in the Nepal Stock Exchange (NEPSE) Index using the XGBoost regressor. A comprehensive feature set is engineered, including lagged log-returns (up to 30 days) and established technical indicators such as short- and medium-term rolling volatility measures and the 14-period Relative Strength Index. Hyperparameter optimization is performed using Optuna with time-series cross-validation on the initial training segment. Out-of-sample performance is rigorously assessed via walk-forward validation under both expanding and fixed-length rolling window schemes across multiple lag configurations, simulating real-world deployment and avoiding lookahead bias. Predictive accuracy is evaluated using root mean squared error, mean absolute error, coefficient of determination (R-squared), and directional accuracy on both log-returns and reconstructed closing prices. Empirical results show that the optimal configuration, an expanding window with 20 lags, outperforms tuned ARIMA and Ridge regression benchmarks, achieving the lowest log-return RMSE (0.013450) and MAE (0.009814) alongside a directional accuracy of 65.15%. While the R-squared remains modest, consistent with the noisy nature of financial returns, primary emphasis is placed on relative error reduction and directional prediction. Feature importance analysis and visual inspection further enhance interpretability. These findings demonstrate the effectiveness of gradient boosting ensembles in modeling nonlinear dynamics in volatile emerging market time series and establish a reproducible benchmark for NEPSE Index forecasting.

</details>


### [11] [DriftGuard: A Hierarchical Framework for Concept Drift Detection and Remediation in Supply Chain Forecasting](https://arxiv.org/abs/2601.08928)
*Shahnawaz Alam,Mohammed Abdul Rahman,Bareera Sadeqa*

Main category: cs.LG

TL;DR: DriftGuard是一个端到端的供应链预测漂移管理系统，通过多方法检测、分层传播分析、SHAP根因诊断和成本感知重训练，解决了传统方法无法及时检测、诊断和修复概念漂移的问题。


<details>
  <summary>Details</summary>
Motivation: 供应链预测模型会随时间退化（概念漂移），导致缺货或库存过剩。当前行业依赖手动监控和定期重训练（3-6个月），浪费计算资源且错过快速漂移事件。现有学术方法仅关注漂移检测，忽略了诊断、修复和供应链数据的层次结构。

Method: DriftGuard包含五个模块：1）四种互补检测方法集成（基于误差监控、统计检验、自编码器异常检测、CUSUM变点分析）；2）分层传播分析确定产品线漂移位置；3）SHAP分析诊断根因；4）成本感知重训练策略选择性更新受影响模型；5）端到端系统管理完整漂移生命周期。

Result: 在M5零售数据集的30,000多个时间序列上评估，DriftGuard在4.2天内达到97.8%的检测召回率，通过针对性修复实现高达417倍的投资回报率。

Conclusion: DriftGuard提供了一个完整的供应链预测漂移管理解决方案，能够早期检测漂移、解释根因并自动修复受影响模型，显著优于当前行业实践和学术方法。

Abstract: Supply chain forecasting models degrade over time as real-world conditions change. Promotions shift, consumer preferences evolve, and supply disruptions alter demand patterns, causing what is known as concept drift. This silent degradation leads to stockouts or excess inventory without triggering any system warnings. Current industry practice relies on manual monitoring and scheduled retraining every 3-6 months, which wastes computational resources during stable periods while missing rapid drift events. Existing academic methods focus narrowly on drift detection without addressing diagnosis or remediation, and they ignore the hierarchical structure inherent in supply chain data. What retailers need is an end-to-end system that detects drift early, explains its root causes, and automatically corrects affected models. We propose DriftGuard, a five-module framework that addresses the complete drift lifecycle. The system combines an ensemble of four complementary detection methods, namely error-based monitoring, statistical tests, autoencoder anomaly detection, and Cumulative Sum (CUSUM) change-point analysis, with hierarchical propagation analysis to identify exactly where drift occurs across product lines. Once detected, Shapley Additive Explanations (SHAP) analysis diagnoses the root causes, and a cost-aware retraining strategy selectively updates only the most affected models. Evaluated on over 30,000 time series from the M5 retail dataset, DriftGuard achieves 97.8% detection recall within 4.2 days and delivers up to 417 return on investment through targeted remediation.

</details>


### [12] [Breaking the Bottlenecks: Scalable Diffusion Models for 3D Molecular Generation](https://arxiv.org/abs/2601.08963)
*Adrita Das,Peiran Jiang,Dantong Zhu,Barnabas Poczos,Jose Lugo-Martinez*

Main category: cs.LG

TL;DR: DDDM通过确定性去噪替代随机反向过程，提升分子生成效率，但理论机制不明确。本文基于RTK框架重新解释DDDM，统一确定性和随机扩散，阐明其高效推理原理，并解决分子扩散中的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在分子设计中表现出强大能力，但存在采样轨迹长、反向过程随机方差大、去噪动态结构意识有限等问题。DDDM通过确定性去噪提高了效率，但其理论机制不明确，需要从理论层面解释确定性更新的原理。

Method: 基于Huang等人（2024）的反向转移核（RTK）框架，将DDDM的反向过程重新解释为近似核算子，表明直接去噪过程隐式优化了噪声样本与干净样本之间的结构化传输映射。这种视角统一了确定性和随机扩散的概率形式化。

Result: RTK视角确保了数值稳定性（通过强制良好条件的反向核）、提高了样本一致性（消除随机方差）、并实现了可扩展且保持对称性的去噪器（尊重SE(3)等变性）。在GEOM-DRUGS数据集上的实验表明，RTK引导的确定性去噪比随机扩散模型收敛更快、结构保真度更高，同时保持化学有效性。

Conclusion: 通过RTK框架对DDDM进行理论重构，不仅阐明了确定性去噪高效推理的理论基础，还解决了分子扩散中的多个长期瓶颈问题，为高效、稳定的分子生成提供了新的理论视角和实践方法。

Abstract: Diffusion models have emerged as a powerful class of generative models for molecular design, capable of capturing complex structural distributions and achieving high fidelity in 3D molecule generation. However, their widespread use remains constrained by long sampling trajectories, stochastic variance in the reverse process, and limited structural awareness in denoising dynamics. The Directly Denoising Diffusion Model (DDDM) mitigates these inefficiencies by replacing stochastic reverse MCMC updates with deterministic denoising step, substantially reducing inference time. Yet, the theoretical underpinnings of such deterministic updates have remained opaque. In this work, we provide a principled reinterpretation of DDDM through the lens of the Reverse Transition Kernel (RTK) framework by Huang et al. 2024, unifying deterministic and stochastic diffusion under a shared probabilistic formalism. By expressing the DDDM reverse process as an approximate kernel operator, we show that the direct denoising process implicitly optimizes a structured transport map between noisy and clean samples. This perspective elucidates why deterministic denoising achieves efficient inference. Beyond theoretical clarity, this reframing resolves several long-standing bottlenecks in molecular diffusion. The RTK view ensures numerical stability by enforcing well-conditioned reverse kernels, improves sample consistency by eliminating stochastic variance, and enables scalable and symmetry-preserving denoisers that respect SE(3) equivariance. Empirically, we demonstrate that RTK-guided deterministic denoising achieves faster convergence and higher structural fidelity than stochastic diffusion models, while preserving chemical validity across GEOM-DRUGS dataset. Code, models, and datasets are publicly available in our project repository.

</details>


### [13] [Continuous Fairness On Data Streams](https://arxiv.org/abs/2601.08976)
*Subhodeep Ghosh,Zhihui Du,Angela Bonifati,Manish Kumar,David Bader,Senjuti Basu Roy*

Main category: cs.LG

TL;DR: 提出了一种在数据流滑动窗口中实施连续组公平性的新方法，通过更细粒度的块级公平性监控和实时重排序算法，显著提升了公平性效果。


<details>
  <summary>Details</summary>
Motivation: 当滑动窗口较大时，窗口级别的组公平性可能过于粗糙，需要在更细粒度上确保公平性。现有方法无法有效处理数据流中的连续公平性监控和实时调整问题。

Method: 提出了块级组公平性模型，设计了基于草图的数据结构来高效监控滑动窗口的公平性，并开发了最优的重排序算法来纠正公平性违规。

Result: 实现了毫秒级处理，平均每秒约30,000次查询吞吐量。重排序算法在某些情况下将块级组公平性提升高达95%，在数据集上平均提升50-60%。

Conclusion: 块级公平性相比窗口级公平性具有显著优势，提出的方法能够高效实时地监控和纠正数据流中的组公平性违规，具有实际应用价值。

Abstract: We study the problem of enforcing continuous group fairness over windows in data streams. We propose a novel fairness model that ensures group fairness at a finer granularity level (referred to as block) within each sliding window. This formulation is particularly useful when the window size is large, making it desirable to enforce fairness at a finer granularity. Within this framework, we address two key challenges: efficiently monitoring whether each sliding window satisfies block-level group fairness, and reordering the current window as effectively as possible when fairness is violated. To enable real-time monitoring, we design sketch-based data structures that maintain attribute distributions with minimal overhead. We also develop optimal, efficient algorithms for the reordering task, supported by rigorous theoretical guarantees. Our evaluation on four real-world streaming scenarios demonstrates the practical effectiveness of our approach. We achieve millisecond-level processing and a throughput of approximately 30,000 queries per second on average, depending on system parameters. The stream reordering algorithm improves block-level group fairness by up to 95% in certain cases, and by 50-60% on average across datasets. A qualitative study further highlights the advantages of block-level fairness compared to window-level fairness.

</details>


### [14] [Optimising for Energy Efficiency and Performance in Machine Learning](https://arxiv.org/abs/2601.08991)
*Emile Dos Santos Ferreira,Neil D. Lawrence,Andrei Paleyes*

Main category: cs.LG

TL;DR: ECOpt是一个超参数调优器，专门优化机器学习模型的能源效率和性能，通过量化这两个指标的权衡关系，帮助从业者在能源成本和环境影响方面做出明智决策。


<details>
  <summary>Details</summary>
Motivation: 机器学习的普及和大模型需求导致能源消耗和环境影响的增加，但当前对ML能源扩展规律了解不足，现有研究主要关注训练成本而忽略了更大的推理成本，且现有能源测量工具无法提供可操作的反馈。

Method: 开发了能源消耗优化器（ECOpt），这是一个超参数调优器，专门优化能源效率和模型性能。ECOpt将这两个指标之间的权衡量化为可解释的帕累托前沿，使ML从业者能够在能源成本和环境影响方面做出明智决策。

Result: 研究发现参数数量和浮点运算次数不能可靠地代表能源消耗，观察到Transformer模型在文本生成任务上的能源效率在不同硬件上相对一致。ECOpt具有净正面的环境影响，并发现了7个在CIFAR-10数据集上同时考虑准确性和能源效率时优于现有最优水平的模型。

Conclusion: 研究结果支持测量和发布ML模型的能源指标的重要性，ECOpt工具能够帮助在模型性能和能源效率之间找到最佳平衡，同时满足新的法规要求，推动机器学习向更可持续的方向发展。

Abstract: The ubiquity of machine learning (ML) and the demand for ever-larger models bring an increase in energy consumption and environmental impact. However, little is known about the energy scaling laws in ML, and existing research focuses on training cost -- ignoring the larger cost of inference. Furthermore, tools for measuring the energy consumption of ML do not provide actionable feedback.
  To address these gaps, we developed Energy Consumption Optimiser (ECOpt): a hyperparameter tuner that optimises for energy efficiency and model performance. ECOpt quantifies the trade-off between these metrics as an interpretable Pareto frontier. This enables ML practitioners to make informed decisions about energy cost and environmental impact, while maximising the benefit of their models and complying with new regulations.
  Using ECOpt, we show that parameter and floating-point operation counts can be unreliable proxies for energy consumption, and observe that the energy efficiency of Transformer models for text generation is relatively consistent across hardware. These findings motivate measuring and publishing the energy metrics of ML models. We further show that ECOpt can have a net positive environmental impact and use it to uncover seven models for CIFAR-10 that improve upon the state of the art, when considering accuracy and energy efficiency together.

</details>


### [15] [Physics-Guided Counterfactual Explanations for Large-Scale Multivariate Time Series: Application in Scalable and Interpretable SEP Event Prediction](https://arxiv.org/abs/2601.08999)
*Pranjal Patil,Anli Ji,Berkay Aydin*

Main category: cs.LG

TL;DR: 提出物理引导的反事实解释框架，用于太阳高能粒子事件预测，在保持物理合理性的同时提升解释质量、稀疏性和计算效率


<details>
  <summary>Details</summary>
Motivation: 太阳高能粒子事件预测对保护卫星、宇航员和空间基础设施至关重要。现有机器学习模型虽然预测能力强，但大多忽略领域特定的可行性约束，反事实解释方法也缺乏物理合理性保证。

Method: 提出物理引导的反事实解释框架，在时间序列分类任务中生成符合物理原理的反事实解释。应用于太阳高能粒子预测，通过物理约束确保解释的合理性和可操作性。

Result: 相比DiCE等基线方法，动态时间规整距离减少80%以上（提升邻近性），生成的反事实解释稀疏性更高，运行时间减少近50%，同时确保物理合理性。

Conclusion: 该框架生成既有效又物理一致的反事实解释，为大数据环境中的可扩展反事实生成奠定基础，增强了科学领域机器学习模型的可解释性和实用性。

Abstract: Accurate prediction of solar energetic particle events is vital for safeguarding satellites, astronauts, and space-based infrastructure. Modern space weather monitoring generates massive volumes of high-frequency, multivariate time series (MVTS) data from sources such as the Geostationary perational Environmental Satellites (GOES). Machine learning (ML) models trained on this data show strong predictive power, but most existing methods overlook domain-specific feasibility constraints. Counterfactual explanations have emerged as a key tool for improving model interpretability, yet existing approaches rarely enforce physical plausibility. This work introduces a Physics-Guided Counterfactual Explanation framework, a novel method for generating counterfactual explanations in time series classification tasks that remain consistent with underlying physical principles. Applied to solar energetic particles (SEP) forecasting, this framework achieves over 80% reduction in Dynamic Time Warping (DTW) distance increasing the proximity, produces counterfactual explanations with higher sparsity, and reduces runtime by nearly 50% compared to state-of-the-art baselines such as DiCE. Beyond numerical improvements, this framework ensures that generated counterfactual explanations are physically plausible and actionable in scientific domains. In summary, the framework generates counterfactual explanations that are both valid and physically consistent, while laying the foundation for scalable counterfactual generation in big data environments.

</details>


### [16] [Universal Dynamics of Warmup Stable Decay: understanding WSD beyond Transformers](https://arxiv.org/abs/2601.09000)
*Annalisa Belloni,Lorenzo Noci,Antonio Orvieto*

Main category: cs.LG

TL;DR: WSD学习率调度器在训练大语言模型时表现出色，但研究者好奇其优异性能是否仅适用于transformer架构。通过比较WSD在语言模型和CNN上的优化路径，发现两者在训练信号、优化路径特征和锐度动态方面具有相似性，表明新旧非凸问题的损失景观具有共享的几何特性。


<details>
  <summary>Details</summary>
Motivation: 探究WSD学习率调度器的卓越性能是否仅限于transformer架构的语言模型，还是反映了更普遍的优化几何特性。通过比较不同架构的优化路径，了解高维非凸问题的损失景观几何特征。

Method: 比较WSD调度器在Adam优化器下，在Pythia-like语言模型和CIFAR10图像分类的小型CNN上的优化路径。分析训练信号、优化路径特征和锐度动态的相似性。

Result: 观察到两种架构在大多数训练信号、优化器路径特征和锐度动态方面具有定性相似性。这表明新旧非凸问题的损失景观具有共享的几何特性。

Conclusion: WSD的性能优势可能反映了更普遍的优化几何特性，而不仅限于transformer架构。这一发现为理解高维优化问题的几何特性提供了新的研究方向。

Abstract: The Warmup Stable Decay (WSD) learning rate scheduler has recently become popular, largely due to its good performance and flexibility when training large language models. It remains an open question whether the remarkable performance of WSD - using a decaying learning rate for only a fraction of training compared to cosine decay - is a phenomenon specific to transformer-based language models that can potentially offer new theoretical insights into their training dynamics. Inspired by the usage of learning rate schedulers as a new lens into understanding landscape geometry (e.g., river valley, connected minima, progressive sharpening), in this work we compare the WSD path of the Adam optimizer on a Pythia-like language model to that of a small CNN trained to classify CIFAR10 images. We observe most training signals, optimizer path features, and sharpness dynamics to be qualitatively similar in such architectures. This consistency points to shared geometric characteristics of the loss landscapes of old and new nonconvex problems, and hints to future research questions around the geometry of high dimensional optimization problems.

</details>


### [17] [Meta-learning to Address Data Shift in Time Series Classification](https://arxiv.org/abs/2601.09018)
*Samuel Myren,Nidhi Parikh,Natalie Klein*

Main category: cs.LG

TL;DR: 本文系统比较了传统深度学习与元学习方法在处理时间序列分类数据偏移问题上的表现，发现元学习在小数据量和模型规模下具有更快、更稳定的适应能力，但随着数据量和模型容量增加，其优势减弱。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据的动态性（数据偏移）导致传统深度学习模型性能快速下降，需要昂贵的重新标注和低效的重新训练。元学习能够快速适应新数据，为解决这一挑战提供了有前景的替代方案。

Method: 引入受控的任务导向地震基准数据集（SeisTask），系统比较传统深度学习与微调以及基于优化的元学习算法，评估它们在处理时间序列分类数据偏移方面的能力。研究任务多样性对元学习的影响。

Result: 元学习通常在数据稀缺情况下实现更快、更稳定的适应，减少过拟合，特别是在小型模型架构中。随着数据可用性和模型容量增加，其优势减弱，传统深度学习与微调表现相当。训练和测试分布的对齐（而非多样性本身）驱动性能提升。

Conclusion: 本研究系统评估了元学习在数据偏移情况下何时以及为何优于传统深度学习，并贡献SeisTask作为时间序列领域自适应学习研究的基准数据集。

Abstract: Across engineering and scientific domains, traditional deep learning (TDL) models perform well when training and test data share the same distribution. However, the dynamic nature of real-world data, broadly termed \textit{data shift}, renders TDL models prone to rapid performance degradation, requiring costly relabeling and inefficient retraining. Meta-learning, which enables models to adapt quickly to new data with few examples, offers a promising alternative for mitigating these challenges. Here, we systematically compare TDL with fine-tuning and optimization-based meta-learning algorithms to assess their ability to address data shift in time-series classification. We introduce a controlled, task-oriented seismic benchmark (SeisTask) and show that meta-learning typically achieves faster and more stable adaptation with reduced overfitting in data-scarce regimes and smaller model architectures. As data availability and model capacity increase, its advantages diminish, with TDL with fine-tuning performing comparably. Finally, we examine how task diversity influences meta-learning and find that alignment between training and test distributions, rather than diversity alone, drives performance gains. Overall, this work provides a systematic evaluation of when and why meta-learning outperforms TDL under data shift and contributes SeisTask as a benchmark for advancing adaptive learning research in time-series domains.

</details>


### [18] [SCaLE: Switching Cost aware Learning and Exploration](https://arxiv.org/abs/2601.09042)
*Neelkamal Bhuyan,Debankur Mukherjee,Adam Wierman*

Main category: cs.LG

TL;DR: SCaLE算法解决了带噪声反馈的在线凸优化中无界度量移动成本问题，首次实现了分布无关的次线性动态遗憾


<details>
  <summary>Details</summary>
Motivation: 解决在线凸优化中无界度量移动成本的基本问题，特别是在高维动态二次命中成本和ℓ₂范数切换成本的噪声反馈模型中

Method: 提出SCaLE算法，采用新颖的谱遗憾分析，分别量化特征值误差驱动的遗憾和特征基扰动驱动的遗憾

Result: 在一般随机环境下，首次实现了分布无关的次线性动态遗憾，无需命中成本结构知识；数值实验验证了算法有效性

Conclusion: SCaLE算法成功解决了无界度量移动成本问题，为噪声反馈模型下的在线凸优化提供了有效的解决方案

Abstract: This work addresses the fundamental problem of unbounded metric movement costs in bandit online convex optimization, by considering high-dimensional dynamic quadratic hitting costs and $\ell_2$-norm switching costs in a noisy bandit feedback model. For a general class of stochastic environments, we provide the first algorithm SCaLE that provably achieves a distribution-agnostic sub-linear dynamic regret, without the knowledge of hitting cost structure. En-route, we present a novel spectral regret analysis that separately quantifies eigenvalue-error driven regret and eigenbasis-perturbation driven regret. Extensive numerical experiments, against online-learning baselines, corroborate our claims, and highlight statistical consistency of our algorithm.

</details>


### [19] [Deep Incomplete Multi-View Clustering via Hierarchical Imputation and Alignment](https://arxiv.org/abs/2601.09051)
*Yiming Du,Ziyu Wang,Jian Li,Rui Ning,Lusi Li*

Main category: cs.LG

TL;DR: DIMVC-HIA：一种结合层次化填补和对齐的深度不完全多视图聚类框架，通过跨视图对比相似性估计缺失聚类分配，利用视图内、簇内统计重建缺失特征，提升聚类性能


<details>
  <summary>Details</summary>
Motivation: 不完全多视图聚类面临准确填补缺失视图而不引入偏差、保持跨视图语义一致性和簇内紧凑性的核心挑战。现有方法在同时处理这些挑战方面存在不足。

Method: 提出DIMVC-HIA框架，包含四个关键组件：1) 视图特定自编码器提取潜在特征，结合视图共享聚类预测器生成软聚类分配；2) 层次化填补模块，先基于跨视图对比相似性估计缺失聚类分配，再利用视图内、簇内统计重建缺失特征；3) 基于能量的语义对齐模块，通过最小化低能量簇锚点周围的能量方差促进簇内紧凑性；4) 对比分配对齐模块，增强跨视图一致性并鼓励自信、分离良好的聚类预测。

Result: 在基准数据集上的实验表明，该框架在不同缺失水平下实现了优越的性能。

Conclusion: DIMVC-HIA通过层次化填补和双重对齐机制有效解决了不完全多视图聚类的核心挑战，在准确填补缺失视图的同时保持了语义一致性和簇内紧凑性。

Abstract: Incomplete multi-view clustering (IMVC) aims to discover shared cluster structures from multi-view data with partial observations. The core challenges lie in accurately imputing missing views without introducing bias, while maintaining semantic consistency across views and compactness within clusters. To address these challenges, we propose DIMVC-HIA, a novel deep IMVC framework that integrates hierarchical imputation and alignment with four key components: (1) view-specific autoencoders for latent feature extraction, coupled with a view-shared clustering predictor to produce soft cluster assignments; (2) a hierarchical imputation module that first estimates missing cluster assignments based on cross-view contrastive similarity, and then reconstructs missing features using intra-view, intra-cluster statistics; (3) an energy-based semantic alignment module, which promotes intra-cluster compactness by minimizing energy variance around low-energy cluster anchors; and (4) a contrastive assignment alignment module, which enhances cross-view consistency and encourages confident, well-separated cluster predictions. Experiments on benchmarks demonstrate that our framework achieves superior performance under varying levels of missingness.

</details>


### [20] [Resolving Predictive Multiplicity for the Rashomon Set](https://arxiv.org/abs/2601.09071)
*Parian Haghighat,Hadis Anahideh,Cynthia Rudin*

Main category: cs.LG

TL;DR: 该论文提出三种方法来减少预测多重性（Rashomon集合中模型预测不一致的问题）：异常值修正、局部修补和成对协调，以提升模型预测的一致性。


<details>
  <summary>Details</summary>
Motivation: 在预测任务中，存在多个准确率相近但预测结果不同的模型（Rashomon集合），这种预测不一致性会降低高风险应用中的可信度。需要解决模型预测不一致的问题。

Method: 提出三种方法：1) 异常值修正：修正那些所有好模型都无法正确预测的异常样本；2) 局部修补：在测试点周围检测并修正模型偏差；3) 成对协调：找到在测试区域有分歧的模型对，修改不一致的预测。这些方法可单独或组合使用。

Result: 在多个数据集上的实验表明，这些方法在保持竞争力的准确率的同时，显著降低了模型间的分歧指标。

Conclusion: 提出的三种方法能有效减少Rashomon集合中模型的预测不一致性，协调后的预测可蒸馏为单个可解释模型用于实际部署，提升了模型预测的一致性。

Abstract: The existence of multiple, equally accurate models for a given predictive task leads to predictive multiplicity, where a ``Rashomon set'' of models achieve similar accuracy but diverges in their individual predictions. This inconsistency undermines trust in high-stakes applications where we want consistent predictions. We propose three approaches to reduce inconsistency among predictions for the members of the Rashomon set. The first approach is \textbf{outlier correction}. An outlier has a label that none of the good models are capable of predicting correctly. Outliers can cause the Rashomon set to have high variance predictions in a local area, so fixing them can lower variance. Our second approach is local patching. In a local region around a test point, models may disagree with each other because some of them are biased. We can detect and fix such biases using a validation set, which also reduces multiplicity. Our third approach is pairwise reconciliation, where we find pairs of models that disagree on a region around the test point. We modify predictions that disagree, making them less biased. These three approaches can be used together or separately, and they each have distinct advantages. The reconciled predictions can then be distilled into a single interpretable model for real-world deployment. In experiments across multiple datasets, our methods reduce disagreement metrics while maintaining competitive accuracy.

</details>


### [21] [SRT: Accelerating Reinforcement Learning via Speculative Rollout with Tree-Structured Cache](https://arxiv.org/abs/2601.09083)
*Chi-Chih Chang,Siqi Zhu,Zhichen Zeng,Haibin Lin,Jiaxuan You,Mohamed S. Abdelfattah,Ziheng Jiang,Xuehai Qian*

Main category: cs.LG

TL;DR: SRT是一种利用树结构缓存加速语言模型强化学习的方法，通过存储历史生成的续写内容作为草稿模型，在生成时进行推测解码，显著降低推理成本并提升训练速度。


<details>
  <summary>Details</summary>
Motivation: 在语言模型的强化学习中，相同提示在不同训练步骤生成的续写内容具有相似性，但传统方法每次都需要重新生成，导致计算成本高、训练速度慢。需要一种方法能够利用这种相似性来加速训练过程。

Method: SRT为每个提示维护一个树结构缓存，存储历史生成的续写内容。在生成时，当前策略使用这个树作为草稿模型进行推测解码。为了保持缓存新鲜度，系统在线更新树结构，并在GPU空闲时主动进行前瞻生成。

Result: SRT集成到标准RL流程（如PPO、GRPO、DAPO）和多轮对话设置中，能够持续降低生成延迟和步骤延迟，减少每个token的推理成本，在rollout阶段实现了最高2.08倍的实时加速。

Conclusion: SRT是一种简单、无模型的加速方法，能够在不牺牲分布正确性的前提下，显著加速语言模型的在线强化学习，通过利用历史生成内容的相似性和推测解码技术，有效降低了训练成本。

Abstract: We present Speculative Rollout with Tree-Structured Cache (SRT), a simple, model-free approach to accelerate on-policy reinforcement learning (RL) for language models without sacrificing distributional correctness. SRT exploits the empirical similarity of rollouts for the same prompt across training steps by storing previously generated continuations in a per-prompt tree-structured cache. During generation, the current policy uses this tree as the draft model for performing speculative decoding. To keep the cache fresh and improve draft model quality, SRT updates trees online from ongoing rollouts and proactively performs run-ahead generation during idle GPU bubbles. Integrated into standard RL pipelines (\textit{e.g.}, PPO, GRPO and DAPO) and multi-turn settings, SRT consistently reduces generation and step latency and lowers per-token inference cost, achieving up to 2.08x wall-clock time speedup during rollout.

</details>


### [22] [MMR-GRPO: Accelerating GRPO-Style Training through Diversity-Aware Reward Reweighting](https://arxiv.org/abs/2601.09085)
*Kangda Wei,Ruihong Huang*

Main category: cs.LG

TL;DR: MMR-GRPO通过整合最大边际相关性来基于完成多样性重新加权奖励，减少训练步骤47.9%和训练时间70.2%，同时保持可比性能


<details>
  <summary>Details</summary>
Motivation: GRPO训练数学推理模型需要每个提示多个完成，计算成本高。虽然最近工作减少了达到峰值性能所需的训练步骤，但由于每步成本更高，总体训练时间通常保持不变甚至增加

Method: 提出MMR-GRPO，整合最大边际相关性(MMR)来基于完成多样性重新加权奖励。核心洞察是语义冗余的完成贡献有限的学习信号；优先考虑多样化的解决方案能产生更多信息化的更新并加速收敛

Result: 在三个模型规模(1.5B, 7B, 8B)、三个GRPO变体和五个数学推理基准上的广泛评估显示，MMR-GRPO达到可比的峰值性能，同时平均需要减少47.9%的训练步骤和70.2%的训练时间。这些收益在不同模型、方法和基准上保持一致

Conclusion: MMR-GRPO通过基于完成多样性重新加权奖励，显著减少了GRPO训练的计算成本，同时保持性能，为高效训练数学推理模型提供了实用解决方案

Abstract: Group Relative Policy Optimization (GRPO) has become a standard approach for training mathematical reasoning models; however, its reliance on multiple completions per prompt makes training computationally expensive. Although recent work has reduced the number of training steps required to reach peak performance, the overall wall-clock training time often remains unchanged or even increases due to higher per-step cost. We propose MMR-GRPO, which integrates Maximal Marginal Relevance to reweigh rewards based on completion diversity. Our key insight is that semantically redundant completions contribute limited marginal learning signal; prioritizing diverse solutions yields more informative updates and accelerates convergence. Extensive evaluations across three model sizes (1.5B, 7B, 8B), three GRPO variants, and five mathematical reasoning benchmarks show that MMR-GRPO achieves comparable peak performance while requiring on average 47.9% fewer training steps and 70.2% less wall-clock time. These gains are consistent across models, methods, and benchmarks. We will release our code, trained models, and experimental protocols.

</details>


### [23] [Hidden States as Early Signals: Step-level Trace Evaluation and Pruning for Efficient Test-Time Scaling](https://arxiv.org/abs/2601.09093)
*Zhixiang Liang,Beichen Huang,Zheng Wang,Minjia Zhang*

Main category: cs.LG

TL;DR: STEP框架通过基于隐藏状态的步骤级评估和动态剪枝，在保持推理准确性的同时显著降低LLM推理延迟


<details>
  <summary>Details</summary>
Motivation: 现有基于相似性或置信度的剪枝方法不能可靠地指示推理轨迹质量，而多采样和长推理轨迹导致计算量大、端到端延迟高

Method: 提出STEP框架：训练轻量级步骤评分器评估推理步骤质量，设计GPU内存感知的剪枝策略，在KV缓存饱和时动态剪枝无前景的推理轨迹

Result: 在多个挑战性推理基准测试中，STEP相比self-consistency平均减少45%-70%的端到端推理延迟，同时提高了推理准确性

Conclusion: STEP通过步骤级评估和动态剪枝有效解决了LLM推理中的延迟问题，在保持准确性的同时显著提升推理效率

Abstract: Large Language Models (LLMs) can enhance reasoning capabilities through test-time scaling by generating multiple traces. However, the combination of lengthy reasoning traces with multiple sampling introduces substantial computation and high end-to-end latency. Prior work on accelerating this process has relied on similarity-based or confidence-based pruning, but these signals do not reliably indicate trace quality. To address these limitations, we propose STEP: Step-level Trace Evaluation and Pruning, a novel pruning framework that evaluates reasoning steps using hidden states and dynamically prunes unpromising traces during generation. We train a lightweight step scorer to estimate trace quality, and design a GPU memory-aware pruning strategy that triggers pruning as the GPU memory is saturated by KV cache to reduce end-to-end latency. Experiments across challenging reasoning benchmarks demonstrate that STEP reduces end-to-end inference latency by 45%-70% on average compared to self-consistency while also improving reasoning accuracy. Our code is released at: https://github.com/Supercomputing-System-AI-Lab/STEP

</details>


### [24] [Enhancing Imbalanced Electrocardiogram Classification: A Novel Approach Integrating Data Augmentation through Wavelet Transform and Interclass Fusion](https://arxiv.org/abs/2601.09103)
*Haijian Shao,Wei Liu,Xing Deng,Daze Lu*

Main category: cs.LG

TL;DR: 本文提出了一种改进的心电图分类器，通过基于小波变换的特征融合技术，同时解决了ECG数据中的类别不平衡和噪声问题，在CPSC 2018数据集上取得了优异的分类性能。


<details>
  <summary>Details</summary>
Motivation: 心电图数据存在类别不平衡问题，某些罕见心脏疾病在数据集中代表性不足，这影响了深度学习ECG分类算法的效果。同时，ECG采集过程中的噪声进一步增加了数据处理的复杂性。现有的数据增强和过采样技术在ECG分类中的有效性尚未达成共识。

Method: 提出基于小波变换的特征融合方法，特别是基于小波变换的类间融合，用于生成训练特征库和测试集特征库。然后将原始训练和测试数据与各自的特征数据库合并，从而获得更平衡的训练和测试数据集。

Result: 在CPSC 2018数据集上，ECG模型对Normal、AF、I-AVB、LBBB、RBBB、PAC、PVC、STD和STE等类别的识别准确率分别达到99%、98%、97%、98%、96%、92%和93%。这些类别的平均识别准确率在92%到98%之间。该方法在CPSC 2018数据集上的ECG分类准确率超过了任何已知算法。

Conclusion: 提出的数据融合方法有效解决了ECG分析中的类别不平衡和噪声问题，显著提升了心电图分类的准确率，为自动化心血管诊断信息处理提供了更有效的解决方案。

Abstract: Imbalanced electrocardiogram (ECG) data hampers the efficacy and resilience of algorithms in the automated processing and interpretation of cardiovascular diagnostic information, which in turn impedes deep learning-based ECG classification. Notably, certain cardiac conditions that are infrequently encountered are disproportionately underrepresented in these datasets. Although algorithmic generation and oversampling of specific ECG signal types can mitigate class skew, there is a lack of consensus regarding the effectiveness of such techniques in ECG classification. Furthermore, the methodologies and scenarios of ECG acquisition introduce noise, further complicating the processing of ECG data. This paper presents a significantly enhanced ECG classifier that simultaneously addresses both class imbalance and noise-related challenges in ECG analysis, as observed in the CPSC 2018 dataset. Specifically, we propose the application of feature fusion based on the wavelet transform, with a focus on wavelet transform-based interclass fusion, to generate the training feature library and the test set feature library. Subsequently, the original training and test data are amalgamated with their respective feature databases, resulting in more balanced training and test datasets. Employing this approach, our ECG model achieves recognition accuracies of up to 99%, 98%, 97%, 98%, 96%, 92%, and 93% for Normal, AF, I-AVB, LBBB, RBBB, PAC, PVC, STD, and STE, respectively. Furthermore, the average recognition accuracy for these categories ranges between 92\% and 98\%. Notably, our proposed data fusion methodology surpasses any known algorithms in terms of ECG classification accuracy in the CPSC 2018 dataset.

</details>


### [25] [EvasionBench: Detecting Evasive Answers in Financial Q&A via Multi-Model Consensus and LLM-as-Judge](https://arxiv.org/abs/2601.09142)
*Shijian Ma,Yan Lin,Yi Yang*

Main category: cs.LG

TL;DR: 提出了EvasionBench数据集和Eva-4B模型，用于检测财报电话会议中的回避性回答，通过多模型标注框架挖掘边界案例，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 检测财报电话会议中的回避性回答对金融透明度至关重要，但缺乏大规模基准数据集阻碍了相关研究的进展。

Method: 引入EvasionBench数据集（3万训练样本+1000人工标注测试样本），采用多模型标注框架：利用前沿大语言模型之间的分歧识别困难样本，通过法官模型解决标签冲突。

Result: 该方法比单模型蒸馏性能提升2.4%，法官解决的样本虽训练损失更高（0.421 vs 0.393）但泛化能力更好。训练的Eva-4B模型（40亿参数）准确率达81.3%，比基础模型提升25个百分点，接近前沿大语言模型性能但推理成本大幅降低。

Conclusion: 分歧挖掘可作为隐式正则化方法，多模型标注框架能有效识别有价值的困难样本，显著提升回避性回答检测模型的性能。

Abstract: Detecting evasive answers in earnings calls is critical for financial transparency, yet progress is hindered by the lack of large-scale benchmarks. We introduce EvasionBench, comprising 30,000 training samples and 1,000 human-annotated test samples (Cohen's Kappa 0.835) across three evasion levels. Our key contribution is a multi-model annotation framework leveraging a core insight: disagreement between frontier LLMs signals hard examples most valuable for training. We mine boundary cases where two strong annotators conflict, using a judge to resolve labels. This approach outperforms single-model distillation by 2.4 percent, with judge-resolved samples improving generalization despite higher training loss (0.421 vs 0.393) - evidence that disagreement mining acts as implicit regularization. Our trained model Eva-4B (4B parameters) achieves 81.3 percent accuracy, outperforming its base by 25 percentage points and approaching frontier LLM performance at a fraction of inference cost.

</details>


### [26] [Discrete Solution Operator Learning for Geometry-Dependent PDEs](https://arxiv.org/abs/2601.09143)
*Jinshuai Bai,Haolin Li,Zahra Sharif Khodaei,M. H. Aliabadi,YuanTong Gu,Xi-Qiao Feng*

Main category: cs.LG

TL;DR: DiSOL是一种离散解算子学习方法，通过模仿经典离散化过程来学习几何依赖PDE的求解程序，而非连续函数空间算子，能处理几何变化引起的离散结构突变。


<details>
  <summary>Details</summary>
Motivation: 传统神经算子学习基于连续函数空间映射，但在工程应用中，几何变化会导致离散结构突变（如拓扑变化、边界条件突变、计算域变化），破坏了平滑变化的前提，需要新的学习范式。

Method: DiSOL将求解器分解为可学习阶段：局部贡献编码、多尺度组装、在嵌入网格上的隐式解重构，这些阶段模仿经典离散化过程，保持过程级一致性同时适应几何相关的离散结构。

Result: 在几何依赖的泊松方程、对流扩散、线性弹性以及时空热传导问题中，DiSOL在分布内和强分布外几何（包括不连续边界和拓扑变化）下都能产生稳定准确的预测。

Conclusion: DiSOL展示了在几何主导机制中需要程序化算子表示，将离散解算子学习定位为科学机器学习中一个独特且互补的方向。

Abstract: Neural operator learning accelerates PDE solution by approximating operators as mappings between continuous function spaces. Yet in many engineering settings, varying geometry induces discrete structural changes, including topological changes, abrupt changes in boundary conditions or boundary types, and changes in the effective computational domain, which break the smooth-variation premise. Here we introduce Discrete Solution Operator Learning (DiSOL), a complementary paradigm that learns discrete solution procedures rather than continuous function-space operators. DiSOL factorizes the solver into learnable stages that mirror classical discretizations: local contribution encoding, multiscale assembly, and implicit solution reconstruction on an embedded grid, thereby preserving procedure-level consistency while adapting to geometry-dependent discrete structures. Across geometry-dependent Poisson, advection-diffusion, linear elasticity, as well as spatiotemporal heat-conduction problems, DiSOL produces stable and accurate predictions under both in-distribution and strongly out-of-distribution geometries, including discontinuous boundaries and topological changes. These results highlight the need for procedural operator representations in geometry-dominated regimes and position discrete solution operator learning as a distinct, complementary direction in scientific machine learning.

</details>


### [27] [KTCF: Actionable Recourse in Knowledge Tracing via Counterfactual Explanations for Education](https://arxiv.org/abs/2601.09156)
*Woojin Kim,Changkwon Lee,Hyeoncheol Kim*

Main category: cs.LG

TL;DR: 该论文提出KTCF方法，为知识追踪生成反事实解释，并将其转化为教育指导序列，以提升AI在教育中的可解释性和实用性。


<details>
  <summary>Details</summary>
Motivation: 利用人工智能改善教学需要更好的适应性和可扩展性。知识追踪在教育中具有重要应用价值，但需要可解释的AI方法。反事实解释具有可操作性、因果性和本地性，易于非专业的教育利益相关者理解，因此研究如何为知识追踪生成反事实解释具有重要意义。

Method: 提出KTCF方法：1）为知识追踪生成反事实解释，考虑知识概念之间的关系；2）通过后处理方案将反事实解释转化为教育指导序列。

Result: 在大规模教育数据集上的实验表明，KTCF方法相比现有方法有显著提升，各项指标改善范围从5.7%到34%。定性评估显示，生成的教育指导有助于减轻学生的学习负担。

Conclusion: 反事实解释有潜力推动AI在教育中的负责任和实际应用。未来可解释AI在知识追踪方面的研究可以从教育基础的概念化和以利益相关者为中心的方法中受益。

Abstract: Using Artificial Intelligence to improve teaching and learning benefits greater adaptivity and scalability in education. Knowledge Tracing (KT) is recognized for student modeling task due to its superior performance and application potential in education. To this end, we conceptualize and investigate counterfactual explanation as the connection from XAI for KT to education. Counterfactual explanations offer actionable recourse, are inherently causal and local, and easy for educational stakeholders to understand who are often non-experts. We propose KTCF, a counterfactual explanation generation method for KT that accounts for knowledge concept relationships, and a post-processing scheme that converts a counterfactual explanation into a sequence of educational instructions. We experiment on a large-scale educational dataset and show our KTCF method achieves superior and robust performance over existing methods, with improvements ranging from 5.7% to 34% across metrics. Additionally, we provide a qualitative evaluation of our post-processing scheme, demonstrating that the resulting educational instructions help in reducing large study burden. We show that counterfactuals have the potential to advance the responsible and practical use of AI in education. Future works on XAI for KT may benefit from educationally grounded conceptualization and developing stakeholder-centered methods.

</details>


### [28] [Efficient Clustering in Stochastic Bandits](https://arxiv.org/abs/2601.09162)
*G Dhinesh Chandran,Kota Srinivas Reddy,Srikrishna Bhashyam*

Main category: cs.LG

TL;DR: 提出EBC和EBC-H两种高效的多臂赌博机聚类算法，在保证渐近最优性的同时显著降低计算复杂度，适用于更广泛的向量参数分布族。


<details>
  <summary>Details</summary>
Motivation: 现有Bandit Clustering算法在固定置信度设置下存在两个主要问题：1）仅适用于高斯分布假设，限制了应用范围；2）每次采样都需要求解优化问题，计算成本高昂。需要设计既能处理更广泛分布族又计算高效的算法。

Method: 提出EBC算法，采用增量优化策略，每次采样时不求解完整优化问题，而是向最优值迈进一步，保持渐近最优性同时降低计算复杂度。进一步提出EBC-H启发式变体，利用停止规则中已计算的量进行臂选择，进一步简化采样规则。

Result: EBC和EBC-H在合成数据集上验证了渐近最优性，在合成和真实数据集上均优于现有方法。计算效率方面，EBC和EBC-H的每样本运行时间显著低于现有算法。

Conclusion: EBC和EBC-H算法在保持渐近最优性的同时显著提高了计算效率，适用于更广泛的分布族，为Bandit Clustering问题提供了实用的解决方案。

Abstract: We study the Bandit Clustering (BC) problem under the fixed confidence setting, where the objective is to group a collection of data sequences (arms) into clusters through sequential sampling from adaptively selected arms at each time step while ensuring a fixed error probability at the stopping time. We consider a setting where arms in a cluster may have different distributions. Unlike existing results in this setting, which assume Gaussian-distributed arms, we study a broader class of vector-parametric distributions that satisfy mild regularity conditions. Existing asymptotically optimal BC algorithms require solving an optimization problem as part of their sampling rule at each step, which is computationally costly. We propose an Efficient Bandit Clustering algorithm (EBC), which, instead of solving the full optimization problem, takes a single step toward the optimal value at each time step, making it computationally efficient while remaining asymptotically optimal. We also propose a heuristic variant of EBC, called EBC-H, which further simplifies the sampling rule, with arm selection based on quantities computed as part of the stopping rule. We highlight the computational efficiency of EBC and EBC-H by comparing their per-sample run time with that of existing algorithms. The asymptotic optimality of EBC is supported through simulations on the synthetic datasets. Through simulations on both synthetic and real-world datasets, we show the performance gain of EBC and EBC-H over existing approaches.

</details>


### [29] [Multi-Teacher Ensemble Distillation: A Mathematical Framework for Probability-Domain Knowledge Aggregation](https://arxiv.org/abs/2601.09165)
*Aaron R. Flouro,Shawn P. Chadwick*

Main category: cs.LG

TL;DR: 提出基于Sparse-KD概率域蒸馏框架的多教师集成知识蒸馏公理化算子理论框架，定义五个核心公理，证明满足这些公理的算子族存在且不唯一，并建立算子无关的理论保证。


<details>
  <summary>Details</summary>
Motivation: 为多教师集成知识蒸馏提供理论基础，避免规定具体的聚合公式，而是建立公理化框架来指导有效的知识聚合操作，为从不同前沿模型进行多教师蒸馏提供理论支持。

Method: 基于Sparse-KD的概率域蒸馏框架，提出算子理论框架，定义五个核心公理：凸性、正性、连续性、权重单调性和温度一致性。证明满足这些公理的算子族存在且不唯一，建立算子无关的理论保证。

Result: 证明多教师聚合能减少随机方差和系统性监督偏差，提供Jensen型边界、对数损失保证和安全衰减特性。对于线性权重算子，在标准独立性假设下建立经典集成方差减少结果，并扩展到相关误差机制。

Conclusion: 该框架为从多样化前沿模型进行多教师蒸馏提供了理论基础，同时允许多种有效的实现策略，为多教师知识蒸馏建立了公理化的理论框架。

Abstract: Building on the probability-domain distillation framework of Sparse-KD, we develop an axiomatic, operator-theoretic framework for multi-teacher ensemble knowledge distillation. Rather than prescribing a specific aggregation formula, we define five core axioms governing valid knowledge aggregation operators, encompassing convexity, positivity, continuity, weight monotonicity, and temperature coherence. We prove the existence and non-uniqueness of operator families satisfying these axioms, establishing that multiple distinct aggregation mechanisms conform to the same foundational principles.
  Within this framework, we establish operator-agnostic guarantees showing that multi-teacher aggregation reduces both stochastic variance and systematic supervisory bias under heterogeneous teachers, while providing Jensen-type bounds, log-loss guarantees, and safety attenuation properties. For aggregation operators linear in teacher weights, we further establish classical ensemble variance-reduction results under standard independence assumptions, with extensions to correlated-error regimes. The framework provides theoretical grounding for multi-teacher distillation from diverse frontier models while admitting multiple valid implementation strategies.

</details>


### [30] [DP-FEDSOFIM: Differentially Private Federated Stochastic Optimization using Regularized Fisher Information Matrix](https://arxiv.org/abs/2601.09166)
*Sidhant R. Nair,Tanmay Sen,Mrinmay Sen*

Main category: cs.LG

TL;DR: DP-FedSOFIM：一种服务器端二阶优化框架，通过Fisher信息矩阵作为预处理器，在差分隐私联邦学习中实现O(d)内存和计算复杂度，显著提升收敛速度和准确性。


<details>
  <summary>Details</summary>
Motivation: 差分隐私联邦学习在严格隐私预算下收敛缓慢，现有二阶方法需要O(d²)内存，不适用于高维模型。需要开发既高效又实用的二阶优化方法。

Method: 提出DP-FedSOFIM框架，利用Fisher信息矩阵作为自然梯度预处理器，通过Sherman-Morrison公式实现高效矩阵求逆，客户端仅需O(d)内存，服务器端进行预处理器更新。

Result: 在CIFAR-10数据集上的实验表明，DP-FedSOFIM在多个隐私机制下相比一阶基线方法获得更优的测试准确率，同时保持(ε,δ)-差分隐私。

Conclusion: DP-FedSOFIM成功解决了差分隐私联邦学习中二阶方法的内存瓶颈问题，实现了高效、实用的二阶优化，为隐私保护机器学习提供了新思路。

Abstract: Differentially private federated learning (DP-FL) suffers from slow convergence under tight privacy budgets due to the overwhelming noise introduced to preserve privacy. While adaptive optimizers can accelerate convergence, existing second-order methods such as DP-FedNew require O(d^2) memory at each client to maintain local feature covariance matrices, making them impractical for high-dimensional models. We propose DP-FedSOFIM, a server-side second-order optimization framework that leverages the Fisher Information Matrix (FIM) as a natural gradient preconditioner while requiring only O(d) memory per client. By employing the Sherman-Morrison formula for efficient matrix inversion, DP-FedSOFIM achieves O(d) computational complexity per round while maintaining the convergence benefits of second-order methods. Our analysis proves that the server-side preconditioning preserves (epsilon, delta)-differential privacy through the post-processing theorem. Empirical evaluation on CIFAR-10 demonstrates that DP-FedSOFIM achieves superior test accuracy compared to first-order baselines across multiple privacy regimes.

</details>


### [31] [Geometric Stability: The Missing Axis of Representations](https://arxiv.org/abs/2601.09173)
*Prashant C. Raju*

Main category: cs.LG

TL;DR: 论文提出"几何稳定性"作为表征分析的新维度，与传统的相似性度量正交，能更可靠地检测表征结构的鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有表征分析主要关注相似性度量，只能揭示表征了什么，但无法评估表征结构是否鲁棒。需要一个新的维度来量化表征几何在扰动下的可靠性

Method: 提出Shesha框架来测量几何稳定性，在7个领域的2463个配置中进行实验，比较稳定性与相似性的关系，并应用于安全监控、可控性和模型选择等场景

Result: 稳定性与相似性几乎不相关（ρ≈0.01），稳定性对细粒度流形结构保持敏感，而相似性度量在移除主成分后会崩溃。稳定性在安全监控中比CKA敏感近2倍，能预测线性可控性（ρ=0.89-0.96），还能揭示迁移优化带来的几何代价

Conclusion: 几何稳定性作为相似性的必要补充，为生物和计算系统的表征审计提供了新维度，能更可靠地评估系统如何保持结构

Abstract: Analysis of learned representations has a blind spot: it focuses on $similarity$, measuring how closely embeddings align with external references, but similarity reveals only what is represented, not whether that structure is robust. We introduce $geometric$ $stability$, a distinct dimension that quantifies how reliably representational geometry holds under perturbation, and present $Shesha$, a framework for measuring it. Across 2,463 configurations in seven domains, we show that stability and similarity are empirically uncorrelated ($ρ\approx 0.01$) and mechanistically distinct: similarity metrics collapse after removing the top principal components, while stability retains sensitivity to fine-grained manifold structure. This distinction yields actionable insights: for safety monitoring, stability acts as a functional geometric canary, detecting structural drift nearly 2$\times$ more sensitively than CKA while filtering out the non-functional noise that triggers false alarms in rigid distance metrics; for controllability, supervised stability predicts linear steerability ($ρ= 0.89$-$0.96$); for model selection, stability dissociates from transferability, revealing a geometric tax that transfer optimization incurs. Beyond machine learning, stability predicts CRISPR perturbation coherence and neural-behavioral coupling. By quantifying $how$ $reliably$ systems maintain structure, geometric stability provides a necessary complement to similarity for auditing representations across biological and computational systems.

</details>


### [32] [$D^2Prune$: Sparsifying Large Language Models via Dual Taylor Expansion and Attention Distribution Awareness](https://arxiv.org/abs/2601.09176)
*Lang Xiong,Ning Liu,Ao Ren,Yuheng Bai,Haining Fang,BinYan Zhang,Zhe Jiang,Yujuan Tan,Duo Liu*

Main category: cs.LG

TL;DR: D²Prune是一种针对大语言模型的新型剪枝方法，通过双泰勒展开联合建模权重和激活扰动实现精确误差估计，并结合注意力感知的动态更新策略保留注意力长尾分布，在各种LLM和视觉模型上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有剪枝方法存在两个关键局限：1) 忽略校准数据与测试数据之间的激活分布偏移，导致误差估计不准确；2) 忽视注意力模块中激活的长尾分布特性。这些限制影响了剪枝效果和模型性能。

Method: 提出D²Prune方法：1) 使用双泰勒展开联合建模权重和激活扰动，实现精确误差估计，指导剪枝掩码选择和权重更新；2) 提出注意力感知的动态更新策略，通过联合最小化注意力分布的KL散度和重构误差来保留长尾注意力模式。

Result: D²Prune在各种LLM（如OPT-125M、LLaMA2/3、Qwen3）上持续优于最先进方法。注意力动态更新机制在ViT视觉模型（如DeiT）上也表现良好，在ImageNet-1K上实现了优越的准确率。

Conclusion: D²Prune通过解决现有剪枝方法的两个关键局限，提供了一种更精确、更有效的模型压缩方法，不仅适用于LLM，还能推广到视觉Transformer模型，具有广泛的适用性和优越性能。

Abstract: Large language models (LLMs) face significant deployment challenges due to their massive computational demands. % While pruning offers a promising compression solution, existing methods suffer from two critical limitations: (1) They neglect activation distribution shifts between calibration data and test data, resulting in inaccurate error estimations; (2) They overlook the long-tail distribution characteristics of activations in the attention module. To address these limitations, this paper proposes a novel pruning method, $D^2Prune$. First, we propose a dual Taylor expansion-based method that jointly models weight and activation perturbations for precise error estimation, leading to precise pruning mask selection and weight updating and facilitating error minimization during pruning. % Second, we propose an attention-aware dynamic update strategy that preserves the long-tail attention pattern by jointly minimizing the KL divergence of attention distributions and the reconstruction error. Extensive experiments show that $D^2Prune$ consistently outperforms SOTA methods across various LLMs (e.g., OPT-125M, LLaMA2/3, and Qwen3). Moreover, the dynamic attention update mechanism also generalizes well to ViT-based vision models like DeiT, achieving superior accuracy on ImageNet-1K.

</details>


### [33] [From Hawkes Processes to Attention: Time-Modulated Mechanisms for Event Sequences](https://arxiv.org/abs/2601.09220)
*Xinzi Tan,Kejian Zhang,Junhan Yu,Doudou Zhou*

Main category: cs.LG

TL;DR: 提出了一种基于霍克斯过程的Hawkes Attention机制，用于标记时间点过程，通过可学习的类型特定神经核来改进传统注意力机制对异质时间效应的建模能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的方法主要通过位置编码注入时间信息，依赖于共享或参数化衰减结构，限制了捕捉异质和类型特定时间效应的能力。

Method: 从多元霍克斯过程理论推导出Hawkes Attention算子，使用可学习的类型特定神经核来调制查询、键和值投影，替代传统注意力中的相应部分。

Result: 实验结果显示该方法相比基线取得了更好的性能，能够统一事件时间和内容交互，从数据中学习时间相关行为和类型特定的激发模式。

Conclusion: Hawkes Attention机制不仅适用于一般标记时间点过程，还可以轻松应用于特定时间结构，如时间序列预测。

Abstract: Marked Temporal Point Processes (MTPPs) arise naturally in medical, social, commercial, and financial domains. However, existing Transformer-based methods mostly inject temporal information only via positional encodings, relying on shared or parametric decay structures, which limits their ability to capture heterogeneous and type-specific temporal effects. Inspired by this observation, we derive a novel attention operator called Hawkes Attention from the multivariate Hawkes process theory for MTPP, using learnable per-type neural kernels to modulate query, key and value projections, thereby replacing the corresponding parts in the traditional attention. Benefited from the design, Hawkes Attention unifies event timing and content interaction, learning both the time-relevant behavior and type-specific excitation patterns from the data. The experimental results show that our method achieves better performance compared to the baselines. In addition to the general MTPP, our attention mechanism can also be easily applied to specific temporal structures, such as time series forecasting.

</details>


### [34] [GIFT: Unlocking Global Optimality in Post-Training via Finite-Temperature Gibbs Initialization](https://arxiv.org/abs/2601.09233)
*Zhengyang Zhao,Lu Ma,Yizhen Jiang,Xiaochen Ma,Zimo Meng,Chengyu Shen,Lexiang Tang,Haoze Sun,Peng Pei,Wentao Zhang*

Main category: cs.LG

TL;DR: 论文提出GIFT方法解决大推理模型后训练中SFT与RL的优化不匹配问题，通过有限温度吉布斯初始化建立分布桥梁，显著提升RL初始化效果。


<details>
  <summary>Details</summary>
Motivation: 当前大推理模型的后训练范式（SFT+RL）存在内在优化不匹配：SFT的刚性监督导致分布坍缩，耗尽后续RL所需的探索空间。

Method: 将SFT重新表述为统一后训练框架，提出GIFT方法。将标准SFT视为抑制基础先验的零温度退化极限，而GIFT将监督作为有限温度能量势，建立分布桥梁确保目标一致性。

Result: 实验表明GIFT作为RL初始化时显著优于标准SFT和其他竞争基线，为后训练实现全局最优性提供了数学原理上的路径。

Conclusion: GIFT通过有限温度吉布斯初始化解决了SFT与RL的优化不匹配问题，建立了数学原理上的后训练优化框架，显著提升大推理模型的训练效果。

Abstract: The prevailing post-training paradigm for Large Reasoning Models (LRMs)--Supervised Fine-Tuning (SFT) followed by Reinforcement Learning (RL)--suffers from an intrinsic optimization mismatch: the rigid supervision inherent in SFT induces distributional collapse, thereby exhausting the exploration space necessary for subsequent RL. In this paper, we reformulate SFT within a unified post-training framework and propose Gibbs Initialization with Finite Temperature (GIFT). We characterize standard SFT as a degenerate zero-temperature limit that suppresses base priors. Conversely, GIFT incorporates supervision as a finite-temperature energy potential, establishing a distributional bridge that ensures objective consistency throughout the post-training pipeline. Our experiments demonstrate that GIFT significantly outperforms standard SFT and other competitive baselines when utilized for RL initialization, providing a mathematically principled pathway toward achieving global optimality in post-training. Our code is available at https://github.com/zzy1127/GIFT.

</details>


### [35] [Reward Learning through Ranking Mean Squared Error](https://arxiv.org/abs/2601.09236)
*Chaitanya Kharyal,Calarina Muslimani,Matthew E. Taylor*

Main category: cs.LG

TL;DR: R4是一种新的基于评分的强化学习方法，使用排序均方误差损失函数，将人类评分作为序数目标，从轨迹-评分对中学习，相比现有方法需要更少反馈且性能更好。


<details>
  <summary>Details</summary>
Motivation: 奖励设计是强化学习应用于现实问题的瓶颈，传统方法需要手动指定奖励函数。虽然已有从人类反馈中学习奖励函数的方法，但现有基于评分的方法缺乏理论保证，且反馈效率有待提高。

Method: 提出R4方法，核心是排序均方误差损失函数。从轨迹-评分对数据集中学习，每个轨迹带有离散评分。训练时采样一组轨迹，预测其回报，使用可微分排序算子获得软排序，然后优化软排序与教师评分之间的均方误差损失。

Result: R4在模拟人类反馈下，在OpenAI Gym和DeepMind Control Suite的机器人运动基准测试中，始终匹配或优于现有的基于评分和偏好的强化学习方法，同时需要显著更少的反馈。

Conclusion: R4提供了一种有效的基于评分的强化学习方法，具有形式化理论保证，能够从更丰富的评分反馈中学习，减少人类监督负担，在机器人控制任务中表现出色。

Abstract: Reward design remains a significant bottleneck in applying reinforcement learning (RL) to real-world problems. A popular alternative is reward learning, where reward functions are inferred from human feedback rather than manually specified. Recent work has proposed learning reward functions from human feedback in the form of ratings, rather than traditional binary preferences, enabling richer and potentially less cognitively demanding supervision. Building on this paradigm, we introduce a new rating-based RL method, Ranked Return Regression for RL (R4). At its core, R4 employs a novel ranking mean squared error (rMSE) loss, which treats teacher-provided ratings as ordinal targets. Our approach learns from a dataset of trajectory-rating pairs, where each trajectory is labeled with a discrete rating (e.g., "bad," "neutral," "good"). At each training step, we sample a set of trajectories, predict their returns, and rank them using a differentiable sorting operator (soft ranks). We then optimize a mean squared error loss between the resulting soft ranks and the teacher's ratings. Unlike prior rating-based approaches, R4 offers formal guarantees: its solution set is provably minimal and complete under mild assumptions. Empirically, using simulated human feedback, we demonstrate that R4 consistently matches or outperforms existing rating and preference-based RL methods on robotic locomotion benchmarks from OpenAI Gym and the DeepMind Control Suite, while requiring significantly less feedback.

</details>


### [36] [XLinear: A Lightweight and Accurate MLP-Based Model for Long-Term Time Series Forecasting with Exogenous Inputs](https://arxiv.org/abs/2601.09237)
*Xinyang Chen,Huidong Jin,Yu Huang,Zaiwen Feng*

Main category: cs.LG

TL;DR: 提出XLinear模型，一个基于MLP的轻量级时间序列预测模型，通过全局token和sigmoid激活的MLP有效利用外生变量信息，在准确性和效率上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现实应用中变量重要性通常不对称，外生变量（如天气）可单向影响内生变量（如湖泊表面温度），而现有Transformer模型计算成本高且存在排列不变性问题，基于patch的变体可能错过局部时间模式。

Method: 提出XLinear模型：1）使用内生变量衍生的全局token作为与外生变量交互的枢纽；2）采用带sigmoid激活的MLP提取时间模式和变量间依赖关系；3）预测头整合这些信号进行内生序列预测。

Result: 在7个标准基准和5个带外生输入的真实数据集上评估，XLinear在多元预测和受外生输入影响的单变量预测方面，相比最先进模型都表现出更优的准确性和效率。

Conclusion: XLinear通过有效利用时间维度和相关外生变量的信息信号，提供了一种轻量级但强大的时间序列预测解决方案，特别适用于存在不对称因果关系和不同数据获取成本的实际应用场景。

Abstract: Despite the prevalent assumption of uniform variable importance in long-term time series forecasting models, real world applications often exhibit asymmetric causal relationships and varying data acquisition costs. Specifically, cost-effective exogenous data (e.g., local weather) can unilaterally influence dynamics of endogenous variables, such as lake surface temperature. Exploiting these links enables more effective forecasts when exogenous inputs are readily available. Transformer-based models capture long-range dependencies but incur high computation and suffer from permutation invariance. Patch-based variants improve efficiency yet can miss local temporal patterns. To efficiently exploit informative signals across both the temporal dimension and relevant exogenous variables, this study proposes XLinear, a lightweight time series forecasting model built upon MultiLayer Perceptrons (MLPs). XLinear uses a global token derived from an endogenous variable as a pivotal hub for interacting with exogenous variables, and employs MLPs with sigmoid activation to extract both temporal patterns and variate-wise dependencies. Its prediction head then integrates these signals to forecast the endogenous series. We evaluate XLinear on seven standard benchmarks and five real-world datasets with exogenous inputs. Compared with state-of-the-art models, XLinear delivers superior accuracy and efficiency for both multivariate forecasts and univariate forecasts influenced by exogenous inputs.

</details>


### [37] [RIFT: Repurposing Negative Samples via Reward-Informed Fine-Tuning](https://arxiv.org/abs/2601.09253)
*Zehua Liu,Shuqi Liu,Tao Zhong,Mingxuan Yuan*

Main category: cs.LG

TL;DR: RIFT是一种利用所有自生成样本进行LLM对齐的奖励感知微调框架，通过重新加权损失函数同时学习正负轨迹，解决了SFT依赖专家数据和RFT丢弃负样本的数据效率问题。


<details>
  <summary>Details</summary>
Motivation: 传统的监督微调(SFT)需要昂贵的专家数据，而拒绝采样微调(RFT)会丢弃有价值的负样本，导致数据效率低下。需要一种能够利用所有自生成样本的微调方法。

Method: 提出奖励感知微调(RIFT)框架，不同于RFT的硬阈值方法，RIFT重新利用负轨迹，通过标量奖励重新加权损失函数，从模型输出的正负轨迹中同时学习。为避免朴素奖励集成导致的训练崩溃，引入了稳定的损失公式确保数值鲁棒性和优化效率。

Result: 在各种基础模型的数学基准测试中，RIFT始终优于RFT，证明了其在使用混合质量自生成数据进行对齐时的鲁棒性和数据效率优势。

Conclusion: RIFT是一种鲁棒且数据高效的替代方案，能够有效利用混合质量的自生成数据进行LLM对齐，解决了现有方法的数据效率问题。

Abstract: While Supervised Fine-Tuning (SFT) and Rejection Sampling Fine-Tuning (RFT) are standard for LLM alignment, they either rely on costly expert data or discard valuable negative samples, leading to data inefficiency. To address this, we propose Reward Informed Fine-Tuning (RIFT), a simple yet effective framework that utilizes all self-generated samples. Unlike the hard thresholding of RFT, RIFT repurposes negative trajectories, reweighting the loss with scalar rewards to learn from both the positive and negative trajectories from the model outputs. To overcome the training collapse caused by naive reward integration, where direct multiplication yields an unbounded loss, we introduce a stabilized loss formulation that ensures numerical robustness and optimization efficiency. Extensive experiments on mathematical benchmarks across various base models show that RIFT consistently outperforms RFT. Our results demonstrate that RIFT is a robust and data-efficient alternative for alignment using mixed-quality, self-generated data.

</details>


### [38] [Learning to Trust Experience: A Monitor-Trust-Regulator Framework for Learning under Unobservable Feedback Reliability](https://arxiv.org/abs/2601.09261)
*Zhipeng Zhang,Zhenjie Yao,Kai Li,Lei Yang*

Main category: cs.LG

TL;DR: 论文提出在不可观测反馈可靠性下的学习问题，引入元认知调节框架，通过自我诊断机制改善认知可识别性。


<details>
  <summary>Details</summary>
Motivation: 在不可观测反馈可靠性（EIUR）的学习环境中，传统鲁棒学习方法可能收敛到高置信度但系统错误的信念，因为无法区分可靠和不可靠的反馈，且数据由学习者自身信念和行动闭环生成。

Method: 提出元认知调节框架，采用Monitor-Trust-Regulator（MTR）模块化分解，实例化为自我诊断机制。该机制维护缓慢变化的经验信任变量，软调节学习更新，无需外部可靠性标签或显式腐败模型。

Result: 在EIUR机制下，自我诊断与改善的认知可识别性相关。在强化学习中实现校准的怀疑和系统腐败奖励下的恢复；在监督学习中揭示了性能恢复与认知恢复的分离：准确率可能反弹而内部信念动态仍被早期误导数据锁定。

Conclusion: MTR框架和自我诊断机制为不可观测可靠性下的自主学习提供了组织抽象和具体设计模板，通过内省诊断实现内在可靠性评估。

Abstract: Learning under unobservable feedback reliability poses a distinct challenge beyond optimization robustness: a system must decide whether to learn from an experience, not only how to learn stably. We study this setting as Epistemic Identifiability under Unobservable Reliability (EIUR), where each experience has a latent credibility, reliable and unreliable feedback can be locally indistinguishable, and data are generated in a closed loop by the learner's own evolving beliefs and actions. In EIUR, standard robust learning can converge stably yet form high-confidence, systematically wrong beliefs.
  We propose metacognitive regulation as a practical response: a second, introspective control loop that infers experience credibility from endogenous evidence in the learner's internal dynamics. We formalize this as a modular Monitor-Trust-Regulator (MTR) decomposition and instantiate it with self-diagnosis, which maintains a slowly varying experience-trust variable that softly modulates learning updates, without exogenous reliability labels or an explicit corruption model.
  Empirically, in the EIUR regimes studied here, self-diagnosis is associated with improved epistemic identifiability. In reinforcement learning, it enables calibrated skepticism and recovery under systematically corrupted rewards. In supervised learning, it exposes a critical dissociation: performance recovery does not imply epistemic recovery. Accuracy can rebound while internal belief dynamics remain locked-in by early misleading data, a failure detectable only through introspective diagnostics. Together, MTR and self-diagnosis provide an organizing abstraction and a concrete design template for intrinsic reliability assessment in autonomous learning under unobservable reliability.

</details>


### [39] [GeoRA: Geometry-Aware Low-Rank Adaptation for RLVR](https://arxiv.org/abs/2601.09361)
*Jiaying Zhang,Lei Shi,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: GeoRA是一种针对强化学习可验证奖励（RLVR）任务的几何感知低秩适配方法，通过SVD提取主方向并冻结残差组件，解决了现有参数高效方法在RLVR中的几何失配和优化不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 现有参数高效方法（如PiSSA和MiLoRA）专为监督微调设计，未考虑RLVR特有的优化动态和几何结构，直接应用会导致谱崩溃和优化不稳定，而依赖更新稀疏性的方法在现代硬件上存在效率瓶颈。

Method: GeoRA利用RL更新子空间的各向异性和可压缩性，通过奇异值分解在几何约束子空间内提取主方向初始化适配器，同时冻结残差组件，保持预训练几何结构并通过密集算子实现高效GPU计算。

Result: 在Qwen和Llama上的实验表明，GeoRA缓解了几何失配导致的优化瓶颈，在关键数学基准测试中持续优于现有低秩基线，达到SOTA结果，并在域外任务中表现出更好的泛化能力和抗灾难性遗忘能力。

Conclusion: GeoRA通过几何感知的低秩适配有效解决了RLVR任务中的优化稳定性问题，为大规模推理模型的强化学习微调提供了高效且性能优越的解决方案。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is crucial for advancing large-scale reasoning models. However, existing parameter-efficient methods, such as PiSSA and MiLoRA, are designed for Supervised Fine-Tuning (SFT) and do not account for the distinct optimization dynamics and geometric structures of RLVR. Applying these methods directly leads to spectral collapse and optimization instability, which severely limit model performance. Meanwhile, alternative approaches that leverage update sparsity encounter significant efficiency bottlenecks on modern hardware due to unstructured computations. To address these challenges, we propose GeoRA (Geometry-Aware Low-Rank Adaptation), which exploits the anisotropic and compressible nature of RL update subspaces. GeoRA initializes adapters by extracting principal directions via Singular Value Decomposition (SVD) within a geometrically constrained subspace while freezing the residual components. This method preserves the pre-trained geometric structure and enables efficient GPU computation through dense operators. Experiments on Qwen and Llama demonstrate that GeoRA mitigates optimization bottlenecks caused by geometric misalignment. It consistently outperforms established low-rank baselines on key mathematical benchmarks, achieving state-of-the-art (SOTA) results. Moreover, GeoRA shows superior generalization and resilience to catastrophic forgetting in out-of-domain tasks.

</details>


### [40] [Enhancing Spatial Reasoning in Large Language Models for Metal-Organic Frameworks Structure Prediction](https://arxiv.org/abs/2601.09285)
*Mianzhi Pan,JianFei Li,Peishuo Liu,Botian Wang,Yawen Ouyang,Yiming Rong,Hao Zhou,Jianbing Zhang*

Main category: cs.LG

TL;DR: MOF-LLM：首个专门用于块级MOF结构预测的LLM框架，通过空间感知持续预训练、结构监督微调和匹配驱动强化学习，显著提升Qwen-3 8B模型的空间推理能力，在MOF结构预测上超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 金属有机框架（MOFs）在碳捕获和药物递送等领域有广泛应用，但准确预测其3D结构仍具挑战性。虽然大语言模型（LLMs）在晶体生成方面显示出潜力，但MOFs的高原子复杂性阻碍了LLMs在该领域的应用。受深度生成模型中块级范式的成功启发，本研究旨在开发首个专门针对MOF结构预测的LLM框架。

Method: 提出MOF-LLM框架，采用三阶段训练范式：1）空间感知持续预训练（CPT）整合空间先验知识；2）结构监督微调（SFT）优化结构预测；3）匹配驱动强化学习（RL）通过Soft Adaptive Policy Optimization（SAPO）优化结构稳定性。该框架基于Qwen-3 8B模型进行适配。

Result: 综合实验表明，MOF-LLM在MOF结构预测任务上超越了最先进的去噪基和LLM基方法，同时展现出卓越的采样效率。模型的空间推理能力得到显著提升，能够准确预测复杂的MOF结构。

Conclusion: MOF-LLM成功将LLMs应用于MOF结构预测领域，通过创新的块级框架和训练范式解决了MOF高原子复杂性的挑战，为MOF设计和发现提供了高效准确的计算工具。

Abstract: Metal-organic frameworks (MOFs) are porous crystalline materials with broad applications such as carbon capture and drug delivery, yet accurately predicting their 3D structures remains a significant challenge. While Large Language Models (LLMs) have shown promise in generating crystals, their application to MOFs is hindered by MOFs' high atomic complexity. Inspired by the success of block-wise paradigms in deep generative models, we pioneer the use of LLMs in this domain by introducing MOF-LLM, the first LLM framework specifically adapted for block-level MOF structure prediction. To effectively harness LLMs for this modular assembly task, our training paradigm integrates spatial-aware continual pre-training (CPT), structural supervised fine-tuning (SFT), and matching-driven reinforcement learning (RL). By incorporating explicit spatial priors and optimizing structural stability via Soft Adaptive Policy Optimization (SAPO), our approach substantially enhances the spatial reasoning capability of a Qwen-3 8B model for accurate MOF structure prediction. Comprehensive experiments demonstrate that MOF-LLM outperforms state-of-the-art denoising-based and LLM-based methods while exhibiting superior sampling efficiency.

</details>


### [41] [Single-Round Clustered Federated Learning via Data Collaboration Analysis for Non-IID Data](https://arxiv.org/abs/2601.09304)
*Sota Sugawara,Yuji Kawamata,Akihiro Toyoda,Tomoru Nakayama,Yukihiko Okada*

Main category: cs.LG

TL;DR: DC-CFL是一种单轮通信的聚类联邦学习框架，通过数据协作分析实现客户端聚类和模型训练，在非IID数据分布下达到与多轮方法相当的精度。


<details>
  <summary>Details</summary>
Motivation: 传统聚类联邦学习方法需要多轮通信进行聚类估计和模型更新，这在通信轮次受限的实际场景中实用性受限，需要一种单轮通信的解决方案。

Method: 使用标签分布的总变差距离量化客户端间相似性，通过层次聚类进行客户端分组，然后利用数据协作分析进行聚类级别的模型学习。

Result: 在多个公开数据集上的实验表明，DC-CFL在代表性非IID条件下能达到与多轮基线方法相当的准确率，同时仅需一轮通信。

Conclusion: DC-CFL为通信轮次受限场景下的协作AI模型开发提供了实用的单轮通信替代方案。

Abstract: Federated Learning (FL) enables distributed learning across multiple clients without sharing raw data. When statistical heterogeneity across clients is severe, Clustered Federated Learning (CFL) can improve performance by grouping similar clients and training cluster-wise models. However, most CFL approaches rely on multiple communication rounds for cluster estimation and model updates, which limits their practicality under tight constraints on communication rounds. We propose Data Collaboration-based Clustered Federated Learning (DC-CFL), a single-round framework that completes both client clustering and cluster-wise learning, using only the information shared in DC analysis. DC-CFL quantifies inter-client similarity via total variation distance between label distributions, estimates clusters using hierarchical clustering, and performs cluster-wise learning via DC analysis. Experiments on multiple open datasets under representative non-IID conditions show that DC-CFL achieves accuracy comparable to multi-round baselines while requiring only one communication round. These results indicate that DC-CFL is a practical alternative for collaborative AI model development when multiple communication rounds are impractical.

</details>


### [42] [On the Hardness of Computing Counterfactual and Semifactual Explanations in XAI](https://arxiv.org/abs/2601.09455)
*André Artelt,Martin Olsen,Kevin Tierney*

Main category: cs.LG

TL;DR: 论文综述了机器学习的反事实和半事实解释的计算复杂性，发现生成这些解释通常是计算困难的，并进一步证明了在某些假设下这些解释也难以近似计算。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在关键应用中部署时，需要提供清晰的解释来增强透明度和可信度。反事实和半事实解释已成为两种重要的解释机制，但生成这些解释的计算复杂性尚未得到充分研究。

Method: 论文首先综述了文献中关于生成反事实和半事实解释的计算复杂性结果，然后通过贡献自己的不可近似性结果来加强这一论点，证明在某些假设下这些解释不仅难以生成，也难以近似计算。

Result: 研究发现生成反事实和半事实解释在多数情况下是计算困难的，并且进一步证明了在某些假设下这些解释也难以近似计算，这为解释的生成带来了显著的计算挑战。

Conclusion: 这些计算复杂性结果对XAI社区和寻求监管AI解释的政策制定者具有重要意义，表明需要开发更高效的算法或接受解释生成的计算限制。

Abstract: Providing clear explanations to the choices of machine learning models is essential for these models to be deployed in crucial applications. Counterfactual and semi-factual explanations have emerged as two mechanisms for providing users with insights into the outputs of their models. We provide an overview of the computational complexity results in the literature for generating these explanations, finding that in many cases, generating explanations is computationally hard. We strengthen the argument for this considerably by further contributing our own inapproximability results showing that not only are explanations often hard to generate, but under certain assumptions, they are also hard to approximate. We discuss the implications of these complexity results for the XAI community and for policymakers seeking to regulate explanations in AI.

</details>


### [43] [Searth Transformer: A Transformer Architecture Incorporating Earth's Geospheric Physical Priors for Global Mid-Range Weather Forecasting](https://arxiv.org/abs/2601.09467)
*Tianye Li,Qi Liu,Hao Li,Lei Chen,Wencong Cheng,Fei Zheng,Xiangao Xia,Ya Wang,Gang Huang,Weiwei Wang,Xuan Tong,Ziqing Zu,Yi Fang,Shenming Fu,Jiang Jiang,Haochen Li,Mingxing Li,Jiangjiang Xia*

Main category: cs.LG

TL;DR: 提出Searth Transformer架构和Relay Autoregressive微调策略，开发YanTian全球中期天气预报模型，在精度和计算效率上超越传统方法


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的天气预报模型大多采用视觉中心架构，忽略了地球的球面几何特征和纬向周期性；传统自回归训练计算成本高且误差累积限制了预报时效

Method: 1) Searth Transformer：将纬向周期性和经向边界融入基于窗口的自注意力机制，实现物理一致的全球信息交换；2) Relay Autoregressive微调策略：在有限内存和计算预算下学习长程大气演变；3) 基于上述方法开发YanTian全球中期天气预报模型

Result: YanTian精度高于欧洲中期天气预报中心的高分辨率预报，与最先进AI模型在1度分辨率上表现相当，计算成本比标准自回归微调低约200倍；Z500的熟练预报时效达到10.3天，超过HRES的9天

Conclusion: 该工作为复杂全球尺度地球物理环流系统的预测建模建立了稳健的算法基础，为地球系统科学提供了新途径

Abstract: Accurate global medium-range weather forecasting is fundamental to Earth system science. Most existing Transformer-based forecasting models adopt vision-centric architectures that neglect the Earth's spherical geometry and zonal periodicity. In addition, conventional autoregressive training is computationally expensive and limits forecast horizons due to error accumulation. To address these challenges, we propose the Shifted Earth Transformer (Searth Transformer), a physics-informed architecture that incorporates zonal periodicity and meridional boundaries into window-based self-attention for physically consistent global information exchange. We further introduce a Relay Autoregressive (RAR) fine-tuning strategy that enables learning long-range atmospheric evolution under constrained memory and computational budgets. Based on these methods, we develop YanTian, a global medium-range weather forecasting model. YanTian achieves higher accuracy than the high-resolution forecast of the European Centre for Medium-Range Weather Forecasts and performs competitively with state-of-the-art AI models at one-degree resolution, while requiring roughly 200 times lower computational cost than standard autoregressive fine-tuning. Furthermore, YanTian attains a longer skillful forecast lead time for Z500 (10.3 days) than HRES (9 days). Beyond weather forecasting, this work establishes a robust algorithmic foundation for predictive modeling of complex global-scale geophysical circulation systems, offering new pathways for Earth system science.

</details>


### [44] [Preliminary Tests of the Anticipatory Classifier System with Hindsight Experience Replay](https://arxiv.org/abs/2601.09400)
*Olgierd Unold,Stanisław Franczyk*

Main category: cs.LG

TL;DR: ACS2HER结合了预期分类器系统(ACS2)与后见经验回放(HER)机制，通过在智能体未能达成主要目标时触发后见学习，将访问状态重新标记为虚拟目标，从而在稀疏奖励环境中加速学习。


<details>
  <summary>Details</summary>
Motivation: ACS2在构建认知地图方面非常有效，但在稀疏奖励环境中性能往往停滞不前。需要一种方法来增强ACS2在稀疏奖励环境中的学习效率。

Method: 提出ACS2HER架构，将ACS2与HER机制集成。当智能体未能达到主要目标时触发后见学习，将访问的状态重新标记为虚拟目标，从而增加学习信号的密度。

Result: 在确定性Maze 6和随机性FrozenLake基准测试中，ACS2HER相比标准ACS2显著加速了知识获取和环境掌握。但效率提升伴随着计算开销增加和分类器数量大幅扩张。

Conclusion: 这是首次将预期机制与回顾性目标重标记结合在学习分类器系统中的分析，展示了ACS2HER在稀疏奖励环境中的有效性，但也指出了计算成本和分类器数量增加的问题。

Abstract: This paper introduces ACS2HER, a novel integration of the Anticipatory Classifier System (ACS2) with the Hindsight Experience Replay (HER) mechanism. While ACS2 is highly effective at building cognitive maps through latent learning, its performance often stagnates in environments characterized by sparse rewards. We propose a specific architectural variant that triggers hindsight learning when the agent fails to reach its primary goal, re-labeling visited states as virtual goals to densify the learning signal. The proposed model was evaluated on two benchmarks: the deterministic \texttt{Maze 6} and the stochastic \texttt{FrozenLake}. The results demonstrate that ACS2HER significantly accelerates knowledge acquisition and environmental mastery compared to the standard ACS2. However, this efficiency gain is accompanied by increased computational overhead and a substantial expansion in classifier numerosity. This work provides the first analysis of combining anticipatory mechanisms with retrospective goal-relabeling in Learning Classifier Systems.

</details>


### [45] [Draw it like Euclid: Teaching transformer models to generate CAD profiles using ruler and compass construction steps](https://arxiv.org/abs/2601.09428)
*Siyi Li,Joseph G. Lambourne,Longfei Zhang,Pradeep Kumar Jayaraman,Karl. D. D. Willis*

Main category: cs.LG

TL;DR: 提出一种通过几何构造序列生成CAD轮廓的新方法，包括曲线偏移、旋转和相交等操作，类似语言模型中的思维链，并通过强化学习进一步提升生成质量。


<details>
  <summary>Details</summary>
Motivation: 传统CAD建模通常需要复杂的参数设置和约束定义，作者希望开发一种更直观、分步的几何构造方法，类似于思维链在语言模型中的作用，提高CAD轮廓生成的质量和可控性。

Method: 提出基于几何构造序列的CAD轮廓生成方法：从设计师提供的初始几何开始，通过一系列简单的几何构造操作（如曲线偏移、旋转、相交等）逐步构建最终轮廓。这些构造序列减少了建模形状的自由度，形成可调整的参数集。同时应用强化学习优化构造序列。

Result: 实验表明，在设计师输入几何和最终轮廓之间添加构造步骤显著提高了生成质量，类似于思维链在语言模型中的效果。构造序列将形状自由度减少到少量可调整参数值，支持浮点精度的参数化编辑。强化学习的应用进一步提升了多维度指标的性能，包括未明确优化的指标。

Conclusion: 几何构造序列方法为CAD轮廓生成提供了一种新颖的、分步的建模框架，结合强化学习优化，在生成质量、参数化编辑和精度控制方面都取得了显著改进，为CAD设计自动化提供了新思路。

Abstract: We introduce a new method of generating Computer Aided Design (CAD) profiles via a sequence of simple geometric constructions including curve offsetting, rotations and intersections. These sequences start with geometry provided by a designer and build up the points and curves of the final profile step by step. We demonstrate that adding construction steps between the designer's input geometry and the final profile improves generation quality in a similar way to the introduction of a chain of thought in language models. Similar to the constraints in a parametric CAD model, the construction sequences reduce the degrees of freedom in the modeled shape to a small set of parameter values which can be adjusted by the designer, allowing parametric editing with the constructed geometry evaluated to floating point precision. In addition we show that applying reinforcement learning to the construction sequences gives further improvements over a wide range of metrics, including some which were not explicitly optimized.

</details>


### [46] [SimMerge: Learning to Select Merge Operators from Similarity Signals](https://arxiv.org/abs/2601.09473)
*Oliver Bolton,Aakanksha,Arash Ahmadian,Sara Hooker,Marzieh Fadaee,Beyza Ermis*

Main category: cs.LG

TL;DR: SimMerge是一种预测性模型合并选择方法，通过任务无关的相似性信号预测合并性能，避免昂贵的合并-评估循环，实现高效的多模型合并。


<details>
  <summary>Details</summary>
Motivation: 模型合并是LLM开发中的重要技术，但大规模合并面临挑战：需要选择合适的合并算子、模型和合并顺序，通常需要进行昂贵的合并-评估搜索。研究人员需要一种更高效的合并选择方法。

Method: 提出SimMerge方法：使用少量无标签探针计算模型的功能和结构特征，基于这些特征预测给定双向合并的性能。利用预测结果选择最佳合并算子、模型子集和合并顺序，无需昂贵的合并-评估循环。

Result: 在7B参数LLM的双向合并中超越了标准合并算子性能；SimMerge能够泛化到多路合并和111B参数LLM合并而无需重新训练；提出了支持动态添加新任务、模型和算子的bandit变体。

Conclusion: 学习如何合并是当检查点目录庞大且评估预算紧张时，实现可扩展模型组合的实用途径。SimMerge为模型合并提供了高效、可扩展的解决方案。

Abstract: Model merging enables multiple large language models (LLMs) to be combined into a single model while preserving performance. This makes it a valuable tool in LLM development, offering a competitive alternative to multi-task training. However, merging can be difficult at scale, as successful merging requires choosing the right merge operator, selecting the right models, and merging them in the right order. This often leads researchers to run expensive merge-and-evaluate searches to select the best merge. In this work, we provide an alternative by introducing \simmerge{}, \emph{a predictive merge-selection method} that selects the best merge using inexpensive, task-agnostic similarity signals between models. From a small set of unlabeled probes, we compute functional and structural features and use them to predict the performance of a given 2-way merge. Using these predictions, \simmerge{} selects the best merge operator, the subset of models to merge, and the merge order, eliminating the expensive merge-and-evaluate loop. We demonstrate that we surpass standard merge-operator performance on 2-way merges of 7B-parameter LLMs, and that \simmerge{} generalizes to multi-way merges and 111B-parameter LLM merges without retraining. Additionally, we present a bandit variant that supports adding new tasks, models, and operators on the fly. Our results suggest that learning how to merge is a practical route to scalable model composition when checkpoint catalogs are large and evaluation budgets are tight.

</details>


### [47] [Toward Understanding Unlearning Difficulty: A Mechanistic Perspective and Circuit-Guided Difficulty Metric](https://arxiv.org/abs/2601.09624)
*Jiali Cheng,Ziheng Chen,Chirag Agarwal,Hadi Amiri*

Main category: cs.LG

TL;DR: 本文提出了一种基于模型电路机制的预遗忘难度度量方法CUD，能够可靠地区分遗忘的难易样本，并揭示了遗忘难度的机制特征。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘对于构建可信赖和合规的语言模型至关重要，但现有遗忘方法在不同样本上的效果差异很大。作者认为这种差异不仅源于数据侧因素，更反映了模型内部编码和保护记忆信息的机制。

Method: 从机制角度研究遗忘问题，基于模型电路（结构化交互路径）提出CUD（Circuit-guided Unlearning Difficulty）度量方法。该方法使用电路级信号为每个样本分配连续难度分数，能够在遗忘前预测样本的遗忘难度。

Result: CUD能够可靠地区分内在易忘和难忘样本，且在不同遗忘方法中保持稳定。研究发现易忘样本与原始模型中更短、更浅、集中于早期到中期部分的交互相关，而难样本则依赖于更长、更深、更接近后期计算的路径。

Conclusion: 相比现有的定性研究，CUD首次为遗忘难度提供了原则性、细粒度和可解释的分析方法，并激励开发基于模型机制的遗忘方法。

Abstract: Machine unlearning is becoming essential for building trustworthy and compliant language models. Yet unlearning success varies considerably across individual samples: some are reliably erased, while others persist despite the same procedure. We argue that this disparity is not only a data-side phenomenon, but also reflects model-internal mechanisms that encode and protect memorized information. We study this problem from a mechanistic perspective based on model circuits--structured interaction pathways that govern how predictions are formed. We propose Circuit-guided Unlearning Difficulty (CUD), a {\em pre-unlearning} metric that assigns each sample a continuous difficulty score using circuit-level signals. Extensive experiments demonstrate that CUD reliably separates intrinsically easy and hard samples, and remains stable across unlearning methods. We identify key circuit-level patterns that reveal a mechanistic signature of difficulty: easy-to-unlearn samples are associated with shorter, shallower interactions concentrated in earlier-to-intermediate parts of the original model, whereas hard samples rely on longer and deeper pathways closer to late-stage computation. Compared to existing qualitative studies, CUD takes a first step toward a principled, fine-grained, and interpretable analysis of unlearning difficulty; and motivates the development of unlearning methods grounded in model mechanisms.

</details>


### [48] [Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection](https://arxiv.org/abs/2601.09684)
*Ziyu Yang,Guibin Chen,Yuxin Yang,Aoxiong Zeng,Xiangquan Yang*

Main category: cs.LG

TL;DR: Ortho-LoRA：针对LoRA多任务学习的梯度投影方法，通过正交化处理解决任务间梯度冲突问题


<details>
  <summary>Details</summary>
Motivation: 多任务学习结合LoRA可以减少存储开销，但存在负迁移问题，不同任务的梯度更新相互冲突，特别是在LoRA的低秩约束下，优化空间有限难以适应多样化的任务需求

Method: 提出Ortho-LoRA方法，针对LoRA的双层结构设计梯度投影机制，动态地将冲突的任务梯度投影到彼此的正交补空间中，在LoRA的固有子空间内实现梯度正交化

Result: 在GLUE基准测试上的广泛实验表明，Ortho-LoRA有效缓解了任务干扰，优于标准联合训练，恢复了多任务与单任务基线之间95%的性能差距，且计算开销可忽略不计

Conclusion: Ortho-LoRA为解决LoRA多任务学习中的负迁移问题提供了一种有效的梯度投影解决方案，在保持参数效率的同时显著提升了多任务性能

Abstract: Multi-Task Learning (MTL) combined with Low-Rank Adaptation (LoRA) has emerged as a promising direction for parameter-efficient deployment of Large Language Models (LLMs). By sharing a single adapter across multiple tasks, one can significantly reduce storage overhead. However, this approach suffers from negative transfer, where conflicting gradient updates from distinct tasks degrade the performance of individual tasks compared to single-task fine-tuning. This problem is exacerbated in LoRA due to the low-rank constraint, which limits the optimization landscape's capacity to accommodate diverse task requirements. In this paper, we propose Ortho-LoRA, a gradient projection method specifically tailored for the bipartite structure of LoRA. Ortho-LoRA dynamically projects conflicting task gradients onto the orthogonal complement of each other within the intrinsic LoRA subspace. Extensive experiments on the GLUE benchmark demonstrate that Ortho-LoRA effectively mitigates task interference, outperforming standard joint training and recovering 95\% of the performance gap between multi-task and single-task baselines with negligible computational overhead.

</details>


### [49] [Terminally constrained flow-based generative models from an optimal control perspective](https://arxiv.org/abs/2601.09474)
*Weiguo Gao,Ming Li,Qianxiao Li*

Main category: cs.LG

TL;DR: 提出TOCFlow方法，利用最优控制理论指导预训练流模型进行终端约束采样，通过黎曼几何方法在计算成本较低的情况下实现约束满足


<details>
  <summary>Details</summary>
Motivation: 解决预训练流基生成模型在终端约束分布采样中的问题，传统方法在满足约束的同时难以保持生成质量

Method: 基于最优控制理论，提出TOCFlow方法，在终端共动坐标系中求解控制问题，得到闭式标量阻尼因子，沿黎曼梯度捕获二阶曲率效应而无需矩阵求逆

Result: 在Darcy流、约束轨迹规划、具有Kolmogorov谱标度的湍流快照生成三个高维科学任务中，TOCFlow相比欧几里得指导和投影基线显著提高了约束满足度，同时保持了参考模型的生成质量

Conclusion: TOCFlow方法通过几何感知的采样时间指导，以标准梯度指导的计算成本实现了与高斯-牛顿更新相当的几何一致性，为预训练流模型的约束采样提供了有效解决方案

Abstract: We address the problem of sampling from terminally constrained distributions with pre-trained flow-based generative models through an optimal control formulation. Theoretically, we characterize the value function by a Hamilton-Jacobi-Bellman equation and derive the optimal feedback control as the minimizer of the associated Hamiltonian. We show that as the control penalty increases, the controlled process recovers the reference distribution, while as the penalty vanishes, the terminal law converges to a generalized Wasserstein projection onto the constraint manifold. Algorithmically, we introduce Terminal Optimal Control with Flow-based models (TOCFlow), a geometry-aware sampling-time guidance method for pre-trained flows. Solving the control problem in a terminal co-moving frame that tracks reference trajectories yields a closed-form scalar damping factor along the Riemannian gradient, capturing second-order curvature effects without matrix inversions. TOCFlow therefore matches the geometric consistency of Gauss-Newton updates at the computational cost of standard gradient guidance. We evaluate TOCFlow on three high-dimensional scientific tasks spanning equality, inequality, and global statistical constraints, namely Darcy flow, constrained trajectory planning, and turbulence snapshot generation with Kolmogorov spectral scaling. Across all settings, TOCFlow improves constraint satisfaction over Euclidean guidance and projection baselines while preserving the reference model's generative quality.

</details>


### [50] [Constraint- and Score-Based Nonlinear Granger Causality Discovery with Kernels](https://arxiv.org/abs/2601.09579)
*Fiona Murphy,Alessio Benavoli*

Main category: cs.LG

TL;DR: 该论文将两种核基格兰杰因果方法统一到核主成分回归框架下，提出基于此统一的方法改进因果识别，并引入带平滑信息准则惩罚的高斯过程评分模型，在非线性时间序列因果发现中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有核基格兰杰因果方法在识别非线性时间序列因果关系方面存在局限性，需要理论统一和性能提升。同时，需要改进非线性时间序列因果发现方法，并解决瞬时因果识别问题。

Method: 1. 将两种最先进的核基格兰杰因果方法统一到核主成分回归框架下；2. 基于此统一提出改进的因果识别方法；3. 引入带平滑信息准则惩罚的高斯过程评分模型；4. 提出完全基于格兰杰因果的瞬时因果识别算法。

Result: 1. 统一框架下的方法能改进因果识别；2. 提出的高斯过程评分模型在非线性时间序列因果发现中表现优于现有最先进方法；3. 提出的瞬时因果识别算法性能可与现有最先进的瞬时时间序列因果发现算法相媲美。

Conclusion: 通过理论统一核基格兰杰因果方法并提出新的高斯过程评分模型，显著提升了非线性时间序列因果发现的性能，同时为瞬时因果识别提供了有效的基于格兰杰因果的解决方案。

Abstract: Kernel-based methods are used in the context of Granger Causality to enable the identification of nonlinear causal relationships between time series variables. In this paper, we show that two state of the art kernel-based Granger Causality (GC) approaches can be theoretically unified under the framework of Kernel Principal Component Regression (KPCR), and introduce a method based on this unification, demonstrating that this approach can improve causal identification. Additionally, we introduce a Gaussian Process score-based model with Smooth Information Criterion penalisation on the marginal likelihood, and demonstrate improved performance over existing state of the art time-series nonlinear causal discovery methods. Furthermore, we propose a contemporaneous causal identification algorithm, fully based on GC, using the proposed score-based $GP_{SIC}$ method, and compare its performance to a state of the art contemporaneous time series causal discovery algorithm.

</details>


### [51] [Energy-Entropy Regularization: The True Power of Minimal Looped Transformers](https://arxiv.org/abs/2601.09588)
*Wai-Lun Lam*

Main category: cs.LG

TL;DR: 提出基于Tsallis熵和哈密顿动力学的训练框架，成功训练单头循环Transformer解决归纳头任务


<details>
  <summary>Details</summary>
Motivation: 当前单头循环Transformer在基准任务上训练常失败或性能不佳，原因是损失函数高度非凸且不规则，优化常陷入局部极小值和鞍点，且其内部机制仍不明确

Method: 提出新颖训练框架，利用Tsallis熵和哈密顿动力学改变损失函数几何形状，将参数更新视为物理流动

Result: 成功训练模型维度d=8的单头循环Transformer，能够处理1000个token的输入序列，解决归纳头任务

Conclusion: 该方法揭示了循环Transformer优越推理能力背后的内部机制，为训练这类模型提供了有效解决方案

Abstract: Recent research suggests that looped Transformers have superior reasoning capabilities compared to standard deep architectures. Current approaches to training single-head looped architectures on benchmark tasks frequently fail or yield suboptimal performance due to a highly non-convex and irregular loss landscape. In these settings, optimization often stagnates in poor local minima and saddle points of the loss landscape, preventing the model from discovering the global minimum point. The internal mechanisms of these single-head looped transformer models remain poorly understood, and training them from scratch remains a significant challenge. In this paper, we propose a novel training framework that leverages Tsallis entropy and Hamiltonian dynamics to transform the geometry of the loss landscape. By treating the parameter updates as a physical flow, we successfully trained a single-head looped Transformer with model dimension $d = 8$ to solve induction head task with input sequence length of 1000 tokens. This success reveals the internal mechanism behind the superior reasoning capability.

</details>


### [52] [Exploring Fine-Tuning for Tabular Foundation Models](https://arxiv.org/abs/2601.09654)
*Aditya Tanna,Pratinav Seth,Mohamed Bouadi,Vinay Kumar Sankarapu*

Main category: cs.LG

TL;DR: 对表格基础模型微调方法的首次全面研究，发现零样本方法已表现良好，而微调效果高度依赖模型和数据，完全监督微调通常降低准确性或校准质量


<details>
  <summary>Details</summary>
Motivation: 表格基础模型最近展示了强大的上下文学习能力，但微调的效果尚不明确。本研究旨在全面评估不同微调方法在表格数据上的效果，为实践提供指导

Method: 在TALENT、OpenML-CC18和TabZilla等基准测试上，系统比较了零样本、元学习、监督微调和参数高效微调四种方法，分析了数据集不平衡、大小和维度等因素的影响

Result: 零样本TFMs已表现良好；微调效果高度模型和数据依赖；元学习和PEFT在特定条件下提供适度增益；完全监督微调通常降低准确性或校准质量

Conclusion: 提供了关于何时微调最有益及其局限性的实用指南，强调需要根据具体模型和数据集特性谨慎选择微调策略

Abstract: Tabular Foundation Models (TFMs) have recently shown strong in-context learning capabilities on structured data, achieving zero-shot performance comparable to traditional machine learning methods. We find that zero-shot TFMs already achieve strong performance, while the benefits of fine-tuning are highly model and data-dependent. Meta-learning and PEFT provide moderate gains under specific conditions, whereas full supervised fine-tuning (SFT) often reduces accuracy or calibration quality. This work presents the first comprehensive study of fine-tuning in TFMs across benchmarks including TALENT, OpenML-CC18, and TabZilla. We compare Zero-Shot, Meta-Learning, Supervised (SFT), and parameter-efficient (PEFT) approaches, analyzing how dataset factors such as imbalance, size, and dimensionality affect outcomes. Our findings cover performance, calibration, and fairness, offering practical guidelines on when fine-tuning is most beneficial and its limitations.

</details>


### [53] [Contrastive Geometric Learning Unlocks Unified Structure- and Ligand-Based Drug Design](https://arxiv.org/abs/2601.09693)
*Lisa Schneckenreiter,Sohvi Luukkonen,Lukas Friedrich,Daniel Kuhn,Günter Klambauer*

Main category: cs.LG

TL;DR: ConGLUDe是一个统一的对比几何学习模型，将结构基础和配体基础的药物设计方法整合到一个框架中，通过对比学习对齐蛋白质全局表示和候选结合位点，无需预定义口袋即可进行虚拟筛选、靶点发现和配体条件口袋预测。


<details>
  <summary>Details</summary>
Motivation: 传统结构基础和配体基础的计算药物设计方法依赖不同的数据源和建模假设，限制了它们在大规模上的联合使用。需要一种统一的模型来整合这两种方法，提高药物设计的效率和准确性。

Method: ConGLUDe采用对比几何学习框架，包含几何蛋白质编码器（生成全蛋白质表示和预测结合位点的隐式嵌入）和快速配体编码器。通过对比学习将配体与全局蛋白质表示和多个候选结合位点对齐，无需预定义口袋。模型在蛋白质-配体复合物和大规模生物活性数据上联合训练。

Result: 在多种基准测试中，ConGLUDe在无需输入结合口袋信息的零样本虚拟筛选中达到最先进性能；在具有挑战性的靶点发现任务上显著优于现有方法；在配体条件口袋选择方面表现出竞争力。

Conclusion: 统一的结构-配体训练方法具有显著优势，ConGLUDe代表了向药物发现通用基础模型迈出的一步，为整合结构基础和配体基础的计算药物设计提供了有效框架。

Abstract: Structure-based and ligand-based computational drug design have traditionally relied on disjoint data sources and modeling assumptions, limiting their joint use at scale. In this work, we introduce Contrastive Geometric Learning for Unified Computational Drug Design (ConGLUDe), a single contrastive geometric model that unifies structure- and ligand-based training. ConGLUDe couples a geometric protein encoder that produces whole-protein representations and implicit embeddings of predicted binding sites with a fast ligand encoder, removing the need for pre-defined pockets. By aligning ligands with both global protein representations and multiple candidate binding sites through contrastive learning, ConGLUDe supports ligand-conditioned pocket prediction in addition to virtual screening and target fishing, while being trained jointly on protein-ligand complexes and large-scale bioactivity data. Across diverse benchmarks, ConGLUDe achieves state-of-the-art zero-shot virtual screening performance in settings where no binding pocket information is provided as input, substantially outperforms existing methods on a challenging target fishing task, and demonstrates competitive ligand-conditioned pocket selection. These results highlight the advantages of unified structure-ligand training and position ConGLUDe as a step toward general-purpose foundation models for drug discovery.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [54] [ConvoLearn: A Dataset of Constructivist Tutor-Student Dialogue](https://arxiv.org/abs/2601.08950)
*Mayank Sharma,Roy Pea,Hari Subramonyam*

Main category: cs.AI

TL;DR: 该研究针对LLM在教育应用中的局限性，开发了基于知识建构理论的ConvoLearn数据集，通过微调使LLM更有效地支持对话式学习而非直接提供答案。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在教育应用中存在根本性教学限制，倾向于直接揭示解决方案而非支持对话式学习。需要开发能够促进知识建构的AI导师。

Method: 1. 基于知识建构理论构建ConvoLearn数据集，包含6个核心教学维度；2. 通过人类教师与模拟学生交互创建1250个中学地球科学对话；3. 使用QLoRA对Mistral 7B进行微调；4. 由31名教师进行人工评估。

Result: 微调后的Mistral 7B（M=4.10, SD=1.03）在整体表现上显著优于基础版本（M=2.59, SD=1.11）和Claude Sonnet 4.5（M=2.87, SD=1.29）。

Conclusion: 该工作建立了一个潜在框架，指导未来建构主义AI导师的开发和评估，证明通过特定数据集微调可以显著改善LLM的教学行为。

Abstract: In educational applications, LLMs exhibit several fundamental pedagogical limitations, such as their tendency to reveal solutions rather than support dialogic learning. We introduce ConvoLearn (https://huggingface.co/datasets/masharma/convolearn ), a dataset grounded in knowledge building theory that operationalizes six core pedagogical dimensions: cognitive engagement, formative assessment, accountability, cultural responsiveness, metacognition, and power dynamics. We construct a semi-synthetic dataset of 1250 tutor-student dialogues (20 turns each) in middle school Earth Science through controlled interactions between human teachers and a simulated student. Using QLoRA, we demonstrate that training on this dataset meaningfully shifts LLM behavior toward knowledge-building strategies. Human evaluation by 31 teachers shows our fine-tuned Mistral 7B (M = 4.10, SD = 1.03) significantly outperforms both its base version (M = 2.59, SD = 1.11) and Claude Sonnet 4.5 (M = 2.87, SD = 1.29) overall. This work establishes a potential framework to guide future development and evaluation of constructivist AI tutors.

</details>


### [55] [Programming over Thinking: Efficient and Robust Multi-Constraint Planning](https://arxiv.org/abs/2601.09097)
*Derrick Goh Xin Deik,Quanyu Long,Zhengyuan Liu,Nancy F. Chen,Wenya Wang*

Main category: cs.AI

TL;DR: SCOPE框架通过分离推理与代码执行，解决了多约束规划中LLM方法的局限性，实现了高效、低成本、可复用的解决方案。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法在多约束规划中存在根本性局限：纯推理范式容易产生不一致、错误累积和高成本；而结合编码或求解器的方法缺乏灵活性，无法跨问题泛化。

Method: 提出SCOPE框架，将查询特定推理与通用代码执行解耦，生成一致、确定且可复用的求解器函数，仅需最小化参数调整。

Result: SCOPE在TravelPlanner任务上达到93.1%成功率，比最佳基线（CoT）提升61.6%，同时降低推理成本1.4倍，减少时间约4.67倍。

Conclusion: SCOPE通过分离推理与执行，实现了多约束规划任务的高性能、低成本解决方案，具有可复用性和可扩展性优势。

Abstract: Multi-constraint planning involves identifying, evaluating, and refining candidate plans while satisfying multiple, potentially conflicting constraints. Existing large language model (LLM) approaches face fundamental limitations in this domain. Pure reasoning paradigms, which rely on long natural language chains, are prone to inconsistency, error accumulation, and prohibitive cost as constraints compound. Conversely, LLMs combined with coding- or solver-based strategies lack flexibility: they often generate problem-specific code from scratch or depend on fixed solvers, failing to capture generalizable logic across diverse problems. To address these challenges, we introduce the Scalable COde Planning Engine (SCOPE), a framework that disentangles query-specific reasoning from generic code execution. By separating reasoning from execution, SCOPE produces solver functions that are consistent, deterministic, and reusable across queries while requiring only minimal changes to input parameters. SCOPE achieves state-of-the-art performance while lowering cost and latency. For example, with GPT-4o, it reaches 93.1% success on TravelPlanner, a 61.6% gain over the best baseline (CoT) while cutting inference cost by 1.4x and time by ~4.67x. Code is available at https://github.com/DerrickGXD/SCOPE.

</details>


### [56] [DScheLLM: Enabling Dynamic Scheduling through a Fine-Tuned Dual-System Large language Model](https://arxiv.org/abs/2601.09100)
*Lixiang Zhang,Chenggong Zhao,Qing Gao,Xiaoke Zhao,Gengyi Bai,Jinhu Lv*

Main category: cs.AI

TL;DR: DScheLLM：基于双系统推理架构（快慢思维）的微调大语言模型动态调度方法，用于处理生产调度中的动态扰动


<details>
  <summary>Details</summary>
Motivation: 传统生产调度方法对动态扰动（如处理时间变化、机器可用性变化、意外任务插入）适应性差，依赖特定事件模型和显式分析公式，难以泛化到未见过的扰动场景

Method: 构建统一的大语言模型框架处理动态事件，使用运筹学求解器获得的精确调度生成快慢推理模式的训练数据集，基于华为OpenPangu Embedded-7B模型使用LoRA进行混合推理范式微调

Result: 在标准作业车间调度基准测试中，快思维模式能高效生成高质量调度方案，慢思维模式能生成与求解器兼容且格式良好的决策输入

Conclusion: 这是最早将大语言模型应用于动态环境作业车间调度的研究之一，展示了LLM在智能自适应调度优化中的巨大潜力

Abstract: Production scheduling is highly susceptible to dynamic disruptions, such as variations in processing times, machine availability, and unexpected task insertions. Conventional approaches typically rely on event-specific models and explicit analytical formulations, which limits their adaptability and generalization across previously unseen disturbances. To overcome these limitations, this paper proposes DScheLLM, a dynamic scheduling approach that leverages fine-tuned large language models within a dual-system (fast-slow) reasoning architecture to address disturbances of different scales. A unified large language model-based framework is constructed to handle dynamic events, where training datasets for both fast and slow reasoning modes are generated using exact schedules obtained from an operations research solver. The Huawei OpenPangu Embedded-7B model is subsequently fine-tuned under the hybrid reasoning paradigms using LoRA. Experimental evaluations on standard job shop scheduling benchmarks demonstrate that the fast-thinking mode can efficiently generate high-quality schedules and the slow-thinking mode can produce solver-compatible and well-formatted decision inputs. To the best of our knowledge, this work represents one of the earliest studies applying large language models to job shop scheduling in dynamic environments, highlighting their considerable potential for intelligent and adaptive scheduling optimization.

</details>


### [57] [AviationLMM: A Large Multimodal Foundation Model for Civil Aviation](https://arxiv.org/abs/2601.09105)
*Wenbin Li,Jingling Wu,Xiaoyong Lin. Jing Chen,Cong Chen*

Main category: cs.AI

TL;DR: 本文提出AviationLMM愿景——一个用于民航的大型多模态基础模型，旨在统一民航异构数据流，实现理解、推理、生成和智能体应用，以解决现有AI方案孤立、单模态的局限性。


<details>
  <summary>Details</summary>
Motivation: 民航是全球交通和商业的基石，但现有AI解决方案在民航领域存在孤立和狭窄的问题，专注于单一任务或单模态，难以整合语音通信、雷达轨迹、传感器流和文本报告等异构数据，限制了态势感知、适应性和实时决策支持能力。

Method: 提出AviationLMM模型架构，能够处理多模态输入（空地语音、监视数据、机载遥测、视频和结构化文本），执行跨模态对齐和融合，并产生灵活输出（从态势摘要、风险警报到预测性诊断和多模态事件重建）。同时识别了实现该愿景需要解决的关键研究机会。

Result: 本文提出了AviationLMM的设计愿景和架构框架，但尚未实现具体模型。通过阐述该设计和挑战，旨在推动民航基础模型的发展，并促进向集成、可信和隐私保护的民航AI生态系统的协调研究努力。

Conclusion: AviationLMM作为民航大型多模态基础模型，有望解决现有AI方案的局限性，通过统一异构数据流提升民航的安全性、效率和客户满意度。实现这一愿景需要解决数据获取、对齐融合、预训练、推理、可信性、隐私、模态缺失鲁棒性和合成场景生成等关键研究挑战。

Abstract: Civil aviation is a cornerstone of global transportation and commerce, and ensuring its safety, efficiency and customer satisfaction is paramount. Yet conventional Artificial Intelligence (AI) solutions in aviation remain siloed and narrow, focusing on isolated tasks or single modalities. They struggle to integrate heterogeneous data such as voice communications, radar tracks, sensor streams and textual reports, which limits situational awareness, adaptability, and real-time decision support. This paper introduces the vision of AviationLMM, a Large Multimodal foundation Model for civil aviation, designed to unify the heterogeneous data streams of civil aviation and enable understanding, reasoning, generation and agentic applications. We firstly identify the gaps between existing AI solutions and requirements. Secondly, we describe the model architecture that ingests multimodal inputs such as air-ground voice, surveillance, on-board telemetry, video and structured texts, and performs cross-modal alignment and fusion, and produces flexible outputs ranging from situation summaries and risk alerts to predictive diagnostics and multimodal incident reconstructions. In order to fully realize this vision, we identify key research opportunities to address, including data acquisition, alignment and fusion, pretraining, reasoning, trustworthiness, privacy, robustness to missing modalities, and synthetic scenario generation. By articulating the design and challenges of AviationLMM, we aim to boost the civil aviation foundation model progress and catalyze coordinated research efforts toward an integrated, trustworthy and privacy-preserving aviation AI ecosystem.

</details>


### [58] [Position on LLM-Assisted Peer Review: Addressing Reviewer Gap through Mentoring and Feedback](https://arxiv.org/abs/2601.09182)
*JungMin Yun,JuneHyoung Kwon,MiHyeon Kim,YoungBin Kim*

Main category: cs.AI

TL;DR: 本文提出以LLM辅助人类审稿人而非替代他们的新范式，通过教育辅助系统提升审稿质量，解决AI研究快速发展带来的审稿人缺口问题。


<details>
  <summary>Details</summary>
Motivation: AI研究的快速扩张加剧了"审稿人缺口"，威胁同行评审的可持续性，并导致低质量评审的恶性循环。现有LLM自动生成评审的方法存在局限，需要转向更有效的解决方案。

Method: 提出以LLM作为辅助和培训人类审稿人的工具，基于高质量同行评审的核心原则，设计了两个互补系统：1) LLM辅助的导师系统，培养审稿人长期能力；2) LLM辅助的反馈系统，帮助审稿人改进评审质量。

Result: 提出了一种以人为本的方法，旨在增强审稿人专业知识，为构建更可持续的学术生态系统做出贡献。

Conclusion: 需要从LLM自动生成评审的范式转向LLM辅助人类审稿人的范式，通过教育辅助系统提升审稿质量，解决同行评审的可持续性问题。

Abstract: The rapid expansion of AI research has intensified the Reviewer Gap, threatening the peer-review sustainability and perpetuating a cycle of low-quality evaluations. This position paper critiques existing LLM approaches that automatically generate reviews and argues for a paradigm shift that positions LLMs as tools for assisting and educating human reviewers. We define the core principles of high-quality peer review and propose two complementary systems grounded in these foundations: (i) an LLM-assisted mentoring system that cultivates reviewers' long-term competencies, and (ii) an LLM-assisted feedback system that helps reviewers refine the quality of their reviews. This human-centered approach aims to strengthen reviewer expertise and contribute to building a more sustainable scholarly ecosystem.

</details>


### [59] [PrivacyReasoner: Can LLM Emulate a Human-like Privacy Mind?](https://arxiv.org/abs/2601.09152)
*Yiwen Tu,Xuan Liu,Lianhui Qin,Haojian Jin*

Main category: cs.AI

TL;DR: PRA是一个AI智能体设计，用于模拟个体用户如何根据现实世界新闻形成隐私担忧，通过整合隐私和认知理论来模拟基于个人评论历史和上下文线索的用户特定隐私推理。


<details>
  <summary>Details</summary>
Motivation: 超越群体层面的情感分析，研究个体用户如何形成隐私担忧，需要模拟用户特定的隐私推理过程，考虑个人历史、上下文线索和有限理性等因素。

Method: PRA通过重建用户的"隐私心智"，使用上下文过滤器动态激活相关隐私记忆（模拟有限理性），生成反映用户对新隐私场景可能反应的合成评论。采用经过隐私担忧分类校准的LLM-as-a-Judge评估器量化生成推理的忠实度。

Result: 在真实世界Hacker News讨论的实验表明，PRA在隐私担忧预测方面优于基线智能体，并能捕捉跨领域（包括AI、电子商务和医疗保健）的可迁移推理模式。

Conclusion: PRA提供了一种模拟个体用户隐私担忧形成过程的有效方法，能够生成忠实于用户特定推理模式的合成评论，为隐私研究和应用提供了有价值的工具。

Abstract: This paper introduces PRA, an AI-agent design for simulating how individual users form privacy concerns in response to real-world news. Moving beyond population-level sentiment analysis, PRA integrates privacy and cognitive theories to simulate user-specific privacy reasoning grounded in personal comment histories and contextual cues. The agent reconstructs each user's "privacy mind", dynamically activates relevant privacy memory through a contextual filter that emulates bounded rationality, and generates synthetic comments reflecting how that user would likely respond to new privacy scenarios. A complementary LLM-as-a-Judge evaluator, calibrated against an established privacy concern taxonomy, quantifies the faithfulness of generated reasoning. Experiments on real-world Hacker News discussions show that \PRA outperforms baseline agents in privacy concern prediction and captures transferable reasoning patterns across domains including AI, e-commerce, and healthcare.

</details>


### [60] [MAXS: Meta-Adaptive Exploration with LLM Agents](https://arxiv.org/abs/2601.09259)
*Jian Zhang,Zhiyuan Wang,Zhangqi Wang,Yu He,Haoran Luo,li yuan,Lingling Zhang,Rui Mao,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: MAXS是一个基于LLM Agent的元自适应推理框架，通过前瞻策略和轨迹收敛机制，解决了现有方法中的局部短视生成和轨迹不稳定问题，在性能和推理效率上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM Agent推理方法存在两个主要问题：1）由于缺乏前瞻性导致局部短视生成；2）早期微小错误会引发轨迹不稳定，导致推理路径发散。这使得难以平衡全局有效性和计算效率。

Method: 提出MAXS框架，采用前瞻策略向前扩展推理路径几步，估计工具使用的优势值，结合步骤一致性方差和步间趋势斜率联合选择稳定、一致且高价值的推理步骤。引入轨迹收敛机制，在路径一致性达到时停止进一步展开，平衡资源效率和全局有效性。

Result: 在三个基础模型（MiMo-VL-7B、Qwen2.5-VL-7B、Qwen2.5-VL-32B）和五个数据集上的广泛实证研究表明，MAXS在性能和推理效率方面均一致优于现有方法。进一步分析证实了前瞻策略和工具使用的有效性。

Conclusion: MAXS通过元自适应推理框架有效解决了LLM Agent推理中的局部短视和轨迹不稳定问题，实现了全局有效性和计算效率的良好平衡，为多工具推理提供了有效的解决方案。

Abstract: Large Language Model (LLM) Agents exhibit inherent reasoning abilities through the collaboration of multiple tools. However, during agent inference, existing methods often suffer from (i) locally myopic generation, due to the absence of lookahead, and (ii) trajectory instability, where minor early errors can escalate into divergent reasoning paths. These issues make it difficult to balance global effectiveness and computational efficiency. To address these two issues, we propose meta-adaptive exploration with LLM agents https://github.com/exoskeletonzj/MAXS, a meta-adaptive reasoning framework based on LLM Agents that flexibly integrates tool execution and reasoning planning. MAXS employs a lookahead strategy to extend reasoning paths a few steps ahead, estimating the advantage value of tool usage, and combines step consistency variance and inter-step trend slopes to jointly select stable, consistent, and high-value reasoning steps. Additionally, we introduce a trajectory convergence mechanism that controls computational cost by halting further rollouts once path consistency is achieved, enabling a balance between resource efficiency and global effectiveness in multi-tool reasoning. We conduct extensive empirical studies across three base models (MiMo-VL-7B, Qwen2.5-VL-7B, Qwen2.5-VL-32B) and five datasets, demonstrating that MAXS consistently outperforms existing methods in both performance and inference efficiency. Further analysis confirms the effectiveness of our lookahead strategy and tool usage.

</details>


### [61] [Efficient Paths and Dense Rewards: Probabilistic Flow Reasoning for Large Language Models](https://arxiv.org/abs/2601.09260)
*Yan Liu,Feng Zhang,Zhanyu Ma,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He,Han Liu,Yangdong Deng*

Main category: cs.AI

TL;DR: CoT-Flow框架将离散推理步骤重新概念化为连续概率流，量化每个步骤对正确答案的贡献，实现推理效率与性能的平衡


<details>
  <summary>Details</summary>
Motivation: 当前思维链方法将推理过程视为不可分割的序列，缺乏量化逐步信息增益的内在机制，导致推理效率低下和优化困难

Method: 提出CoT-Flow框架，将离散推理步骤重新概念化为连续概率流，量化每个步骤对正确答案的贡献，实现流引导解码和基于流的强化学习两种方法

Result: 在具有挑战性的基准测试中，CoT-Flow在推理效率和推理性能之间实现了优越的平衡

Conclusion: CoT-Flow通过连续概率流框架有效解决了当前思维链方法的局限性，为高效推理提供了新的方法论

Abstract: High-quality chain-of-thought has demonstrated strong potential for unlocking the reasoning capabilities of large language models. However, current paradigms typically treat the reasoning process as an indivisible sequence, lacking an intrinsic mechanism to quantify step-wise information gain. This granularity gap manifests in two limitations: inference inefficiency from redundant exploration without explicit guidance, and optimization difficulty due to sparse outcome supervision or costly external verifiers. In this work, we propose CoT-Flow, a framework that reconceptualizes discrete reasoning steps as a continuous probabilistic flow, quantifying the contribution of each step toward the ground-truth answer. Built on this formulation, CoT-Flow enables two complementary methodologies: flow-guided decoding, which employs a greedy flow-based decoding strategy to extract information-efficient reasoning paths, and flow-based reinforcement learning, which constructs a verifier-free dense reward function. Experiments on challenging benchmarks demonstrate that CoT-Flow achieves a superior balance between inference efficiency and reasoning performance.

</details>


### [62] [Coordinated Pandemic Control with Large Language Model Agents as Policymaking Assistants](https://arxiv.org/abs/2601.09264)
*Ziyi Shi,Xusen Guo,Hongliang Lu,Mingxing Peng,Haotian Wang,Zheng Zhu,Zhenning Li,Yuxuan Liang,Xinhu Zheng,Hai Yang*

Main category: cs.AI

TL;DR: 提出基于大语言模型的多智能体政策制定框架，用于协调、主动的跨区域疫情防控，相比现实政策可显著降低感染和死亡人数


<details>
  <summary>Details</summary>
Motivation: 传统疫情防控政策制定存在碎片化、反应式的问题，各区域政策孤立制定且仅在疫情升级后才调整，缺乏主动干预和全局协调，影响整体防控效果

Method: 为每个行政区域分配一个LLM智能体作为AI政策助手，智能体基于区域特定流行病学动态进行推理，同时通过结构化通信与其他智能体协作，整合真实数据、疫情演化模拟器和闭环模拟过程，共同探索反事实干预场景并制定协调政策

Result: 使用美国2020年4-12月州级COVID-19数据、真实流动记录和观察到的政策干预进行验证，相比现实疫情结果，在州级层面分别减少累计感染和死亡63.7%和40.1%，跨州汇总后分别减少39.0%和27.0%

Conclusion: LLM多智能体系统能够通过协调政策制定实现更有效的疫情防控，为公共卫生危机管理提供了新的技术路径

Abstract: Effective pandemic control requires timely and coordinated policymaking across administrative regions that are intrinsically interdependent. However, human-driven responses are often fragmented and reactive, with policies formulated in isolation and adjusted only after outbreaks escalate, undermining proactive intervention and global pandemic mitigation. To address this challenge, here we propose a large language model (LLM) multi-agent policymaking framework that supports coordinated and proactive pandemic control across regions. Within our framework, each administrative region is assigned an LLM agent as an AI policymaking assistant. The agent reasons over region-specific epidemiological dynamics while communicating with other agents to account for cross-regional interdependencies. By integrating real-world data, a pandemic evolution simulator, and structured inter-agent communication, our framework enables agents to jointly explore counterfactual intervention scenarios and synthesize coordinated policy decisions through a closed-loop simulation process. We validate the proposed framework using state-level COVID-19 data from the United States between April and December 2020, together with real-world mobility records and observed policy interventions. Compared with real-world pandemic outcomes, our approach reduces cumulative infections and deaths by up to 63.7% and 40.1%, respectively, at the individual state level, and by 39.0% and 27.0%, respectively, when aggregated across states. These results demonstrate that LLM multi-agent systems can enable more effective pandemic control with coordinated policymaking...

</details>


### [63] [RISER: Orchestrating Latent Reasoning Skills for Adaptive Activation Steering](https://arxiv.org/abs/2601.09269)
*Wencheng Ye,Liang Peng,Xiaoyang Yuan,Yi Bin,Pengpeng Zeng,Hengyu Jin,Heng Tao Shen*

Main category: cs.AI

TL;DR: RISER：基于路由器的自适应激活干预框架，通过动态组合可重用推理向量提升LLM推理能力，无需参数更新


<details>
  <summary>Details</summary>
Motivation: 现有领域特定推理方法依赖训练密集的参数更新，而激活干预方法使用静态手动干预，无法适应复杂推理的动态特性

Method: 提出RISER框架：1）构建可重用推理向量库；2）使用轻量级路由器动态组合向量；3）通过强化学习优化路由器，在任务级奖励下激活潜在认知原语

Result: 在7个多样化基准测试中，相比基础模型获得3.4-6.5%的平均零样本准确率提升；相比CoT推理，token效率提高2-3倍且保持稳健准确率增益

Conclusion: RISER能够自主组合多个向量形成可解释的精确控制策略，为更可控、高效的LLM推理指明了方向

Abstract: Recent work on domain-specific reasoning with large language models (LLMs) often relies on training-intensive approaches that require parameter updates. While activation steering has emerged as a parameter efficient alternative, existing methods apply static, manual interventions that fail to adapt to the dynamic nature of complex reasoning. To address this limitation, we propose RISER (Router-based Intervention for Steerable Enhancement of Reasoning), a plug-and-play intervention framework that adaptively steers LLM reasoning in activation space. RISER constructs a library of reusable reasoning vectors and employs a lightweight Router to dynamically compose them for each input. The Router is optimized via reinforcement learning under task-level rewards, activating latent cognitive primitives in an emergent and compositional manner. Across seven diverse benchmarks, RISER yields 3.4-6.5% average zero-shot accuracy improvements over the base model while surpassing CoT-style reasoning with 2-3x higher token efficiency and robust accuracy gains. Further analysis shows that RISER autonomously combines multiple vectors into interpretable, precise control strategies, pointing toward more controllable and efficient LLM reasoning.

</details>


### [64] [$A^3$-Bench: Benchmarking Memory-Driven Scientific Reasoning via Anchor and Attractor Activation](https://arxiv.org/abs/2601.09274)
*Jian Zhang,Yu He,Zhiyuan Wang,Zhangqi Wang,Kai He,Fangzhi Xu,Qika Lin,Jun Liu*

Main category: cs.AI

TL;DR: 该论文提出了A³-Bench基准，用于评估科学推理中的记忆驱动机制，重点关注锚点和吸引子的激活与利用。


<details>
  <summary>Details</summary>
Motivation: 现有基准主要评估最终答案或逐步推理的连贯性，但忽视了人类科学推理中基于记忆驱动的机制，即激活先验知识锚点和吸引子并将其整合到多步推理中的过程。

Method: 1) 使用SAPM过程（主体、锚点&吸引子、问题、记忆发展）标注了2,198个跨领域的科学推理问题；2) 提出了基于锚点和吸引子的双尺度记忆评估框架；3) 引入了AAUI（锚点-吸引子利用指数）指标来衡量记忆激活率。

Result: 通过在不同基础模型和范式上的实验验证了A³-Bench的有效性，并分析了记忆激活如何影响推理性能，为理解记忆驱动的科学推理提供了见解。

Conclusion: 该研究填补了科学推理评估中记忆驱动机制的空白，提出的A³-Bench基准和AAUI指标能够有效评估模型在科学推理中激活和利用先验知识的能力，为改进AI系统的科学推理能力提供了新方向。

Abstract: Scientific reasoning relies not only on logical inference but also on activating prior knowledge and experiential structures. Memory can efficiently reuse knowledge and enhance reasoning consistency and stability. However, existing benchmarks mainly evaluate final answers or step-by-step coherence, overlooking the \textit{memory-driven} mechanisms that underlie human reasoning, which involves activating anchors and attractors, then integrating them into multi-step inference. To address this gap, we propose $A^3$-Bench~ https://a3-bench.github.io, a benchmark designed to evaluate scientific reasoning through dual-scale memory-driven activation, grounded in Anchor and Attractor Activation. First, we annotate 2,198 science reasoning problems across domains using the SAPM process(subject, anchor & attractor, problem, and memory developing). Second, we introduce a dual-scale memory evaluation framework utilizing anchors and attractors, along with the AAUI(Anchor--Attractor Utilization Index) metric to measure memory activation rates. Finally, through experiments with various base models and paradigms, we validate $A^3$-Bench and analyze how memory activation impacts reasoning performance, providing insights into memory-driven scientific reasoning.

</details>


### [65] [STaR: Sensitive Trajectory Regulation for Unlearning in Large Reasoning Models](https://arxiv.org/abs/2601.09281)
*Jingjing Zhou,Gaoxiang Cong,Li Su,Liang Li*

Main category: cs.AI

TL;DR: STaR是一个无需参数的推理时遗忘框架，专门解决大型推理模型在生成复杂思维链过程中的隐私风险问题，通过语义检测、安全约束注入、轨迹感知抑制和自适应过滤实现全链条隐私保护。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型能够生成复杂的思维链轨迹，但这引入了严重的隐私风险，因为敏感信息可能深度嵌入整个推理过程中。现有的LLM遗忘方法通常只关注修改最终答案，无法移除中间步骤的敏感内容，导致持续的隐私泄露和安全退化。

Method: STaR框架包含四个关键步骤：1) 通过语义感知检测识别敏感内容；2) 通过安全提示前缀注入全局安全约束；3) 执行轨迹感知抑制，动态阻止整个推理链中的敏感内容；4) 应用令牌级自适应过滤，防止生成精确和转述的敏感令牌。

Result: 在R-TOFU基准测试上的实验表明，STaR实现了全面稳定的遗忘效果，同时保持最小的效用损失。此外，作者还引入了两个新评估指标：多解码一致性评估和多粒度成员推理攻击评估，为隐私保护推理设立了新标准。

Conclusion: STaR框架为大型推理模型提供了有效的推理时隐私保护解决方案，解决了现有方法在思维链隐私保护方面的不足，通过全链条的敏感内容抑制实现了更全面的隐私保护。

Abstract: Large Reasoning Models (LRMs) have advanced automated multi-step reasoning, but their ability to generate complex Chain-of-Thought (CoT) trajectories introduces severe privacy risks, as sensitive information may be deeply embedded throughout the reasoning process. Existing Large Language Models (LLMs) unlearning approaches that typically focus on modifying only final answers are insufficient for LRMs, as they fail to remove sensitive content from intermediate steps, leading to persistent privacy leakage and degraded security. To address these challenges, we propose Sensitive Trajectory Regulation (STaR), a parameter-free, inference-time unlearning framework that achieves robust privacy protection throughout the reasoning process. Specifically, we first identify sensitive content via semantic-aware detection. Then, we inject global safety constraints through secure prompt prefix. Next, we perform trajectory-aware suppression to dynamically block sensitive content across the entire reasoning chain. Finally, we apply token-level adaptive filtering to prevent both exact and paraphrased sensitive tokens during generation. Furthermore, to overcome the inadequacies of existing evaluation protocols, we introduce two metrics: Multi-Decoding Consistency Assessment (MCS), which measures the consistency of unlearning across diverse decoding strategies, and Multi-Granularity Membership Inference Attack (MIA) Evaluation, which quantifies privacy protection at both answer and reasoning-chain levels. Experiments on the R-TOFU benchmark demonstrate that STaR achieves comprehensive and stable unlearning with minimal utility loss, setting a new standard for privacy-preserving reasoning in LRMs.

</details>


### [66] [Cluster Workload Allocation: Semantic Soft Affinity Using Natural Language Processing](https://arxiv.org/abs/2601.09282)
*Leszek Sliwko,Jolanta Mizeria-Pietraszko*

Main category: cs.AI

TL;DR: 论文提出基于自然语言处理的语义化意图驱动调度范式，通过LLM解析自然语言分配提示注释，实现集群工作负载的简化配置。


<details>
  <summary>Details</summary>
Motivation: 集群工作负载分配通常需要复杂配置，存在可用性差距。传统调度系统配置复杂，用户需要掌握专业技术知识，这限制了集群系统的易用性和可访问性。

Method: 采用语义化意图驱动调度范式，将大型语言模型（LLM）通过Kubernetes调度器扩展器集成，解析自然语言分配提示注释以表达软亲和性偏好。开发了原型系统，包含集群状态缓存和意图分析器（使用AWS Bedrock）。

Result: LLM解析准确率高（在评估数据集上>95%的子集准确率），顶级模型如Amazon Nova Pro/Premier和Mistral Pixtral Large显著优于基线引擎。在六个场景的调度质量测试中，原型系统实现优于或等同于标准Kubernetes配置的放置效果，在复杂和定量场景以及处理冲突软偏好方面表现突出。

Conclusion: 验证了使用LLM进行可访问调度的可行性，确认了语义化软亲和性简化工作负载编排的可行性。但指出了同步LLM延迟等限制，建议采用异步处理以实现生产就绪。

Abstract: Cluster workload allocation often requires complex configurations, creating a usability gap. This paper introduces a semantic, intent-driven scheduling paradigm for cluster systems using Natural Language Processing. The system employs a Large Language Model (LLM) integrated via a Kubernetes scheduler extender to interpret natural language allocation hint annotations for soft affinity preferences. A prototype featuring a cluster state cache and an intent analyzer (using AWS Bedrock) was developed. Empirical evaluation demonstrated high LLM parsing accuracy (>95% Subset Accuracy on an evaluation ground-truth dataset) for top-tier models like Amazon Nova Pro/Premier and Mistral Pixtral Large, significantly outperforming a baseline engine. Scheduling quality tests across six scenarios showed the prototype achieved superior or equivalent placement compared to standard Kubernetes configurations, particularly excelling in complex and quantitative scenarios and handling conflicting soft preferences. The results validate using LLMs for accessible scheduling but highlight limitations like synchronous LLM latency, suggesting asynchronous processing for production readiness. This work confirms the viability of semantic soft affinity for simplifying workload orchestration.

</details>


### [67] [Long-term Task-oriented Agent: Proactive Long-term Intent Maintenance in Dynamic Environments](https://arxiv.org/abs/2601.09382)
*Qinglong Shi,Donghai Wang,Hantao Zhou,Jiguo Li,Jun Xu,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.AI

TL;DR: 提出主动式任务导向智能体新范式，解决现有语言模型只能被动响应用户查询的问题，通过意图条件监控和事件触发跟进实现长期意图维护和环境适应


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型智能体主要采用被动响应范式，只能在短期会话中响应用户即时查询，无法维持用户的长期意图和适应动态变化的外部环境

Method: 1. 形式化主动性为两个关键能力：意图条件监控（基于对话历史自主制定触发条件）和事件触发跟进（检测到有用环境更新时主动与用户互动）
2. 引入高质量数据合成流水线构建动态环境中的复杂多轮对话数据
3. 提出新的基准测试ChronosBench评估动态环境中任务导向交互
4. 使用合成数据进行监督学习微调模型

Result: 1. 评估当前领先的闭源和开源模型，揭示它们在长期任务导向交互中的缺陷
2. 使用合成数据微调的模型在包含用户意图转变的复杂任务中达到85.19%的任务完成率，优于其他测试模型
3. 验证了数据驱动策略的有效性

Conclusion: 提出的主动式任务导向智能体新范式能够弥合相对静态的用户需求与动态环境之间的差距，通过意图条件监控和事件触发跟进实现长期意图维护和环境适应，数据驱动方法在复杂任务中表现出色

Abstract: Current large language model agents predominantly operate under a reactive paradigm, responding only to immediate user queries within short-term sessions. This limitation hinders their ability to maintain long-term user's intents and dynamically adapt to evolving external environments. In this paper, we propose a novel interaction paradigm for proactive Task-oriented Agents capable of bridging the gap between relatively static user's needs and a dynamic environment. We formalize proactivity through two key capabilities, (i) Intent-Conditioned Monitoring: The agent autonomously formulates trigger conditions based on dialog history; (ii) Event-Triggered Follow-up: The agent actively engages the user upon detecting useful environmental updates. We introduce a high-quality data synthesis pipeline to construct complex, multi-turn dialog data in a dynamic environment. Furthermore, we attempt to address the lack of evaluation criteria of task-oriented interaction in a dynamic environment by proposing a new benchmark, namely ChronosBench. We evaluated some leading close-source and open-source models at present and revealed their flaws in long-term task-oriented interaction. Furthermore, our fine-tuned model trained using synthetic data for supervised learning achieves a task completion rate of 85.19% for complex tasks including shifts in user intent, outperforming other models under test. And the result validated the effectiveness of our data-driven strategy.

</details>


### [68] [What Do LLM Agents Know About Their World? Task2Quiz: A Paradigm for Studying Environment Understanding](https://arxiv.org/abs/2601.09503)
*Siyuan Liu,Hongbang Yuan,Xinze Li,Ziyue Zhu,Yixin Cao,Yu-Gang Jiang*

Main category: cs.AI

TL;DR: 论文提出T2Q评估范式，通过将任务执行与环境理解解耦来评估LLM智能体的泛化能力，发现任务成功不能有效反映环境理解水平。


<details>
  <summary>Details</summary>
Motivation: 当前LLM智能体在复杂决策和工具使用任务中表现出色，但其在不同环境中的泛化能力研究不足。现有评估主要依赖基于轨迹的任务成功率指标，无法评估智能体是否真正掌握了可迁移的环境模型。

Method: 提出Task-to-Quiz（T2Q）评估范式，将任务执行与环境理解解耦。具体实现为T2QBench评估套件，包含30个环境和1,967个基于真实状态的问答对，涵盖多个难度级别。

Result: 实验表明：1）任务成功率不能有效反映环境理解水平；2）当前记忆机制无法有效帮助智能体获得真实的环境模型；3）主动探索和细粒度状态表示是主要瓶颈。

Conclusion: T2Q评估范式为开发更具泛化能力的自主智能体提供了坚实基础，识别了环境理解的关键瓶颈，为未来研究指明了方向。

Abstract: Large language model (LLM) agents have demonstrated remarkable capabilities in complex decision-making and tool-use tasks, yet their ability to generalize across varying environments remains a under-examined concern. Current evaluation paradigms predominantly rely on trajectory-based metrics that measure task success, while failing to assess whether agents possess a grounded, transferable model of the environment. To address this gap, we propose Task-to-Quiz (T2Q), a deterministic and automated evaluation paradigm designed to decouple task execution from world-state understanding. We instantiate this paradigm in T2QBench, a suite comprising 30 environments and 1,967 grounded QA pairs across multiple difficulty levels. Our extensive experiments reveal that task success is often a poor proxy for environment understanding, and that current memory machanism can not effectively help agents acquire a grounded model of the environment. These findings identify proactive exploration and fine-grained state representation as primary bottlenecks, offering a robust foundation for developing more generalizable autonomous agents.

</details>


### [69] [Omni-R1: Towards the Unified Generative Paradigm for Multimodal Reasoning](https://arxiv.org/abs/2601.09536)
*Dongjie Cheng,Yongqi Li,Zhixin Ma,Hongru Cai,Yupeng Hu,Wenjie Wang,Liqiang Nie,Wenjie Li*

Main category: cs.AI

TL;DR: Omni-R1提出了一种统一的生成式多模态推理框架，通过在推理过程中生成中间图像来统一多种多模态推理技能，无需特定任务模式。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs要么专注于纯文本推理，要么采用单一任务特定的推理模式，限制了在不同多模态任务上的泛化能力。许多多模态任务需要多样化的推理技能，如放大特定区域或在图像中标记对象。

Method: 提出统一的生成式多模态推理范式，通过生成中间图像来统一多样化的多模态推理技能。具体实现为Omni-R1，采用两阶段SFT+RL框架，包含感知对齐损失和感知奖励，实现功能性图像生成。还提出Omni-R1-Zero，从纯文本推理数据中引导生成逐步可视化，无需多模态标注。

Result: 实验结果表明，Omni-R1能够在广泛的多模态任务上实现统一的生成式推理。Omni-R1-Zero在平均性能上能够匹配甚至超越Omni-R1，显示了生成式多模态推理的潜力。

Conclusion: 统一的生成式多模态推理范式通过生成中间图像来统一多样化推理技能，Omni-R1和Omni-R1-Zero展示了这一方法的有效性，为生成式多模态推理提供了有前景的方向。

Abstract: Multimodal Large Language Models (MLLMs) are making significant progress in multimodal reasoning. Early approaches focus on pure text-based reasoning. More recent studies have incorporated multimodal information into the reasoning steps; however, they often follow a single task-specific reasoning pattern, which limits their generalizability across various multimodal tasks. In fact, there are numerous multimodal tasks requiring diverse reasoning skills, such as zooming in on a specific region or marking an object within an image. To address this, we propose unified generative multimodal reasoning, which unifies diverse multimodal reasoning skills by generating intermediate images during the reasoning process. We instantiate this paradigm with Omni-R1, a two-stage SFT+RL framework featuring perception alignment loss and perception reward, thereby enabling functional image generation. Additionally, we introduce Omni-R1-Zero, which eliminates the need for multimodal annotations by bootstrapping step-wise visualizations from text-only reasoning data. Empirical results show that Omni-R1 achieves unified generative reasoning across a wide range of multimodal tasks, and Omni-R1-Zero can match or even surpass Omni-R1 on average, suggesting a promising direction for generative multimodal reasoning.

</details>


### [70] [Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning](https://arxiv.org/abs/2601.09667)
*Zhiyuan Hu,Yunhai Hu,Juncheng Liu,Shuyue Stella Li,Yucheng Wang,Zhen Xu,See-Kiong Ng,Anh Tuan Luu,Xinxing Xu,Bryan Hooi,Cynthia Breazeal,Hae Won Park*

Main category: cs.AI

TL;DR: MATTRL框架通过测试时强化学习，在多智能体推理过程中注入结构化文本经验，无需训练即可提升多智能体系统的性能


<details>
  <summary>Details</summary>
Motivation: 多智能体系统虽然通过多样性和交叉验证获得鲁棒性，但多智能体强化学习训练资源密集且不稳定，存在非平稳性和稀疏高方差奖励问题

Method: 提出多智能体测试时强化学习框架，组建专家团队进行多轮讨论，检索并整合测试时经验，通过共识机制进行最终决策，并研究信用分配机制构建轮次级经验池

Result: 在医学、数学和教育等挑战性基准测试中，MATTRL比多智能体基线平均准确率提高3.67%，比单智能体基线提高8.67%

Conclusion: MATTRL提供了一种稳定、有效且高效的路径，无需调优即可实现分布偏移鲁棒的多智能体推理

Abstract: Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.

</details>
