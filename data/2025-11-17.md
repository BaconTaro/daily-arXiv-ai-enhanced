<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 23]
- [cs.HC](#cs.HC) [Total: 5]
- [cs.LG](#cs.LG) [Total: 50]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Co-EPG: A Framework for Co-Evolution of Planning and Grounding in Autonomous GUI Agents](https://arxiv.org/abs/2511.10705)
*Yuan Zhao,Hualei Zhu,Tingyu Jiang,Shen Li,Xiaohang Xu,Hao Henry Wang*

Main category: cs.AI

TL;DR: Co-EPG是一个自迭代训练框架，通过规划和接地模型的协同进化来解决GUI任务自动化中的挑战，无需外部数据即可实现持续性能提升。


<details>
  <summary>Details</summary>
Motivation: 当前GUI智能体方法存在两个根本限制：(1)跨模型协同利用不足，(2)过度依赖合成数据生成但利用不足。需要一种能够充分利用模型间协同效应的方法。

Method: 提出Co-EPG框架，建立规划和接地模型的迭代正反馈循环：规划模型通过GRPO在接地奖励指导下探索策略，生成多样化数据优化接地模型；优化的接地模型为后续规划模型训练提供更有效的奖励，实现持续改进。

Result: 在Multimodal-Mind2Web和AndroidControl基准测试中，仅经过三次迭代就超越了现有最先进方法，无需外部数据。智能体在每次迭代中持续改进，展现出强大的自我增强能力。

Conclusion: 这项工作为GUI智能体建立了新的训练范式，从孤立优化转向集成、自驱动的协同进化方法。

Abstract: Graphical User Interface (GUI) task automation constitutes a critical frontier in artificial intelligence research. While effective GUI agents synergistically integrate planning and grounding capabilities, current methodologies exhibit two fundamental limitations: (1) insufficient exploitation of cross-model synergies, and (2) over-reliance on synthetic data generation without sufficient utilization. To address these challenges, we propose Co-EPG, a self-iterative training framework for Co-Evolution of Planning and Grounding. Co-EPG establishes an iterative positive feedback loop: through this loop, the planning model explores superior strategies under grounding-based reward guidance via Group Relative Policy Optimization (GRPO), generating diverse data to optimize the grounding model. Concurrently, the optimized Grounding model provides more effective rewards for subsequent GRPO training of the planning model, fostering continuous improvement. Co-EPG thus enables iterative enhancement of agent capabilities through self-play optimization and training data distillation. On the Multimodal-Mind2Web and AndroidControl benchmarks, our framework outperforms existing state-of-the-art methods after just three iterations without requiring external data. The agent consistently improves with each iteration, demonstrating robust self-enhancement capabilities. This work establishes a novel training paradigm for GUI agents, shifting from isolated optimization to an integrated, self-driven co-evolution approach.

</details>


### [2] [Picking a Representative Set of Solutions in Multiobjective Optimization: Axioms, Algorithms, and Experiments](https://arxiv.org/abs/2511.10716)
*Niclas Boehmer,Maximilian T. Wittmann*

Main category: cs.AI

TL;DR: 本文研究了多目标优化中的帕累托剪枝问题，将其重新定义为多赢家投票问题，分析了现有质量度量的不足，提出了新的定向覆盖度量，并研究了不同质量度量的计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 现实决策问题常涉及多目标优化，帕累托最优解数量众多，决策者难以选择。为减轻决策者认知负担，需要从帕累托最优解集中选择固定大小的代表性子集。

Method: 将帕累托剪枝问题重新构建为多赢家投票问题，进行公理分析，引入新的定向覆盖度量，分析各种质量度量的计算复杂性，并进行实验评估。

Result: 发现现有质量度量存在反直觉行为，提出了新的定向覆盖度量，识别了可处理和难处理问题的边界，实验表明质量度量选择对解集特征有决定性影响。

Conclusion: 定向覆盖度量在各种设置下表现竞争力强，质量度量的选择对帕累托剪枝结果具有重要影响，计算复杂性分析为实际问题提供了指导。

Abstract: Many real-world decision-making problems involve optimizing multiple objectives simultaneously, rendering the selection of the most preferred solution a non-trivial problem: All Pareto optimal solutions are viable candidates, and it is typically up to a decision maker to select one for implementation based on their subjective preferences. To reduce the cognitive load on the decision maker, previous work has introduced the Pareto pruning problem, where the goal is to compute a fixed-size subset of Pareto optimal solutions that best represent the full set, as evaluated by a given quality measure. Reframing Pareto pruning as a multiwinner voting problem, we conduct an axiomatic analysis of existing quality measures, uncovering several unintuitive behaviors. Motivated by these findings, we introduce a new measure, directed coverage. We also analyze the computational complexity of optimizing various quality measures, identifying previously unknown boundaries between tractable and intractable cases depending on the number and structure of the objectives. Finally, we present an experimental evaluation, demonstrating that the choice of quality measure has a decisive impact on the characteristics of the selected set of solutions and that our proposed measure performs competitively or even favorably across a range of settings.

</details>


### [3] [Structure-Aware Encodings of Argumentation Properties for Clique-width](https://arxiv.org/abs/2511.10767)
*Yasir Mahmood,Markus Hecher,Johanna Groven,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文研究了图参数clique-width在编码中的应用，特别针对抽象论证框架。设计了从论证问题到(Q)SAT的新颖归约，这些归约线性保持clique-width，建立了有向分解引导(DDG)归约方法。


<details>
  <summary>Details</summary>
Motivation: 图的结构度量（如treewidth）在计算复杂性中是核心工具，现代SAT求解器在小treewidth实例上表现高效。clique-width比treewidth更通用，可在稠密图上保持较小值，但关于其编码能力的研究较少。本文旨在理解clique-width的编码能力，选择抽象论证框架作为研究对象。

Method: 设计了从论证问题到(Q)SAT的新颖归约方法，这些归约线性保持clique-width，形成了有向分解引导(DDG)归约。该方法适用于所有论证语义，包括计数问题。

Result: 建立了所有论证语义（包括计数）的新结果。证明了DDG归约引起的开销在合理假设下无法显著改进。

Conclusion: 本文开启了理解clique-width编码能力的研究方向，通过抽象论证框架展示了clique-width在编码中的潜力，为更广泛的图参数编码研究奠定了基础。

Abstract: Structural measures of graphs, such as treewidth, are central tools in computational complexity resulting in efficient algorithms when exploiting the parameter. It is even known that modern SAT solvers work efficiently on instances of small treewidth. Since these solvers are widely applied, research interests in compact encodings into (Q)SAT for solving and to understand encoding limitations. Even more general is the graph parameter clique-width, which unlike treewidth can be small for dense graphs. Although algorithms are available for clique-width, little is known about encodings. We initiate the quest to understand encoding capabilities with clique-width by considering abstract argumentation, which is a robust framework for reasoning with conflicting arguments. It is based on directed graphs and asks for computationally challenging properties, making it a natural candidate to study computational properties. We design novel reductions from argumentation problems to (Q)SAT. Our reductions linearly preserve the clique-width, resulting in directed decomposition-guided (DDG) reductions. We establish novel results for all argumentation semantics, including counting. Notably, the overhead caused by our DDG reductions cannot be significantly improved under reasonable assumptions.

</details>


### [4] [Potential Outcome Rankings for Counterfactual Decision Making](https://arxiv.org/abs/2511.10776)
*Yuta Kawakami,Jin Tian*

Main category: cs.AI

TL;DR: 本文提出了两种新的反事实决策指标：潜在结果排序概率(PoR)和获得最佳潜在结果概率(PoB)，用于在不确定性下进行因果推理的决策制定。


<details>
  <summary>Details</summary>
Motivation: 在不确定性下进行反事实决策时，决策者需要比较不同候选行动的预期潜在结果。现有方法存在局限，需要新的决策规则来更准确地评估行动偏好。

Method: 引入PoR和PoB两个新指标，建立识别定理和边界推导，提出估计方法，并通过数值实验验证估计器的有限样本性质。

Result: 建立了PoR和PoB的识别理论框架，推导了边界条件，开发了有效的估计方法，并在真实数据集上展示了应用效果。

Conclusion: PoR和PoB为反事实决策提供了新的有效工具，能够更准确地评估个体层面的行动偏好和最优结果概率。

Abstract: Counterfactual decision-making in the face of uncertainty involves selecting the optimal action from several alternatives using causal reasoning. Decision-makers often rank expected potential outcomes (or their corresponding utility and desirability) to compare the preferences of candidate actions. In this paper, we study new counterfactual decision-making rules by introducing two new metrics: the probabilities of potential outcome ranking (PoR) and the probability of achieving the best potential outcome (PoB). PoR reveals the most probable ranking of potential outcomes for an individual, and PoB indicates the action most likely to yield the top-ranked outcome for an individual. We then establish identification theorems and derive bounds for these metrics, and present estimation methods. Finally, we perform numerical experiments to illustrate the finite-sample properties of the estimators and demonstrate their application to a real-world dataset.

</details>


### [5] [From Efficiency to Adaptivity: A Deeper Look at Adaptive Reasoning in Large Language Models](https://arxiv.org/abs/2511.10788)
*Chao Wu,Baoheng Li,Mingchen Gao,Zhenyi Wang*

Main category: cs.AI

TL;DR: 本调查从适应性角度重新审视LLM推理，提出将推理努力根据输入难度和不确定性进行分配的能力作为核心评估标准，建立了训练式和训练无关方法的系统分类法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM对简单和复杂任务采用统一的推理策略，导致对简单问题生成过长推理链而对困难任务推理不足，需要开发适应性推理能力。

Method: 将适应性推理形式化为控制增强的策略优化问题，提出系统分类法将现有方法分为训练式方法（强化学习、监督微调、学习控制器）和训练无关方法（提示条件化、反馈驱动停止、模块化组合）。

Result: 建立了连接经典认知范式与算法实现的框架，澄清了不同机制在实践中实现适应性推理的方式，实现了跨策略的系统比较。

Conclusion: 识别了自我评估、元推理和人类对齐推理控制等开放挑战，为LLM推理研究提供了新的理论框架和分类体系。

Abstract: Recent advances in large language models (LLMs) have made reasoning a central benchmark for evaluating intelligence. While prior surveys focus on efficiency by examining how to shorten reasoning chains or reduce computation, this view overlooks a fundamental challenge: current LLMs apply uniform reasoning strategies regardless of task complexity, generating long traces for trivial problems while failing to extend reasoning for difficult tasks. This survey reframes reasoning through the lens of {adaptivity}: the capability to allocate reasoning effort based on input characteristics such as difficulty and uncertainty. We make three contributions. First, we formalize deductive, inductive, and abductive reasoning within the LLM context, connecting these classical cognitive paradigms with their algorithmic realizations. Second, we formalize adaptive reasoning as a control-augmented policy optimization problem balancing task performance with computational cost, distinguishing learned policies from inference-time control mechanisms. Third, we propose a systematic taxonomy organizing existing methods into training-based approaches that internalize adaptivity through reinforcement learning, supervised fine-tuning, and learned controllers, and training-free approaches that achieve adaptivity through prompt conditioning, feedback-driven halting, and modular composition. This framework clarifies how different mechanisms realize adaptive reasoning in practice and enables systematic comparison across diverse strategies. We conclude by identifying open challenges in self-evaluation, meta-reasoning, and human-aligned reasoning control.

</details>


### [6] [HyperComplEx: Adaptive Multi-Space Knowledge Graph Embeddings](https://arxiv.org/abs/2511.10842)
*Jugal Gajjar,Kaustik Ranaware,Kamalasankari Subramaniakuppusamy,Vaibhav Gandhi*

Main category: cs.AI

TL;DR: HyperComplEx是一个混合知识图谱嵌入框架，通过注意力机制自适应结合双曲空间、复数空间和欧几里得空间，解决了现有方法在处理不同关系类型时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱嵌入方法在处理大规模多样化关系类型时存在关键限制：欧几里得模型难以处理层次结构，向量空间模型无法捕捉不对称关系，双曲模型在对称关系上表现不佳。

Method: 提出关系特定的空间加权策略，通过学习注意力机制动态为每种关系类型选择最优几何空间，并使用多空间一致性损失确保跨空间预测的一致性。

Result: 在从1K论文到10M论文的知识图谱上评估，相比TransE、RotatE等基线方法获得持续改进。在10M论文数据集上达到0.612 MRR，相对最佳基线提升4.8%，推理速度85ms/三元组。

Conclusion: HyperComplEx通过自适应几何空间组合有效解决了知识图谱嵌入的多样性关系建模问题，具有良好的可扩展性和效率。

Abstract: Knowledge graphs have emerged as fundamental structures for representing complex relational data across scientific and enterprise domains. However, existing embedding methods face critical limitations when modeling diverse relationship types at scale: Euclidean models struggle with hierarchies, vector space models cannot capture asymmetry, and hyperbolic models fail on symmetric relations. We propose HyperComplEx, a hybrid embedding framework that adaptively combines hyperbolic, complex, and Euclidean spaces via learned attention mechanisms. A relation-specific space weighting strategy dynamically selects optimal geometries for each relation type, while a multi-space consistency loss ensures coherent predictions across spaces. We evaluate HyperComplEx on computer science research knowledge graphs ranging from 1K papers (~25K triples) to 10M papers (~45M triples), demonstrating consistent improvements over state-of-the-art baselines including TransE, RotatE, DistMult, ComplEx, SEPA, and UltraE. Additional tests on standard benchmarks confirm significantly higher results than all baselines. On the 10M-paper dataset, HyperComplEx achieves 0.612 MRR, a 4.8% relative gain over the best baseline, while maintaining efficient training, achieving 85 ms inference per triple. The model scales near-linearly with graph size through adaptive dimension allocation. We release our implementation and dataset family to facilitate reproducible research in scalable knowledge graph embeddings.

</details>


### [7] [Advanced Tool for Traffic Crash Analysis: An AI-Driven Multi-Agent Approach to Pre-Crash Reconstruction](https://arxiv.org/abs/2511.10853)
*Gerui Xu,Boyou Chen,Huizhong Guo,Dave LeBlanc,Ananna Ahmed,Zhaonan Sun,Shan Bao*

Main category: cs.AI

TL;DR: 本研究开发了一个多智能体AI框架，用于从碎片化的碰撞数据中重建事故前场景并推断车辆行为。该框架在39个复杂追尾事故案例中实现了100%的准确率，超越了人类专家92%的准确率。


<details>
  <summary>Details</summary>
Motivation: 传统的交通事故重建依赖人类专家，在处理不完整多模态数据时往往产生不一致的结果。需要开发能够处理异构碰撞数据的AI系统来提高重建精度。

Method: 采用两阶段协作框架：第一阶段从多模态输入生成自然语言事故重建；第二阶段结合这些重建与时间性事件数据记录器数据进行深度事故推理。系统处理了277起追尾前车减速事故数据。

Result: 在39个复杂追尾事故案例评估中，框架实现了完美准确率，成功识别最相关EDR事件并正确区分撞击与被撞车辆，超越了人类研究人员92%的准确率。即使在处理不完整数据时也保持稳健性能。

Conclusion: 本研究展示了AI在处理异构碰撞数据方面的卓越能力，在重建碰撞动力学和表征事故前行为方面提供了前所未有的精度。

Abstract: Traffic collision reconstruction traditionally relies on human expertise, often yielding inconsistent results when analyzing incomplete multimodal data. This study develops a multi-agent AI framework that reconstructs pre-crash scenarios and infers vehicle behaviors from fragmented collision data. We present a two-phase collaborative framework combining reconstruction and reasoning phases. The system processes 277 rear-end lead vehicle deceleration (LVD) collisions from the Crash Investigation Sampling System, integrating textual crash reports, structured tabular data, and visual scene diagrams. Phase I generates natural-language crash reconstructions from multimodal inputs. Phase II performs in-depth crash reasoning by combining these reconstructions with temporal Event Data Recorder (EDR).For validation, we applied it to all LVD cases, focusing on a subset of 39 complex crashes where multiple EDR records per collision introduced ambiguity (e.g., due to missing or conflicting data).The evaluation of the 39 LVD crash cases revealed our framework achieved perfect accuracy across all test cases, successfully identifying both the most relevant EDR event and correctly distinguishing striking versus struck vehicles, surpassing the 92% accuracy achieved by human researchers on the same challenging dataset. The system maintained robust performance even when processing incomplete data, including missing or erroneous EDR records and ambiguous scene diagrams. This study demonstrates superior AI capabilities in processing heterogeneous collision data, providing unprecedented precision in reconstructing impact dynamics and characterizing pre-crash behaviors.

</details>


### [8] [Enhancing Demand-Oriented Regionalization with Agentic AI and Local Heterogeneous Data for Adaptation Planning](https://arxiv.org/abs/2511.10857)
*Seyedeh Mobina Noorani,Shangde Gao,Changjie Chen,Karla Saldana Ochoa*

Main category: cs.AI

TL;DR: 提出一个基于智能AI的规划支持系统，用于创建动态规划单元，通过结合人类参与原则实现透明和适应性，应用于灾害规划中的需求导向区域生成。


<details>
  <summary>Details</summary>
Motivation: 传统规划单元（如人口普查区、邮政编码区）无法准确捕捉当地社区的具体需求，缺乏灵活性来实施有效的灾害预防或响应策略。

Method: 基于代表性初始化空间约束自组织映射（RepSC-SOM），扩展传统SOM方法，结合自适应地理过滤和区域生长细化，AI智能体通过推理、规划和行动来指导过程。

Result: 通过佛罗里达州杰克逊维尔洪水相关风险的案例研究，展示了平台允许用户交互式探索、生成和评估区域化的能力。

Conclusion: 该平台将计算严谨性与用户驱动决策相结合，为灾害规划提供了有效的动态规划单元生成工具。

Abstract: Conventional planning units or urban regions, such as census tracts, zip codes, or neighborhoods, often do not capture the specific demands of local communities and lack the flexibility to implement effective strategies for hazard prevention or response. To support the creation of dynamic planning units, we introduce a planning support system with agentic AI that enables users to generate demand-oriented regions for disaster planning, integrating the human-in-the-loop principle for transparency and adaptability. The platform is built on a representative initialized spatially constrained self-organizing map (RepSC-SOM), extending traditional SOM with adaptive geographic filtering and region-growing refinement, while AI agents can reason, plan, and act to guide the process by suggesting input features, guiding spatial constraints, and supporting interactive exploration. We demonstrate the capabilities of the platform through a case study on the flooding-related risk in Jacksonville, Florida, showing how it allows users to explore, generate, and evaluate regionalization interactively, combining computational rigor with user-driven decision making.

</details>


### [9] [LLM enhanced graph inference for long-term disease progression modelling](https://arxiv.org/abs/2511.10890)
*Tiantian He,An Zhao,Elinor Thompson,Anna Schroder,Ahmed Abdulaal,Frederik Barkhof,Daniel C. Alexander*

Main category: cs.AI

TL;DR: 提出了一种利用大型语言模型作为专家指导的新框架，从不规则采样的纵向患者数据中学习神经退行性疾病进展，特别是阿尔茨海默病的病理传播。


<details>
  <summary>Details</summary>
Motivation: 当前方法过于简化大脑连接性，假设单一模态连接组作为疾病传播基质，导致病理传播预测不准确。纯数据驱动方法由于缺乏适当约束而面临可识别性问题。

Method: 利用大型语言模型作为区域变量交互的专家指导，同时优化长期疾病轨迹构建和生物约束的图结构学习，结合tau-PET成像数据。

Result: 新框架在阿尔茨海默病队列中表现出比传统方法更优的预测准确性和可解释性，揭示了超出传统连接性测量的额外疾病驱动因素。

Conclusion: 该框架通过整合LLMs的多模态关系合成能力，能够更准确地建模神经退行性疾病进展，为理解疾病机制提供了新的见解。

Abstract: Understanding the interactions between biomarkers among brain regions during neurodegenerative disease is essential for unravelling the mechanisms underlying disease progression. For example, pathophysiological models of Alzheimer's Disease (AD) typically describe how variables, such as regional levels of toxic proteins, interact spatiotemporally within a dynamical system driven by an underlying biological substrate, often based on brain connectivity. However, current methods grossly oversimplify the complex relationship between brain connectivity by assuming a single-modality brain connectome as the disease-spreading substrate. This leads to inaccurate predictions of pathology spread, especially during the long-term progression period. Meanhwile, other methods of learning such a graph in a purely data-driven way face the identifiability issue due to lack of proper constraint. We thus present a novel framework that uses Large Language Models (LLMs) as expert guides on the interaction of regional variables to enhance learning of disease progression from irregularly sampled longitudinal patient data. By leveraging LLMs' ability to synthesize multi-modal relationships and incorporate diverse disease-driving mechanisms, our method simultaneously optimizes 1) the construction of long-term disease trajectories from individual-level observations and 2) the biologically-constrained graph structure that captures interactions among brain regions with better identifiability. We demonstrate the new approach by estimating the pathology propagation using tau-PET imaging data from an Alzheimer's disease cohort. The new framework demonstrates superior prediction accuracy and interpretability compared to traditional approaches while revealing additional disease-driving factors beyond conventional connectivity measures.

</details>


### [10] [Multi-Agent Legal Verifier Systems for Data Transfer Planning](https://arxiv.org/abs/2511.10925)
*Ha-Thanh Nguyen,Wachara Fungwacharakorn,Ken Satoh*

Main category: cs.AI

TL;DR: 提出多智能体法律验证器，将合规检查分解为专门智能体，在AP隐私法规下实现72%准确率，比单智能体基线提高21个百分点。


<details>
  <summary>Details</summary>
Motivation: 在严格隐私法规（如日本个人信息保护法APPI）下，AI驱动的数据传输规划中的法律合规性日益重要。

Method: 多智能体法律验证器，将合规检查分解为法规解释、业务背景评估和风险评估三个专门智能体，通过结构化合成协议协调。

Result: 在200个AP案例数据集上，系统达到72%准确率，比单智能体基线提高21个百分点，在明确合规案例上达到90%准确率（基线为16%），同时保持对明确违规的完美检测。

Conclusion: 领域专业化和协调推理能显著提高法律AI性能，为可信赖和可解释的自动化合规验证提供可扩展的法规感知框架。

Abstract: Legal compliance in AI-driven data transfer planning is becoming increasingly critical under stringent privacy regulations such as the Japanese Act on the Protection of Personal Information (APPI). We propose a multi-agent legal verifier that decomposes compliance checking into specialized agents for statutory interpretation, business context evaluation, and risk assessment, coordinated through a structured synthesis protocol. Evaluated on a stratified dataset of 200 Amended APPI Article 16 cases with clearly defined ground truth labels and multiple performance metrics, the system achieves 72% accuracy, which is 21 percentage points higher than a single-agent baseline, including 90% accuracy on clear compliance cases (vs. 16% for the baseline) while maintaining perfect detection of clear violations. While challenges remain in ambiguous scenarios, these results show that domain specialization and coordinated reasoning can meaningfully improve legal AI performance, providing a scalable and regulation-aware framework for trustworthy and interpretable automated compliance verification.

</details>


### [11] [Requirements for Aligned, Dynamic Resolution of Conflicts in Operational Constraints](https://arxiv.org/abs/2511.10952)
*Steven J. Jones,Robert E. Wray,John E. Laird*

Main category: cs.AI

TL;DR: 本文探讨了自主AI系统在遇到训练数据未覆盖的复杂场景时，如何构建、评估和证明候选行动方案，以满足人类期望和价值观。


<details>
  <summary>Details</summary>
Motivation: 自主AI系统在实际部署中必然会遇到训练数据未覆盖的新场景，需要超越训练策略来构建和评估行动方案，以实现与人类期望和价值观一致的目标。

Method: 通过理论分析和实证案例研究，探讨了智能体如何整合规范性、实用性和情境性理解来选择更符合人类期望的行动方案。

Result: 识别了智能体在复杂现实环境中做出稳健决策所需的知识类型，包括规范性知识、实用性知识和情境性知识。

Conclusion: 智能体需要整合多种知识类型来在复杂现实环境中选择和执行更符合人类期望的行动方案，这超出了传统策略训练的范围。

Abstract: Deployed, autonomous AI systems must often evaluate multiple plausible courses of action (extended sequences of behavior) in novel or under-specified contexts. Despite extensive training, these systems will inevitably encounter scenarios where no available course of action fully satisfies all operational constraints (e.g., operating procedures, rules, laws, norms, and goals). To achieve goals in accordance with human expectations and values, agents must go beyond their trained policies and instead construct, evaluate, and justify candidate courses of action. These processes require contextual "knowledge" that may lie outside prior (policy) training. This paper characterizes requirements for agent decision making in these contexts. It also identifies the types of knowledge agents require to make decisions robust to agent goals and aligned with human expectations. Drawing on both analysis and empirical case studies, we examine how agents need to integrate normative, pragmatic, and situational understanding to select and then to pursue more aligned courses of action in complex, real-world environments.

</details>


### [12] [Faster Symmetry Breaking Constraints for Abstract Structures](https://arxiv.org/abs/2511.11029)
*Özgür Akgün,Mun See Chang,Ian P. Gent,Christopher Jefferson*

Main category: cs.AI

TL;DR: 提出了一种新的不完全方法来打破抽象结构的对称性，通过更好地利用其表示来处理不可区分对象产生的对称性，比先前方法更快。


<details>
  <summary>Details</summary>
Motivation: 在约束编程中，抽象结构需要转换为求解器支持的表示，但对称性破坏技术应用于抽象变量会产生大量复杂约束，性能较差。

Method: 通过更好地利用抽象结构的表示来打破对称性，特别针对不可区分对象产生的对称性，采用不完全方法。

Result: 新方法比Akgün等人2025年提出的先前方法更快。

Conclusion: 通过更好地利用抽象结构的表示，可以更有效地打破对称性，提高求解效率。

Abstract: In constraint programming and related paradigms, a modeller specifies their problem in a modelling language for a solver to search and return its solution(s). Using high-level modelling languages such as Essence, a modeller may express their problems in terms of abstract structures. These are structures not natively supported by the solvers, and so they have to be transformed into or represented as other structures before solving. For example, nested sets are abstract structures, and they can be represented as matrices in constraint solvers. Many problems contain symmetries and one very common and highly successful technique used in constraint programming is to "break" symmetries, to avoid searching for symmetric solutions. This can speed up the solving process by many orders of magnitude. Most of these symmetry-breaking techniques involve placing some kind of ordering for the variables of the problem, and picking a particular member under the symmetries, usually the smallest. Unfortunately, applying this technique to abstract variables produces a very large number of complex constraints that perform poorly in practice. In this paper, we demonstrate a new incomplete method of breaking the symmetries of abstract structures by better exploiting their representations. We apply the method in breaking the symmetries arising from indistinguishable objects, a commonly occurring type of symmetry, and show that our method is faster than the previous methods proposed in (Akgün et al. 2025).

</details>


### [13] [Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?](https://arxiv.org/abs/2511.11040)
*Qian Zhang,Yan Zheng,Jinyi Liu,Hebin Liang,Lanjun Wang*

Main category: cs.AI

TL;DR: 本文研究了多智能体辩论中角色分配策略的重要性，提出了"Truth Last"策略可提升推理任务性能22%，并开发了MADC策略来解决实际应用中未知真相的问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体辩论在增强推理能力方面显示出潜力，但角色分配策略这一关键方面尚未得到充分探索。

Method: 提出"Truth Last"角色分配策略，并开发了多智能体辩论一致性策略，通过路径一致性评估独立角色间的一致性，将一致性最高的角色模拟为真相。

Result: 在9个LLM模型上的验证显示，MADC策略持续表现出先进性能，有效克服了MAD的性能瓶颈，推理任务性能提升达22%。

Conclusion: MADC策略为LLM智能体扩展提供了关键改进路径，系统性地优化了多智能体辩论的核心机制。

Abstract: Recent studies on LLM agent scaling have highlighted the potential of Multi-Agent Debate (MAD) to enhance reasoning abilities. However, the critical aspect of role allocation strategies remains underexplored. In this study, we demonstrate that allocating roles with differing viewpoints to specific positions significantly impacts MAD's performance in reasoning tasks. Specifically, we find a novel role allocation strategy, "Truth Last", which can improve MAD performance by up to 22% in reasoning tasks. To address the issue of unknown truth in practical applications, we propose the Multi-Agent Debate Consistency (MADC) strategy, which systematically simulates and optimizes its core mechanisms. MADC incorporates path consistency to assess agreement among independent roles, simulating the role with the highest consistency score as the truth. We validated MADC across a range of LLMs (9 models), including the DeepSeek-R1 Distilled Models, on challenging reasoning tasks. MADC consistently demonstrated advanced performance, effectively overcoming MAD's performance bottlenecks and providing a crucial pathway for further improvements in LLM agent scaling.

</details>


### [14] [Autonomous Vehicle Path Planning by Searching With Differentiable Simulation](https://arxiv.org/abs/2511.11043)
*Asen Nachkov,Jan-Nico Zaech,Danda Pani Paudel,Xi Wang,Luc Van Gool*

Main category: cs.AI

TL;DR: 提出了DSS框架，利用可微分模拟器Waymax作为状态预测器和评估器，通过梯度下降优化动作序列，显著提升了自动驾驶中的跟踪和路径规划精度。


<details>
  <summary>Details</summary>
Motivation: 在自动驾驶中，规划对于避免碰撞和在复杂密集交通场景中导航至关重要。传统方法在学习策略、状态预测器和评估器时面临挑战。

Method: 使用可微分模拟器Waymax作为状态预测器和评估器，利用其硬编码动力学实现准确状态预测，通过可微分特性在动作序列上进行有效搜索，使用梯度下降优化想象未来轨迹中的动作。

Result: 实验表明，DSS（规划梯度和随机搜索的组合）相比序列预测、模仿学习、无模型RL和其他规划方法，显著提高了跟踪和路径规划的准确性。

Conclusion: DSS框架通过结合规划梯度和随机搜索，在自动驾驶规划任务中表现出优越性能，证明了可微分模拟器在动作序列优化中的有效性。

Abstract: Planning allows an agent to safely refine its actions before executing them in the real world. In autonomous driving, this is crucial to avoid collisions and navigate in complex, dense traffic scenarios. One way to plan is to search for the best action sequence. However, this is challenging when all necessary components - policy, next-state predictor, and critic - have to be learned. Here we propose Differentiable Simulation for Search (DSS), a framework that leverages the differentiable simulator Waymax as both a next state predictor and a critic. It relies on the simulator's hardcoded dynamics, making state predictions highly accurate, while utilizing the simulator's differentiability to effectively search across action sequences. Our DSS agent optimizes its actions using gradient descent over imagined future trajectories. We show experimentally that DSS - the combination of planning gradients and stochastic search - significantly improves tracking and path planning accuracy compared to sequence prediction, imitation learning, model-free RL, and other planning methods.

</details>


### [15] [Satisficing and Optimal Generalised Planning via Goal Regression (Extended Version)](https://arxiv.org/abs/2511.11095)
*Dillon Z. Chen,Till Hofmann,Toryn Q. Klassen,Sheila A. McIlraith*

Main category: cs.AI

TL;DR: 本文提出了一种新的广义规划方法，通过从训练问题中学习条件-动作规则来合成解决相关规划问题族的程序。


<details>
  <summary>Details</summary>
Motivation: 广义规划旨在合成能够解决相关规划问题族的程序，现有方法在合成成本、规划覆盖率和解决方案质量方面存在改进空间。

Method: 对每个训练问题，按顺序为目标原子计算最优规划，对结果规划执行目标回归，并将输出提升为一阶条件-动作规则集合。

Result: 实验表明，该方法在合成成本、规划覆盖率和解决方案质量三个指标上显著优于最先进的广义规划器。

Conclusion: 该方法能够学习有效的广义规划和状态空间剪枝公理，在各种经典和数值规划领域中表现出色。

Abstract: Generalised planning (GP) refers to the task of synthesising programs that solve families of related planning problems. We introduce a novel, yet simple method for GP: given a set of training problems, for each problem, compute an optimal plan for each goal atom in some order, perform goal regression on the resulting plans, and lift the corresponding outputs to obtain a set of first-order $\textit{Condition} \rightarrow \textit{Actions}$ rules. The rules collectively constitute a generalised plan that can be executed as is or alternatively be used to prune the planning search space. We formalise and prove the conditions under which our method is guaranteed to learn valid generalised plans and state space pruning axioms for search. Experiments demonstrate significant improvements over state-of-the-art (generalised) planners with respect to the 3 metrics of synthesis cost, planning coverage, and solution quality on various classical and numeric planning domains.

</details>


### [16] [GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models](https://arxiv.org/abs/2511.11134)
*Jingxuan Wei,Caijun Jia,Xi Bai,Xinglong Xu,Siyuan Li,Linzhuang Sun,Bihui Yu,Conghui He,Lijun Wu,Cheng Tan*

Main category: cs.AI

TL;DR: GGBench是一个专门评估几何生成推理能力的基准测试，旨在填补统一多模态模型在集成认知过程评估方面的空白。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试主要评估判别性理解或无约束图像生成，无法衡量生成推理的集成认知过程，几何构造作为需要语言理解和精确视觉生成融合的任务提供了理想的测试平台。

Method: 提出GGBench基准测试，通过几何构造任务系统性地诊断模型的理解、推理和主动构建解决方案的能力。

Result: GGBench为下一代智能系统设定了更严格的标准，提供了评估几何生成推理能力的综合框架。

Conclusion: 几何构造是评估统一多模态模型生成推理能力的理想测试平台，GGBench填补了现有评估方法的空白。

Abstract: The advent of Unified Multimodal Models (UMMs) signals a paradigm shift in artificial intelligence, moving from passive perception to active, cross-modal generation. Despite their unprecedented ability to synthesize information, a critical gap persists in evaluation: existing benchmarks primarily assess discriminative understanding or unconstrained image generation separately, failing to measure the integrated cognitive process of generative reasoning. To bridge this gap, we propose that geometric construction provides an ideal testbed as it inherently demands a fusion of language comprehension and precise visual generation. We introduce GGBench, a benchmark designed specifically to evaluate geometric generative reasoning. It provides a comprehensive framework for systematically diagnosing a model's ability to not only understand and reason but to actively construct a solution, thereby setting a more rigorous standard for the next generation of intelligent systems. Project website: https://opendatalab-raiser.github.io/GGBench/.

</details>


### [17] [STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models](https://arxiv.org/abs/2511.11233)
*Huajian Zhang,Mingyue Cheng,Yucong Luo,Xiaoyu Tao*

Main category: cs.AI

TL;DR: STaR框架通过慢思考机制解决表格推理中的深度推理和稳定性问题，采用两阶段难度感知强化学习和轨迹级不确定性量化，在多个基准测试中表现出优越性能和增强的推理稳定性。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在表格推理中存在两个关键限制：(i) 推理过程缺乏人类认知的深度和迭代精炼特性；(ii) 推理过程不稳定，影响下游应用的可靠性。

Method: STaR框架为LLMs配备慢思考能力，通过显式建模逐步思考和不确定性感知推理。训练时采用两阶段难度感知强化学习，从简单到复杂查询逐步学习；推理时通过集成标记级置信度和答案一致性进行轨迹级不确定性量化。

Result: 在基准测试上的广泛实验表明，STaR实现了优越的性能和增强的推理稳定性。在领域外数据集上的强泛化能力进一步证明了其作为可靠表格推理解决方案的潜力。

Conclusion: STaR是一个可靠且受认知启发的表格推理解决方案，通过慢思考机制显著提升了LLMs在表格推理中的性能和稳定性。

Abstract: Table reasoning with the large language models (LLMs) is a fundamental path toward building intelligent systems that can understand and analyze over structured data. While recent progress has shown promising results, they still suffer from two key limitations: (i) the reasoning processes lack the depth and iterative refinement characteristic of human cognition; and (ii) the reasoning processes exhibit instability, which compromises their reliability in downstream applications. In this work, we present STaR (slow-thinking for table reasoning), a new framework achieving cognitive table reasoning, in which LLMs are equipped with slow-thinking capabilities by explicitly modeling step-by-step thinking and uncertainty-aware inference. During training, STaR employs two-stage difficulty-aware reinforcement learning (DRL), progressively learning from simple to complex queries under a composite reward. During inference, STaR performs trajectory-level uncertainty quantification by integrating token-level confidence and answer consistency, enabling selection of more credible reasoning paths. Extensive experiments on benchmarks demonstrate that STaR achieves superior performance and enhanced reasoning stability. Moreover, strong generalization over out-of-domain datasets further demonstrates STaR's potential as a reliable and cognitively inspired solution for table reasoning with LLMs.

</details>


### [18] [A Workflow for Full Traceability of AI Decisions](https://arxiv.org/abs/2511.11275)
*Julius Wenzel,Syeda Umaima Alam,Andreas Schmidt,Hanwei Zhang,Holger Hermanns*

Main category: cs.AI

TL;DR: 本文提出了一种通过强制记录AI训练和推理过程中所有组件的方法，来解决AI决策缺乏可追溯性的问题，并展示了首个支持生成防篡改、可验证且全面的AI决策跟踪的工作流程。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在决策过程文档化方面做得很少，这阻碍了追溯决策依据的能力，而这是重建责任链的前提。特别是在AI决策违反法律时，需要能在法庭上站得住脚的文档记录。

Method: 扩展DBOM概念，利用机密计算技术构建有效的工作流程，强制记录训练和推理过程中的每个组件，生成防篡改、可验证且全面的AI决策跟踪。

Result: 开发了一个运行工作流程，支持生成防篡改、可验证且详尽的AI决策跟踪，并通过一个区分有毒和可食用蘑菇的应用程序演示了工作流程的内部运作。

Conclusion: 该方法为解决AI决策可追溯性问题提供了一种激进但实用的方法，为建立AI决策责任链奠定了基础。

Abstract: An ever increasing number of high-stake decisions are made or assisted by automated systems employing brittle artificial intelligence technology. There is a substantial risk that some of these decision induce harm to people, by infringing their well-being or their fundamental human rights. The state-of-the-art in AI systems makes little effort with respect to appropriate documentation of the decision process. This obstructs the ability to trace what went into a decision, which in turn is a prerequisite to any attempt of reconstructing a responsibility chain. Specifically, such traceability is linked to a documentation that will stand up in court when determining the cause of some AI-based decision that inadvertently or intentionally violates the law.
  This paper takes a radical, yet practical, approach to this problem, by enforcing the documentation of each and every component that goes into the training or inference of an automated decision. As such, it presents the first running workflow supporting the generation of tamper-proof, verifiable and exhaustive traces of AI decisions. In doing so, we expand the DBOM concept into an effective running workflow leveraging confidential computing technology. We demonstrate the inner workings of the workflow in the development of an app to tell poisonous and edible mushrooms apart, meant as a playful example of high-stake decision support.

</details>


### [19] [Can You Tell the Difference? Contrastive Explanations for ABox Entailments](https://arxiv.org/abs/2511.11281)
*Patrick Koopmann,Yasir Mahmood,Axel-Cyrille Ngonga Ngomo,Balram Tiwari*

Main category: cs.AI

TL;DR: 本文提出了对比ABox解释的概念，用于回答"为什么a是C的实例而b不是"这类问题。与单独解释正面蕴含或缺失蕴含的方法不同，对比解释同时考虑两者，聚焦于a和b之间的相关共性和差异。


<details>
  <summary>Details</summary>
Motivation: 现有的解释方法要么只解释为什么知识库蕴含C(a)，要么只解释为什么不蕴含C(b)，缺乏同时考虑两者的对比视角。对比解释能够更好地揭示个体间的相关差异，提供更全面的解释。

Method: 为描述逻辑本体论中的ABox推理开发了对比解释的适当概念，分析了不同最优性标准下各种变体的计算复杂度，涵盖了轻量级和更富有表达力的描述逻辑。实现了一种计算对比解释变体的方法，并在现实知识库的生成问题上进行了评估。

Result: 提出了对比ABox解释的形式化定义，分析了不同描述逻辑下对比解释的计算复杂度特性，并实现了相应的计算方法。

Conclusion: 对比ABox解释为描述逻辑推理提供了更全面的解释框架，能够同时处理正面和负面实例问题，有助于更好地理解个体间的语义差异。

Abstract: We introduce the notion of contrastive ABox explanations to answer questions of the type "Why is a an instance of C, but b is not?". While there are various approaches for explaining positive entailments (why is C(a) entailed by the knowledge base) as well as missing entailments (why is C(b) not entailed) in isolation, contrastive explanations consider both at the same time, which allows them to focus on the relevant commonalities and differences between a and b. We develop an appropriate notion of contrastive explanations for the special case of ABox reasoning with description logic ontologies, and analyze the computational complexity for different variants under different optimality criteria, considering lightweight as well as more expressive description logics. We implemented a first method for computing one variant of contrastive explanations, and evaluated it on generated problems for realistic knowledge bases.

</details>


### [20] [RLSLM: A Hybrid Reinforcement Learning Framework Aligning Rule-Based Social Locomotion Model with Human Social Norms](https://arxiv.org/abs/2511.11323)
*Yitian Kou,Yihe Gu,Chen Zhou,DanDan Zhu,Shuguang Kuai*

Main category: cs.AI

TL;DR: RLSLM是一个混合强化学习框架，将基于规则的社会运动模型集成到强化学习的奖励函数中，实现社会意识导航，在用户体验上优于最先进的基于规则模型。


<details>
  <summary>Details</summary>
Motivation: 解决在人口密集环境中导航时避免引起不适的问题。基于规则的方法可解释但缺乏泛化性，数据驱动方法能学习复杂行为但效率低且不透明。需要结合两者优势。

Method: 提出RLSLM混合强化学习框架，将基于经验行为实验的社会运动模型集成到强化学习奖励函数中，生成方向敏感的社会舒适场，联合优化机械能和社会舒适度。

Result: 在沉浸式VR实验中，RLSLM在用户体验上优于最先进的基于规则模型。消融和敏感性分析显示模型比传统数据驱动方法具有显著改进的可解释性。

Conclusion: 这项工作提出了一种可扩展的、以人为中心的方法论，有效整合认知科学和机器学习，实现现实世界的社会导航。

Abstract: Navigating human-populated environments without causing discomfort is a critical capability for socially-aware agents. While rule-based approaches offer interpretability through predefined psychological principles, they often lack generalizability and flexibility. Conversely, data-driven methods can learn complex behaviors from large-scale datasets, but are typically inefficient, opaque, and difficult to align with human intuitions. To bridge this gap, we propose RLSLM, a hybrid Reinforcement Learning framework that integrates a rule-based Social Locomotion Model, grounded in empirical behavioral experiments, into the reward function of a reinforcement learning framework. The social locomotion model generates an orientation-sensitive social comfort field that quantifies human comfort across space, enabling socially aligned navigation policies with minimal training. RLSLM then jointly optimizes mechanical energy and social comfort, allowing agents to avoid intrusions into personal or group space. A human-agent interaction experiment using an immersive VR-based setup demonstrates that RLSLM outperforms state-of-the-art rule-based models in user experience. Ablation and sensitivity analyses further show the model's significantly improved interpretability over conventional data-driven methods. This work presents a scalable, human-centered methodology that effectively integrates cognitive science and machine learning for real-world social navigation.

</details>


### [21] [Robust and Efficient Communication in Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2511.11393)
*Zejiao Liu,Yi Li,Jiali Wang,Junqi Tu,Yitian Hong,Fangfei Li,Yang Liu,Toshiharu Sugawara,Yang Tang*

Main category: cs.AI

TL;DR: 本文系统综述了多智能体强化学习在现实约束下的鲁棒高效通信策略，包括消息扰动、传输延迟和带宽限制等挑战，并聚焦于协同自动驾驶、分布式SLAM和联邦学习三个应用领域。


<details>
  <summary>Details</summary>
Motivation: 现有MARL方法大多假设通信是瞬时、可靠且带宽无限的，但这些条件在现实部署中很少满足，因此需要研究在现实约束下的通信策略。

Method: 系统性地综述了近期在MARL中针对消息扰动、传输延迟和有限带宽等现实约束的鲁棒高效通信策略研究进展。

Result: 识别了低延迟可靠性、带宽密集型数据共享和通信隐私权衡等核心挑战，并分析了在协同自动驾驶、分布式SLAM和联邦学习三个应用领域的解决方案。

Conclusion: 提出了统一的方法论，建议共同设计通信、学习和鲁棒性，以弥合理论MARL模型与实际实现之间的差距，并指出了关键开放挑战和未来研究方向。

Abstract: Multi-agent reinforcement learning (MARL) has made significant strides in enabling coordinated behaviors among autonomous agents. However, most existing approaches assume that communication is instantaneous, reliable, and has unlimited bandwidth; these conditions are rarely met in real-world deployments. This survey systematically reviews recent advances in robust and efficient communication strategies for MARL under realistic constraints, including message perturbations, transmission delays, and limited bandwidth. Furthermore, because the challenges of low-latency reliability, bandwidth-intensive data sharing, and communication-privacy trade-offs are central to practical MARL systems, we focus on three applications involving cooperative autonomous driving, distributed simultaneous localization and mapping, and federated learning. Finally, we identify key open challenges and future research directions, advocating a unified approach that co-designs communication, learning, and robustness to bridge the gap between theoretical MARL models and practical implementations.

</details>


### [22] [CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction](https://arxiv.org/abs/2511.11423)
*Cong-Tinh Dao,Nguyen Minh Thao Phan,Jun-En Ding,Chenwei Wu,David Restrepo,Dongsheng Luo,Fanyi Zhao,Chun-Chieh Liao,Wen-Chih Peng,Chi-Te Wang,Pei-Fu Chen,Ling Chen,Xinglong Ju,Feng Liu,Fang-Ming Hung*

Main category: cs.AI

TL;DR: CURENet是一个多模态模型，通过整合非结构化临床笔记、实验室测试和患者时间序列数据，利用LLM处理临床文本和文本化实验室测试，以及使用transformer编码器处理纵向序列就诊数据，来预测慢性疾病。


<details>
  <summary>Details</summary>
Motivation: 电子健康记录包含多种数据类型，但现有预测模型未能充分利用多模态数据之间的交互、冗余和时间模式，通常只关注单一数据类型或忽略这些复杂性。

Method: 使用大型语言模型处理临床文本和文本化实验室测试，使用transformer编码器处理纵向序列就诊数据，整合这三种数据模态来构建预测模型。

Result: 在MIMIC-III和FEMH数据集上评估，CURENet在多标签框架中预测前10种慢性疾病的准确率超过94%。

Conclusion: 多模态EHR整合有潜力增强临床决策制定并改善患者预后。

Abstract: Electronic health records (EHRs) are designed to synthesize diverse data types, including unstructured clinical notes, structured lab tests, and time-series visit data. Physicians draw on these multimodal and temporal sources of EHR data to form a comprehensive view of a patient's health, which is crucial for informed therapeutic decision-making. Yet, most predictive models fail to fully capture the interactions, redundancies, and temporal patterns across multiple data modalities, often focusing on a single data type or overlooking these complexities. In this paper, we present CURENet, a multimodal model (Combining Unified Representations for Efficient chronic disease prediction) that integrates unstructured clinical notes, lab tests, and patients' time-series data by utilizing large language models (LLMs) for clinical text processing and textual lab tests, as well as transformer encoders for longitudinal sequential visits. CURENet has been capable of capturing the intricate interaction between different forms of clinical data and creating a more reliable predictive model for chronic illnesses. We evaluated CURENet using the public MIMIC-III and private FEMH datasets, where it achieved over 94\% accuracy in predicting the top 10 chronic conditions in a multi-label framework. Our findings highlight the potential of multimodal EHR integration to enhance clinical decision-making and improve patient outcomes.

</details>


### [23] [Experience-Guided Adaptation of Inference-Time Reasoning Strategies](https://arxiv.org/abs/2511.11519)
*Adam Stein,Matthew Trager,Benjamin Bowman,Michael Kleinman,Aditya Chattopadhyay,Wei Xia,Stefano Soatto*

Main category: cs.AI

TL;DR: EGuR是一个经验引导的推理系统，能够在推理时动态生成完整的计算策略（包括LLM调用、工具、采样参数和控制逻辑），基于积累的经验进行自适应优化。


<details>
  <summary>Details</summary>
Motivation: 现有AI系统在训练后难以自适应调整问题解决方法，要么只能修改文本输入而无法改变采样参数、工具配置等核心组件，要么需要离线优化且部署后无法动态调整。

Method: 使用基于LLM的元策略来生成完整策略，包含两个组件：Guide基于当前问题和结构化记忆生成候选策略，Consolidator整合执行反馈来改进未来策略生成。

Result: 在五个挑战性基准测试（AIME 2025、3-SAT和三个Big Bench Extra Hard任务）上，EGuR相比最强基线实现了高达14%的准确率提升，同时计算成本降低了111倍，且随着经验积累两项指标持续改善。

Conclusion: EGuR通过动态生成完整策略的方法，实现了推理时的自适应优化，在保持高性能的同时显著降低了计算成本，为智能体AI系统的自适应问题解决提供了新思路。

Abstract: Enabling agentic AI systems to adapt their problem-solving approaches based on post-training interactions remains a fundamental challenge. While systems that update and maintain a memory at inference time have been proposed, existing designs only steer the system by modifying textual input to a language model or agent, which means that they cannot change sampling parameters, remove tools, modify system prompts, or switch between agentic and workflow paradigms. On the other hand, systems that adapt more flexibly require offline optimization and remain static once deployed. We present Experience-Guided Reasoner (EGuR), which generates tailored strategies -- complete computational procedures involving LLM calls, tools, sampling parameters, and control logic -- dynamically at inference time based on accumulated experience. We achieve this using an LLM-based meta-strategy -- a strategy that outputs strategies -- enabling adaptation of all strategy components (prompts, sampling parameters, tool configurations, and control logic). EGuR operates through two components: a Guide generates multiple candidate strategies conditioned on the current problem and structured memory of past experiences, while a Consolidator integrates execution feedback to improve future strategy generation. This produces complete, ready-to-run strategies optimized for each problem, which can be cached, retrieved, and executed as needed without wasting resources. Across five challenging benchmarks (AIME 2025, 3-SAT, and three Big Bench Extra Hard tasks), EGuR achieves up to 14% accuracy improvements over the strongest baselines while reducing computational costs by up to 111x, with both metrics improving as the system gains experience.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [24] [Surveillance and Disability in Online Proctored Exams: Student Perspectives and Design Implications](https://arxiv.org/abs/2511.10826)
*Monika Blue Kwapisz,Yoav Ackerman,Jennifer Nguyen,Prashanth Rajivan*

Main category: cs.HC

TL;DR: 在线监考系统(OPS)对残疾学生造成隐私侵犯和歧视性问题，导致焦虑、认知负担增加，并迫使他们做出隐私和便利设施的妥协。


<details>
  <summary>Details</summary>
Motivation: 研究在线监考系统如何对残疾学生产生负面影响，特别是隐私侵犯、不公平标记以及与学生残疾的互动问题。

Method: 采用反思性主题分析，访谈有在线监考考试经验且需要残疾便利设施的学生。

Result: 学生表现出对监控与残疾互动的焦虑，担心被误解，考试认知负担增加，并被迫在隐私和便利设施之间做出妥协。

Conclusion: 需要重新设计在线监考系统以减轻残疾学生面临的问题，平衡监控需求与学生隐私和公平性。

Abstract: Online proctoring systems (OPS) are technologies and services that are used to monitor students during an online exam to deter cheating. However, OPS often violates student privacy by implementing overly intrusive surveillance to which students cannot consent meaningfully. The technologies used in OPS have been shown to unfairly flag students with disabilities. Our reflexive thematic analysis of interviews with students who have first-hand experience with online invigilated exams and who have disability accommodations points to their anxiety about the interaction between surveillance and their disabilities, leading to fears about misrepresentation and increased cognitive load on the exam. Students describe the compromises they need to make with their privacy and accommodations to take remote tests and share their privacy values. We present the implications for the design of OPS to mitigate the issues faced by disabled students.

</details>


### [25] [ReTrace: Interactive Visualizations for Reasoning Traces of Large Reasoning Models](https://arxiv.org/abs/2511.11187)
*Ludwig Felder,Jacob Miller,Markus Wallinger,Stephen Kobourov,Chunyang Chen*

Main category: cs.HC

TL;DR: ReTrace是一个交互式系统，通过结构化处理和可视化大型语言模型的推理轨迹，帮助用户更准确、更轻松地理解模型推理过程。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型产生的推理轨迹虽然提供了模型思考过程的透明度，但通常冗长复杂，给用户带来认知负担，需要更好的方式来支持理解。

Method: 使用经过验证的推理分类法将文本推理轨迹转化为结构化数据，并研究两种交互式可视化方法。

Result: 用户研究表明，两种可视化方法都比原始文本基线更能准确理解模型推理，且感知努力更少。

Conclusion: 该研究为处理冗长复杂的机器生成推理过程提供了设计启示，是AI可解释性的重要一步。

Abstract: Recent advances in Large Language Models have led to Large Reasoning Models, which produce step-by-step reasoning traces. These traces offer insight into how models think and their goals, improving explainability and helping users follow the logic, learn the process, and even debug errors. These traces, however, are often verbose and complex, making them cognitively demanding to comprehend. We address this challenge with ReTrace, an interactive system that structures and visualizes textual reasoning traces to support understanding. We use a validated reasoning taxonomy to produce structured reasoning data and investigate two types of interactive visualizations thereof. In a controlled user study, both visualizations enabled users to comprehend the model's reasoning more accurately and with less perceived effort than a raw text baseline. The results of this study could have design implications for making long and complex machine-generated reasoning processes more usable and transparent, an important step in AI explainability.

</details>


### [26] [Towards Usable Privacy Management for IoT TAPs: Deriving Privacy Clusters and Preference Profiles](https://arxiv.org/abs/2511.11209)
*Piero Romare,Farzaneh Karegar,Simone Fischer-Hübner*

Main category: cs.HC

TL;DR: 本文为物联网触发-动作平台(TAPs)开发了基于用户隐私关注和需求的隐私集群与配置文件，通过问卷研究将用户分为基础、中等和高隐私三个集群，为TAPs提供更可用的隐私控制方案。


<details>
  <summary>Details</summary>
Motivation: 物联网触发-动作平台通常提供粗粒度的权限控制，即使有细粒度控制，用户也因设置隐私偏好的复杂性而不堪重负，需要更可用的隐私管理方案。

Method: 开发并验证基于用户隐私关注和透明度需求的问卷，通过在线研究(N=301)将用户按隐私关注和需求聚类为三个隐私集群，并使用因子情境方法分析用户数据共享偏好。

Result: 研究发现三个不同的隐私配置文件：基础、中等和高隐私集群，这些集群在数据类别、数据接收者类型和数据共享目的方面表现出不同的数据共享偏好。

Conclusion: 三个不同的隐私配置文件为TAPs提供了更可用隐私控制的基础，可以半自动地分配或建议给用户，从而改善用户体验。

Abstract: IoT Trigger-Action Platforms (TAPs) typically offer coarse-grained permission controls. Even when fine-grained controls are available, users are likely overwhelmed by the complexity of setting privacy preferences. This paper contributes to usable privacy management for TAPs by deriving privacy clusters and profiles for different types of users that can be semi-automatically assigned or suggested to them. We developed and validated a questionnaire, based on users' privacy concerns regarding confidentiality and control and their requirements towards transparency in TAPs. In an online study (N=301), where participants were informed about potential privacy risks, we clustered users by their privacy concerns and requirements into Basic, Medium and High Privacy clusters. These clusters were then characterized by the users' data sharing preferences, based on a factorial vignette approach, considering the data categories, the data recipient types, and the purpose of data sharing. Our findings show three distinct privacy profiles, providing a foundation for more usable privacy controls in TAPs.

</details>


### [27] [Devising Experiments with Interactive Environments](https://arxiv.org/abs/2511.11229)
*Pavlos Panagiotidis,Jocelyn Spence,Nils Jager*

Main category: cs.HC

TL;DR: 本文研究在沉浸式表演中无需编写代码即可创作响应式灯光和声音的方法，通过视觉逻辑层将实时手势、位置和语音输入与场景输出相连接，并在六个工作坊中验证了该系统的可行性。


<details>
  <summary>Details</summary>
Motivation: 探索在沉浸式表演创作中，如何让表演者无需编程技能就能直接参与交互式灯光和声音的创作过程，降低技术门槛。

Method: 开发模块化系统，通过视觉逻辑层连接实时输入（手势、位置、语音）和场景输出；组织六个工作坊，采用从并行训练到集成戏剧创作的渐进式方法。

Result: 通过工作坊视频日志、焦点小组和引导者笔记分析，发现了三个使技术在混合创作/设计实践中可行的策略：角色轮换、拥抱可控的不完美、使用技术描述性隐喻。

Conclusion: 视觉逻辑系统能够有效支持表演者在沉浸式表演中创作交互式元素，通过特定的创作策略可以成功整合技术到表演实践中。

Abstract: This paper reports a practice-based investigation into authoring responsive light and sound in immersive performance without writing code. A modular system couples live gesture, position, and speech inputs to scenographic outputs through a visual logic layer that performers can operate in rehearsal. Across six workshops with eight professional performance-makers, we staged a progression from parallel ensemble and technical training to integrated dramaturgy, culminating in a single-spectator scratch immersive performance with interactive elements. This paper details the system's building blocks and the workshop arc. A reflexive reading of workshop video logs, post-workshop focus groups, and facilitator notes surfaced three ensemble-level strategies that made the technology workable in a hybrid devising/design practice: rotating roles between operator, performer, and mediator; embracing controlled imperfection as a creative resource; and using technology-describing metaphors to support creative practice.

</details>


### [28] [Context-aware Adaptive Visualizations for Critical Decision Making](https://arxiv.org/abs/2511.11476)
*Angela Lopez-Cardona,Mireia Masias Bruns,Nuwan T. Attygalle,Sebastian Idesis,Matteo Salvatori,Konstantinos Raftopoulos,Konstantinos Oikonomou,Saravanakumar Duraisamy,Parvin Emami,Nacera Latreche,Alaa Eddine Anis Sahraoui,Michalis Vakallelis,Jean Vanderdonckt,Ioannis Arapakis,Luis A. Leiva*

Main category: cs.HC

TL;DR: Symbiotik是一个智能自适应可视化系统，通过神经生理信号实时估计用户心理负荷，并使用强化学习动态调整可视化仪表板，提高任务表现和参与度。


<details>
  <summary>Details</summary>
Motivation: 传统信息可视化仪表板很少适应用户的认知状态，更不用说实时适应。需要开发能够根据用户认知状态动态调整的可视化系统。

Method: 利用神经生理信号估计心理负荷，通过强化学习算法动态调整可视化仪表板，构建了可扩展的实时适应架构。

Result: 在120名参与者和三种可视化类型的用户研究中，证明了该方法能够改善任务表现和用户参与度。

Conclusion: Symbiotik提供了一个经过验证的神经自适应用户界面方法学，以及可扩展的实时适应架构，为智能自适应可视化系统开辟了新途径。

Abstract: Effective decision-making often relies on timely insights from complex visual data. While Information Visualization (InfoVis) dashboards can support this process, they rarely adapt to users' cognitive state, and less so in real time. We present Symbiotik, an intelligent, context-aware adaptive visualization system that leverages neurophysiological signals to estimate mental workload (MWL) and dynamically adapt visual dashboards using reinforcement learning (RL). Through a user study with 120 participants and three visualization types, we demonstrate that our approach improves task performance and engagement. Symbiotik offers a scalable, real-time adaptation architecture, and a validated methodology for neuroadaptive user interfaces.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [29] [LT-Soups: Bridging Head and Tail Classes via Subsampled Model Soups](https://arxiv.org/abs/2511.10683)
*Masih Aminbeidokhti,Subhankar Roy,Eric Granger,Elisa Ricci,Marco Pedersoli*

Main category: cs.LG

TL;DR: LT-Soups是一个两阶段模型融合框架，用于解决长尾数据分布下参数高效微调方法在尾类性能提升时牺牲头类准确率的问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界数据集通常呈现长尾分布，现有参数高效微调方法在提升尾类性能时会损害头类准确率，且头尾类比例这一关键因素被忽视。

Method: 提出LT-Soups两阶段框架：第一阶段在平衡子集上微调模型并平均以减少头类偏差；第二阶段仅在完整数据集上微调分类器以恢复头类准确率。

Result: 在六个基准数据集上的实验表明，LT-Soups在各种不平衡机制下相比PEFT和传统模型融合方法实现了更优的权衡。

Conclusion: LT-Soups能够有效应对不同长尾分布场景，在保持尾类性能的同时不牺牲头类准确率，提供了更好的性能权衡。

Abstract: Real-world datasets typically exhibit long-tailed (LT) distributions, where a few head classes dominate and many tail classes are severely underrepresented. While recent work shows that parameter-efficient fine-tuning (PEFT) methods like LoRA and AdaptFormer preserve tail-class performance on foundation models such as CLIP, we find that they do so at the cost of head-class accuracy. We identify the head-tail ratio, the proportion of head to tail classes, as a crucial but overlooked factor influencing this trade-off. Through controlled experiments on CIFAR100 with varying imbalance ratio ($ρ$) and head-tail ratio ($η$), we show that PEFT excels in tail-heavy scenarios but degrades in more balanced and head-heavy distributions. To overcome these limitations, we propose LT-Soups, a two-stage model soups framework designed to generalize across diverse LT regimes. In the first stage, LT-Soups averages models fine-tuned on balanced subsets to reduce head-class bias; in the second, it fine-tunes only the classifier on the full dataset to restore head-class accuracy. Experiments across six benchmark datasets show that LT-Soups achieves superior trade-offs compared to both PEFT and traditional model soups across a wide range of imbalance regimes.

</details>


### [30] [Fast Neural Tangent Kernel Alignment, Norm and Effective Rank via Trace Estimation](https://arxiv.org/abs/2511.10796)
*James Hazelden*

Main category: cs.LG

TL;DR: 提出了一种基于迹估计的矩阵自由方法，用于快速计算神经正切核(NTK)的迹、Frobenius范数、有效秩和对齐度，无需计算完整的NTK矩阵。


<details>
  <summary>Details</summary>
Motivation: 计算完整的NTK矩阵对于循环架构等复杂模型通常不可行，需要一种更高效的NTK分析方法。

Method: 使用Hutch++迹估计器和单边自动微分方法，仅需前向或反向模式自动微分，无需同时使用两种模式。

Result: 矩阵自由随机化方法可以实现多个数量级的加速，在低样本情况下单边估计器优于Hutch++。

Conclusion: 矩阵自由随机化方法能够显著加速NTK的分析和应用，特别适用于参数数量与模型状态差距较大的情况。

Abstract: The Neural Tangent Kernel (NTK) characterizes how a model's state evolves over Gradient Descent. Computing the full NTK matrix is often infeasible, especially for recurrent architectures. Here, we introduce a matrix-free perspective, using trace estimation to rapidly analyze the empirical, finite-width NTK. This enables fast computation of the NTK's trace, Frobenius norm, effective rank, and alignment. We provide numerical recipes based on the Hutch++ trace estimator with provably fast convergence guarantees. In addition, we show that, due to the structure of the NTK, one can compute the trace using only forward- or reverse-mode automatic differentiation, not requiring both modes. We show these so-called one-sided estimators can outperform Hutch++ in the low-sample regime, especially when the gap between the model state and parameter count is large. In total, our results demonstrate that matrix-free randomized approaches can yield speedups of many orders of magnitude, leading to faster analysis and applications of the NTK.

</details>


### [31] [Near-optimal Linear Predictive Clustering in Non-separable Spaces via Mixed Integer Programming and Quadratic Pseudo-Boolean Reductions](https://arxiv.org/abs/2511.10809)
*Jiazhou Liang,Hassan Khurram,Scott Sanner*

Main category: cs.LG

TL;DR: 本文提出了两种改进线性预测聚类（LPC）全局优化的新方法，通过利用可分离性的理论特性推导出具有可证明误差界的近似最优解，显著降低了混合整数规划（MIP）的复杂性并提高了可扩展性。


<details>
  <summary>Details</summary>
Motivation: 现有的LPC方法存在局限性：贪婪优化方法缺乏全局最优性，在非可分聚类场景中表现不佳；而基于MIP的方法虽然能保证全局最优性但可扩展性差。本文旨在开发更高效的全局优化方法来解决这些问题。

Method: 1. 基于可分离性理论特性推导出具有可证明误差界的近似最优解，显著降低MIP公式的复杂性
2. 将LPC进一步近似为二次伪布尔优化（QPBO）问题，在某些设置下实现显著的计算改进

Result: 在合成和真实数据集上的比较分析表明，所提方法始终能获得近似最优解，回归误差显著低于贪婪优化方法，同时相比现有MIP公式展现出更优的可扩展性。

Conclusion: 本文提出的两种新方法在保持全局优化优势的同时，显著改善了LPC的计算效率和可扩展性，为实际应用提供了更实用的解决方案。

Abstract: Linear Predictive Clustering (LPC) partitions samples based on shared linear relationships between feature and target variables, with numerous applications including marketing, medicine, and education. Greedy optimization methods, commonly used for LPC, alternate between clustering and linear regression but lack global optimality. While effective for separable clusters, they struggle in non-separable settings where clusters overlap in feature space. In an alternative constrained optimization paradigm, Bertsimas and Shioda (2007) formulated LPC as a Mixed-Integer Program (MIP), ensuring global optimality regardless of separability but suffering from poor scalability. This work builds on the constrained optimization paradigm to introduce two novel approaches that improve the efficiency of global optimization for LPC. By leveraging key theoretical properties of separability, we derive near-optimal approximations with provable error bounds, significantly reducing the MIP formulation's complexity and improving scalability. Additionally, we can further approximate LPC as a Quadratic Pseudo-Boolean Optimization (QPBO) problem, achieving substantial computational improvements in some settings. Comparative analyses on synthetic and real-world datasets demonstrate that our methods consistently achieve near-optimal solutions with substantially lower regression errors than greedy optimization while exhibiting superior scalability over existing MIP formulations.

</details>


### [32] [Transformers know more than they can tell -- Learning the Collatz sequence](https://arxiv.org/abs/2511.10811)
*François Charton,Ashvni Narayanan*

Main category: cs.LG

TL;DR: 本文研究了Transformer模型在预测Collatz序列长步长方面的表现，发现模型准确率随编码基数变化，最高可达99.7%，最低仅25%。所有模型都遵循相同的学习模式，逐步学习输入按模2^p分类的规律，这与Collatz序列的数学性质相关。


<details>
  <summary>Details</summary>
Motivation: 使用数学问题作为工具来理解、解释并可能改进语言模型，探索模型学习复杂算术函数的能力和机制。

Method: 通过在不同基数下编码输入和输出，训练Transformer模型预测Collatz序列的长步长，分析模型的学习模式和错误类型。

Result: 模型准确率随编码基数变化显著（24和32基数下99.7%，11和3基数下仅25-37%）。模型学习输入按模2^p分类的规律，错误主要源于循环长度估计错误而非幻觉。

Conclusion: 学习复杂算术函数的困难在于理解计算的控制结构（循环长度）。使用数学问题作为分析工具的方法可广泛应用于其他问题，有望获得丰硕成果。

Abstract: We investigate transformer prediction of long Collatz steps, a complex arithmetic function that maps odd integers to their distant successors in the Collatz sequence ( $u_{n+1}=u_n/2$ if $u_n$ is even, $u_{n+1}=(3u_n+1)/2$ if $u_n$ is odd). Model accuracy varies with the base used to encode input and output. It can be as high as $99.7\%$ for bases $24$ and $32$, and as low as $37$ and $25\%$ for bases $11$ and $3$. Yet, all models, no matter the base, follow a common learning pattern. As training proceeds, they learn a sequence of classes of inputs that share the same residual modulo $2^p$. Models achieve near-perfect accuracy on these classes, and less than $1\%$ for all other inputs. This maps to a mathematical property of Collatz sequences: the length of the loops involved in the computation of a long Collatz step can be deduced from the binary representation of its input. The learning pattern reflects the model learning to predict inputs associated with increasing loop lengths. An analysis of failure cases reveals that almost all model errors follow predictable patterns. Hallucination, a common feature of large language models, almost never happens. In over $90\%$ of failures, the model performs the correct calculation, but wrongly estimates loop lengths. Our observations give a full account of the algorithms learned by the models. They suggest that the difficulty of learning such complex arithmetic function lies in figuring the control structure of the computation -- the length of the loops. We believe that the approach outlined here, using mathematical problems as tools for understanding, explaining, and perhaps improving language models, can be applied to a broad range of problems and bear fruitful results.

</details>


### [33] [Towards Universal Neural Operators through Multiphysics Pretraining](https://arxiv.org/abs/2511.10829)
*Mikhail Masliaev,Dmitry Gusarov,Ilya Markov,Alexander Hvatov*

Main category: cs.LG

TL;DR: 研究评估了基于Transformer的神经算子在迁移学习中的表现，证明其能够有效在不同PDE问题间传递知识。


<details>
  <summary>Details</summary>
Motivation: 虽然神经算子广泛用于数据驱动的物理模拟，但其训练计算成本高昂。下游学习通过预训练简化问题再微调复杂问题来解决此问题。

Method: 在更通用的迁移学习设置中评估基于Transformer的神经算子，测试其在多种PDE问题上的表现，包括参数外推、新变量引入和多方程数据集迁移。

Result: 结果表明，先进的神经算子架构能够有效在PDE问题间传递知识。

Conclusion: 基于Transformer的神经算子在迁移学习场景中表现出色，能够成功实现跨PDE问题的知识转移。

Abstract: Although neural operators are widely used in data-driven physical simulations, their training remains computationally expensive. Recent advances address this issue via downstream learning, where a model pretrained on simpler problems is fine-tuned on more complex ones. In this research, we investigate transformer-based neural operators, which have previously been applied only to specific problems, in a more general transfer learning setting. We evaluate their performance across diverse PDE problems, including extrapolation to unseen parameters, incorporation of new variables, and transfer from multi-equation datasets. Our results demonstrate that advanced neural operator architectures can effectively transfer knowledge across PDE problems.

</details>


### [34] [Benchmarking Quantum Kernels Across Diverse and Complex Data](https://arxiv.org/abs/2511.10831)
*Yuhan Jiang,Matthew Otten*

Main category: cs.LG

TL;DR: 本文提出了一个变分量子核框架，用于高维真实世界数据的分类任务，并在8个具有挑战性的数据集上进行了基准测试，结果显示量子核性能优于经典RBF核。


<details>
  <summary>Details</summary>
Motivation: 当前量子核方法研究主要局限于低维或合成数据集，无法充分评估其在真实世界高维数据上的潜力，需要填补这一研究空白。

Method: 开发了使用资源高效ansatz的变分量子核框架，并引入了参数缩放技术来加速收敛，在8个高维真实世界数据集上进行全面基准测试。

Result: 经典模拟结果显示，提出的量子核在性能上明显优于标准经典核（如RBF核），证明了量子核作为高性能工具的潜力。

Conclusion: 适当设计的量子核可以作为多功能、高性能的工具，为真实世界机器学习中的量子增强应用奠定基础，但需要进一步研究来全面评估实际的量子优势。

Abstract: Quantum kernel methods are a promising branch of quantum machine learning, yet their practical advantage on diverse, high-dimensional, real-world data remains unverified. Current research has largely been limited to low-dimensional or synthetic datasets, preventing a thorough evaluation of their potential. To address this gap, we developed a variational quantum kernel framework utilizing resource-efficient ansätze for complex classification tasks and introduced a parameter scaling technique to accelerate convergence. We conducted a comprehensive benchmark of this framework on eight challenging, real world and high-dimensional datasets covering tabular, image, time series, and graph data. Our classically simulated results show that the proposed quantum kernel demonstrated a clear performance advantage over standard classical kernels, such as the radial basis function (RBF) kernel. This work demonstrates that properly designed quantum kernels can function as versatile, high-performance tools, laying a foundation for quantum-enhanced applications in real-world machine learning. Further research is needed to fully assess the practical quantum advantage.

</details>


### [35] [SURFACEBENCH: Can Self-Evolving LLMs Find the Equations of 3D Scientific Surfaces?](https://arxiv.org/abs/2511.10833)
*Sanchit Kabra,Shobhnik Kriplani,Parshin Shojaee,Chandan K. Reddy*

Main category: cs.LG

TL;DR: SurfaceBench是一个用于符号曲面发现的综合基准，包含183个任务，涵盖15种符号复杂性类别，支持显式、隐式和参数方程表示形式，并采用几何感知指标评估方程发现质量。


<details>
  <summary>Details</summary>
Motivation: 现有符号回归方法主要关注标量函数，缺乏领域基础，且依赖脆弱的字符串匹配指标，无法捕捉科学等价性。需要建立一个能够评估符号推理与几何重建能力的基准。

Method: 构建包含183个任务的综合基准，涵盖15种符号复杂性类别，支持三种方程表示形式（显式、隐式、参数），每个任务包含真实方程、变量语义和合成三维数据，采用新颖符号组合抵抗LLM记忆。

Result: 实验表明，最先进的框架虽然在特定函数族上偶尔成功，但在跨表示类型和曲面复杂性方面难以泛化。SurfaceBench建立了具有挑战性的诊断测试平台。

Conclusion: SurfaceBench为组合泛化、数据驱动科学归纳和几何感知推理提供了原则性基准，连接了符号推理与几何重建，推动了LLM在科学发现中的应用。

Abstract: Equation discovery from data is a core challenge in machine learning for science, requiring the recovery of concise symbolic expressions that govern complex physical and geometric phenomena. Recent approaches with large language models (LLMs) show promise in symbolic regression, but their success often hinges on memorized formulas or overly simplified functional forms. Existing benchmarks exacerbate this limitation: they focus on scalar functions, ignore domain grounding, and rely on brittle string-matching based metrics that fail to capture scientific equivalence. We introduce SurfaceBench, first comprehensive benchmark for symbolic surface discovery. SurfaceBench comprises 183 tasks across 15 categories of symbolic complexity, spanning explicit, implicit, and parametric equation representation forms. Each task includes ground-truth equations, variable semantics, and synthetically sampled three dimensional data. Unlike prior SR datasets, our tasks reflect surface-level structure, resist LLM memorization through novel symbolic compositions, and are grounded in scientific domains such as fluid dynamics, robotics, electromagnetics, and geometry. To evaluate equation discovery quality, we pair symbolic checks with geometry-aware metrics such as Chamfer and Hausdorff distances, capturing both algebraic fidelity and spatial reconstruction accuracy. Our experiments reveal that state-of-the-art frameworks, while occasionally successful on specific families, struggle to generalize across representation types and surface complexities. SurfaceBench thus establishes a challenging and diagnostic testbed that bridges symbolic reasoning with geometric reconstruction, enabling principled benchmarking of progress in compositional generalization, data-driven scientific induction, and geometry-aware reasoning with LLMs. We release the code here: https://github.com/Sanchit-404/surfacebench

</details>


### [36] [Multi-Joint Physics-Informed Deep Learning Framework for Time-Efficient Inverse Dynamics](https://arxiv.org/abs/2511.10878)
*Shuhao Ma,Zeyi Huang,Yu Cao,Wesley Doorsamy,Chaoyang Shi,Jun Li,Zhi-Qiang Zhang*

Main category: cs.LG

TL;DR: 提出了一种物理信息驱动的深度学习框架PI-MJCA-BiGRU，直接从运动学数据估计肌肉激活和力，无需标注数据即可实现生理一致的预测，并在多关节协调建模方面优于基线架构。


<details>
  <summary>Details</summary>
Motivation: 传统方法计算成本高且缺乏高质量多关节标注数据集，需要开发时间高效且无需标注数据的肌肉激活和力估计方法。

Method: 使用新颖的多关节交叉注意力模块和双向门控循环单元层来捕捉关节间协调，通过将多关节动力学、关节间耦合和外部力交互嵌入损失函数实现物理信息驱动学习。

Result: 在两个数据集上的实验验证表明，该方法性能与传统监督方法相当但无需真实标签，MJCA模块显著提升了关节间协调建模能力。

Conclusion: PI-MJCA-BiGRU框架能够实现时间高效、生理一致的肌肉激活和力估计，为临床评估和辅助设备控制提供了有效解决方案。

Abstract: Time-efficient estimation of muscle activations and forces across multi-joint systems is critical for clinical assessment and assistive device control. However, conventional approaches are computationally expensive and lack a high-quality labeled dataset for multi-joint applications. To address these challenges, we propose a physics-informed deep learning framework that estimates muscle activations and forces directly from kinematics. The framework employs a novel Multi-Joint Cross-Attention (MJCA) module with Bidirectional Gated Recurrent Unit (BiGRU) layers to capture inter-joint coordination, enabling each joint to adaptively integrate motion information from others. By embedding multi-joint dynamics, inter-joint coupling, and external force interactions into the loss function, our Physics-Informed MJCA-BiGRU (PI-MJCA-BiGRU) delivers physiologically consistent predictions without labeled data while enabling time-efficient inference. Experimental validation on two datasets demonstrates that PI-MJCA-BiGRU achieves performance comparable to conventional supervised methods without requiring ground-truth labels, while the MJCA module significantly enhances inter-joint coordination modeling compared to other baseline architectures.

</details>


### [37] [The Map of Misbelief: Tracing Intrinsic and Extrinsic Hallucinations Through Attention Patterns](https://arxiv.org/abs/2511.10837)
*Elyes Hajji,Aymen Bouguerra,Fabio Arnez*

Main category: cs.LG

TL;DR: 本文提出了一个区分外在和内在幻觉的评估框架，并利用基于注意力的不确定性量化算法来改进幻觉检测性能。研究发现基于采样的方法适用于外在幻觉检测，而基于注意力聚合的方法更适合内在幻觉。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在安全关键领域部署时容易产生幻觉，现有检测方法计算成本高且未区分幻觉类型，需要更有效的检测策略。

Method: 引入区分外在和内在幻觉的评估框架，利用基于注意力的不确定性量化算法，提出新颖的注意力聚合策略。

Result: 基于采样的方法如语义熵能有效检测外在幻觉但在内在幻觉上表现不佳；基于输入令牌注意力聚合的方法更适合内在幻觉检测。

Conclusion: 注意力是量化模型不确定性的丰富信号，为根据幻觉性质调整检测策略提供了新方向。

Abstract: Large Language Models (LLMs) are increasingly deployed in safety-critical domains, yet remain susceptible to hallucinations. While prior works have proposed confidence representation methods for hallucination detection, most of these approaches rely on computationally expensive sampling strategies and often disregard the distinction between hallucination types. In this work, we introduce a principled evaluation framework that differentiates between extrinsic and intrinsic hallucination categories and evaluates detection performance across a suite of curated benchmarks. In addition, we leverage a recent attention-based uncertainty quantification algorithm and propose novel attention aggregation strategies that improve both interpretability and hallucination detection performance. Our experimental findings reveal that sampling-based methods like Semantic Entropy are effective for detecting extrinsic hallucinations but generally fail on intrinsic ones. In contrast, our method, which aggregates attention over input tokens, is better suited for intrinsic hallucinations. These insights provide new directions for aligning detection strategies with the nature of hallucination and highlight attention as a rich signal for quantifying model uncertainty.

</details>


### [38] [FlowPath: Learning Data-Driven Manifolds with Invertible Flows for Robust Irregularly-sampled Time Series Classification](https://arxiv.org/abs/2511.10841)
*YongKyung Oh,Dong-Young Lim,Sungil Kim*

Main category: cs.LG

TL;DR: FlowPath提出了一种通过可逆神经流学习控制路径几何形状的新方法，用于处理稀疏和不规则采样的时间序列数据，相比固定插值方案显著提升了分类准确性。


<details>
  <summary>Details</summary>
Motivation: 神经控制微分方程在处理稀疏和不规则采样时间序列时，其性能高度依赖于从离散观测构建的控制路径选择。现有方法使用固定插值方案，这些方案强加了过于简化的几何假设，往往无法准确表示底层数据流形，特别是在高缺失率情况下。

Method: FlowPath通过可逆神经流学习控制路径的几何形状，构建连续且数据自适应的流形。该方法受到可逆性约束的指导，强制实施信息保持和良好行为的变换，这种归纳偏置使其区别于先前无约束的可学习路径模型。

Result: 在18个基准数据集和一个真实世界案例研究上的实证评估表明，FlowPath在使用固定插值器或非可逆架构的基线方法上，分类准确性始终获得统计显著性的改进。

Conclusion: 这些结果强调了不仅需要建模路径上的动态，还需要建模路径本身的几何形状的重要性，为从不规则时间序列中学习提供了稳健且可泛化的解决方案。

Abstract: Modeling continuous-time dynamics from sparse and irregularly-sampled time series remains a fundamental challenge. Neural controlled differential equations provide a principled framework for such tasks, yet their performance is highly sensitive to the choice of control path constructed from discrete observations. Existing methods commonly employ fixed interpolation schemes, which impose simplistic geometric assumptions that often misrepresent the underlying data manifold, particularly under high missingness. We propose FlowPath, a novel approach that learns the geometry of the control path via an invertible neural flow. Rather than merely connecting observations, FlowPath constructs a continuous and data-adaptive manifold, guided by invertibility constraints that enforce information-preserving and well-behaved transformations. This inductive bias distinguishes FlowPath from prior unconstrained learnable path models. Empirical evaluations on 18 benchmark datasets and a real-world case study demonstrate that FlowPath consistently achieves statistically significant improvements in classification accuracy over baselines using fixed interpolants or non-invertible architectures. These results highlight the importance of modeling not only the dynamics along the path but also the geometry of the path itself, offering a robust and generalizable solution for learning from irregular time series.

</details>


### [39] [Behaviour Policy Optimization: Provably Lower Variance Return Estimates for Off-Policy Reinforcement Learning](https://arxiv.org/abs/2511.10843)
*Alexander W. Goodall,Edwin Hamel-De le Court,Francesco Belardinelli*

Main category: cs.LG

TL;DR: 本文提出利用行为策略收集离线数据来降低回报估计方差的方法，将其扩展到在线强化学习设置中，通过改进两种策略梯度方法，在多样环境中实现了更好的样本效率和性能。


<details>
  <summary>Details</summary>
Motivation: 许多强化学习算法依赖回报估计进行策略改进，但高方差的回报估计会导致样本效率低下和训练不稳定。研究发现精心设计的行为策略可以收集离线数据来获得方差更低的回报估计，这一发现挑战了传统认为在线收集数据方差最优的观点。

Method: 将离线评估的关键见解扩展到在线强化学习设置，在策略评估和改进交替进行时学习最优策略。扩展两种策略梯度方法，使用单一行为策略收集数据用于策略改进，获得方差更低的回报估计。

Result: 实验表明，在多样环境中，改进后的方法相比传统方法具有更好的样本效率和性能表现。

Conclusion: 通过利用行为策略收集离线数据来降低回报估计方差，可以在在线强化学习环境中实现更高效的策略学习，这一方法为改进强化学习算法的样本效率提供了新的思路。

Abstract: Many reinforcement learning algorithms, particularly those that rely on return estimates for policy improvement, can suffer from poor sample efficiency and training instability due to high-variance return estimates. In this paper we leverage new results from off-policy evaluation; it has recently been shown that well-designed behaviour policies can be used to collect off-policy data for provably lower variance return estimates. This result is surprising as it means collecting data on-policy is not variance optimal. We extend this key insight to the online reinforcement learning setting, where both policy evaluation and improvement are interleaved to learn optimal policies. Off-policy RL has been well studied (e.g., IMPALA), with correct and truncated importance weighted samples for de-biasing and managing variance appropriately. Generally these approaches are concerned with reconciling data collected from multiple workers in parallel, while the policy is updated asynchronously, mismatch between the workers and policy is corrected in a mathematically sound way. Here we consider only one worker - the behaviour policy, which is used to collect data for policy improvement, with provably lower variance return estimates. In our experiments we extend two policy-gradient methods with this regime, demonstrating better sample efficiency and performance over a diverse set of environments.

</details>


### [40] [STAMP: Spatial-Temporal Adapter with Multi-Head Pooling](https://arxiv.org/abs/2511.10848)
*Brad Shook,Abby Turner,Jieshi Chen,Michał Wiliński,Mononito Goswami,Jonathan Elmer,Artur Dubrawski*

Main category: cs.LG

TL;DR: 本文提出了STAMP适配器，将通用时间序列基础模型应用于EEG数据，在8个基准数据集上达到与专用EEG基础模型相当的性能。


<details>
  <summary>Details</summary>
Motivation: 目前缺乏对EEG专用基础模型与通用时间序列基础模型在EEG任务上的比较分析，需要探索如何有效利用通用TSFM处理EEG数据。

Method: 提出STAMP适配器，利用通用TSFM的单变量嵌入，隐式建模EEG数据的时空特征，采用多头池化技术。

Result: STAMP在8个临床EEG分类数据集上表现优异，性能与最先进的EEG专用基础模型相当，且参数量轻量、输入灵活。

Conclusion: STAMP证明了通用TSFM通过适配器可以有效处理EEG数据，为EEG建模提供了轻量灵活的解决方案。

Abstract: Time series foundation models (TSFMs) pretrained on data from multiple domains have shown strong performance on diverse modeling tasks. Various efforts have been made to develop foundation models specific to electroencephalography (EEG) data, which records brain electrical activity as time series. However, no comparative analysis of EEG-specific foundation models (EEGFMs) versus general TSFMs has been performed on EEG-specific tasks. We introduce a novel Spatial-Temporal Adapter with Multi-Head Pooling (STAMP), which leverages univariate embeddings produced by a general TSFM, implicitly models spatial-temporal characteristics of EEG data, and achieves performance comparable to state-of-the-art EEGFMs. A comprehensive analysis is performed on 8 benchmark datasets of clinical tasks using EEG for classification, along with ablation studies. Our proposed adapter is lightweight in trainable parameters and flexible in the inputs it can accommodate, supporting easy modeling of EEG data using TSFMs.

</details>


### [41] [ExPairT-LLM: Exact Learning for LLM Code Selection by Pairwise Queries](https://arxiv.org/abs/2511.10855)
*Tom Yuviler,Dana Drachsler-Cohen*

Main category: cs.LG

TL;DR: ExPairT-LLM是一种精确的代码选择算法，通过向LLM提出成对成员资格和成对等价性查询来从多个生成的程序中选择最佳程序，在四个流行代码数据集上平均比最先进算法提升13.0%的pass@1成功率。


<details>
  <summary>Details</summary>
Motivation: 现有代码选择算法可能无法识别正确程序，因为它们可能错误识别非等价程序，或者依赖LLM并假设它总能正确确定每个输入的输出。

Method: 提出ExPairT-LLM算法，通过向LLM提出两种新型查询（成对成员资格和成对等价性）来通过锦标赛方式选择程序，这些查询对LLM更简单且能容忍一些LLM错误。

Result: 在四个流行代码数据集上，ExPairT-LLM的pass@1（成功率）平均比最先进代码选择算法高出13.0%，最高达27.1%，并将执行复杂推理的LLM的pass@1提高了24.0%。

Conclusion: ExPairT-LLM通过更简单的查询类型和锦标赛机制，有效提高了代码选择的准确性，对LLM错误具有鲁棒性。

Abstract: Despite recent advances in LLMs, the task of code generation is still challenging. To cope, code selection algorithms select the best program from multiple programs generated by an LLM. However, existing algorithms can fail to identify the correct program, either because they can misidentify nonequivalent programs or because they rely on an LLM and assume it always correctly determines the output for every input. We present ExPairT-LLM, an exact learning algorithm for code selection that selects a program by posing to an LLM oracle two new types of queries: pairwise membership and pairwise equivalence. These queries are simpler for LLMs and enable ExPairT-LLM to identify the correct program through a tournament, which is robust to some LLM mistakes. We evaluate ExPairT-LLM on four popular code datasets. Its pass@1 (success rate) outperforms the state-of-the-art code selection algorithm on average by +13.0% and up to +27.1%. It also improves the pass@1 of LLMs performing complex reasoning by +24.0%.

</details>


### [42] [Private Zeroth-Order Optimization with Public Data](https://arxiv.org/abs/2511.10859)
*Xuchen Gong,Tian Li*

Main category: cs.LG

TL;DR: 提出PAZO框架，利用公共数据指导私有零阶优化算法，在保持隐私的同时显著提升效用和运行效率。


<details>
  <summary>Details</summary>
Motivation: 解决一阶差分隐私机器学习算法的高计算和内存成本问题，零阶方法虽然更容易实现隐私保护但效用较低，需要改进梯度近似质量。

Method: 利用公共数据指导私有零阶优化，提出PAZO框架，在相似性假设下提供理论分析。

Result: 在视觉和文本任务中实现优越的隐私/效用权衡，在高度隐私场景下优于最佳一阶基线，提供高达16倍的运行加速。

Conclusion: PAZO框架有效结合公共数据与零阶优化，在保持隐私的同时显著提升算法性能和效率。

Abstract: One of the major bottlenecks for deploying popular first-order differentially private (DP) machine learning algorithms (e.g., DP-SGD) lies in their high computation and memory cost, despite the existence of optimized implementations. Zeroth-order methods have promise in mitigating the overhead, as they leverage function evaluations to approximate the gradients, hence significantly easier to privatize. While recent works have explored zeroth-order approaches in both private and non-private settings, they still suffer from relatively low utilities compared with DP-SGD, and have only been evaluated in limited application domains. In this work, we propose to leverage public information to guide and improve gradient approximation of private zeroth-order algorithms. We explore a suite of public-data-assisted zeroth-order optimizers (PAZO) with minimal overhead. We provide theoretical analyses of the PAZO framework under an assumption of the similarity between public and private data. Empirically, we demonstrate that PAZO achieves superior privacy/utility tradeoffs across vision and text tasks in both pre-training and fine-tuning settings, outperforming the best first-order baselines (with public data) especially in highly private regimes, while offering up to $16\times$ runtime speedup.

</details>


### [43] [Go-UT-Bench: A Fine-Tuning Dataset for LLM-Based Unit Test Generation in Go](https://arxiv.org/abs/2511.10868)
*Yashshi Pipalani,Hritik Raj,Rajat Ghosh,Vaishnavi Bhargava,Debojyoti Dutta*

Main category: cs.LG

TL;DR: GO UT Bench是一个包含5264对代码和单元测试的基准数据集，用于解决代码LLM在训练数据不平衡问题，特别是在Golang等低资源语言中。


<details>
  <summary>Details</summary>
Motivation: 当前代码LLM的训练数据严重不平衡，过度代表开源代码而低估了更广泛的软件工程任务，导致模型在代码自动补全方面表现出色，但在单元测试生成等实际开发工作流程中表现不佳。

Method: 从10个许可宽松的Golang仓库中收集数据，构建GO UT Bench基准数据集，并在两种LLM家族（专家混合和密集解码器）上进行微调评估。

Result: 微调后的模型在超过75%的基准任务上优于其基础模型。

Conclusion: GO UT Bench作为微调数据集能有效提升代码LLM在单元测试生成等实际开发任务上的性能。

Abstract: Training data imbalance poses a major challenge for code LLMs. Most available data heavily over represents raw opensource code while underrepresenting broader software engineering tasks, especially in low resource languages like Golang. As a result, models excel at code autocompletion but struggle with real world developer workflows such as unit test generation. To address this gap, we introduce GO UT Bench, a benchmark dataset of 5264 pairs of code and unit tests, drawn from 10 permissively licensed Golang repositories spanning diverse domain. We evaluate its effectiveness as a fine tuning dataset across two LLM families i.e. mixture of experts and dense decoders. Our results show that finetuned models outperform their base counterparts on more than 75% of benchmark tasks.

</details>


### [44] [Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework](https://arxiv.org/abs/2511.10915)
*Guanxiong He,Jie Wang,Liaoyuan Tang,Zheng Wang,Rong Wang,Feiping Nie*

Main category: cs.LG

TL;DR: 提出SPP-FGC算法，通过本地结构图进行隐私保护的联邦聚类，解决传统方法在性能与隐私之间的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 解决联邦聚类中传输嵌入表示会导致敏感数据泄露，而仅共享抽象聚类原型又会降低模型准确性的两难困境。

Method: 基于客户端-服务器架构，客户端构建私有结构图捕获数据关系，服务器安全聚合和对齐形成全局图，从中推导统一聚类结构。提供SPP-FGC（单轮通信）和SPP-FGC+（迭代优化）两种模式。

Result: 在广泛实验中达到最先进性能，相比联邦基线将聚类准确率提升高达10%（NMI），同时保持可证明的隐私保证。

Conclusion: SPP-FGC框架通过结构图作为隐私保护知识共享媒介，成功解决了联邦聚类中性能与隐私的权衡问题，为不同需求提供灵活解决方案。

Abstract: Federated clustering addresses the critical challenge of extracting patterns from decentralized, unlabeled data. However, it is hampered by the flaw that current approaches are forced to accept a compromise between performance and privacy: \textit{transmitting embedding representations risks sensitive data leakage, while sharing only abstract cluster prototypes leads to diminished model accuracy}. To resolve this dilemma, we propose Structural Privacy-Preserving Federated Graph Clustering (SPP-FGC), a novel algorithm that innovatively leverages local structural graphs as the primary medium for privacy-preserving knowledge sharing, thus moving beyond the limitations of conventional techniques. Our framework operates on a clear client-server logic; on the client-side, each participant constructs a private structural graph that captures intrinsic data relationships, which the server then securely aggregates and aligns to form a comprehensive global graph from which a unified clustering structure is derived. The framework offers two distinct modes to suit different needs. SPP-FGC is designed as an efficient one-shot method that completes its task in a single communication round, ideal for rapid analysis. For more complex, unstructured data like images, SPP-FGC+ employs an iterative process where clients and the server collaboratively refine feature representations to achieve superior downstream performance. Extensive experiments demonstrate that our framework achieves state-of-the-art performance, improving clustering accuracy by up to 10\% (NMI) over federated baselines while maintaining provable privacy guarantees.

</details>


### [45] [Cascading Bandits With Feedback](https://arxiv.org/abs/2511.10938)
*R Sri Prakash,Nikhil Karamchandani,Sharayu Moharir*

Main category: cs.LG

TL;DR: 该论文研究了边缘推理中的级联赌博机模型，分析了四种决策策略的理论遗憾保证，发现LCB和Thompson Sampling因持续自适应而获得常数遗憾，优于固定排序的Explore-then-Commit和Action Elimination策略。


<details>
  <summary>Details</summary>
Motivation: 受边缘推理挑战的驱动，研究每个臂对应具有相关准确度和错误概率的推理模型的级联赌博机变体。

Method: 分析四种决策策略：Explore-then-Commit、Action Elimination、Lower Confidence Bound (LCB)和Thompson Sampling，并提供每种策略的尖锐理论遗憾保证。

Result: 与经典赌博机设置不同，Explore-then-Commit和Action Elimination因在探索阶段后承诺固定排序而遭受次优遗憾。LCB和Thompson Sampling基于观察反馈持续更新决策，实现常数O(1)遗憾。模拟验证了这些理论发现。

Conclusion: 在不确定性下的高效边缘推理中，自适应性起着关键作用，LCB和Thompson Sampling因持续自适应而优于固定排序策略。

Abstract: Motivated by the challenges of edge inference, we study a variant of the cascade bandit model in which each arm corresponds to an inference model with an associated accuracy and error probability. We analyse four decision-making policies-Explore-then-Commit, Action Elimination, Lower Confidence Bound (LCB), and Thompson Sampling-and provide sharp theoretical regret guarantees for each. Unlike in classical bandit settings, Explore-then-Commit and Action Elimination incur suboptimal regret because they commit to a fixed ordering after the exploration phase, limiting their ability to adapt. In contrast, LCB and Thompson Sampling continuously update their decisions based on observed feedback, achieving constant O(1) regret. Simulations corroborate these theoretical findings, highlighting the crucial role of adaptivity for efficient edge inference under uncertainty.

</details>


### [46] [From Parameter to Representation: A Closed-Form Approach for Controllable Model Merging](https://arxiv.org/abs/2511.10943)
*Jialin Wu,Jian Yang,Handing Wang,Jiajun Wen,Zhiyong Yu*

Main category: cs.LG

TL;DR: 本文提出了一种新的可控模型融合方法，通过直接修正模型最终表示而非参数空间优化，使用最优线性变换的闭式解替代昂贵的离线多目标优化，实现了线性复杂度的高效模型生成。


<details>
  <summary>Details</summary>
Motivation: 现有模型融合方法面临参数干扰问题，且现有可控融合方法采用编译-查询范式，需要进行昂贵的离线多目标优化，复杂度随任务数量指数增长，限制了实际应用。

Method: 将视角从参数空间优化转向直接修正模型最终表示，将该修正建模为最优线性变换，得到闭式解，用单步架构无关计算替代整个离线优化过程。

Result: 实验结果表明，该方法生成的Pareto前沿更优，偏好对齐更精确，计算成本大幅降低，复杂度仅随任务数量线性增长。

Conclusion: 提出的方法通过表示空间直接修正实现了高效可控的模型融合，解决了现有方法计算复杂度高的问题，为多任务模型融合提供了更实用的解决方案。

Abstract: Model merging combines expert models for multitask performance but faces challenges from parameter interference. This has sparked recent interest in controllable model merging, giving users the ability to explicitly balance performance trade-offs. Existing approaches employ a compile-then-query paradigm, performing a costly offline multi-objective optimization to enable fast, preference-aware model generation. This offline stage typically involves iterative search or dedicated training, with complexity that grows exponentially with the number of tasks. To overcome these limitations, we shift the perspective from parameter-space optimization to a direct correction of the model's final representation. Our approach models this correction as an optimal linear transformation, yielding a closed-form solution that replaces the entire offline optimization process with a single-step, architecture-agnostic computation. This solution directly incorporates user preferences, allowing a Pareto-optimal model to be generated on-the-fly with complexity that scales linearly with the number of tasks. Experimental results show our method generates a superior Pareto front with more precise preference alignment and drastically reduced computational cost.

</details>


### [47] [How Data Quality Affects Machine Learning Models for Credit Risk Assessment](https://arxiv.org/abs/2511.10964)
*Andrea Maurino*

Main category: cs.LG

TL;DR: 本文研究了数据质量问题（缺失值、噪声属性、异常值和标签错误）对信用风险评估机器学习模型预测准确性的影响，通过受控数据污染实验评估了10种常用模型的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 机器学习模型在信用风险评估中的应用日益增多，但其有效性很大程度上取决于输入数据的质量。本文旨在探究各种数据质量问题对模型性能的影响。

Method: 使用开源数据集，通过Pucktrick库引入受控数据污染，评估包括随机森林、SVM和逻辑回归在内的10种常用模型的鲁棒性。

Result: 实验结果显示，不同模型对数据退化的鲁棒性存在显著差异，具体取决于数据质量问题的性质和严重程度。

Conclusion: 提出的方法和配套工具为从业者增强数据管道鲁棒性提供了实用支持，并为研究人员在数据为中心的AI环境中进一步实验提供了灵活框架。

Abstract: Machine Learning (ML) models are being increasingly employed for credit risk evaluation, with their effectiveness largely hinging on the quality of the input data. In this paper we investigate the impact of several data quality issues, including missing values, noisy attributes, outliers, and label errors, on the predictive accuracy of the machine learning model used in credit risk assessment. Utilizing an open-source dataset, we introduce controlled data corruption using the Pucktrick library to assess the robustness of 10 frequently used models like Random Forest, SVM, and Logistic Regression and so on. Our experiments show significant differences in model robustness based on the nature and severity of the data degradation. Moreover, the proposed methodology and accompanying tools offer practical support for practitioners seeking to enhance data pipeline robustness, and provide researchers with a flexible framework for further experimentation in data-centric AI contexts.

</details>


### [48] [Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm](https://arxiv.org/abs/2511.11009)
*Fuxiang Huang,Xiaowei Fu,Shiyu Ye,Lina Ma,Wen Li,Xinbo Gao,David Zhang,Lei Zhang*

Main category: cs.LG

TL;DR: 本文提出了无监督鲁棒域自适应（URDA）范式，解决了传统无监督域自适应方法忽视对抗攻击鲁棒性的问题，并开发了分离对抗鲁棒训练（DART）算法。


<details>
  <summary>Details</summary>
Motivation: 传统无监督域自适应方法强调迁移能力但忽视对抗攻击鲁棒性，而标准对抗训练在UDA中效果不佳，需要探索新的鲁棒域自适应范式。

Method: 提出URDA范式，揭示UDA+VAT的内在纠缠问题，开发DART算法——两阶段训练：先预训练任意UDA模型，再通过分离蒸馏进行瞬时鲁棒化后训练。

Result: 在四个基准数据集上的实验表明，DART在有无攻击情况下都能有效增强鲁棒性并保持域适应能力，验证了URDA范式和理论。

Conclusion: 本文首次建立了URDA范式和理论，DART算法简单有效，能同时确保迁移性和鲁棒性，为鲁棒域自适应提供了新思路。

Abstract: Unsupervised domain adaptation (UDA) aims to transfer knowledge from a label-rich source domain to an unlabeled target domain by addressing domain shifts. Most UDA approaches emphasize transfer ability, but often overlook robustness against adversarial attacks. Although vanilla adversarial training (VAT) improves the robustness of deep neural networks, it has little effect on UDA. This paper focuses on answering three key questions: 1) Why does VAT, known for its defensive effectiveness, fail in the UDA paradigm? 2) What is the generalization bound theory under attacks and how does it evolve from classical UDA theory? 3) How can we implement a robustification training procedure without complex modifications? Specifically, we explore and reveal the inherent entanglement challenge in general UDA+VAT paradigm, and propose an unsupervised robust domain adaptation (URDA) paradigm. We further derive the generalization bound theory of the URDA paradigm so that it can resist adversarial noise and domain shift. To the best of our knowledge, this is the first time to establish the URDA paradigm and theory. We further introduce a simple, novel yet effective URDA algorithm called Disentangled Adversarial Robustness Training (DART), a two-step training procedure that ensures both transferability and robustness. DART first pre-trains an arbitrary UDA model, and then applies an instantaneous robustification post-training step via disentangled distillation.Experiments on four benchmark datasets with/without attacks show that DART effectively enhances robustness while maintaining domain adaptability, and validate the URDA paradigm and theory.

</details>


### [49] [Scalable Population Training for Zero-Shot Coordination](https://arxiv.org/abs/2511.11083)
*Bingyu Hui,Lebin Yu,Quanming Yao,Yunpeng Qu,Xudong Zhang,Jian Wang*

Main category: cs.LG

TL;DR: 本文提出了可扩展群体训练(ScaPT)框架，通过元智能体和互信息正则化器来解决零样本协调中群体规模扩展的计算资源限制问题，在Hanabi游戏中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 现有基于群体的训练方法受限于计算资源，主要在小群体中优化多样性，而忽视了通过扩大群体规模带来的潜在性能提升。

Method: 提出ScaPT框架，包含两个关键组件：通过选择性共享参数实现群体的元智能体，以及保证群体多样性的互信息正则化器。

Result: 在Hanabi游戏中与代表性框架进行对比评估，证实了ScaPT方法的优越性。

Conclusion: ScaPT框架能够有效解决零样本协调中群体规模扩展的计算挑战，提供更好的协调性能。

Abstract: Zero-shot coordination(ZSC) has become a hot topic in reinforcement learning research recently. It focuses on the generalization ability of agents, requiring them to coordinate well with collaborators that are not seen before without any fine-tuning. Population-based training has been proven to provide good zero-shot coordination performance; nevertheless, existing methods are limited by computational resources, mainly focusing on optimizing diversity in small populations while neglecting the potential performance gains from scaling population size. To address this issue, this paper proposes the Scalable Population Training (ScaPT), an efficient training framework comprising two key components: a meta-agent that efficiently realizes a population by selectively sharing parameters across agents, and a mutual information regularizer that guarantees population diversity. To empirically validate the effectiveness of ScaPT, this paper evaluates it along with representational frameworks in Hanabi and confirms its superiority.

</details>


### [50] [Improving Continual Learning of Knowledge Graph Embeddings via Informed Initialization](https://arxiv.org/abs/2511.11118)
*Gerard Pons,Besim Bilalli,Anna Queralt*

Main category: cs.LG

TL;DR: 提出一种新的知识图谱嵌入初始化策略，利用图谱模式和已有嵌入为新实体生成初始表示，提升持续学习效果


<details>
  <summary>Details</summary>
Motivation: 知识图谱频繁更新需要嵌入模型持续学习，而新实体嵌入初始化对最终准确性和训练时间有重要影响，特别是对于小型频繁更新

Method: 利用知识图谱模式和先前学习到的嵌入，基于实体所属类别为新实体获取初始表示，可无缝集成到现有持续学习方法中

Result: 实验分析表明该初始化策略提高了预测性能，增强了知识保留，加速了知识获取并减少了训练轮次和时间

Conclusion: 该方法在各类知识图谱嵌入学习模型中均表现出优势，能够有效提升新知识获取同时减少灾难性遗忘

Abstract: Many Knowledege Graphs (KGs) are frequently updated, forcing their Knowledge Graph Embeddings (KGEs) to adapt to these changes. To address this problem, continual learning techniques for KGEs incorporate embeddings for new entities while updating the old ones. One necessary step in these methods is the initialization of the embeddings, as an input to the KGE learning process, which can have an important impact in the accuracy of the final embeddings, as well as in the time required to train them. This is especially relevant for relatively small and frequent updates. We propose a novel informed embedding initialization strategy, which can be seamlessly integrated into existing continual learning methods for KGE, that enhances the acquisition of new knowledge while reducing catastrophic forgetting. Specifically, the KG schema and the previously learned embeddings are utilized to obtain initial representations for the new entities, based on the classes the entities belong to. Our extensive experimental analysis shows that the proposed initialization strategy improves the predictive performance of the resulting KGEs, while also enhancing knowledge retention. Furthermore, our approach accelerates knowledge acquisition, reducing the number of epochs, and therefore time, required to incrementally learn new embeddings. Finally, its benefits across various types of KGE learning models are demonstrated.

</details>


### [51] [Anomaly Detection in High-Dimensional Bank Account Balances via Robust Methods](https://arxiv.org/abs/2511.11143)
*Federico Maddanu,Tommaso Proietti,Riccardo Crupi*

Main category: cs.LG

TL;DR: 提出并评估了几种在中等和高维数据集中计算效率高的稳健方法，用于检测银行账户余额中的点异常，应用于260万条匿名用户银行账户余额的每日记录。


<details>
  <summary>Details</summary>
Motivation: 检测银行账户余额中的点异常对金融机构至关重要，可以识别潜在的欺诈、操作问题或其他异常情况。稳健统计在标记异常值和提供不受污染观测影响的数据分布参数估计方面很有用，但在高维设置下通常效率较低且计算成本高。

Method: 提出并评估了几种稳健方法，这些方法在中等和高维数据集中具有高崩溃点和低计算时间，可能计算效率高。

Result: 应用处理了约260万条匿名用户银行账户余额的每日记录。

Conclusion: 所提出的稳健方法在高维银行账户余额数据中具有实际应用价值，能够有效检测异常同时保持计算效率。

Abstract: Detecting point anomalies in bank account balances is essential for financial institutions, as it enables the identification of potential fraud, operational issues, or other irregularities. Robust statistics is useful for flagging outliers and for providing estimates of the data distribution parameters that are not affected by contaminated observations. However, such a strategy is often less efficient and computationally expensive under high dimensional setting. In this paper, we propose and evaluate empirically several robust approaches that may be computationally efficient in medium and high dimensional datasets, with high breakdown points and low computational time. Our application deals with around 2.6 million daily records of anonymous users' bank account balances.

</details>


### [52] [Bias-Restrained Prefix Representation Finetuning for Mathematical Reasoning](https://arxiv.org/abs/2511.10707)
*Sirui Liang,Pengfei Cao,Jian Zhao,Cong Huang,Jun Zhao,Kang Liu*

Main category: cs.LG

TL;DR: 本文提出BREP ReFT方法，通过截断训练数据优化初始推理前缀生成、干预早期推理阶段防止错误累积、约束干预向量幅度避免干扰数值编码，显著提升了ReFT在数学推理任务上的性能。


<details>
  <summary>Details</summary>
Motivation: ReFT方法在数学推理任务上表现显著下降，主要原因是难以生成有效的推理前缀以及干扰数值编码导致错误累积。

Method: 提出BREP ReFT方法：1）截断训练数据优化初始推理前缀生成；2）干预早期推理阶段防止错误累积；3）约束干预向量幅度避免干扰数值编码。

Result: 在多种模型架构上的广泛实验表明，BREP在数学推理任务上优于标准ReFT和基于权重的PEFT方法，具有更好的效果、效率和鲁棒泛化能力。

Conclusion: BREP ReFT通过针对性地解决ReFT在数学推理中的核心问题，显著提升了性能，同时保持了参数效率优势。

Abstract: Parameter-Efficient finetuning (PEFT) enhances model performance on downstream tasks by updating a minimal subset of parameters. Representation finetuning (ReFT) methods further improve efficiency by freezing model weights and optimizing internal representations with fewer parameters than PEFT, outperforming PEFT on several tasks. However, ReFT exhibits a significant performance decline on mathematical reasoning tasks. To address this problem, the paper demonstrates that ReFT's poor performance on mathematical tasks primarily stems from its struggle to generate effective reasoning prefixes during the early inference phase. Moreover, ReFT disturbs the numerical encoding and the error accumulats during the CoT stage. Based on these observations, this paper proposes Bias-REstrained Prefix Representation FineTuning (BREP ReFT), which enhances ReFT's mathematical reasoning capability by truncating training data to optimize the generation of initial reasoning prefixes, intervening on the early inference stage to prevent error accumulation, and constraining the intervention vectors' magnitude to avoid disturbing numerical encoding. Extensive experiments across diverse model architectures demonstrate BREP's superior effectiveness, efficiency, and robust generalization capability, outperforming both standard ReFT and weight-based PEFT methods on the task of mathematical reasoning. The source code is available at https://github.com/LiangThree/BREP.

</details>


### [53] [Deep Learning for Short-Term Precipitation Prediction in Four Major Indian Cities: A ConvLSTM Approach with Explainable AI](https://arxiv.org/abs/2511.11152)
*Tanmay Ghosh,Shaurabh Anand,Rakesh Gomaji Nannewar,Nithin Nagaraj*

Main category: cs.LG

TL;DR: 开发了一个可解释的深度学习框架用于印度四个主要城市的短期降水预测，通过混合CNN-ConvLSTM架构实现准确预测，并利用多种可解释性方法分析模型行为。


<details>
  <summary>Details</summary>
Motivation: 深度学习降水预测模型通常作为黑盒运行，限制了在实际天气预测中的应用。为了在保持准确性的同时提高透明度，需要开发可解释的深度学习框架。

Method: 采用混合时间分布CNN-ConvLSTM架构，在多年代ERA5再分析数据上训练。为每个城市优化卷积滤波器数量：班加罗尔(32)、孟买和德里(64)、加尔各答(128)。使用排列重要性、Grad-CAM、时间遮挡和反事实扰动进行可解释性分析。

Result: 模型实现了RMSE值：班加罗尔0.21毫米/天、孟买0.52毫米/天、德里0.48毫米/天、加尔各答1.80毫米/天。预测范围从班加罗尔的1天到加尔各答的5天。模型依赖城市特定变量。

Conclusion: 研究表明可解释AI能够提供准确的降水预测，并在不同城市环境中提供透明的降水模式洞察，证明了xAI在降水预测中的实用性。

Abstract: Deep learning models for precipitation forecasting often function as black boxes, limiting their adoption in real-world weather prediction. To enhance transparency while maintaining accuracy, we developed an interpretable deep learning framework for short-term precipitation prediction in four major Indian cities: Bengaluru, Mumbai, Delhi, and Kolkata, spanning diverse climate zones. We implemented a hybrid Time-Distributed CNN-ConvLSTM (Convolutional Neural Network-Long Short-Term Memory) architecture, trained on multi-decadal ERA5 reanalysis data. The architecture was optimized for each city with a different number of convolutional filters: Bengaluru (32), Mumbai and Delhi (64), and Kolkata (128). The models achieved root mean square error (RMSE) values of 0.21 mm/day (Bengaluru), 0.52 mm/day (Mumbai), 0.48 mm/day (Delhi), and 1.80 mm/day (Kolkata). Through interpretability analysis using permutation importance, Gradient-weighted Class Activation Mapping (Grad-CAM), temporal occlusion, and counterfactual perturbation, we identified distinct patterns in the model's behavior. The model relied on city-specific variables, with prediction horizons ranging from one day for Bengaluru to five days for Kolkata. This study demonstrates how explainable AI (xAI) can provide accurate forecasts and transparent insights into precipitation patterns in diverse urban environments.

</details>


### [54] [Towards Uncertainty Quantification in Generative Model Learning](https://arxiv.org/abs/2511.10710)
*Giorgio Morales,Frederic Jurie,Jalal Fadili*

Main category: cs.LG

TL;DR: 本文提出生成模型学习中的不确定性量化问题，讨论了使用集成精度-召回曲线等研究方向，并在合成数据集上验证了聚合精度-召回曲线在捕捉模型近似不确定性方面的有效性。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型评估方法主要关注学习分布与目标分布的接近程度，但忽略了这些测量中固有的不确定性，这影响了生成模型的可靠性评估。

Method: 提出了不确定性量化在生成模型学习中的形式化问题，探讨了基于集成精度-召回曲线等研究方向的潜在方法。

Result: 在合成数据集上的初步实验表明，聚合精度-召回曲线能有效捕捉模型近似不确定性，使基于不确定性特征的系统性模型架构比较成为可能。

Conclusion: 不确定性量化是生成模型可靠性的关键但被忽视的方面，需要开发新的评估方法来系统评估模型分布近似的不确定性。

Abstract: While generative models have become increasingly prevalent across various domains, fundamental concerns regarding their reliability persist. A crucial yet understudied aspect of these models is the uncertainty quantification surrounding their distribution approximation capabilities. Current evaluation methodologies focus predominantly on measuring the closeness between the learned and the target distributions, neglecting the inherent uncertainty in these measurements. In this position paper, we formalize the problem of uncertainty quantification in generative model learning. We discuss potential research directions, including the use of ensemble-based precision-recall curves. Our preliminary experiments on synthetic datasets demonstrate the effectiveness of aggregated precision-recall curves in capturing model approximation uncertainty, enabling systematic comparison among different model architectures based on their uncertainty characteristics.

</details>


### [55] [Power Ensemble Aggregation for Improved Extreme Event AI Prediction](https://arxiv.org/abs/2511.11170)
*Julien Collard,Pierre Gentine,Tian Zheng*

Main category: cs.LG

TL;DR: 本文提出使用幂均值聚合集成预测来改进热浪等气候极端事件的机器学习预测方法，相比传统均值预测能获得更好的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决气候极端事件（特别是热浪）预测的关键挑战，改进机器学习方法对极端天气的预测能力。

Method: 将问题构建为分类问题，预测地表气温是否会在指定时间内超过其局部q分位数；通过使机器学习天气预报模型具有生成性，并应用幂均值这种非线性聚合方法来整合集成预测。

Result: 幂均值聚合显著提高了分类器的性能，在预测极端热事件方面比相同模型的典型均值预测具有更好的准确性；该方法的最优性能随所选分位数阈值而变化，对更高极端值的预测效果更好。

Conclusion: 幂均值聚合方法在气候极端事件预测中显示出良好的前景和适应性，特别适用于更高极端值的预测。

Abstract: This paper addresses the critical challenge of improving predictions of climate extreme events, specifically heat waves, using machine learning methods. Our work is framed as a classification problem in which we try to predict whether surface air temperature will exceed its q-th local quantile within a specified timeframe. Our key finding is that aggregating ensemble predictions using a power mean significantly enhances the classifier's performance. By making a machine-learning based weather forecasting model generative and applying this non-linear aggregation method, we achieve better accuracy in predicting extreme heat events than with the typical mean prediction from the same model. Our power aggregation method shows promise and adaptability, as its optimal performance varies with the quantile threshold chosen, demonstrating increased effectiveness for higher extremes prediction.

</details>


### [56] [On-line learning of dynamic systems: sparse regression meets Kalman filtering](https://arxiv.org/abs/2511.11178)
*Gianluigi Pillonetto,Akram Yazdani,Aleksandr Aravkin*

Main category: cs.LG

TL;DR: 本文提出了Sindy Kalman Filter (SKF)算法，将稀疏驱动的Sindy方法与卡尔曼滤波相结合，实现了非线性动态系统的实时学习，能够处理参数漂移或切换的复杂系统。


<details>
  <summary>Details</summary>
Motivation: 从数据中学习控制方程对于理解物理系统行为至关重要，但现有方法难以实现实时学习复杂、时变的非线性模型。

Method: 将Sindy算法与卡尔曼滤波结合，将未知系统参数作为状态变量处理，通过前瞻误差增强参数识别策略。

Result: 在参数漂移或切换的混沌Lorenz系统和真实飞行数据构建的稀疏非线性飞机模型上验证了SKF的有效性。

Conclusion: SKF统一了稀疏驱动和控制理论框架，实现了传统方法无法达到的复杂时变非线性模型的实时推断能力。

Abstract: Learning governing equations from data is central to understanding the behavior of physical systems across diverse scientific disciplines, including physics, biology, and engineering. The Sindy algorithm has proven effective in leveraging sparsity to identify concise models of nonlinear dynamical systems. In this paper, we extend sparsity-driven approaches to real-time learning by integrating a cornerstone algorithm from control theory -- the Kalman filter (KF). The resulting Sindy Kalman Filter (SKF) unifies both frameworks by treating unknown system parameters as state variables, enabling real-time inference of complex, time-varying nonlinear models unattainable by either method alone. Furthermore, SKF enhances KF parameter identification strategies, particularly via look-ahead error, significantly simplifying the estimation of sparsity levels, variance parameters, and switching instants. We validate SKF on a chaotic Lorenz system with drifting or switching parameters and demonstrate its effectiveness in the real-time identification of a sparse nonlinear aircraft model built from real flight data.

</details>


### [57] [Dynamic Deep Graph Learning for Incomplete Multi-View Clustering with Masked Graph Reconstruction Loss](https://arxiv.org/abs/2511.11181)
*Zhenghao Zhang,Jun Xie,Xingchen Chen,Tao Yu,Hongzhu Yi,Kaixin Xu,Yuanxiang Wang,Tianyu Zong,Xinming Wang,Jiahuan Chen,Guoqing Chao,Feng Chen,Zhepeng Wang,Jungang Xu*

Main category: cs.LG

TL;DR: 该论文提出了一种名为DGIMVCM的动态深度图学习方法，用于解决不完整多视图聚类中的图构建噪声和优化梯度噪声问题。


<details>
  <summary>Details</summary>
Motivation: 现实世界中多视图数据的普遍性使得不完整多视图聚类成为重要研究方向。现有基于GNN的方法存在两个主要挑战：(1) 使用KNN构建静态图会引入噪声并降低图拓扑的鲁棒性；(2) 使用MSE损失作为图重构损失会导致优化过程中产生大量梯度噪声。

Method: 提出DGIMVCM方法：首先从不完整原始数据构建缺失鲁棒的全局图，通过图卷积嵌入层提取主要特征和精炼的动态视图特定图结构，利用全局图进行缺失视图补全，并通过图结构对比学习识别视图间一致性；其次引入图自注意力编码器基于补全特征和视图特定图提取高层表示，使用掩码图重构损失优化以减少梯度噪声；最后构建聚类模块并通过伪标签自监督训练机制优化。

Result: 在多个数据集上的广泛实验验证了DGIMVCM的有效性和优越性。

Conclusion: DGIMVCM通过动态深度图学习和掩码图重构损失有效解决了不完整多视图聚类中的图构建噪声和优化梯度噪声问题，提高了聚类性能。

Abstract: The prevalence of real-world multi-view data makes incomplete multi-view clustering (IMVC) a crucial research. The rapid development of Graph Neural Networks (GNNs) has established them as one of the mainstream approaches for multi-view clustering. Despite significant progress in GNNs-based IMVC, some challenges remain: (1) Most methods rely on the K-Nearest Neighbors (KNN) algorithm to construct static graphs from raw data, which introduces noise and diminishes the robustness of the graph topology. (2) Existing methods typically utilize the Mean Squared Error (MSE) loss between the reconstructed graph and the sparse adjacency graph directly as the graph reconstruction loss, leading to substantial gradient noise during optimization. To address these issues, we propose a novel \textbf{D}ynamic Deep \textbf{G}raph Learning for \textbf{I}ncomplete \textbf{M}ulti-\textbf{V}iew \textbf{C}lustering with \textbf{M}asked Graph Reconstruction Loss (DGIMVCM). Firstly, we construct a missing-robust global graph from the raw data. A graph convolutional embedding layer is then designed to extract primary features and refined dynamic view-specific graph structures, leveraging the global graph for imputation of missing views. This process is complemented by graph structure contrastive learning, which identifies consistency among view-specific graph structures. Secondly, a graph self-attention encoder is introduced to extract high-level representations based on the imputed primary features and view-specific graphs, and is optimized with a masked graph reconstruction loss to mitigate gradient noise during optimization. Finally, a clustering module is constructed and optimized through a pseudo-label self-supervised training mechanism. Extensive experiments on multiple datasets validate the effectiveness and superiority of DGIMVCM.

</details>


### [58] [LoRaCompass: Robust Reinforcement Learning to Efficiently Search for a LoRa Tag](https://arxiv.org/abs/2511.11190)
*Tianlang He,Zhongming Lin,Tianrui Jiang,S. -H. Gary Chan*

Main category: cs.LG

TL;DR: LoRaCompass是一种强化学习模型，用于在未知环境中高效定位周期性广播的LoRa标签，通过空间感知特征提取器和策略蒸馏损失函数实现鲁棒搜索，在80km²的多样化环境中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LoRa标签定位方法容易受到域偏移和信号波动的影响，导致决策错误累积和定位不准确，需要开发更鲁棒的搜索方法。

Method: 提出LoRaCompass模型，包含：1）空间感知特征提取器和策略蒸馏损失函数，从RSSI学习鲁棒空间表示；2）受上置信界启发的探索函数，引导传感器以递增置信度向标签移动。

Result: 在超过80km²的多样化未见环境中验证，LoRaCompass在100米范围内定位成功率超过90%（比现有方法提高40%），搜索路径长度与初始距离呈线性比例关系。

Conclusion: LoRaCompass能够实现鲁棒高效的LoRa标签搜索，在域偏移和信号波动条件下仍能保持高成功率和搜索效率。

Abstract: The Long-Range (LoRa) protocol, known for its extensive range and low power, has increasingly been adopted in tags worn by mentally incapacitated persons (MIPs) and others at risk of going missing. We study the sequential decision-making process for a mobile sensor to locate a periodically broadcasting LoRa tag with the fewest moves (hops) in general, unknown environments, guided by the received signal strength indicator (RSSI). While existing methods leverage reinforcement learning for search, they remain vulnerable to domain shift and signal fluctuation, resulting in cascading decision errors that culminate in substantial localization inaccuracies. To bridge this gap, we propose LoRaCompass, a reinforcement learning model designed to achieve robust and efficient search for a LoRa tag. For exploitation under domain shift and signal fluctuation, LoRaCompass learns a robust spatial representation from RSSI to maximize the probability of moving closer to a tag, via a spatially-aware feature extractor and a policy distillation loss function. It further introduces an exploration function inspired by the upper confidence bound (UCB) that guides the sensor toward the tag with increasing confidence. We have validated LoRaCompass in ground-based and drone-assisted scenarios within diverse unseen environments covering an area of over 80km^2. It has demonstrated high success rate (>90%) in locating the tag within 100m proximity (a 40% improvement over existing methods) and high efficiency with a search path length (in hops) that scales linearly with the initial distance.

</details>


### [59] [When to Stop Federated Learning: Zero-Shot Generation of Synthetic Validation Data with Generative AI for Early Stopping](https://arxiv.org/abs/2511.11208)
*Youngjoon Lee,Hyukjoon Lee,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出了一种基于生成式AI的零样本合成验证框架，用于联邦学习中的早期停止，可减少高达74%的训练轮次，同时保持准确率在最优值的1%以内。


<details>
  <summary>Details</summary>
Motivation: 联邦学习通常运行预定义的全局轮次，导致在达到最优性能后仍进行不必要的计算，或者模型无法达到有意义性能时继续训练，造成效率低下。

Method: 引入零样本合成验证框架，利用生成式AI监控模型性能并确定早期停止点，自适应地在接近最优轮次时停止训练。

Result: 在多标签胸部X射线分类任务上的数值结果显示，该方法减少了高达74%的训练轮次，同时准确率保持在最优值的1%以内。

Conclusion: 该方法有效节省计算资源，并支持快速超参数调整，提高了联邦学习的训练效率。

Abstract: Federated Learning (FL) enables collaborative model training across decentralized devices while preserving data privacy. However, FL methods typically run for a predefined number of global rounds, often leading to unnecessary computation when optimal performance is reached earlier. In addition, training may continue even when the model fails to achieve meaningful performance. To address this inefficiency, we introduce a zero-shot synthetic validation framework that leverages generative AI to monitor model performance and determine early stopping points. Our approach adaptively stops training near the optimal round, thereby conserving computational resources and enabling rapid hyperparameter adjustments. Numerical results on multi-label chest X-ray classification demonstrate that our method reduces training rounds by up to 74% while maintaining accuracy within 1% of the optimal.

</details>


### [60] [A Best-of-Both-Worlds Proof for Tsallis-INF without Fenchel Conjugates](https://arxiv.org/abs/2511.11211)
*Wei-Cheng Lee,Francesco Orabona*

Main category: cs.LG

TL;DR: 本文提供了一个对Tsallis-INF多臂老虎机算法最佳两界保证的简化推导，避免了共轭函数的使用，采用在线凸优化工具，并为了简洁证明而不优化边界常数。


<details>
  <summary>Details</summary>
Motivation: 为Tsallis-INF算法的最佳两界保证提供一个更简单、更直接的证明方法，避免复杂数学工具的使用。

Method: 使用现代在线凸优化工具，避免共轭函数，不优化边界常数以保持证明的简洁性。

Result: 成功推导出Tsallis-INF算法在随机和对抗性老虎机问题中的最佳两界性能保证。

Conclusion: 提出的简化证明方法有效展示了Tsallis-INF算法的理论保证，为相关研究提供了更易理解的证明框架。

Abstract: In this short note, we present a simple derivation of the best-of-both-world guarantee for the Tsallis-INF multi-armed bandit algorithm from J. Zimmert and Y. Seldin. Tsallis-INF: An optimal algorithm for stochastic and adversarial bandits. Journal of Machine Learning Research, 22(28):1-49, 2021. URL https://jmlr.csail.mit.edu/papers/volume22/19-753/19-753.pdf. In particular, the proof uses modern tools from online convex optimization and avoid the use of conjugate functions. Also, we do not optimize the constants in the bounds in favor of a slimmer proof.

</details>


### [61] [HealSplit: Towards Self-Healing through Adversarial Distillation in Split Federated Learning](https://arxiv.org/abs/2511.11240)
*Yuhan Xie,Chen Lyu*

Main category: cs.LG

TL;DR: HealSplit是首个专门为Split Federated Learning设计的统一防御框架，提供端到端的检测和恢复功能，针对五种复杂的投毒攻击。


<details>
  <summary>Details</summary>
Motivation: Split Federated Learning虽然是一种新兴的隐私保护分布式学习范式，但仍然容易受到针对本地特征、标签、粉碎数据和模型权重的复杂数据投毒攻击。现有的防御方法主要从传统联邦学习改编而来，在SFL中效果较差，因为无法完全访问模型更新。

Method: HealSplit包含三个关键组件：(1)拓扑感知检测模块，通过在粉碎数据上构建图来识别投毒样本；(2)生成恢复管道，为检测到的异常合成语义一致的替代品；(3)对抗性多教师蒸馏框架，使用语义监督和异常感知信号训练学生模型。

Result: 在四个基准数据集上的广泛实验表明，HealSplit始终优于十种最先进的防御方法，在各种攻击场景下实现了卓越的鲁棒性和防御效果。

Conclusion: HealSplit为Split Federated Learning提供了一个有效的统一防御框架，能够有效检测和恢复多种复杂的投毒攻击，显著提升了SFL系统的安全性。

Abstract: Split Federated Learning (SFL) is an emerging paradigm for privacy-preserving distributed learning. However, it remains vulnerable to sophisticated data poisoning attacks targeting local features, labels, smashed data, and model weights. Existing defenses, primarily adapted from traditional Federated Learning (FL), are less effective under SFL due to limited access to complete model updates. This paper presents HealSplit, the first unified defense framework tailored for SFL, offering end-to-end detection and recovery against five sophisticated types of poisoning attacks. HealSplit comprises three key components: (1) a topology-aware detection module that constructs graphs over smashed data to identify poisoned samples via topological anomaly scoring (TAS); (2) a generative recovery pipeline that synthesizes semantically consistent substitutes for detected anomalies, validated by a consistency validation student; and (3) an adversarial multi-teacher distillation framework trains the student using semantic supervision from a Vanilla Teacher and anomaly-aware signals from an Anomaly-Influence Debiasing (AD) Teacher, guided by the alignment between topological and gradient-based interaction matrices. Extensive experiments on four benchmark datasets demonstrate that HealSplit consistently outperforms ten state-of-the-art defenses, achieving superior robustness and defense effectiveness across diverse attack scenarios.

</details>


### [62] [Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria](https://arxiv.org/abs/2511.11293)
*Jiheum Park,Chao Pang,Tristan Y. Lee,Jeong Yun Yang,Jacob Berkowitz,Alexander Z. Wei,Nicholas Tatonetti*

Main category: cs.LG

TL;DR: 本研究评估了基于电子健康记录（EHR）的预测模型与传统风险因素在识别八种主要癌症高风险个体方面的临床效用。结果显示，EHR模型在识别高风险个体中真实癌症病例的富集度比传统风险因素高3-6倍。


<details>
  <summary>Details</summary>
Motivation: 当前癌症筛查指南仅覆盖少数癌症类型，且依赖年龄或单一风险因素等狭窄标准来识别高风险个体。基于EHR的预测模型可能通过检测癌症的微妙前诊断信号，提供更有效的识别工具。

Method: 使用All of Us研究计划的数据，整合了超过865,000名参与者的EHR、基因组和调查数据，系统评估EHR预测模型与传统风险因素（包括基因突变和癌症家族史）在八种主要癌症中的表现。

Result: 即使使用基线建模方法，EHR模型在识别高风险个体中真实癌症病例的富集度比传统风险因素高3-6倍。EHR基础模型进一步提高了26种癌症类型的预测性能。

Conclusion: 基于EHR的预测模型具有支持更精确和可扩展的早期检测策略的临床潜力，可作为独立或补充工具使用。

Abstract: Current cancer screening guidelines cover only a few cancer types and rely on narrowly defined criteria such as age or a single risk factor like smoking history, to identify high-risk individuals. Predictive models using electronic health records (EHRs), which capture large-scale longitudinal patient-level health information, may provide a more effective tool for identifying high-risk groups by detecting subtle prediagnostic signals of cancer. Recent advances in large language and foundation models have further expanded this potential, yet evidence remains limited on how useful HER-based models are compared with traditional risk factors currently used in screening guidelines. We systematically evaluated the clinical utility of EHR-based predictive models against traditional risk factors, including gene mutations and family history of cancer, for identifying high-risk individuals across eight major cancers (breast, lung, colorectal, prostate, ovarian, liver, pancreatic, and stomach), using data from the All of Us Research Program, which integrates EHR, genomic, and survey data from over 865,000 participants. Even with a baseline modeling approach, EHR-based models achieved a 3- to 6-fold higher enrichment of true cancer cases among individuals identified as high risk compared with traditional risk factors alone, whether used as a standalone or complementary tool. The EHR foundation model, a state-of-the-art approach trained on comprehensive patient trajectories, further improved predictive performance across 26 cancer types, demonstrating the clinical potential of EHR-based predictive modeling to support more precise and scalable early detection strategies.

</details>


### [63] [Retrofit: Continual Learning with Bounded Forgetting for Security Applications](https://arxiv.org/abs/2511.11439)
*Yiling He,Junchi Lei,Hongyu She,Shuo Shao,Xinran Zheng,Yiping Liu,Zhan Qin,Lorenzo Cavallaro*

Main category: cs.LG

TL;DR: RETROFIT是一种无需历史数据的持续学习方法，通过参数级模型合并和知识仲裁机制，在安全分析场景中有效缓解灾难性遗忘问题，在恶意软件检测和二进制摘要任务上显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决安全分析中深度学习模型因威胁环境演变和数据表示漂移导致的性能下降问题，同时克服传统持续学习方法依赖历史数据重放或完全重训练在数据敏感环境中的不可行性。

Method: 提出RETROFIT方法：1) 将先前训练模型和新微调模型作为新旧知识的教师模型进行参数级合并；2) 使用低秩和稀疏更新将参数变化限制在独立子空间；3) 基于模型置信度的知识仲裁机制动态平衡教师模型贡献。

Result: 在恶意软件检测任务中，RETROFIT将保留分数从基线的20.2%提升至38.6%，并在新数据上超过oracle上界；在二进制摘要任务中，BLEU分数达到先前工作中迁移学习方法的两倍，并在跨表示泛化方面超越所有基线。

Conclusion: RETROFIT通过无历史数据的持续学习方法，在安全分析场景中有效平衡了知识保留和适应性，为数据敏感环境下的模型持续学习提供了可行解决方案。

Abstract: Modern security analytics are increasingly powered by deep learning models, but their performance often degrades as threat landscapes evolve and data representations shift. While continual learning (CL) offers a promising paradigm to maintain model effectiveness, many approaches rely on full retraining or data replay, which are infeasible in data-sensitive environments. Moreover, existing methods remain inadequate for security-critical scenarios, facing two coupled challenges in knowledge transfer: preserving prior knowledge without old data and integrating new knowledge with minimal interference.
  We propose RETROFIT, a data retrospective-free continual learning method that achieves bounded forgetting for effective knowledge transfer. Our key idea is to consolidate previously trained and newly fine-tuned models, serving as teachers of old and new knowledge, through parameter-level merging that eliminates the need for historical data. To mitigate interference, we apply low-rank and sparse updates that confine parameter changes to independent subspaces, while a knowledge arbitration dynamically balances the teacher contributions guided by model confidence. Our evaluation on two representative applications demonstrates that RETROFIT consistently mitigates forgetting while maintaining adaptability. In malware detection under temporal drift, it substantially improves the retention score, from 20.2% to 38.6% over CL baselines, and exceeds the oracle upper bound on new data. In binary summarization across decompilation levels, where analyzing stripped binaries is especially challenging, RETROFIT achieves around twice the BLEU score of transfer learning used in prior work and surpasses all baselines in cross-representation generalization.

</details>


### [64] [Epistemic Error Decomposition for Multi-step Time Series Forecasting: Rethinking Bias-Variance in Recursive and Direct Strategies](https://arxiv.org/abs/2511.11461)
*Riku Green,Huw Day,Zahraa S. Abdallah,Telmo M. Silva Filho*

Main category: cs.LG

TL;DR: 本文重新审视了多步预测中递归策略和直接策略的传统偏见-方差权衡观点，通过理论分析发现非线性模型中递归策略可能同时具有更低偏差和更高方差，并通过实验验证了这一发现。


<details>
  <summary>Details</summary>
Motivation: 传统观点认为递归策略具有高偏差低方差，直接策略具有低偏差高方差。本文旨在重新检验这一经验法则，为多步预测策略选择提供更准确的理论指导。

Method: 将多步预测误差分解为不可约噪声、结构近似差距和估计方差三个部分。对于线性预测器，证明结构差距为零；对于非线性预测器，分析递归组合如何影响模型表达能力。还推导了递归策略估计方差的雅可比放大因子表达式。

Result: 理论分析表明，非线性模型中递归策略可能同时具有更低偏差和更高方差。在ETTm1数据集上的多层感知机实验证实了这一发现。

Conclusion: 选择递归或直接策略应基于模型非线性和噪声特性，而非传统的偏见-方差直觉，这为多步预测策略选择提供了更实用的指导。

Abstract: Multi-step forecasting is often described through a simple rule of thumb: recursive strategies are said to have high bias and low variance, while direct strategies are said to have low bias and high variance. We revisit this belief by decomposing the expected multi-step forecast error into three parts: irreducible noise, a structural approximation gap, and an estimation-variance term. For linear predictors we show that the structural gap is identically zero for any dataset. For nonlinear predictors, however, the repeated composition used in recursion can increase model expressivity, making the structural gap depend on both the model and the data. We further show that the estimation variance of the recursive strategy at any horizon can be written as the one-step variance multiplied by a Jacobian-based amplification factor that measures how sensitive the composed predictor is to parameter error. This perspective explains when recursive forecasting may simultaneously have lower bias and higher variance than direct forecasting. Experiments with multilayer perceptrons on the ETTm1 dataset confirm these findings. The results offer practical guidance for choosing between recursive and direct strategies based on model nonlinearity and noise characteristics, rather than relying on traditional bias-variance intuition.

</details>


### [65] [Toward Multi-Fidelity Machine Learning Force Field for Cathode Materials](https://arxiv.org/abs/2511.11361)
*Guangyi Dong,Zhihui Wang*

Main category: cs.LG

TL;DR: 本文开发了一个多保真度机器学习力场框架，用于提高锂离子电池正极材料计算的数据效率，能够同时利用低保真度非磁性和高保真度磁性计算数据集进行训练。


<details>
  <summary>Details</summary>
Motivation: 目前机器学习力场在锂离子电池正极材料中的应用相对有限，主要由于正极材料复杂的电子结构特性和高质量计算数据集的稀缺。

Method: 开发多保真度机器学习力场框架，同时利用低保真度非磁性和高保真度高保真度磁性计算数据集进行训练。

Result: 在锂锰铁磷酸盐正极材料系统上的测试证明了这种多保真度方法的有效性。

Conclusion: 这项工作有助于以较低的训练数据集成本实现正极材料的高精度机器学习力场训练，并为机器学习力场在正极材料计算模拟中的应用提供了新视角。

Abstract: Machine learning force fields (MLFFs), which employ neural networks to map atomic structures to system energies, effectively combine the high accuracy of first-principles calculation with the computational efficiency of empirical force fields. They are widely used in computational materials simulations. However, the development and application of MLFFs for lithium-ion battery cathode materials remain relatively limited. This is primarily due to the complex electronic structure characteristics of cathode materials and the resulting scarcity of high-quality computational datasets available for force field training. In this work, we develop a multi-fidelity machine learning force field framework to enhance the data efficiency of computational results, which can simultaneously utilize both low-fidelity non-magnetic and high-fidelity magnetic computational datasets of cathode materials for training. Tests conducted on the lithium manganese iron phosphate (LMFP) cathode material system demonstrate the effectiveness of this multi-fidelity approach. This work helps to achieve high-accuracy MLFF training for cathode materials at a lower training dataset cost, and offers new perspectives for applying MLFFs to computational simulations of cathode materials.

</details>


### [66] [On-Device Fine-Tuning via Backprop-Free Zeroth-Order Optimization](https://arxiv.org/abs/2511.11362)
*Prabodh Katti,Sangwoo Park,Bipin Rajendran,Osvaldo Simeone*

Main category: cs.LG

TL;DR: 本文分析了在边缘设备内存限制下，使用内存高效零阶优化（MeZO）相比传统反向传播（BP）训练的优势，MeZO通过仅使用前向评估估计梯度，无需存储中间激活或优化器状态，从而支持更大模型的部署。


<details>
  <summary>Details</summary>
Motivation: 边缘AI系统需要在严格内存约束下适应不同任务，传统BP训练需要存储层激活和优化器状态，限制了可部署模型的最大规模。

Method: 使用内存高效零阶优化（MeZO），仅通过前向评估估计梯度，无需存储中间激活或优化器状态；提供理论分析比较BP和MeZO可容纳的相对模型大小，并进行数值验证。

Result: MeZO在设备内存约束下表现出精度优势，前提是有足够的微调时间；相比BP训练，MeZO能够支持显著更大的模型部署在片上内存中。

Conclusion: MeZO通过消除存储中间激活和优化器状态的需求，缓解了边缘设备内存瓶颈，使更大模型能够在内存受限环境中部署，尽管可能需要更长的微调时间。

Abstract: On-device fine-tuning is a critical capability for edge AI systems, which must support adaptation to different agentic tasks under stringent memory constraints. Conventional backpropagation (BP)-based training requires storing layer activations and optimizer states, a demand that can be only partially alleviated through checkpointing. In edge deployments in which the model weights must reside entirely in device memory, this overhead severely limits the maximum model size that can be deployed. Memory-efficient zeroth-order optimization (MeZO) alleviates this bottleneck by estimating gradients using forward evaluations alone, eliminating the need for storing intermediate activations or optimizer states. This enables significantly larger models to fit within on-chip memory, albeit at the cost of potentially longer fine-tuning wall-clock time. This paper first provides a theoretical estimate of the relative model sizes that can be accommodated under BP and MeZO training. We then numerically validate the analysis, demonstrating that MeZO exhibits accuracy advantages under on-device memory constraints, provided sufficient wall-clock time is available for fine-tuning.

</details>


### [67] [SPOT: Single-Shot Positioning via Trainable Near-Field Rainbow Beamforming](https://arxiv.org/abs/2511.11391)
*Yeyue Cai,Jianhua Mo,Meixia Tao*

Main category: cs.LG

TL;DR: 本文提出了一种端到端的深度学习方案，同时设计彩虹波束并估计用户位置，通过将相位器和真时延系数作为可训练变量来优化定位精度，相比现有方法显著降低了开销和定位误差。


<details>
  <summary>Details</summary>
Motivation: 相位-时间阵列（整合相位器和真时延）已成为宽带感知和定位中生成频率相关彩虹波束的成本效益架构，但现有方法在定位精度和开销方面仍有改进空间。

Method: 采用端到端深度学习方案，将相位器和真时延系数作为可训练变量来设计任务导向波束，然后通过轻量级全连接模块从用户反馈的最大量化接收功率及其对应子载波索引中恢复用户的角距坐标。

Result: 与现有分析和基于学习的方法相比，所提方法将开销降低了一个数量级，并持续提供更低的二维定位误差。

Conclusion: 该深度学习方案能够有效设计彩虹波束并实现高精度用户定位，在减少开销的同时显著提升了定位性能。

Abstract: Phase-time arrays, which integrate phase shifters (PSs) and true-time delays (TTDs), have emerged as a cost-effective architecture for generating frequency-dependent rainbow beams in wideband sensing and localization. This paper proposes an end-to-end deep learning-based scheme that simultaneously designs the rainbow beams and estimates user positions. Treating the PS and TTD coefficients as trainable variables allows the network to synthesize task-oriented beams that maximize localization accuracy. A lightweight fully connected module then recovers the user's angle-range coordinates from its feedback of the maximum quantized received power and its corresponding subcarrier index after a single downlink transmission. Compared with existing analytical and learning-based schemes, the proposed method reduces overhead by an order of magnitude and delivers consistently lower two-dimensional positioning error.

</details>


### [68] [Multicalibration yields better matchings](https://arxiv.org/abs/2511.11413)
*Riccardo Colini Baldeschi,Simone Di Gregorio,Simone Fioravanti,Federico Fusco,Ido Guy,Daniel Haimovich,Stefano Leonardi,Fridolin Linder,Lorenzo Perini,Matteo Russo,Niek Tax*

Main category: cs.LG

TL;DR: 该论文提出使用多校准方法来改进加权图中的最佳匹配问题，通过构建多校准预测器来补偿不完美预测器的误差，使其性能与在原始预测器上应用最佳决策规则相竞争。


<details>
  <summary>Details</summary>
Motivation: 在加权图中寻找最佳匹配时，通常只能获得基于上下文的随机权重预测。虽然贝叶斯最优预测器能提供最优解，但现实中预测器往往不完美，因此需要设计能补偿预测误差的决策规则。

Method: 提出使用多校准方法，要求预测器在受保护上下文集合的每个元素上无偏。给定匹配算法类C和任意边权重预测器γ，构建特定的多校准预测器γ̂。

Result: 基于γ̂输出选择的最佳匹配与在原始预测器γ上应用C类中最佳决策规则的性能具有竞争力。

Conclusion: 多校准方法能有效处理不完美预测器的问题，并提供样本复杂度界限来支持理论结果。

Abstract: Consider the problem of finding the best matching in a weighted graph where we only have access to predictions of the actual stochastic weights, based on an underlying context. If the predictor is the Bayes optimal one, then computing the best matching based on the predicted weights is optimal. However, in practice, this perfect information scenario is not realistic. Given an imperfect predictor, a suboptimal decision rule may compensate for the induced error and thus outperform the standard optimal rule.
  In this paper, we propose multicalibration as a way to address this problem. This fairness notion requires a predictor to be unbiased on each element of a family of protected sets of contexts. Given a class of matching algorithms $\mathcal C$ and any predictor $γ$ of the edge-weights, we show how to construct a specific multicalibrated predictor $\hat γ$, with the following property. Picking the best matching based on the output of $\hat γ$ is competitive with the best decision rule in $\mathcal C$ applied onto the original predictor $γ$. We complement this result by providing sample complexity bounds.

</details>


### [69] [Differentiation Strategies for Acoustic Inverse Problems: Admittance Estimation and Shape Optimization](https://arxiv.org/abs/2511.11415)
*Nikolas Borrel-Jensen,Josiah Bjorgaard*

Main category: cs.LG

TL;DR: 本文提出了一种实用的可微分编程方法用于声学逆问题，包括导纳估计和共振阻尼的形状优化。通过JAX-FEM的自动微分实现边界导纳的精确估计，并结合随机有限差分进行声学形状优化，显著减少了计算成本。


<details>
  <summary>Details</summary>
Motivation: 解决声学逆问题中传统方法需要手动推导伴随方程和计算成本高的问题，利用现代可微分软件栈实现快速优化工作流原型设计。

Method: 使用JAX-FEM进行自动微分实现边界导纳估计，结合PyTorch3D进行网格操作，采用随机有限差分方法进行声学形状优化，将物理驱动的边界优化与几何驱动的内部网格适配分离。

Result: 边界导纳估计达到3位数精度，形状优化在目标频率下实现48.1%的能量减少，相比标准有限差分方法减少了30倍FEM求解次数。

Conclusion: 现代可微分软件栈能够快速构建基于物理的逆问题优化工作流，自动微分适用于参数估计，有限差分与自动微分结合适用于几何设计。

Abstract: We demonstrate a practical differentiable programming approach for acoustic inverse problems through two applications: admittance estimation and shape optimization for resonance damping. First, we show that JAX-FEM's automatic differentiation (AD) enables direct gradient-based estimation of complex boundary admittance from sparse pressure measurements, achieving 3-digit precision without requiring manual derivation of adjoint equations. Second, we apply randomized finite differences to acoustic shape optimization, combining JAX-FEM for forward simulation with PyTorch3D for mesh manipulation through AD. By separating physics-driven boundary optimization from geometry-driven interior mesh adaptation, we achieve 48.1% energy reduction at target frequencies with 30-fold fewer FEM solutions compared to standard finite difference on the full mesh. This work showcases how modern differentiable software stacks enable rapid prototyping of optimization workflows for physics-based inverse problems, with automatic differentiation for parameter estimation and a combination of finite differences and AD for geometric design.

</details>


### [70] [Low-Bit, High-Fidelity: Optimal Transport Quantization for Flow Matching](https://arxiv.org/abs/2511.11418)
*Dara Varam,Diaa A. Abuhani,Imran Zualkernan,Raghad AlDamani,Lujain Khalil*

Main category: cs.LG

TL;DR: 本文研究Flow Matching生成模型的量化压缩，通过最优传输理论进行后训练量化，在2-3位精度下保持生成质量，优于传统量化方法。


<details>
  <summary>Details</summary>
Motivation: Flow Matching模型虽然训练高效且采样确定，但实际部署面临高精度参数需求挑战，需要有效的压缩方法。

Method: 采用基于最优传输的后训练量化方法，最小化量化权重与原始权重之间的2-Wasserstein距离，并与均匀、分段和对数量化方案进行系统比较。

Result: 在五个不同复杂度的基准数据集上，OT量化在2-3位精度下仍能保持视觉生成质量和潜在空间稳定性，而其他方法在此精度下失效。

Conclusion: OT量化是压缩FM生成模型用于边缘和嵌入式AI应用的原则性有效方法。

Abstract: Flow Matching (FM) generative models offer efficient simulation-free training and deterministic sampling, but their practical deployment is challenged by high-precision parameter requirements. We adapt optimal transport (OT)-based post-training quantization to FM models, minimizing the 2-Wasserstein distance between quantized and original weights, and systematically compare its effectiveness against uniform, piecewise, and logarithmic quantization schemes. Our theoretical analysis provides upper bounds on generative degradation under quantization, and empirical results across five benchmark datasets of varying complexity show that OT-based quantization preserves both visual generation quality and latent space stability down to 2-3 bits per parameter, where alternative methods fail. This establishes OT-based quantization as a principled, effective approach to compress FM generative models for edge and embedded AI applications.

</details>


### [71] [DiffPro: Joint Timestep and Layer-Wise Precision Optimization for Efficient Diffusion Inference](https://arxiv.org/abs/2511.11446)
*Farhana Amin,Sabiha Afroz,Kanchon Gharami,Mona Moghadampanah,Dimitrios S. Nikolopoulos*

Main category: cs.LG

TL;DR: DiffPro是一个后训练框架，通过联合优化时间步数和每层精度来加速扩散模型推理，无需重新训练即可实现6.25倍模型压缩、50%时间步减少和2.8倍推理加速。


<details>
  <summary>Details</summary>
Motivation: 扩散模型生成高质量图像但推理成本高昂，因为需要大量去噪步骤和繁重的矩阵运算。

Method: 结合三部分：流形感知敏感度度量分配权重比特、动态激活量化稳定跨时间步激活、基于师生漂移的预算时间步选择器。

Result: 在标准基准测试中实现Delta FID <= 10，模型压缩达6.25倍，时间步减少50%，推理速度提升2.8倍。

Conclusion: DiffPro将步数减少和精度规划统一为单一可部署计划，实现实时节能的扩散推理。

Abstract: Diffusion models produce high quality images but inference is costly due to many denoising steps and heavy matrix operations. We present DiffPro, a post-training, hardware-faithful framework that works with the exact integer kernels used in deployment and jointly tunes timesteps and per-layer precision in Diffusion Transformers (DiTs) to reduce latency and memory without any training. DiffPro combines three parts: a manifold-aware sensitivity metric to allocate weight bits, dynamic activation quantization to stabilize activations across timesteps, and a budgeted timestep selector guided by teacher-student drift. In experiments DiffPro achieves up to 6.25x model compression, fifty percent fewer timesteps, and 2.8x faster inference with Delta FID <= 10 on standard benchmarks, demonstrating practical efficiency gains. DiffPro unifies step reduction and precision planning into a single budgeted deployable plan for real-time energy-aware diffusion inference.

</details>


### [72] [FairReweighing: Density Estimation-Based Reweighing Framework for Improving Separation in Fair Regression](https://arxiv.org/abs/2511.11459)
*Xiaoyin Xi,Zhe Yu*

Main category: cs.LG

TL;DR: 本文提出了一种基于密度估计的FairReweighing预处理算法，用于解决回归任务中的公平性问题，通过互信息度量分离违规，并在合成和真实数据上验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: AI软件在公共部门和工业应用中广泛使用，但缺乏透明度引发了关于公平性的担忧。现有研究主要关注二元分类任务的公平性，而回归任务的公平性研究相对不足。

Method: 采用基于互信息的度量来评估分离违规，并扩展到分类和回归问题。提出基于密度估计的FairReweighing预处理算法，确保学习模型满足分离准则。

Result: 理论上证明在数据独立性假设下，FairReweighing算法能保证训练数据的分离。实证表明，在合成和真实数据上，该算法在提高分离性的同时保持高准确率，优于现有最先进的回归公平性解决方案。

Conclusion: FairReweighing算法有效解决了回归任务中的公平性问题，为AI软件在敏感应用中的公平部署提供了可行方案。

Abstract: There has been a prevalence of applying AI software in both high-stakes public-sector and industrial contexts. However, the lack of transparency has raised concerns about whether these data-informed AI software decisions secure fairness against people of all racial, gender, or age groups. Despite extensive research on emerging fairness-aware AI software, up to now most efforts to solve this issue have been dedicated to binary classification tasks. Fairness in regression is relatively underexplored. In this work, we adopted a mutual information-based metric to assess separation violations. The metric is also extended so that it can be directly applied to both classification and regression problems with both binary and continuous sensitive attributes. Inspired by the Reweighing algorithm in fair classification, we proposed a FairReweighing pre-processing algorithm based on density estimation to ensure that the learned model satisfies the separation criterion. Theoretically, we show that the proposed FairReweighing algorithm can guarantee separation in the training data under a data independence assumption. Empirically, on both synthetic and real-world data, we show that FairReweighing outperforms existing state-of-the-art regression fairness solutions in terms of improving separation while maintaining high accuracy.

</details>


### [73] [MoCap2Radar: A Spatiotemporal Transformer for Synthesizing Micro-Doppler Radar Signatures from Motion Capture](https://arxiv.org/abs/2511.11462)
*Kevin Chen,Kenneth W. Parker,Anish Arora*

Main category: cs.LG

TL;DR: 提出了一种基于纯机器学习的从运动捕捉数据合成雷达频谱图的方法，使用基于Transformer的模型将MoCap数据转换为多普勒雷达频谱图。


<details>
  <summary>Details</summary>
Motivation: 利用更丰富的运动捕捉数据来增强稀缺的雷达数据集，为更高级别的应用提供训练数据，同时相比基于物理的方法需要更少的计算资源。

Method: 将MoCap到频谱图的转换制定为窗口化序列到序列任务，使用基于Transformer的模型共同捕捉MoCap标记之间的空间关系和跨帧的时间动态。

Result: 实验表明该方法能够生成视觉和数量上合理的多普勒雷达频谱图，并具有良好的泛化能力。消融实验显示模型具备将多部位运动转换为多普勒特征的能力，并能理解人体不同部位之间的空间关系。

Conclusion: 这是使用Transformer进行时间序列信号处理的有趣示例，特别适用于边缘计算和物联网雷达，能够通过更丰富的MoCap数据增强稀缺的雷达数据集，且计算量远小于基于物理的方法。

Abstract: We present a pure machine learning process for synthesizing radar spectrograms from Motion-Capture (MoCap) data. We formulate MoCap-to-spectrogram translation as a windowed sequence-to-sequence task using a transformer-based model that jointly captures spatial relations among MoCap markers and temporal dynamics across frames. Real-world experiments show that the proposed approach produces visually and quantitatively plausible doppler radar spectrograms and achieves good generalizability. Ablation experiments show that the learned model includes both the ability to convert multi-part motion into doppler signatures and an understanding of the spatial relations between different parts of the human body.
  The result is an interesting example of using transformers for time-series signal processing. It is especially applicable to edge computing and Internet of Things (IoT) radars. It also suggests the ability to augment scarce radar datasets using more abundant MoCap data for training higher-level applications. Finally, it requires far less computation than physics-based methods for generating radar data.

</details>


### [74] [Quantifying and Improving Adaptivity in Conformal Prediction through Input Transformations](https://arxiv.org/abs/2511.11472)
*Sooyong Jang,Insup Lee*

Main category: cs.LG

TL;DR: 本文提出了一种新的自适应预测集评估方法，通过输入变换和均匀质量分箱来更准确地评估方法的适应性，并基于此提出了新的自适应预测集算法。


<details>
  <summary>Details</summary>
Motivation: 现有适应性评估方法存在分箱不平衡问题，导致覆盖率和集合大小估计不准确，需要更可靠的评估指标。

Method: 使用输入变换按难度排序样本，然后进行均匀质量分箱，提出两个新指标来评估适应性，并开发基于分组条件共形预测的新算法。

Result: 实验表明，新指标与期望的适应性属性相关性更强，新算法在图像分类和医疗任务上均优于现有方法。

Conclusion: 提出的分箱方法和评估指标能更准确评估预测集适应性，基于分组的自适应预测集算法在多个任务上表现优异。

Abstract: Conformal prediction constructs a set of labels instead of a single point prediction, while providing a probabilistic coverage guarantee. Beyond the coverage guarantee, adaptiveness to example difficulty is an important property. It means that the method should produce larger prediction sets for more difficult examples, and smaller ones for easier examples. Existing evaluation methods for adaptiveness typically analyze coverage rate violation or average set size across bins of examples grouped by difficulty. However, these approaches often suffer from imbalanced binning, which can lead to inaccurate estimates of coverage or set size. To address this issue, we propose a binning method that leverages input transformations to sort examples by difficulty, followed by uniform-mass binning. Building on this binning, we introduce two metrics to better evaluate adaptiveness. These metrics provide more reliable estimates of coverage rate violation and average set size due to balanced binning, leading to more accurate adaptivity assessment. Through experiments, we demonstrate that our proposed metric correlates more strongly with the desired adaptiveness property compared to existing ones. Furthermore, motivated by our findings, we propose a new adaptive prediction set algorithm that groups examples by estimated difficulty and applies group-conditional conformal prediction. This allows us to determine appropriate thresholds for each group. Experimental results on both (a) an Image Classification (ImageNet) (b) a medical task (visual acuity prediction) show that our method outperforms existing approaches according to the new metrics.

</details>


### [75] [FarSkip-Collective: Unhobbling Blocking Communication in Mixture of Experts Models](https://arxiv.org/abs/2511.11505)
*Yonatan Dukler,Guihong Li,Deval Shah,Vikram Appia,Emad Barsoum*

Main category: cs.LG

TL;DR: FarSkip-Collective通过修改现代模型架构，在分布式环境中实现计算与通信的重叠，解决了MoEs通信阻塞问题，并在16B到109B参数的大模型中保持与原模型相当的准确度。


<details>
  <summary>Details</summary>
Motivation: 在分布式环境中运行MoEs时，通信阻塞是主要效率障碍，需要找到方法在不牺牲模型能力的前提下实现计算与通信的重叠。

Method: 修改模型架构，跳过模型中的连接，通过自蒸馏等技术将16B到109B参数的大模型完全转换，实现通信重叠。

Result: 转换后的模型在广泛的下游评估中平均准确度与原开源版本相当，例如109B参数的Llama 4 Scout模型准确度差距在1%以内。

Conclusion: FarSkip-Collective方法成功实现了大模型架构修改，在保持准确度的同时通过计算与通信重叠加速了训练和推理过程。

Abstract: Blocking communication presents a major hurdle in running MoEs efficiently in distributed settings. To address this, we present FarSkip-Collective which modifies the architecture of modern models to enable overlapping of their computation with communication. Our approach modifies the architecture to skip connections in the model and it is unclear a priori whether the modified model architecture can remain as capable, especially for large state-of-the-art models and while modifying all of the model layers. We answer this question in the affirmative and fully convert a series of state-of-the-art models varying from 16B to 109B parameters to enable overlapping of their communication while achieving accuracy on par with their original open-source releases. For example, we convert Llama 4 Scout (109B) via self-distillation and achieve average accuracy within 1% of its instruction tuned release averaged across a wide range of downstream evaluations. In addition to demonstrating retained accuracy of the large modified models, we realize the benefits of FarSkip-Collective through optimized implementations that explicitly overlap communication with computation, accelerating both training and inference in existing frameworks.

</details>


### [76] [Generalizing Fair Clustering to Multiple Groups: Algorithms and Applications](https://arxiv.org/abs/2511.11539)
*Diptarka Chakraborty,Kushagra Chatterjee,Debarati Das,Tien-Long Nguyen*

Main category: cs.LG

TL;DR: 本文研究了多组公平聚类问题，将最接近公平聚类扩展到任意数量组的情况，证明了问题的NP难度，并提出了近似算法，同时改进了公平相关聚类和公平共识聚类的近似保证。


<details>
  <summary>Details</summary>
Motivation: 传统的聚类方法经常无法为受保护属性定义的边缘化群体提供公平表示，这是由于训练数据中的偏见造成的。现有研究仅限于两个组的情况，而实际数据通常包含多个受保护属性（如年龄、种族、性别等），因此需要研究多组公平聚类问题。

Method: 将最接近公平聚类问题推广到任意数量组的情况，证明该问题在组大小相等时也是NP难的，并提出近线性时间的近似算法来处理任意大小的多组情况。

Result: 提出了处理多组公平聚类的高效近似算法，改进了公平相关聚类问题的近似保证，并首次为多组公平共识聚类问题提供了近似算法。

Conclusion: 本研究成功解决了多组公平聚类问题，填补了现有研究的空白，为处理包含多个受保护属性的实际数据提供了有效的解决方案。

Abstract: Clustering is a fundamental task in machine learning and data analysis, but it frequently fails to provide fair representation for various marginalized communities defined by multiple protected attributes -- a shortcoming often caused by biases in the training data. As a result, there is a growing need to enhance the fairness of clustering outcomes, ideally by making minimal modifications, possibly as a post-processing step after conventional clustering. Recently, Chakraborty et al. [COLT'25] initiated the study of \emph{closest fair clustering}, though in a restricted scenario where data points belong to only two groups. In practice, however, data points are typically characterized by many groups, reflecting diverse protected attributes such as age, ethnicity, gender, etc.
  In this work, we generalize the study of the \emph{closest fair clustering} problem to settings with an arbitrary number (more than two) of groups. We begin by showing that the problem is NP-hard even when all groups are of equal size -- a stark contrast with the two-group case, for which an exact algorithm exists. Next, we propose near-linear time approximation algorithms that efficiently handle arbitrary-sized multiple groups, thereby answering an open question posed by Chakraborty et al. [COLT'25].
  Leveraging our closest fair clustering algorithms, we further achieve improved approximation guarantees for the \emph{fair correlation clustering} problem, advancing the state-of-the-art results established by Ahmadian et al. [AISTATS'20] and Ahmadi et al. [2020]. Additionally, we are the first to provide approximation algorithms for the \emph{fair consensus clustering} problem involving multiple (more than two) groups, thus addressing another open direction highlighted by Chakraborty et al. [COLT'25].

</details>


### [77] [Multistability of Self-Attention Dynamics in Transformers](https://arxiv.org/abs/2511.11553)
*Claudio Altafini*

Main category: cs.LG

TL;DR: 本文研究了自注意力机制中的连续时间多智能体模型，揭示了其与多智能体版Oja流的关系，并将单头自注意力系统的平衡点分为四类：共识、二分共识、聚类和多边形平衡点。


<details>
  <summary>Details</summary>
Motivation: 研究自注意力动态与Oja流之间的关系，以更好地理解transformer中注意力机制的计算特性。

Method: 将自注意力动态建模为连续时间多智能体系统，分析其与多智能体Oja流的关系，并对平衡点进行分类研究。

Result: 发现自注意力动态中存在四种平衡点类型，其中前三种类型通常共存多个渐近稳定平衡点。共识和二分共识平衡点总是与价值矩阵的特征向量对齐。

Conclusion: 自注意力动态与Oja流密切相关，其平衡点结构复杂多样，为理解transformer的注意力机制提供了新的理论视角。

Abstract: In machine learning, a self-attention dynamics is a continuous-time multiagent-like model of the attention mechanisms of transformers. In this paper we show that such dynamics is related to a multiagent version of the Oja flow, a dynamical system that computes the principal eigenvector of a matrix corresponding for transformers to the value matrix. We classify the equilibria of the ``single-head'' self-attention system into four classes: consensus, bipartite consensus, clustering and polygonal equilibria. Multiple asymptotically stable equilibria from the first three classes often coexist in the self-attention dynamics. Interestingly, equilibria from the first two classes are always aligned with the eigenvectors of the value matrix, often but not exclusively with the principal eigenvector.

</details>


### [78] [Optimizing Mixture of Block Attention](https://arxiv.org/abs/2511.11571)
*Guangxuan Xiao,Junxian Guo,Kasra Mazaheri,Song Han*

Main category: cs.LG

TL;DR: 本文分析了Mixture of Block Attention (MoBA)的性能机制，提出了改进路由准确性的方法（小分块和键卷积），并开发了高效的GPU实现FlashMoBA，使MoBA在保持性能的同时实现显著加速。


<details>
  <summary>Details</summary>
Motivation: MoBA虽然能有效降低长上下文处理的计算成本，但其性能机制不明确且缺乏高效GPU实现，阻碍了实际应用。本文旨在理解MoBA的设计原理并解决其实现效率问题。

Method: 首先建立统计模型分析MoBA机制，发现路由准确性是关键；然后提出使用小分块和键卷积来改进路由；最后开发硬件感知的CUDA内核FlashMoBA来高效实现小分块MoBA。

Result: 改进后的MoBA模型在性能上能与密集注意力基线相媲美，FlashMoBA相比FlashAttention-2在小分块情况下实现了最高14.7倍的加速。

Conclusion: 通过理论分析和硬件优化，本文成功提升了MoBA的性能和效率，使其成为实际可行的长上下文处理方案。

Abstract: Mixture of Block Attention (MoBA) (Lu et al., 2025) is a promising building block for efficiently processing long contexts in LLMs by enabling queries to sparsely attend to a small subset of key-value blocks, drastically reducing computational cost. However, the design principles governing MoBA's performance are poorly understood, and it lacks an efficient GPU implementation, hindering its practical adoption. In this paper, we first develop a statistical model to analyze MoBA's underlying mechanics. Our model reveals that performance critically depends on the router's ability to accurately distinguish relevant from irrelevant blocks based on query-key affinities. We derive a signal-to-noise ratio that formally connects architectural parameters to this retrieval accuracy. Guided by our analysis, we identify two key pathways for improvement: using smaller block sizes and applying a short convolution on keys to cluster relevant signals, which enhances routing accuracy. While theoretically better, small block sizes are inefficient on GPUs. To bridge this gap, we introduce FlashMoBA, a hardware-aware CUDA kernel that enables efficient MoBA execution even with the small block sizes our theory recommends. We validate our insights by training LLMs from scratch, showing that our improved MoBA models match the performance of dense attention baselines. FlashMoBA achieves up to 14.7x speedup over FlashAttention-2 for small blocks, making our theoretically-grounded improvements practical. Code is available at: https://github.com/mit-han-lab/flash-moba.

</details>
