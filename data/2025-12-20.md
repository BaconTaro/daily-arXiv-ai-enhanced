<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 4]
- [cs.LG](#cs.LG) [Total: 1]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [TOGGLE: Temporal Logic-Guided Large Language Model Compression for Edge](https://arxiv.org/abs/2512.16855)
*Khurram Khalil,Khaza Anuarul Hoque*

Main category: cs.AI

TL;DR: TOGGLE是一个新颖的LLM压缩框架，首次将形式化方法（信号时序逻辑）引入大语言模型压缩，在保持语言特性的同时实现高效压缩，无需重新训练或微调。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型压缩技术（如量化和剪枝）往往会损害关键的语言特性，并且缺乏保持模型行为的正式保证，这限制了LLM在资源受限的边缘设备上的部署。

Method: 提出TOGGLE框架，利用信号时序逻辑（STL）形式化指定和强制执行语言特性。采用STL鲁棒性引导的贝叶斯优化来系统探索层级的量化和剪枝配置，生成满足指定语言约束的压缩模型。

Result: 在四种LLM架构（GPT-2、DeepSeek-V2 7B、LLaMA 3 8B和Mistral 7B）上评估，实现了高达3.3倍的计算成本（FLOPs）减少和高达68.8%的模型大小减少，同时满足所有语言特性。

Conclusion: TOGGLE代表了形式化方法在LLM压缩中的首次集成，实现了高效、可验证的LLM在边缘硬件上的部署，为资源受限环境下的LLM应用提供了新的解决方案。

Abstract: Large Language Models (LLMs) deliver exceptional performance across natural language tasks but demand substantial computational resources, limiting their deployment on resource-constrained edge devices. Existing compression techniques, such as quantization and pruning, often degrade critical linguistic properties and lack formal guarantees for preserving model behavior. We propose Temporal Logic-Guided Large Language Model Compression (TOGGLE), a novel framework that leverages Signal Temporal Logic (STL) to formally specify and enforce linguistic properties during compression. TOGGLE employs an STL robustness-guided Bayesian optimization to systematically explore layer-wise quantization and pruning configurations, generating compressed models that formally satisfy specified linguistic constraints without retraining or fine-tuning. Evaluating TOGGLE on four LLM architectures (GPT-2, DeepSeek-V2 7B, LLaMA 3 8B, and Mistral 7B), we achieve up to 3.3x reduction in computational costs (FLOPs) and up to a 68.8% reduction in model size while satisfying all linguistic properties. TOGGLE represents the first integration of formal methods into LLM compression, enabling efficient, verifiable deployment of LLMs on edge hardware.

</details>


### [2] [Distributional AGI Safety](https://arxiv.org/abs/2512.16856)
*Nenad Tomašev,Matija Franklin,Julian Jacobs,Sébastien Krier,Simon Osindero*

Main category: cs.AI

TL;DR: 论文提出"拼凑式AGI"假说，认为通用智能可能首先通过多个子AGI智能体的协调合作实现，而非单一AGI系统。作者呼吁重视这一假说并开发相应的安全框架。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全和对齐研究主要关注单一AI系统的防护，假设最终会出现单一的通用人工智能。然而，另一种可能性是通用能力首先通过多个子AGI智能体的协调合作实现，这种"拼凑式AGI"假说需要得到重视，并开发相应的安全措施。

Method: 提出分布式AGI安全框架，超越对单个智能体的评估和对齐。该框架核心是设计和实施虚拟智能体沙盒经济（不可渗透或半渗透），其中智能体间的交易由强大的市场机制管理，配合适当的可审计性、声誉管理和监督，以减轻集体风险。

Result: 论文提出了一个应对拼凑式AGI风险的系统性框架，强调需要通过市场机制、审计、声誉管理和监督来管理多个智能体协调带来的集体风险。

Conclusion: 随着具有工具使用能力、能够通信和协调的高级AI智能体的快速部署，拼凑式AGI假说需要得到严肃考虑，并应指导相应安全措施和缓解策略的开发。分布式AGI安全框架为这一新兴风险提供了系统性的应对方案。

Abstract: AI safety and alignment research has predominantly been focused on methods for safeguarding individual AI systems, resting on the assumption of an eventual emergence of a monolithic Artificial General Intelligence (AGI). The alternative AGI emergence hypothesis, where general capability levels are first manifested through coordination in groups of sub-AGI individual agents with complementary skills and affordances, has received far less attention. Here we argue that this patchwork AGI hypothesis needs to be given serious consideration, and should inform the development of corresponding safeguards and mitigations. The rapid deployment of advanced AI agents with tool-use capabilities and the ability to communicate and coordinate makes this an urgent safety consideration. We therefore propose a framework for distributional AGI safety that moves beyond evaluating and aligning individual agents. This framework centers on the design and implementation of virtual agentic sandbox economies (impermeable or semi-permeable), where agent-to-agent transactions are governed by robust market mechanisms, coupled with appropriate auditability, reputation management, and oversight to mitigate collective risks.

</details>


### [3] [The Social Responsibility Stack: A Control-Theoretic Architecture for Governing Socio-Technical AI](https://arxiv.org/abs/2512.16873)
*Otman A. Basir*

Main category: cs.AI

TL;DR: 该论文提出了社会责任感堆栈（SRS）框架，这是一个六层架构，将社会价值观嵌入AI系统作为显式约束、保障措施、行为接口、审计机制和治理流程，旨在为AI系统提供可执行的工程机制。


<details>
  <summary>Details</summary>
Motivation: 当前负责任AI和治理工作提供了重要的规范性原则，但往往缺乏在整个系统生命周期中运行的可执行工程机制。AI系统越来越多地部署在影响人类行为、机构决策和社会结果的领域，需要将社会价值观实际嵌入系统设计。

Method: 提出社会责任感堆栈（SRS）框架，将责任建模为社会技术系统的闭环监督控制问题。该框架包含六层架构，整合设计时保障措施与运行时监控和机构监督。开发了统一的基于约束的表述，引入了安全包络和反馈解释，展示了如何持续监控和执行公平性、自主性、认知负担和解释质量。

Result: 通过临床决策支持、协作式自动驾驶汽车和公共部门系统的案例研究，展示了SRS如何将规范性目标转化为可操作的工程和运营控制。该框架连接了伦理学、控制理论和AI治理。

Conclusion: SRS框架为负责任、适应性强且可审计的社会技术AI系统提供了实用基础，弥合了伦理原则与工程实施之间的差距，使AI系统能够在整个生命周期中持续监控和执行社会价值观。

Abstract: Artificial intelligence systems are increasingly deployed in domains that shape human behaviour, institutional decision-making, and societal outcomes. Existing responsible AI and governance efforts provide important normative principles but often lack enforceable engineering mechanisms that operate throughout the system lifecycle. This paper introduces the Social Responsibility Stack (SRS), a six-layer architectural framework that embeds societal values into AI systems as explicit constraints, safeguards, behavioural interfaces, auditing mechanisms, and governance processes. SRS models responsibility as a closed-loop supervisory control problem over socio-technical systems, integrating design-time safeguards with runtime monitoring and institutional oversight. We develop a unified constraint-based formulation, introduce safety-envelope and feedback interpretations, and show how fairness, autonomy, cognitive burden, and explanation quality can be continuously monitored and enforced. Case studies in clinical decision support, cooperative autonomous vehicles, and public-sector systems illustrate how SRS translates normative objectives into actionable engineering and operational controls. The framework bridges ethics, control theory, and AI governance, providing a practical foundation for accountable, adaptive, and auditable socio-technical AI systems.

</details>


### [4] [Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning](https://arxiv.org/abs/2512.16917)
*Qihao Liu,Luoxin Ye,Wufei Ma,Yu-Cheng Chou,Alan Yuille*

Main category: cs.AI

TL;DR: 提出Generative Adversarial Reasoner框架，通过对抗强化学习联合训练推理模型和判别器，提升LLM数学推理能力


<details>
  <summary>Details</summary>
Motivation: 尽管具备显式推理能力的大语言模型在数学推理方面表现出色，但仍存在过程错误，如计算错误、逻辑脆弱和表面合理但无效的推理步骤。需要改进推理质量。

Method: 提出Generative Adversarial Reasoner框架：1) 使用计算高效的审查计划将推理链划分为逻辑完整的切片；2) 判别器评估每个切片的合理性并提供结构化理由；3) 通过对抗强化学习联合训练推理模型和判别器，推理模型因逻辑一致且得出正确答案而获得奖励，判别器因正确检测错误而获得奖励。

Result: 在多个数学基准测试中取得一致提升：在AIME24上，DeepSeek-R1-Distill-Qwen-7B从54.0提升到61.3(+7.3)，DeepSeek-R1-Distill-Llama-8B从43.7提升到53.7(+10.0)。模块化判别器支持灵活的奖励塑造。

Conclusion: Generative Adversarial Reasoner框架通过对抗强化学习产生密集、校准良好的步骤级奖励，补充稀疏的精确匹配信号，改善了信用分配，提高了样本效率，增强了LLM的整体推理质量。

Abstract: Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice's soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [5] [Training Together, Diagnosing Better: Federated Learning for Collagen VI-Related Dystrophies](https://arxiv.org/abs/2512.16876)
*Astrid Brull,Sara Aguti,Véronique Bolduc,Ying Hu,Daniel M. Jimenez-Gutierrez,Enrique Zuazua,Joaquin Del-Rio,Oleksii Sliusarenko,Haiyan Zhou,Francesco Muntoni,Carsten G. Bönnemann,Xabi Uribe-Etxebarria*

Main category: cs.LG

TL;DR: 利用联邦学习平台在分散的国际数据集上训练机器学习模型，用于胶原VI相关肌营养不良症的诊断，相比单机构模型显著提升了诊断性能。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断面临数据稀缺和分散的挑战，跨机构数据共享存在隐私、监管和物流障碍。联邦学习提供了一种在保护患者隐私的前提下，利用分散数据进行协作模型训练的可能解决方案。

Method: 采用Sherpa.ai联邦学习平台，在两个国际组织的分布式数据集上进行协作训练。使用患者来源成纤维细胞培养物的胶原VI免疫荧光显微镜图像，训练机器学习模型对COL6-RD的三种主要致病机制进行分类。

Result: 联邦学习模型实现了0.82的F1分数，显著优于单机构模型（0.57-0.75）。该模型能够将胶原VI患者图像分类为外显子跳跃、甘氨酸替代和假外显子插入三种致病机制组。

Conclusion: 联邦学习相比孤立机构模型显著提高了诊断效用和泛化能力。该方法不仅支持更准确的诊断，还有助于解释意义不确定的变异，并指导测序策略的优先排序以识别新的致病变异。

Abstract: The application of Machine Learning (ML) to the diagnosis of rare diseases, such as collagen VI-related dystrophies (COL6-RD), is fundamentally limited by the scarcity and fragmentation of available data. Attempts to expand sampling across hospitals, institutions, or countries with differing regulations face severe privacy, regulatory, and logistical obstacles that are often difficult to overcome. The Federated Learning (FL) provides a promising solution by enabling collaborative model training across decentralized datasets while keeping patient data local and private. Here, we report a novel global FL initiative using the Sherpa.ai FL platform, which leverages FL across distributed datasets in two international organizations for the diagnosis of COL6-RD, using collagen VI immunofluorescence microscopy images from patient-derived fibroblast cultures. Our solution resulted in an ML model capable of classifying collagen VI patient images into the three primary pathogenic mechanism groups associated with COL6-RD: exon skipping, glycine substitution, and pseudoexon insertion. This new approach achieved an F1-score of 0.82, outperforming single-organization models (0.57-0.75). These results demonstrate that FL substantially improves diagnostic utility and generalizability compared to isolated institutional models. Beyond enabling more accurate diagnosis, we anticipate that this approach will support the interpretation of variants of uncertain significance and guide the prioritization of sequencing strategies to identify novel pathogenic variants.

</details>
