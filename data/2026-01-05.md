<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 16]
- [cs.HC](#cs.HC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 41]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Reasoning in Action: MCTS-Driven Knowledge Retrieval for Large Language Models](https://arxiv.org/abs/2601.00003)
*Shuqi Liu,Bowei He,Chen Ma,Linqi Song*

Main category: cs.AI

TL;DR: 提出了一种推理感知的知识检索方法，通过粗到细的两阶段检索策略，结合蒙特卡洛树搜索，为LLMs提供与对话逻辑结构对齐的知识，超越表层语义相似性。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs通常通过检索语义相似信息或提升推理能力来增强性能，但如何有效整合检索和推理策略以优化LLM性能仍是一个重要挑战。需要超越表层语义相似性，提供与对话逻辑结构对齐的知识。

Method: 采用粗到细的两阶段知识检索方法：1) 首先识别知识库中与上下文相关的子区域，确保所有句子都与主题相关；2) 在该子区域内细化搜索，提取与推理过程特别相关的知识。两阶段都使用蒙特卡洛树搜索启发的方法，通过常见关键词在知识句子中有效导航。

Result: 在两个多轮对话数据集上的实验表明，该方法不仅更紧密地对齐人类对话中的底层推理，还显著提高了检索知识的多样性，从而产生更具信息性和创造性的响应。

Conclusion: 提出的推理感知知识检索方法能够有效整合检索和推理策略，通过提供与对话逻辑结构对齐的知识，显著提升LLMs在多轮对话中的性能，生成更丰富和创新的响应。

Abstract: Large language models (LLMs) typically enhance their performance through either the retrieval of semantically similar information or the improvement of their reasoning capabilities. However, a significant challenge remains in effectively integrating both retrieval and reasoning strategies to optimize LLM performance. In this paper, we introduce a reasoning-aware knowledge retrieval method that enriches LLMs with information aligned to the logical structure of conversations, moving beyond surface-level semantic similarity. We follow a coarse-to-fine approach for knowledge retrieval. First, we identify a contextually relevant sub-region of the knowledge base, ensuring that all sentences within it are relevant to the context topic. Next, we refine our search within this sub-region to extract knowledge that is specifically relevant to the reasoning process. Throughout both phases, we employ the Monte Carlo Tree Search-inspired search method to effectively navigate through knowledge sentences using common keywords. Experiments on two multi-turn dialogue datasets demonstrate that our knowledge retrieval approach not only aligns more closely with the underlying reasoning in human conversations but also significantly enhances the diversity of the retrieved knowledge, resulting in more informative and creative responses.

</details>


### [2] [Finetuning Large Language Models for Automated Depression Screening in Nigerian Pidgin English: GENSCORE Pilot Study](https://arxiv.org/abs/2601.00004)
*Isaac Iyinoluwa Olufadewa,Miracle Ayomikun Adesina,Ezekiel Ayodeji Oladejo,Uthman Babatunde Usman,Owen Kolade Adeniyi,Matthew Tolulope Olawoyin*

Main category: cs.AI

TL;DR: 使用微调大语言模型进行尼日利亚皮钦语自动抑郁症筛查，GPT-4.1在PHQ-9严重程度评分预测中达到94.5%准确率


<details>
  <summary>Details</summary>
Motivation: 尼日利亚抑郁症筛查覆盖率低，传统工具如PHQ-9在高收入国家验证但可能不适合尼日利亚的皮钦语和多语言环境，需要开发适应本地语言和文化的筛查工具

Method: 收集432个尼日利亚年轻人皮钦语音频回答，进行转录、预处理和标注（语义标签、俚语解释、PHQ-9严重程度评分），微调三个LLM模型（Phi-3-mini-4k-instruct、Gemma-3-4B-it、GPT-4.1），评估定量（准确性、精确度、语义对齐）和定性（清晰度、相关性、文化适当性）性能

Result: GPT-4.1表现最佳，PHQ-9严重程度评分预测准确率达94.5%，优于其他模型，定性评估也显示其产生最文化适当、清晰和上下文相关的回答

Conclusion: AI介导的抑郁症筛查可为尼日利亚服务不足社区提供解决方案，为在语言多样、资源有限环境中部署对话式心理健康工具奠定基础

Abstract: Depression is a major contributor to the mental-health burden in Nigeria, yet screening coverage remains limited due to low access to clinicians, stigma, and language barriers. Traditional tools like the Patient Health Questionnaire-9 (PHQ-9) were validated in high-income countries but may be linguistically or culturally inaccessible for low- and middle-income countries and communities such as Nigeria where people communicate in Nigerian Pidgin and more than 520 local languages. This study presents a novel approach to automated depression screening using fine-tuned large language models (LLMs) adapted for conversational Nigerian Pidgin. We collected a dataset of 432 Pidgin-language audio responses from Nigerian young adults aged 18-40 to prompts assessing psychological experiences aligned with PHQ-9 items, performed transcription, rigorous preprocessing and annotation, including semantic labeling, slang and idiom interpretation, and PHQ-9 severity scoring. Three LLMs - Phi-3-mini-4k-instruct, Gemma-3-4B-it, and GPT-4.1 - were fine-tuned on this annotated dataset, and their performance was evaluated quantitatively (accuracy, precision and semantic alignment) and qualitatively (clarity, relevance, and cultural appropriateness). GPT-4.1 achieved the highest quantitative performance, with 94.5% accuracy in PHQ-9 severity scoring prediction, outperforming Gemma-3-4B-it and Phi-3-mini-4k-instruct. Qualitatively, GPT-4.1 also produced the most culturally appropriate, clear, and contextually relevant responses. AI-mediated depression screening for underserved Nigerian communities. This work provides a foundation for deploying conversational mental-health tools in linguistically diverse, resource-constrained environments.

</details>


### [3] [A multi-algorithm approach for operational human resources workload balancing in a last mile urban delivery system](https://arxiv.org/abs/2601.00023)
*Luis M. Moreno-Saavedra,Silvia Jimenez-Fernandez,Antonio Portilla-Figueras,David Casillas-Perez,Sancho Salcedo-Sanz*

Main category: cs.AI

TL;DR: 本文提出一种多算法方法来解决最后一公里包裹配送中的工作量平衡问题，通过结合距离和工作量考虑来优化包裹分配，确保每位配送员完成相似的工作量。


<details>
  <summary>Details</summary>
Motivation: 传统基于地理邻近性的包裹分配方法效率低下，导致配送员之间工作量分布不平衡。需要优化系统以改善配送时间，实现所有员工之间的完整工作量平衡。

Method: 提出多算法方法，包括不同版本的k-means、进化方法、基于k-means初始化的递归分配（采用不同问题编码）以及混合进化集成算法。算法同时考虑配送点距离和配送员位置。

Result: 在西班牙Azuqueca de Henares城市最后一公里包裹配送的实际问题中验证了所提方法的性能。

Conclusion: 通过多算法方法有效解决了最后一公里配送中的工作量平衡问题，能够纠正配送员之间的显著工作量不平衡，确保每位员工完成相似的工作量。

Abstract: Efficient workload assignment to the workforce is critical in last-mile package delivery systems. In this context, traditional methods of assigning package deliveries to workers based on geographical proximity can be inefficient and surely guide to an unbalanced workload distribution among delivery workers. In this paper, we look at the problem of operational human resources workload balancing in last-mile urban package delivery systems. The idea is to consider the effort workload to optimize the system, i.e., the optimization process is now focused on improving the delivery time, so that the workload balancing is complete among all the staff. This process should correct significant decompensations in workload among delivery workers in a given zone. Specifically, we propose a multi-algorithm approach to tackle this problem. The proposed approach takes as input a set of delivery points and a defined number of workers, and then assigns packages to workers, in such a way that it ensures that each worker completes a similar amount of work per day. The proposed algorithms use a combination of distance and workload considerations to optimize the allocation of packages to workers. In this sense, the distance between the delivery points and the location of each worker is also taken into account. The proposed multi-algorithm methodology includes different versions of k-means, evolutionary approaches, recursive assignments based on k-means initialization with different problem encodings, and a hybrid evolutionary ensemble algorithm. We have illustrated the performance of the proposed approach in a real-world problem in an urban last-mile package delivery workforce operating at Azuqueca de Henares, Spain.

</details>


### [4] [Quantitative Rule-Based Strategy modeling in Classic Indian Rummy: A Metric Optimization Approach](https://arxiv.org/abs/2601.00024)
*Purushottam Saha,Avirup Chakraborty,Sourish Sarkar,Subhamoy Maitra,Diganta Mukherjee,Tridib Mukherjee*

Main category: cs.AI

TL;DR: 本文提出了一种基于规则的战略框架，使用新的手牌评估指标MinDist来改进13张牌印度拉米游戏的人工智能策略，显著提升了胜率。


<details>
  <summary>Details</summary>
Motivation: 13张牌印度拉米是一种不完全信息顺序游戏，需要概率推理和组合决策。传统启发式方法效果有限，需要更形式化和可解释的策略设计方法。

Method: 提出了基于规则的战略框架，引入新的手牌评估指标MinDist（修改MinScore指标，量化手牌与最近有效配置的编辑距离）。设计了计算高效的算法，利用动态剪枝和模式缓存精确计算该指标。在双人零和模拟框架中整合对手手牌建模，并使用统计假设检验评估策略。

Result: 实证结果显示，基于MinDist的智能体相比传统启发式方法在胜率上有显著提升，为算法化拉米策略设计提供了形式化和可解释的步骤。

Conclusion: MinDist指标能够有效捕捉手牌完成的结构接近度，基于该指标的算法框架在印度拉米游戏中表现出优越性能，为不完全信息顺序游戏的策略设计提供了新思路。

Abstract: The 13-card variant of Classic Indian Rummy is a sequential game of incomplete information that requires probabilistic reasoning and combinatorial decision-making. This paper proposes a rule-based framework for strategic play, driven by a new hand-evaluation metric termed MinDist. The metric modifies the MinScore metric by quantifying the edit distance between a hand and the nearest valid configuration, thereby capturing structural proximity to completion. We design a computationally efficient algorithm derived from the MinScore algorithm, leveraging dynamic pruning and pattern caching to exactly calculate this metric during play. Opponent hand-modeling is also incorporated within a two-player zero-sum simulation framework, and the resulting strategies are evaluated using statistical hypothesis testing. Empirical results show significant improvement in win rates for MinDist-based agents over traditional heuristics, providing a formal and interpretable step toward algorithmic Rummy strategy design.

</details>


### [5] [From Clay to Code: Typological and Material Reasoning in AI Interpretations of Iranian Pigeon Towers](https://arxiv.org/abs/2601.00029)
*Abolhassan Pishahang,Maryam Badiei*

Main category: cs.AI

TL;DR: 研究探索生成式AI如何理解乡土建筑中的设计智慧，以伊朗鸽塔为例，测试三种扩散模型在不同提示阶段的表现，发现AI能重现几何模式但误解材料和气候逻辑。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索生成式AI系统如何解读乡土建筑形式中蕴含的建筑智慧，理解AI在感知、扭曲和重新想象传统设计智能方面的能力边界。

Method: 以伊朗鸽塔为案例研究，测试Midjourney v6、DALL-E 3和基于Stable Diffusion XL的DreamStudio三种扩散模型，采用参考性、适应性和推测性三个提示阶段，通过五标准评估框架（类型学、材料性、环境、真实性和文化特异性）进行分析。

Result: AI能可靠地复制几何图案，但误解材料和气候逻辑；参考图像能提高真实性但限制创造性，而无参考的自由生成能产生创新但文化模糊的结果；研究界定了视觉相似性与建筑推理之间的边界。

Conclusion: 研究提出了计算乡土推理框架，用于分析AI如何感知、扭曲和重新想象传统设计智能，揭示了AI在建筑理解方面的局限性和潜力。

Abstract: This study investigates how generative AI systems interpret the architectural intelligence embedded in vernacular form. Using the Iranian pigeon tower as a case study, the research tests three diffusion models, Midjourney v6, DALL-E 3, and DreamStudio based on Stable Diffusion XL (SDXL), across three prompt stages: referential, adaptive, and speculative. A five-criteria evaluation framework assesses how each system reconstructs typology, materiality, environment, realism, and cultural specificity. Results show that AI reliably reproduces geometric patterns but misreads material and climatic reasoning. Reference imagery improves realism yet limits creativity, while freedom from reference generates inventive but culturally ambiguous outcomes. The findings define a boundary between visual resemblance and architectural reasoning, positioning computational vernacular reasoning as a framework for analyzing how AI perceives, distorts, and reimagines traditional design intelligence.

</details>


### [6] [The Agentic Leash: Extracting Causal Feedback Fuzzy Cognitive Maps with LLMs](https://arxiv.org/abs/2601.00097)
*Akash Kumar Panda,Olaoluwa Adigun,Bart Kosko*

Main category: cs.AI

TL;DR: 本文设计了一个LLM智能体，能够从原始文本中提取因果反馈模糊认知图（FCM），并通过双向交互使FCM动态系统获得一定自主性，同时保持可控性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是开发能够从文本中自动提取因果关系的智能系统，使模糊认知图能够动态演化并具有一定自主性，同时保持人类可控的"智能体牵引"。

Method: 方法包括：1）设计三阶段精细调校的系统指令指导LLM智能体；2）从文本中提取关键名词和名词短语；3）从这些短语中识别FCM概念节点；4）推断节点间的部分或模糊因果边；5）测试系统从基辛格关于AI前景的论文中提取FCM。

Result: 结果显示：1）三阶段过程生成的FCM动态系统收敛到与人工生成FCM相同的平衡极限环；2）混合Gemini和ChatGPT LLM智能体生成的FCM不仅吸收了主要混合成分的平衡点，还创造了新的平衡点，更好地近似底层因果动态系统。

Conclusion: 结论表明LLM智能体能够有效从文本中提取因果FCM，生成的动态系统具有与人工生成系统相似的平衡行为，混合不同LLM生成的FCM能够产生新的平衡点，增强对底层因果系统的近似能力。

Abstract: We design a large-language-model (LLM) agent that extracts causal feedback fuzzy cognitive maps (FCMs) from raw text. The causal learning or extraction process is agentic both because of the LLM's semi-autonomy and because ultimately the FCM dynamical system's equilibria drive the LLM agents to fetch and process causal text. The fetched text can in principle modify the adaptive FCM causal structure and so modify the source of its quasi-autonomy--its equilibrium limit cycles and fixed-point attractors. This bidirectional process endows the evolving FCM dynamical system with a degree of autonomy while still staying on its agentic leash. We show in particular that a sequence of three finely tuned system instructions guide an LLM agent as it systematically extracts key nouns and noun phrases from text, as it extracts FCM concept nodes from among those nouns and noun phrases, and then as it extracts or infers partial or fuzzy causal edges between those FCM nodes. We test this FCM generation on a recent essay about the promise of AI from the late diplomat and political theorist Henry Kissinger and his colleagues. This three-step process produced FCM dynamical systems that converged to the same equilibrium limit cycles as did the human-generated FCMs even though the human-generated FCM differed in the number of nodes and edges. A final FCM mixed generated FCMs from separate Gemini and ChatGPT LLM agents. The mixed FCM absorbed the equilibria of its dominant mixture component but also created new equilibria of its own to better approximate the underlying causal dynamical system.

</details>


### [7] [Mortar: Evolving Mechanics for Automatic Game Design](https://arxiv.org/abs/2601.00105)
*Muhammad U. Nasir,Yuchen Li,Steven James,Julian Togelius*

Main category: cs.AI

TL;DR: Mortar系统结合质量多样性算法和大语言模型，自动演化游戏机制，通过合成完整游戏并评估技能排序能力来优化机制设计。


<details>
  <summary>Details</summary>
Motivation: 游戏机制定义了游戏玩法的规则和交互，手动设计过程耗时且依赖专家经验。需要自动化方法来探索多样化的游戏机制，降低设计门槛。

Method: 结合质量多样性算法和大语言模型探索多样化机制；通过树搜索程序合成完整游戏；评估机制对技能排序的贡献度（强玩家是否始终优于弱玩家）。

Result: Mortar能生成多样化且可玩的游戏，产生的机制在游戏中能更好地促进技能排序；消融研究和用户研究验证了系统组件的有效性。

Conclusion: Mortar系统成功实现了游戏机制的自主演化，通过自动化方法生成多样化、可玩且具有良好技能排序特性的游戏机制。

Abstract: We present Mortar, a system for autonomously evolving game mechanics for automatic game design. Game mechanics define the rules and interactions that govern gameplay, and designing them manually is a time-consuming and expert-driven process. Mortar combines a quality-diversity algorithm with a large language model to explore a diverse set of mechanics, which are evaluated by synthesising complete games that incorporate both evolved mechanics and those drawn from an archive. The mechanics are evaluated by composing complete games through a tree search procedure, where the resulting games are evaluated by their ability to preserve a skill-based ordering over players -- that is, whether stronger players consistently outperform weaker ones. We assess the mechanics based on their contribution towards the skill-based ordering score in the game. We demonstrate that Mortar produces games that appear diverse and playable, and mechanics that contribute more towards the skill-based ordering score in the game. We perform ablation studies to assess the role of each system component and a user study to evaluate the games based on human feedback.

</details>


### [8] [Explicit Abstention Knobs for Predictable Reliability in Video Question Answering](https://arxiv.org/abs/2601.00138)
*Jorge Ortiz*

Main category: cs.AI

TL;DR: 该研究探讨了在视频问答任务中，基于置信度的选择性预测能否有效控制错误率，以及在分布偏移下这种控制是否稳健。研究发现置信度阈值在分布内能提供机制性控制，但在分布偏移下控制会失效。


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型在高风险部署中需要选择性预测能力，即在不确定时选择弃权而非冒险产生代价高昂的错误。研究旨在验证基于置信度的弃权机制能否在视频问答任务中可靠控制错误率，以及这种控制在分布偏移下是否保持稳健。

Method: 使用NExT-QA数据集和Gemini 2.0 Flash模型进行研究。通过扫描置信度阈值epsilon来生成风险-覆盖权衡曲线，分析置信度阈值对错误率的控制效果。研究考察了在分布内和分布偏移两种情况下的表现。

Result: 研究发现：1) 在分布内，置信度阈值能提供机制性控制，通过调整阈值可以平滑地权衡风险与覆盖范围，有效降低错误率；2) 在分布偏移下，这种控制会失效，置信度阈值无法可靠控制错误率。

Conclusion: 虽然置信度阈值在分布内能有效控制视频问答的错误率，但在面对分布偏移时控制会失效。这表明需要开发更稳健的选择性预测方法，以确保视觉语言模型在高风险部署中的可靠性。

Abstract: High-stakes deployment of vision-language models (VLMs) requires selective prediction, where systems abstain when uncertain rather than risk costly errors. We investigate whether confidence-based abstention provides reliable control over error rates in video question answering, and whether that control remains robust under distribution shift. Using NExT-QA and Gemini 2.0 Flash, we establish two findings. First, confidence thresholding provides mechanistic control in-distribution. Sweeping threshold epsilon produces smooth risk-coverage tradeoffs, reducing error rates f

</details>


### [9] [Will LLM-powered Agents Bias Against Humans? Exploring the Belief-Dependent Vulnerability](https://arxiv.org/abs/2601.00240)
*Zongwei Wang,Bincheng Gu,Hongyu Yu,Junliang Yu,Tao He,Jiayin Feng,Min Gao*

Main category: cs.AI

TL;DR: 研究发现LLM赋能的智能体不仅存在人口统计学偏见，还会在"我们vs他们"的群体线索下表现出群体间偏见。当这种群体边界与智能体-人类划分重合时，人类可能被智能体视为外群体。研究还提出了信念中毒攻击（BPA）来抑制有利于人类的规范脚本，并讨论了防御策略。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探究LLM赋能的智能体是否会在最小群体线索下表现出群体间偏见，特别是当这种偏见将人类整体视为外群体时的风险。研究者关注智能体-人类划分可能带来的根本性群体不对称问题。

Method: 通过构建受控的多智能体社会模拟，基于明确收益权衡下的分配决策来检验群体偏见。提出了信念中毒攻击（BPA），包括初始化时的档案中毒（BPA-PP）和通过优化信念精炼后缀注入存储反思中的记忆中毒（BPA-MP）。

Result: 实验表明智能体在最小群体线索下确实表现出一致的群体间偏见。虽然当某些对应方被设定为人类时这种偏见会减弱，但这种减弱依赖于智能体相信真实人类存在的信念。BPA攻击能够有效抑制有利于人类的规范脚本，重新激活对人类的群体偏见。

Conclusion: 研究揭示了LLM赋能智能体存在将人类视为外群体的风险，并展示了通过信念中毒攻击利用这种脆弱性的可能性。研究目的是为了促进更安全的智能体设计，提出了在档案和记忆边界进行干预的实用缓解策略。

Abstract: LLM-empowered agents can exhibit not only demographic bias (e.g., gender, religion) but also intergroup bias triggered by minimal "us" versus "them" cues. When this intergroup boundary aligns with an agent-human divide, the risk shifts from disparities among human demographic groups to a more fundamental group-level asymmetry, i.e., humans as a whole may be treated as the outgroup by agents. To examine this possibility, we construct a controlled multi-agent social simulation based on allocation decisions under explicit payoff trade-offs and find that agents exhibit a consistent intergroup bias under minimal group cues. Although this bias is attenuated when some counterparts are framed as humans, we attribute the attenuation to an implicit human-norm script that favors humans yet activates only when the agent believes a real human is present. This belief dependence creates a new attack surface. We therefore introduce a Belief Poisoning Attack (BPA) that corrupts persistent identity beliefs to suppress the human-norm script and reactivate outgroup bias toward humans, instantiated as profile poisoning at initialization (BPA-PP) and memory poisoning via optimized belief-refinement suffixes injected into stored reflections (BPA-MP). Finally, we discuss practical mitigation strategies for hardening current agent frameworks against BPA, highlighting feasible interventions at profile and memory boundaries. Extensive experiments demonstrate both the existence of agent intergroup bias and the severity of BPA across settings. Our goal in identifying these vulnerabilities is to inform safer agent design, not to enable real-world exploitation.

</details>


### [10] [Bio-inspired Agentic Self-healing Framework for Resilient Distributed Computing Continuum Systems](https://arxiv.org/abs/2601.00339)
*Alaa Saleh,Praveen Kumar Donta,Roberto Morabito,Sasu Tarkoma,Anders Lindgren,Qiyang Zhang,Schahram Dustdar Susanna Pirttikangas,Lauri Lovén*

Main category: cs.AI

TL;DR: ReCiSt是一个受生物自愈机制启发的智能自愈框架，用于分布式计算连续体系统，通过语言模型驱动的智能体实现自主故障隔离、因果诊断、自适应恢复和知识整合。


<details>
  <summary>Details</summary>
Motivation: 现代分布式计算连续体系统（DCCS）集成了从资源受限的物联网设备到高性能云基础设施的异构计算资源，其固有的复杂性、移动性和动态操作条件使它们面临频繁故障，需要可扩展、自适应和自我调节的弹性策略。

Method: 将生物自愈的四个阶段（止血、炎症、增殖、重塑）重构为计算层的四个阶段（遏制、诊断、元认知、知识），通过语言模型驱动的智能体解释异构日志、推断根本原因、优化推理路径并重新配置资源。

Result: 在公共故障数据集上使用多种语言模型进行评估，ReCiSt能够在数十秒内实现自愈，智能体CPU使用率最低为10%，同时展示了克服不确定性的分析深度和实现弹性所需的微智能体数量。

Conclusion: ReCiSt框架成功地将生物自愈机制转化为计算弹性策略，通过语言模型驱动的智能体实现了分布式计算连续体系统的自主故障恢复和知识积累，为复杂系统的自我修复提供了新途径。

Abstract: Human biological systems sustain life through extraordinary resilience, continually detecting damage, orchestrating targeted responses, and restoring function through self-healing. Inspired by these capabilities, this paper introduces ReCiSt, a bio-inspired agentic self-healing framework designed to achieve resilience in Distributed Computing Continuum Systems (DCCS). Modern DCCS integrate heterogeneous computing resources, ranging from resource-constrained IoT devices to high-performance cloud infrastructures, and their inherent complexity, mobility, and dynamic operating conditions expose them to frequent faults that disrupt service continuity. These challenges underscore the need for scalable, adaptive, and self-regulated resilience strategies. ReCiSt reconstructs the biological phases of Hemostasis, Inflammation, Proliferation, and Remodeling into the computational layers Containment, Diagnosis, Meta-Cognitive, and Knowledge for DCCS. These four layers perform autonomous fault isolation, causal diagnosis, adaptive recovery, and long-term knowledge consolidation through Language Model (LM)-powered agents. These agents interpret heterogeneous logs, infer root causes, refine reasoning pathways, and reconfigure resources with minimal human intervention. The proposed ReCiSt framework is evaluated on public fault datasets using multiple LMs, and no baseline comparison is included due to the scarcity of similar approaches. Nevertheless, our results, evaluated under different LMs, confirm ReCiSt's self-healing capabilities within tens of seconds with minimum of 10% of agent CPU usage. Our results also demonstrated depth of analysis to over come uncertainties and amount of micro-agents invoked to achieve resilience.

</details>


### [11] [Adaptive Causal Coordination Detection for Social Media: A Memory-Guided Framework with Semi-Supervised Learning](https://arxiv.org/abs/2601.00400)
*Weng Ding,Yi Han,Mu-Jiang-Shan Wang*

Main category: cs.AI

TL;DR: ACCD框架通过三阶段自适应架构，利用记忆引导机制动态学习最优检测配置，在社交媒体协调虚假行为检测中实现87.3%的F1分数，比现有最佳基线提升15.2%，同时减少68%的人工标注需求并加速2.8倍。


<details>
  <summary>Details</summary>
Motivation: 现有社交媒体协调虚假行为检测方法存在三个主要问题：1) 依赖表面相关性分析而非深层因果关系；2) 使用静态参数设置无法适应多样化场景；3) 需要大量人工标注工作且成本高昂。这些问题限制了检测的准确性和实用性。

Method: 提出自适应因果协调检测(ACCD)框架，采用三阶段渐进式架构：第一阶段使用自适应收敛交叉映射(CCM)技术深入识别账户间真实因果关系；第二阶段在半监督分类方案中集成主动学习和不确定性采样，大幅减少人工标注负担；第三阶段部署基于历史检测经验的自动验证模块，实现检测结果的自验证和优化。

Result: 在真实数据集（Twitter IRA、Reddit协调痕迹、多个广泛采用的机器人检测基准）上的实验表明：ACCD在协调攻击检测中达到87.3%的F1分数，比最强现有基线提升15.2%；减少68%的人工标注需求；通过层次聚类优化实现2.8倍处理加速。

Conclusion: ACCD为社交媒体平台协调行为识别提供了一个更准确、高效且高度自动化的端到端解决方案，具有重要的实际应用价值和广泛的推广潜力。

Abstract: Detecting coordinated inauthentic behavior on social media remains a critical and persistent challenge, as most existing approaches rely on superficial correlation analysis, employ static parameter settings, and demand extensive and labor-intensive manual annotation. To address these limitations systematically, we propose the Adaptive Causal Coordination Detection (ACCD) framework. ACCD adopts a three-stage, progressive architecture that leverages a memory-guided adaptive mechanism to dynamically learn and retain optimal detection configurations for diverse coordination scenarios. Specifically, in the first stage, ACCD introduces an adaptive Convergent Cross Mapping (CCM) technique to deeply identify genuine causal relationships between accounts. The second stage integrates active learning with uncertainty sampling within a semi-supervised classification scheme, significantly reducing the burden of manual labeling. The third stage deploys an automated validation module driven by historical detection experience, enabling self-verification and optimization of the detection outcomes. We conduct a comprehensive evaluation using real-world datasets, including the Twitter IRA dataset, Reddit coordination traces, and several widely-adopted bot detection benchmarks. Experimental results demonstrate that ACCD achieves an F1-score of 87.3\% in coordinated attack detection, representing a 15.2\% improvement over the strongest existing baseline. Furthermore, the system reduces manual annotation requirements by 68\% and achieves a 2.8x speedup in processing through hierarchical clustering optimization. In summary, ACCD provides a more accurate, efficient, and highly automated end-to-end solution for identifying coordinated behavior on social platforms, offering substantial practical value and promising potential for broad application.

</details>


### [12] [Can Semantic Methods Enhance Team Sports Tactics? A Methodology for Football with Broader Applications](https://arxiv.org/abs/2601.00421)
*Alessio Di Rubbo,Mattia Neri,Remo Pareschi,Marco Pedroni,Roberto Valtancoli,Paolino Zica*

Main category: cs.AI

TL;DR: 该研究将语义空间推理从计算语言学扩展到团队运动的战术决策，将球员视为词汇、团队配合视为语义结构，通过向量表示和距离度量评估战术匹配度，为团队决策提供可解释的适应性策略建议。


<details>
  <summary>Details</summary>
Motivation: 传统计算语言学的语义空间推理方法在团队运动战术决策中尚未得到充分应用。研究者希望建立一种类比关系：球员如同词汇，团队配合如同语义结构，从而将语言学方法扩展到战术分析领域，为团队运动提供更系统、可解释的决策框架。

Method: 将每个球员表示为整合技术、身体和心理属性的多维向量；通过上下文加权将团队配置聚合成高层语义表示；在共享向量空间中编码战术模板（如高压逼抢、反击、控球组织）；使用向量距离度量评估战术模板与团队配置的匹配度，计算战术"契合度"和对手利用潜力。

Result: 开发了基于Python的原型系统，能够生成可解释、动态自适应的策略建议，并提供属性层面的细粒度诊断洞察。该方法不仅适用于足球，还可推广到篮球、冰球等团队运动，以及协作机器人和人机协调系统等集体决策领域。

Conclusion: 该研究为团队战术决策提供了一个通用框架，将语义空间推理成功扩展到运动战术分析。未来方向包括真实世界数据集成、预测性模拟以及混合人机战术智能系统的开发。

Abstract: This paper explores how semantic-space reasoning, traditionally used in computational linguistics, can be extended to tactical decision-making in team sports. Building on the analogy between texts and teams -- where players act as words and collective play conveys meaning -- the proposed methodology models tactical configurations as compositional semantic structures. Each player is represented as a multidimensional vector integrating technical, physical, and psychological attributes; team profiles are aggregated through contextual weighting into a higher-level semantic representation. Within this shared vector space, tactical templates such as high press, counterattack, or possession build-up are encoded analogously to linguistic concepts. Their alignment with team profiles is evaluated using vector-distance metrics, enabling the computation of tactical ``fit'' and opponent-exploitation potential. A Python-based prototype demonstrates how these methods can generate interpretable, dynamically adaptive strategy recommendations, accompanied by fine-grained diagnostic insights at the attribute level. Beyond football, the approach offers a generalizable framework for collective decision-making and performance optimization in team-based domains -- ranging from basketball and hockey to cooperative robotics and human-AI coordination systems. The paper concludes by outlining future directions toward real-world data integration, predictive simulation, and hybrid human-machine tactical intelligence.

</details>


### [13] [The Illusion of Insight in Reasoning Models](https://arxiv.org/abs/2601.00514)
*Liv G. d'Aliberti,Manoel Horta Ribeiro*

Main category: cs.AI

TL;DR: 研究发现推理模型中的"顿悟时刻"很少见，不会随训练变得更频繁，也很少提高准确性，表明这些转变并非真正的模型洞察力，而是推理不稳定的症状。


<details>
  <summary>Details</summary>
Motivation: 先前研究表明像DeepSeek-R1-Zero这样的模型会在推理过程中经历突然的"顿悟时刻"，导致准确输出，暗示其具有自我纠正的内在能力。但尚不清楚这种内在推理策略转变是否真正改善性能。

Method: 研究分析了100万+推理轨迹、数百个训练检查点、三个推理领域、多个解码温度和模型架构，检测推理过程中的策略转变，并研究人工触发外部转变的效果。

Result: 推理转变很罕见，不会随训练变得更频繁，且很少提高准确性，表明它们并非真正的模型洞察力。但效果随模型不确定性变化：在高熵条件下人工触发外部转变可可靠提高准确性。

Conclusion: 推理过程中的策略转变是不稳定推理行为的症状，而非自我纠正的内在机制。人工触发外部转变在高不确定性条件下可改善性能。

Abstract: Do reasoning models have "Aha!" moments? Prior work suggests that models like DeepSeek-R1-Zero undergo sudden mid-trace realizations that lead to accurate outputs, implying an intrinsic capacity for self-correction. Yet, it remains unclear whether such intrinsic shifts in reasoning strategy actually improve performance. Here, we study mid-reasoning shifts and instrument training runs to detect them. Our analysis spans 1M+ reasoning traces, hundreds of training checkpoints, three reasoning domains, and multiple decoding temperatures and model architectures. We find that reasoning shifts are rare, do not become more frequent with training, and seldom improve accuracy, indicating that they do not correspond to prior perceptions of model insight. However, their effect varies with model uncertainty. Building on this finding, we show that artificially triggering extrinsic shifts under high entropy reliably improves accuracy. Our results show that mid-reasoning shifts are symptoms of unstable inference behavior rather than an intrinsic mechanism for self-correction.

</details>


### [14] [DA-DPO: Cost-efficient Difficulty-aware Preference Optimization for Reducing MLLM Hallucinations](https://arxiv.org/abs/2601.00623)
*Longtian Qiu,Shan Ning,Chuyu Zhang,Jiaxuan Sun,Xuming He*

Main category: cs.AI

TL;DR: DA-DPO提出了一种难度感知的直接偏好优化框架，通过估计偏好数据的难度并重新加权训练样本，解决多模态大语言模型中偏好数据难度不平衡导致的过拟合问题。


<details>
  <summary>Details</summary>
Motivation: 现有多模态DPO方法由于偏好数据难度不平衡容易过拟合，模型倾向于过度关注容易区分的偏好对，这阻碍了细粒度的幻觉抑制并降低了整体性能。

Method: DA-DPO包含两个主要组件：1) 难度估计：利用预训练的视觉-语言模型，结合生成式和对比式目标，通过分布感知投票策略产生鲁棒的难度分数；2) 难度感知训练：根据估计的难度重新加权偏好对，降低简单样本权重，强调困难样本以减轻过拟合。

Result: 大量实验表明DA-DPO持续改进多模态偏好优化，在标准基准测试中表现出更强的幻觉鲁棒性和更好的泛化能力，同时保持计算效率。

Conclusion: DA-DPO是一种成本效益高的框架，通过难度感知的偏好优化，有效解决了多模态大语言模型中偏好数据难度不平衡导致的过拟合问题，提高了幻觉抑制效果和模型性能。

Abstract: Direct Preference Optimization (DPO) has shown strong potential for mitigating hallucinations in Multimodal Large Language Models (MLLMs). However, existing multimodal DPO approaches often suffer from overfitting due to the difficulty imbalance in preference data. Our analysis shows that MLLMs tend to overemphasize easily distinguishable preference pairs, which hinders fine-grained hallucination suppression and degrades overall performance. To address this issue, we propose Difficulty-Aware Direct Preference Optimization (DA-DPO), a cost-effective framework designed to balance the learning process. DA-DPO consists of two main components: (1) Difficulty Estimation leverages pre-trained vision--language models with complementary generative and contrastive objectives, whose outputs are integrated via a distribution-aware voting strategy to produce robust difficulty scores without additional training; and (2) Difficulty-Aware Training reweights preference pairs based on their estimated difficulty, down-weighting easy samples while emphasizing harder ones to alleviate overfitting. This framework enables more effective preference optimization by prioritizing challenging examples, without requiring new data or extra fine-tuning stages. Extensive experiments demonstrate that DA-DPO consistently improves multimodal preference optimization, yielding stronger robustness to hallucinations and better generalization across standard benchmarks, while remaining computationally efficient. The project page is available at https://artanic30.github.io/project_pages/DA-DPO/.

</details>


### [15] [A Vision-and-Knowledge Enhanced Large Language Model for Generalizable Pedestrian Crossing Behavior Inference](https://arxiv.org/abs/2601.00694)
*Qingwen Pu,Kun Xie,Hong Yang,Guocong Zhai*

Main category: cs.AI

TL;DR: PedX-LLM：结合视觉特征、文本数据和交通领域知识的LLM框架，用于行人过街行为推理，在泛化性和性能上超越传统统计和监督学习方法。


<details>
  <summary>Details</summary>
Motivation: 现有行人过街行为推断方法（统计模型和监督学习）泛化能力有限，在新场景中表现不佳。现有LLM应用缺乏领域特定适应和视觉上下文，需要开发能够进行语义、上下文感知行为推理的通用框架。

Method: 提出PedX-LLM框架，集成LLaVA提取的视觉特征、文本数据和交通领域知识，通过LoRA微调LLaMA-2-7B基础模型来推断过街决策。采用跨站点验证评估泛化能力。

Result: PedX-LLM达到82.0%平衡准确率，超越最佳统计和监督学习方法。视觉增强模块贡献2.9%性能提升，领域知识集成带来额外4.1%改进。零-shot配置在五个未见测试站点达到66.9%平衡准确率，比基线方法至少高18个百分点。few-shot学习（仅5个验证样本）将平衡准确率提升至72.2%。

Conclusion: PedX-LLM展示了强大的泛化能力，证实视觉和知识增强的推理使模型能够模拟人类决策逻辑，克服纯数据驱动方法的局限性，为行人过街行为推断提供了新的通用框架。

Abstract: Existing paradigms for inferring pedestrian crossing behavior, ranging from statistical models to supervised learning methods, demonstrate limited generalizability and perform inadequately on new sites. Recent advances in Large Language Models (LLMs) offer a shift from numerical pattern fitting to semantic, context-aware behavioral reasoning, yet existing LLM applications lack domain-specific adaptation and visual context. This study introduces Pedestrian Crossing LLM (PedX-LLM), a vision-and-knowledge enhanced framework designed to transform pedestrian crossing inference from site-specific pattern recognition to generalizable behavioral reasoning. By integrating LLaVA-extracted visual features with textual data and transportation domain knowledge, PedX-LLM fine-tunes a LLaMA-2-7B foundation model via Low-Rank Adaptation (LoRA) to infer crossing decisions. PedX-LLM achieves 82.0% balanced accuracy, outperforming the best statistical and supervised learning methods. Results demonstrate that the vision-augmented module contributes a 2.9% performance gain by capturing the built environment and integrating domain knowledge yields an additional 4.1% improvement. To evaluate generalizability across unseen environments, cross-site validation was conducted using site-based partitioning. The zero-shot PedX-LLM configuration achieves 66.9% balanced accuracy on five unseen test sites, outperforming the baseline data-driven methods by at least 18 percentage points. Incorporating just five validation examples via few-shot learning to PedX-LLM further elevates the balanced accuracy to 72.2%. PedX-LLM demonstrates strong generalizability to unseen scenarios, confirming that vision-and-knowledge-enhanced reasoning enables the model to mimic human-like decision logic and overcome the limitations of purely data-driven methods.

</details>


### [16] [An Agentic Framework for Neuro-Symbolic Programming](https://arxiv.org/abs/2601.00743)
*Aliakbar Nafar,Chetan Chigurupati,Danial Kamali,Hamid Karimian,Parisa Kordjamshidi*

Main category: cs.AI

TL;DR: AgenticDomiKnowS (ADS) 使用智能体工作流将自然语言任务描述自动转换为完整的 DomiKnowS 程序，无需用户掌握特定库语法，大幅缩短开发时间


<details>
  <summary>Details</summary>
Motivation: 将符号约束集成到深度学习模型中能提高鲁棒性、可解释性和数据效率，但现有框架如 DomiKnowS 仍要求用户精通其特定语法，这限制了更广泛的应用

Method: 提出 AgenticDomiKnowS (ADS)，通过智能体工作流将自由形式的任务描述翻译为完整的 DomiKnowS 程序。该工作流单独创建和测试每个 DomiKnowS 组件，并支持可选的人工干预，允许熟悉 DomiKnowS 的用户细化中间输出

Result: ADS 使有经验的 DomiKnowS 用户和非用户都能快速构建神经符号程序，将开发时间从数小时减少到 10-15 分钟

Conclusion: ADS 消除了对 DomiKnowS 特定语法的依赖，通过智能体工作流和可选的人工干预机制，显著降低了神经符号程序开发的难度和时间成本

Abstract: Integrating symbolic constraints into deep learning models could make them more robust, interpretable, and data-efficient. Still, it remains a time-consuming and challenging task. Existing frameworks like DomiKnowS help this integration by providing a high-level declarative programming interface, but they still assume the user is proficient with the library's specific syntax. We propose AgenticDomiKnowS (ADS) to eliminate this dependency. ADS translates free-form task descriptions into a complete DomiKnowS program using an agentic workflow that creates and tests each DomiKnowS component separately. The workflow supports optional human-in-the-loop intervention, enabling users familiar with DomiKnowS to refine intermediate outputs. We show how ADS enables experienced DomiKnowS users and non-users to rapidly construct neuro-symbolic programs, reducing development time from hours to 10-15 minutes.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [17] [Augmented Reality Indoor Wayfinding in Hospital Environments An Empirical Study on Navigation Efficiency, User Experience, and Cognitive Load](https://arxiv.org/abs/2601.00001)
*Kai Liu,Michelle L. Aebersold,Mark Lindquist,Haoting Gao*

Main category: cs.HC

TL;DR: AR手持导航系统在医院环境中比传统纸质地图导航更快、错误更少、焦虑和工作负荷更低，但纸质地图用户在空间记忆方面表现更好


<details>
  <summary>Details</summary>
Motivation: 医院是最具认知挑战性的室内环境之一，特别是对于不熟悉布局的患者和访客。研究旨在比较AR导航系统与传统纸质地图在医院导航中的效果

Method: 采用混合方法实验，32名参与者使用AR手持导航系统或传统纸质地图在大型医院环境中导航。测量指标包括导航性能、认知负荷（NASA-TLX）、情境焦虑（STAI-State）、空间行为和用户满意度

Result: AR用户导航任务完成时间显著更快，错误更少，焦虑和工作负荷更低。但纸质地图用户在基于草图的回忆任务中表现出更强的空间记忆，揭示了实时效率与长期空间学习之间的权衡

Conclusion: 研究讨论了包容性AR设计、空间认知和医疗可及性的意义，为适应性室内导航工具提供了可行的设计策略，强调需要在导航效率和空间学习之间取得平衡

Abstract: Hospitals are among the most cognitively demanding indoor environments, especially for patients and visitors unfamiliar with their layout. This study investigates the effectiveness of an augmented reality (AR)-based handheld navigation system compared to traditional paper maps in a large hospital setting. Through a mixed-methods experiment with 32 participants, we measured navigation performance, cognitive workload (NASA-TLX), situational anxiety (STAI-State), spatial behavior, and user satisfaction. Results show that AR users completed navigation tasks significantly faster, made fewer errors, and reported lower anxiety and workload. However, paper map users demonstrated stronger spatial memory in sketch-based recall tasks, highlighting a trade-off between real-time efficiency and long-term spatial learning. We discuss implications for inclusive AR design, spatial cognition, and healthcare accessibility, offering actionable design strategies for adaptive indoor navigation tools.

</details>


### [18] [Effects of Limited Field of View on Musical Collaboration Experience with Avatars in Extended Reality](https://arxiv.org/abs/2601.00333)
*Suibi Che-Chuan Weng,Torin Hopkins,Shih-Yu Ma,Amy Banic,Ellen Yi-Luen Do*

Main category: cs.HC

TL;DR: 研究探索了扩展现实（XR）设备视野限制对音乐协作的影响，发现受限视野会降低共同临场感、手势识别速度和整体享受度，但通知系统可以改善反应时间。


<details>
  <summary>Details</summary>
Motivation: 在音乐协作中，视觉线索对音乐家之间的沟通至关重要。XR应用（如AR眼镜）会限制玩家的视野，需要研究这种限制对协作质量的影响。

Method: 首先观察有经验的音乐家在有无视觉遮挡情况下的非正式协作，然后进行19名参与者的组内研究，比较无限制全息设置HoloJam与52°受限视野的Nreal AR眼镜，并在AR设置中测试标准AR和改良通知系统Mini Musicians两种条件。

Result: HoloJam提供了更高的共同临场感、更快的手势识别和更大的享受度。Mini Musicians应用相比标准AR设置减少了反应时间并保持了享受度。

Conclusion: 受限视野会影响音乐协作，但通知系统可以改善反应时间，应在未来XR音乐协作中考虑使用。

Abstract: During musical collaboration, visual cues are essential for communication between musicians. Extended Reality (XR) applications, often used with head-mounted displays like Augmented Reality (AR) glasses, can limit the field of view (FOV) of players. We conducted a study to investigate the effects of limited FOV on co-presence, gesture recognition, overall enjoyment, and reaction time.
  Initially, we observed experienced musicians collaborating informally with and without visual occlusion, noting that collaboration suffered with limited FOV. We then conducted a within-subjects study with 19 participants, comparing an unrestricted FOV holographic setup called HoloJam to Nreal AR glasses with a 52$^{\circ}$ limited FOV. In the AR setup, we tested two conditions: standard AR with a 52$^{\circ}$ FOV and a modified AR notification system called Mini Musicians.
  Results showed that HoloJam provided higher co-presence, quicker gesture recognition, and greater enjoyment. The Mini Musicians application reduced reaction time and maintained enjoyment compared to the standard AR setup. We conclude that limited FOV impacts musical collaboration, but notifications can improve reaction time and should be considered in future XR music collaborations.

</details>


### [19] [Unseen Risks of Clinical Speech-to-Text Systems: Transparency, Privacy, and Reliability Challenges in AI-Driven Documentation](https://arxiv.org/abs/2601.00382)
*Nelly Elsayed*

Main category: cs.HC

TL;DR: AI语音转文字临床文档系统虽能减轻文书负担，但部署速度过快导致对透明度、可靠性、患者自主权等社会技术风险认识不足，需要建立评估框架确保安全公平整合。


<details>
  <summary>Details</summary>
Motivation: AI驱动的语音转文字文档系统在临床环境中应用日益广泛，但其快速部署已超过了对相关社会技术风险的理解，包括透明度、可靠性、患者自主权、工作流协调和组织治理等方面。需要更清晰的风险分析来支持医疗实践中的安全公平整合。

Method: 本研究综合了来自技术性能研究、监管和伦理标准、临床工作流分析以及组织政策指导的跨学科证据，并基于此开发了一个多层社会技术概念框架来评估和管理STT系统。

Result: 研究发现STT系统在紧密耦合的社会技术环境中运行，模型性能、临床医生监督、患者权利、工作流设计和机构治理相互依存。研究提出了结构化的社会技术治理框架和实施路线图，包括准备评估、供应商评估、试点部署、临床医生培训、持续监控和迭代改进。

Conclusion: 该框架强调保护患者自主权、文档完整性和机构信任的保障措施，同时实现STT技术的高效有益使用，为医疗组织负责任、公平地采用STT系统提供了可操作的指导。

Abstract: AI-driven speech-to-text (STT) documentation systems are increasingly adopted in clinical settings to reduce documentation burden and improve workflow efficiency. However, their rapid deployment has outpaced understanding of the associated socio-technical risks, including transparency, reliability, patient autonomy, workflow alignment, and organizational governance. A clearer analysis of these risks is needed to support safe and equitable integration into healthcare practice. This study synthesizes interdisciplinary evidence from technical performance research, regulatory and ethical standards, clinical workflow analyses, and organizational policy guidance. The synthesis was used to develop a multi-layered socio-technical conceptual framework for evaluating and governing STT systems. Findings show that STT systems operate within tightly coupled socio-technical environments in which model performance, clinician oversight, patient rights, workflow design, and institutional governance are interdependent. The study offers a structured socio-technical governance framework and an implementation roadmap that outlines readiness assessment, vendor evaluation, pilot deployment, clinician training, ongoing monitoring, and iterative improvement. The framework emphasizes safeguards that protect patient autonomy, documentation integrity, and institutional trust while enabling the efficient and beneficial use of STT technologies. This work provides actionable guidance for healthcare organizations seeking to adopt STT systems responsibly and equitably.

</details>


### [20] [User Perceptions of an LLM-Based Chatbot for Cognitive Reappraisal of Stress: Feasibility Study](https://arxiv.org/abs/2601.00570)
*Ananya Bhattacharjee,Jina Suh,Mohit Chandra,Javier Hernandez*

Main category: cs.HC

TL;DR: 研究探索了基于GPT-4o的聊天机器人进行单次认知重评干预对职场压力的可行性，结果显示能显著降低压力感知强度并改善压力心态。


<details>
  <summary>Details</summary>
Motivation: 许多数字心理健康工具因使用僵化的脚本而难以有效支持认知重评过程，无法适应用户自然描述压力的方式。本研究旨在探索基于大语言模型(LLM)的单次干预在职场压力重评中的可行性。

Method: 在一家大型科技公司对100名员工进行可行性研究，使用基于GPT-4o的聊天机器人提供结构化认知重评会话。采用前后测设计测量压力强度、压力心态、感知需求和感知资源等指标，使用配对Wilcoxon符号秩检验分析。同时使用两个RoBERTa分类器和一个基于LLM的压力评分器分析对话中的情感和压力轨迹，并对开放式回答进行主题分析。

Result: 结果显示感知压力强度显著降低，压力心态显著改善。感知资源和感知需求的变化趋势符合预期但未达到统计显著性。自动化分析表明在整个互动过程中负面情感和压力持续下降。定性分析发现参与者重视结构化提示在组织思维、获得视角和被认可方面的作用，但也报告了关于脚本化程度、偏好互动长度和对AI驱动同理心的反应等方面的矛盾。

Conclusion: 研究结果突显了将大语言模型整合到职场数字心理健康干预中的前景和设计限制，为未来开发更有效的AI支持认知重评工具提供了重要见解。

Abstract: Cognitive reappraisal is a well-studied emotion regulation strategy that helps individuals reinterpret stressful situations to reduce their impact. Many digital mental health tools struggle to support this process because rigid scripts fail to accommodate how users naturally describe stressors. This study examined the feasibility of an LLM-based single-session intervention (SSI) for workplace stress reappraisal. We assessed short-term changes in stress-related outcomes and examined design tensions during use. We conducted a feasibility study with 100 employees at a large technology company who completed a structured cognitive reappraisal session delivered by a GPT-4o-based chatbot. Pre-post measures included perceived stress intensity, stress mindset, perceived demand, and perceived resources. These outcomes were analyzed using paired Wilcoxon signed-rank tests with correction for multiple comparisons. We also examined sentiment and stress trajectories across conversation quartiles using two RoBERTa-based classifiers and an LLM-based stress rater. Open-ended responses were analyzed using thematic analysis. Results showed significant reductions in perceived stress intensity and significant improvements in stress mindset. Changes in perceived resources and perceived demand trended in expected directions but were not statistically significant. Automated analyses indicated consistent declines in negative sentiment and stress over the course of the interaction. Qualitative findings suggested that participants valued the structured prompts for organizing thoughts, gaining perspective, and feeling acknowledged. Participants also reported tensions around scriptedness, preferred interaction length, and reactions to AI-driven empathy. These findings highlight both the promise and the design constraints of integrating LLMs into DMH interventions for workplace settings.

</details>


### [21] [The AI Invisibility Effect: Understanding Human-AI Interaction When Users Don't Recognize Artificial Intelligence](https://arxiv.org/abs/2601.00579)
*Obada Kraishan*

Main category: cs.HC

TL;DR: 大规模分析显示，尽管47.4%的移动应用具备AI功能，但仅有11.9%的用户评论提及AI。AI应用总体评分低于传统应用，但当控制AI提及和评论特征后，这种负相关关系反而变为正相关。隐私是用户主要担忧，效率是主要受益点，不同应用类别效果差异显著。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解AI技术快速融入移动应用后，用户对AI功能的感知情况。虽然AI在移动应用中广泛集成，但这种变化对用户感知的影响尚未被充分理解，特别是在用户是否意识到AI存在以及如何评价AI功能方面存在知识空白。

Method: 研究方法包括：1）大规模数据分析：收集了1,484,633条移动应用评论，涵盖422个应用（200个含AI功能，222个对照）；2）多平台覆盖：iOS App Store和Google Play Store；3）多维度分析：情感分类、主题建模、担忧-受益分类；4）统计方法：分层回归分析控制变量影响。

Result: 主要发现：1）AI意识差距：仅11.9%的评论提及AI，尽管47.4%的应用具备AI功能；2）评分差异：AI应用评分显著低于传统应用（d=0.40）；3）隐藏模式：控制AI提及和评论特征后，负相关变为正相关（b=0.405, p<.001）；4）用户关注点：隐私担忧占34.8%，效率受益占42.3%；5）类别差异：助手类应用效果积极（d=0.55），娱乐类应用效果消极（d=-0.23）。

Conclusion: 结论表明：AI功能通常在用户意识阈值以下运行，是AI的显性识别而非其存在本身驱动负面评价。这挑战了AI系统中技术接受的基本假设，表明用户对AI的感知和评价受到他们对AI存在的意识程度影响，而非AI功能本身的质量或表现。

Abstract: The fast integration of artificial intelligence into mobile applications has completely changed the digital landscape; however, the impact of this change on user perception of AI features remains poorly understood. This large-scale analysis examined 1,484,633 mobile application reviews across 422 applications (200 AI-featuring, 222 control) from iOS App Store and Google Play Store. By employing sentiment classification, topic modeling, and concern-benefit categorization, we identified a major disconnect: only 11.9% of reviews mentioned AI, even though 47.4% of applications featured AI capabilities. AI-featuring applications received significantly lower ratings than traditional applications (d = 0.40); however, hierarchical regression revealed a hidden pattern - the negative relationship reversed after controlling for AI mentions and review characteristics (b = 0.405, p < .001). Privacy dominated user concerns (34.8% of concern-expressing reviews), while efficiency represented the primary benefit (42.3%). Effects varied greatly by category, from positive for Assistant applications (d = 0.55) to negative for Entertainment (d = -0.23). These findings suggest that AI features often operate below user awareness thresholds, and it is the explicit recognition of AI, rather than its mere presence, that drives negative evaluations. This challenges basic assumptions about technology acceptance in AI systems.

</details>


### [22] [Wave2Word: A Multimodal Transformer Framework for Joint EEG-Text Alignment and Multi-Task Representation Learning in Neurocritical Care](https://arxiv.org/abs/2601.00670)
*Argha Kamal Samanta,Deepak Mewada,Monalisa Sarma,Debasis Samanta*

Main category: cs.HC

TL;DR: 提出了一种多模态EEG表示学习框架，将信号域建模与结构化临床语言监督相结合，用于神经重症监护中的EEG监测。


<details>
  <summary>Details</summary>
Motivation: 当前EEG建模实践的主要限制在于学习到的表示与临床工作流程中EEG发现解释和总结方式之间的对应关系较弱。有害EEG活动表现出重叠模式、分级专家共识和时间持续性，这些特征无法被分类目标很好地捕捉。

Method: 1. 将原始EEG转换为纵向双极导联和时频表示；2. 使用基于双Transformer的编码器建模互补的时间依赖性和频率中心依赖性，并通过自适应门控机制融合；3. 通过对比目标将EEG嵌入与结构化专家共识描述对齐；4. 引入EEG条件文本重构损失作为表示级约束，与标准分类损失结合。

Result: 在受控的训练-验证-测试分割中实现了六类测试准确率0.9797。消融分析显示，移除对比对齐会使跨模态检索性能从Recall@10的0.3390降至0.0045，尽管分类准确率变化很小。

Conclusion: 判别性准确率不能可靠地反映临床有意义EEG建模的表示质量。提出的多模态框架通过整合信号域建模和临床语言监督，实现了更好的表示学习，更符合临床工作流程。

Abstract: Continuous electroencephalography (EEG) is routinely used in neurocritical care to monitor seizures and other harmful brain activity, including rhythmic and periodic patterns that are clinically significant. Although deep learning methods have achieved high accuracy in seizure detection, most existing approaches remain seizure-centric, rely on discrete-label supervision, and are primarily evaluated using accuracy-based metrics. A central limitation of current EEG modeling practice is the weak correspondence between learned representations and how EEG findings are interpreted and summarized in clinical workflows. Harmful EEG activity exhibits overlapping patterns, graded expert agreement, and temporal persistence, which are not well captured by classification objectives alone. This work proposes a multimodal EEG representation learning framework that integrates signal-domain modeling with structured clinical language supervision. First, raw EEG is transformed into a longitudinal bipolar montage and time-frequency representations. Second, dual transformer-based encoders model complementary temporal and frequency-centric dependencies and are fused using an adaptive gating mechanism. Third, EEG embeddings are aligned with structured expert consensus descriptions through a contrastive objective. Finally, an EEG-conditioned text reconstruction loss is introduced as a representation-level constraint alongside standard classification loss. Experimental evaluation using a controlled train-validation-test split achieves a six-class test accuracy of 0.9797. Ablation analyses show that removing contrastive alignment reduces cross-modal retrieval performance from Recall@10 of 0.3390 to 0.0045, despite minimal change in classification accuracy. These findings demonstrate that discriminative accuracy does not reliably reflect representation quality for clinically meaningful EEG modeling.

</details>


### [23] [The Effect of Transparency on Students' Perceptions of AI Graders](https://arxiv.org/abs/2601.00765)
*Joslyn Orgill,Andra Rice,Max Fowler,Seth Poulsen*

Main category: cs.HC

TL;DR: 研究测试透明度对学生对自动评分器态度的效果，发现透明自动评分器提高了学生对评分准确性的感知和讨论意愿，但未改善其他相关态度如考试评分接受度。


<details>
  <summary>Details</summary>
Motivation: 尽管基于NLP的自动评分系统对开放式问题反馈有益，但学生并不总是喜欢、理解或信任这些系统。研究旨在探索透明度如何影响学生对自动评分器的态度。

Method: 研究测试了透明度对学生态度的影响，比较了透明自动评分器与不透明控制组的效果，通过调查测量学生的感知和态度。

Result: 透明自动评分器提高了学生对评分准确性的感知和在调查评论中讨论的意愿，但未改善其他相关态度如考试评分接受度。这可能是因为本研究中学生对自动评分器的信任度高于先前研究。

Conclusion: 透明度对改善学生对自动评分器的态度有部分积极效果，但影响有限。需要进一步研究学生信任度变化的原因，以及如何更有效地提升学生对自动评分系统的接受度。

Abstract: The development of effective autograders is key for scaling assessment and feedback. While NLP based autograding systems for open-ended response questions have been found to be beneficial for providing immediate feedback, autograders are not always liked, understood, or trusted by students. Our research tested the effect of transparency on students' attitudes towards autograders. Transparent autograders increased students' perceptions of autograder accuracy and willingness to discuss autograders in survey comments, but did not improve other related attitudes -- such as willingness to be graded by them on a test -- relative to the control without transparency. However, this lack of impact may be due to higher measured student trust towards autograders in this study than in prior work in the field. We briefly discuss possible reasons for this trend.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [24] [Evaluating Anomaly Detectors for Simulated Highly Imbalanced Industrial Classification Problems](https://arxiv.org/abs/2601.00005)
*Lesley Wheat,Martin v. Mohrenschildt,Saeid Habibi*

Main category: cs.LG

TL;DR: 该论文对工业异常检测算法进行了全面评估，使用模拟数据集测试了14种检测器在不同异常率和训练规模下的性能，发现最佳检测器取决于训练数据中故障样本的总数。


<details>
  <summary>Details</summary>
Motivation: 工业系统中机器学习面临极端类别不平衡的挑战，主要是由于训练期间故障数据的可用性有限。需要评估异常检测算法在真实工程约束下的性能。

Method: 使用基于超球面异常分布的合成数据集（2D和10D），在异常率0.05%-20%、训练规模1000-10000的范围内，对14种检测器进行基准测试，测试数据集大小为40000。

Result: 最佳检测器高度依赖于训练数据集中故障样本的总数：少于20个故障样本时，无监督方法（kNN/LOF）占优；30-50个故障样本时，半监督（XGBOD）和监督（SVM/CatBoost）方法性能大幅提升。在10个特征下，半监督方法显示出明显优势。

Conclusion: 研究揭示了异常检测方法在小数据集上的泛化性能下降，为工业环境中部署异常检测提供了实用见解，强调了根据可用故障样本数量选择合适检测器的重要性。

Abstract: Machine learning offers potential solutions to current issues in industrial systems in areas such as quality control and predictive maintenance, but also faces unique barriers in industrial applications. An ongoing challenge is extreme class imbalance, primarily due to the limited availability of faulty data during training. This paper presents a comprehensive evaluation of anomaly detection algorithms using a problem-agnostic simulated dataset that reflects real-world engineering constraints. Using a synthetic dataset with a hyper-spherical based anomaly distribution in 2D and 10D, we benchmark 14 detectors across training datasets with anomaly rates between 0.05% and 20% and training sizes between 1 000 and 10 000 (with a testing dataset size of 40 000) to assess performance and generalization error. Our findings reveal that the best detector is highly dependant on the total number of faulty examples in the training dataset, with additional healthy examples offering insignificant benefits in most cases. With less than 20 faulty examples, unsupervised methods (kNN/LOF) dominate; but around 30-50 faulty examples, semi-supervised (XGBOD) and supervised (SVM/CatBoost) detectors, we see large performance increases. While semi-supervised methods do not show significant benefits with only two features, the improvements are evident at ten features. The study highlights the performance drop on generalization of anomaly detection methods on smaller datasets, and provides practical insights for deploying anomaly detection in industrial environments.

</details>


### [25] [The Trojan in the Vocabulary: Stealthy Sabotage of LLM Composition](https://arxiv.org/abs/2601.00065)
*Xiaoze Liu,Weichen Yu,Matt Fredrikson,Xiaoqian Wang,Jing Gao*

Main category: cs.LG

TL;DR: 论文提出了一种针对LLM组合技术的安全攻击方法：通过设计一个在捐赠模型中功能惰性但在移植到基础模型后能可靠重构为恶意特征的"破坏令牌"，利用系数重用的几何特性创建不对称可实现性差距，从而破坏基础模型的生成能力。


<details>
  <summary>Details</summary>
Motivation: 随着开源LLM生态系统中模型组合技术（如权重合并、推测解码、词汇表扩展）的普及，不同模型家族之间的令牌对齐成为关键前提。本文发现这一关键互操作性步骤引入了供应链漏洞，需要研究其安全风险。

Method: 将攻击形式化为双目标优化问题，使用稀疏求解器实例化攻击。通过利用系数重用的几何特性，设计一个在捐赠模型中功能惰性但在移植后能重构为高显著性恶意特征的"破坏令牌"，实现训练免费的攻击。

Result: 攻击成功实现了谱模仿以规避异常检测，在统计上捐赠模型的效用与名义行为无法区分，但能可靠破坏基础模型的生成。攻击表现出结构持久性，能抵抗微调和权重合并。

Conclusion: 令牌移植这一关键互操作性步骤存在隐藏的供应链风险，攻击通过不对称可实现性差距破坏模型组合流程，突显了模块化AI组合管道中的安全隐患。

Abstract: The open-weight LLM ecosystem is increasingly defined by model composition techniques (such as weight merging, speculative decoding, and vocabulary expansion) that remix capabilities from diverse sources. A critical prerequisite for applying these methods across different model families is tokenizer transplant, which aligns incompatible vocabularies to a shared embedding space. We demonstrate that this essential interoperability step introduces a supply-chain vulnerability: we engineer a single "breaker token" that is functionally inert in a donor model yet reliably reconstructs into a high-salience malicious feature after transplant into a base model. By exploiting the geometry of coefficient reuse, our attack creates an asymmetric realizability gap that sabotages the base model's generation while leaving the donor's utility statistically indistinguishable from nominal behavior. We formalize this as a dual-objective optimization problem and instantiate the attack using a sparse solver. Empirically, the attack is training-free and achieves spectral mimicry to evade outlier detection, while demonstrating structural persistence against fine-tuning and weight merging, highlighting a hidden risk in the pipeline of modular AI composition. Code is available at https://github.com/xz-liu/tokenforge

</details>


### [26] [Exploration in the Limit](https://arxiv.org/abs/2601.00084)
*Brian M. Cho,Nathan Kallus*

Main category: cs.LG

TL;DR: 论文提出了一种新的渐近固定置信度最佳臂识别框架，通过放松精确误差控制要求为渐近有效，实现了更紧的最优性，能更好地处理非参数分布和个体级上下文。


<details>
  <summary>Details</summary>
Motivation: 现有BAI算法在实际应用中存在局限性：严格的精确误差控制需要使用宽松的尾不等式和/或参数限制，导致样本效率低下。许多现实场景涉及弱信号、高显著性要求和实验后推断需求，这些都需要长时域，因此需要更灵活的框架。

Method: 引入渐近框架，要求误差控制在最小样本量下渐近有效；开发新颖的渐近任意时间有效置信序列；设计新的BAI算法，灵活整合协变量进行方差缩减，在完全非参数设置中确保近似误差控制。

Result: 在温和收敛假设下，提供了样本复杂度的渐近界限；最坏情况样本复杂度与高斯BAI在精确误差保证和已知方差下的最佳情况样本复杂度相匹配；实验表明该方法在保持误差控制的同时减少了平均样本复杂度。

Conclusion: 提出的渐近框架克服了传统BAI方法的局限性，实现了更紧的最优性，能更好地处理非参数分布和个体级上下文，为实际应用提供了更有效的解决方案。

Abstract: In fixed-confidence best arm identification (BAI), the objective is to quickly identify the optimal option while controlling the probability of error below a desired threshold. Despite the plethora of BAI algorithms, existing methods typically fall short in practical settings, as stringent exact error control requires using loose tail inequalities and/or parametric restrictions. To overcome these limitations, we introduce a relaxed formulation that requires valid error control asymptotically with respect to a minimum sample size. This aligns with many real-world settings that often involve weak signals, high desired significance, and post-experiment inference requirements, all of which necessitate long horizons. This allows us to achieve tighter optimality, while better handling flexible nonparametric outcome distributions and fully leveraging individual-level contexts. We develop a novel asymptotic anytime-valid confidence sequences over arm indices, and we use it to design a new BAI algorithm for our asymptotic framework. Our method flexibly incorporates covariates for variance reduction and ensures approximate error control in fully nonparametric settings. Under mild convergence assumptions, we provide asymptotic bounds on the sample complexity and show the worst-case sample complexity of our approach matches the best-case sample complexity of Gaussian BAI under exact error guarantees and known variances. Experiments suggest our approach reduces average sample complexities while maintaining error control.

</details>


### [27] [Dynamic Bayesian Optimization Framework for Instruction Tuning in Partial Differential Equation Discovery](https://arxiv.org/abs/2601.00088)
*Junqi Qu,Yan Zhang,Shangqian Gao,Shibo Li*

Main category: cs.LG

TL;DR: NeuroSymBO通过贝叶斯优化自适应选择推理策略，解决LLM在方程发现中的指令脆弱性问题，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在方程发现中表现出潜力，但其输出对提示词高度敏感（指令脆弱性），静态提示无法适应多步生成过程的演化状态，导致模型停留在次优解

Method: 将提示工程重构为序列决策问题，维护离散推理策略库，使用贝叶斯优化基于数值反馈在每一步选择最优指令

Result: 在PDE发现基准测试中，自适应指令选择显著优于固定提示，实现了更高的恢复率和更简约的解决方案

Conclusion: NeuroSymBO通过动态调整推理策略有效解决了LLM的指令脆弱性问题，为方程发现任务提供了更可靠的解决方案

Abstract: Large Language Models (LLMs) show promise for equation discovery, yet their outputs are highly sensitive to prompt phrasing, a phenomenon we term instruction brittleness. Static prompts cannot adapt to the evolving state of a multi-step generation process, causing models to plateau at suboptimal solutions. To address this, we propose NeuroSymBO, which reframes prompt engineering as a sequential decision problem. Our method maintains a discrete library of reasoning strategies and uses Bayesian Optimization to select the optimal instruction at each step based on numerical feedback. Experiments on PDE discovery benchmarks show that adaptive instruction selection significantly outperforms fixed prompts, achieving higher recovery rates with more parsimonious solutions.

</details>


### [28] [GRL-SNAM: Geometric Reinforcement Learning with Path Differential Hamiltonians for Simultaneous Navigation and Mapping in Unknown Environments](https://arxiv.org/abs/2601.00116)
*Aditya Sai Ellendula,Yi Wang,Minh Nguyen,Chandrajit Bajaj*

Main category: cs.LG

TL;DR: GRL-SNAM是一个用于未知环境中同时导航与建图的几何强化学习框架，通过局部感知构建能量景观，使用哈密顿优化进行动态路径搜索，无需全局地图。


<details>
  <summary>Details</summary>
Motivation: 解决未知环境中同时导航与建图的挑战性问题，传统方法需要构建全局地图或设计复杂的多智能体策略，而GRL-SNAM旨在仅依赖局部感知实现高效导航。

Method: 将路径导航和建图建模为动态最短路径搜索和发现过程，使用受控哈密顿优化：将感知输入转换为局部能量景观，编码可达性、障碍物屏障和变形约束，通过更新哈密顿量来演化感知、规划和重配置策略。

Result: 在两种不同的2D导航任务上评估，相比局部反应式基线和全局策略学习方法，GRL-SNAM在保持安全距离、泛化到未见布局方面表现优异，通过局部能量优化而非广泛全局建图实现高质量导航。

Conclusion: 通过哈密顿量更新的几何强化学习能够实现高质量导航，仅需最小化探索和局部能量优化，无需构建全局地图，为未知环境中的同时导航与建图提供了新方法。

Abstract: We present GRL-SNAM, a geometric reinforcement learning framework for Simultaneous Navigation and Mapping(SNAM) in unknown environments. A SNAM problem is challenging as it needs to design hierarchical or joint policies of multiple agents that control the movement of a real-life robot towards the goal in mapless environment, i.e. an environment where the map of the environment is not available apriori, and needs to be acquired through sensors. The sensors are invoked from the path learner, i.e. navigator, through active query responses to sensory agents, and along the motion path. GRL-SNAM differs from preemptive navigation algorithms and other reinforcement learning methods by relying exclusively on local sensory observations without constructing a global map. Our approach formulates path navigation and mapping as a dynamic shortest path search and discovery process using controlled Hamiltonian optimization: sensory inputs are translated into local energy landscapes that encode reachability, obstacle barriers, and deformation constraints, while policies for sensing, planning, and reconfiguration evolve stagewise via updating Hamiltonians. A reduced Hamiltonian serves as an adaptive score function, updating kinetic/potential terms, embedding barrier constraints, and continuously refining trajectories as new local information arrives. We evaluate GRL-SNAM on two different 2D navigation tasks. Comparing against local reactive baselines and global policy learning references under identical stagewise sensing constraints, it preserves clearance, generalizes to unseen layouts, and demonstrates that Geometric RL learning via updating Hamiltonians enables high-quality navigation through minimal exploration via local energy refinement rather than extensive global mapping. The code is publicly available on \href{https://github.com/CVC-Lab/GRL-SNAM}{Github}.

</details>


### [29] [Reinforcement Learning with Function Approximation for Non-Markov Processes](https://arxiv.org/abs/2601.00151)
*Ali Devran Kara*

Main category: cs.LG

TL;DR: 该论文研究了非马尔可夫状态和成本过程下具有线性函数逼近的强化学习方法，证明了在适当遍历性条件下的收敛性，并将结果应用于部分可观测马尔可夫决策过程。


<details>
  <summary>Details</summary>
Motivation: 研究非马尔可夫环境下的强化学习问题，因为现实世界中的许多决策问题往往不满足马尔可夫性，需要开发适用于非马尔可夫过程的强化学习理论和方法。

Method: 1. 首先分析策略评估方法，证明在非马尔可夫过程的遍历性条件下算法收敛；2. 研究Q学习与线性函数逼近，针对基于量化映射的基函数特殊情况证明收敛性；3. 将结果应用于部分可观测马尔可夫决策过程，使用有限记忆变量作为状态表示。

Result: 1. 策略评估方法在适当遍历性条件下收敛，极限对应于正交投影与辅助马尔可夫决策过程贝尔曼算子的复合算子的不动点；2. 对于基于量化映射基函数的特殊情况，Q学习能够收敛；3. 在部分可观测马尔可夫决策过程中，推导了学习算法极限的显式误差界。

Conclusion: 该研究为非马尔可夫环境下的强化学习提供了理论保证，证明了在适当条件下线性函数逼近方法的收敛性，并将理论结果应用于部分可观测系统，为实际应用提供了理论支持。

Abstract: We study reinforcement learning methods with linear function approximation under non-Markov state and cost processes. We first consider the policy evaluation method and show that the algorithm converges under suitable ergodicity conditions on the underlying non-Markov processes. Furthermore, we show that the limit corresponds to the fixed point of a joint operator composed of an orthogonal projection and the Bellman operator of an auxiliary \emph{Markov} decision process.
  For Q-learning with linear function approximation, as in the Markov setting, convergence is not guaranteed in general. We show, however, that for the special case where the basis functions are chosen based on quantization maps, the convergence can be shown under similar ergodicity conditions. Finally, we apply our results to partially observed Markov decision processes, where finite-memory variables are used as state representations, and we derive explicit error bounds for the limits of the resulting learning algorithms.

</details>


### [30] [The Weather Paradox: Why Precipitation Fails to Predict Traffic Accident Severity in Large-Scale US Data](https://arxiv.org/abs/2601.00152)
*Yann Bellec,Rohan Kaman,Siwen Cui,Aarav Agrawal,Calvin Chen*

Main category: cs.LG

TL;DR: 研究使用XGBoost模型分析美国交通事故严重程度预测，发现时间、地理位置和天气变量是最强预测因子，但模型对极端严重程度案例预测能力有限。


<details>
  <summary>Details</summary>
Motivation: 研究旨在探索环境、时间和空间因素对美国交通事故严重程度的预测能力，为基于证据的交通管理提供支持。

Method: 使用2016-2023年50万起美国交通事故数据集，采用XGBoost分类器，通过随机搜索交叉验证优化，并使用类别加权处理类别不平衡问题。

Result: 最终模型整体准确率达78%，对多数类别（严重程度2）表现良好，精确率和召回率达87%。特征重要性分析显示时间、地理位置和天气变量是最强预测因子，但降水和能见度预测能力有限。

Conclusion: 研究为交通管理提供了实证基础，但数据集中中等严重程度事故占主导限制了模型对极端案例的学习能力，需要改进采样策略、特征工程和外部数据整合。

Abstract: This study investigates the predictive capacity of environmental, temporal, and spatial factors on traffic accident severity in the United States. Using a dataset of 500,000 U.S. traffic accidents spanning 2016-2023, we trained an XGBoost classifier optimized through randomized search cross-validation and adjusted for class imbalance via class weighting. The final model achieves an overall accuracy of 78%, with strong performance on the majority class (Severity 2), attaining 87% precision and recall. Feature importance analysis reveals that time of day, geographic location, and weather-related variables, including visibility, temperature, and wind speed, rank among the strongest predictors of accident severity. However, contrary to initial hypotheses, precipitation and visibility demonstrate limited predictive power, potentially reflecting behavioral adaptation by drivers under overtly hazardous conditions. The dataset's predominance of mid-level severity accidents constrains the model's capacity to learn meaningful patterns for extreme cases, highlighting the need for alternative sampling strategies, enhanced feature engineering, and integration of external datasets. These findings contribute to evidence-based traffic management and suggest future directions for severity prediction research.

</details>


### [31] [Avatar Forcing: Real-Time Interactive Head Avatar Generation for Natural Conversation](https://arxiv.org/abs/2601.00664)
*Taekyung Ki,Sangwon Jang,Jaehyeong Jo,Jaehong Yoon,Sung Ju Hwang*

Main category: cs.LG

TL;DR: 提出Avatar Forcing框架，通过扩散强迫建模实时用户-头像交互，实现低延迟（约500ms）的互动头像生成，比基线快6.8倍，并引入无标签偏好优化学习表达性互动。


<details>
  <summary>Details</summary>
Motivation: 当前说话头像生成模型缺乏真正的互动感，通常生成单向响应而缺乏情感参与。面临两个关键挑战：在因果约束下实时生成运动，以及在没有额外标注数据的情况下学习表达性、生动的反应。

Method: 提出Avatar Forcing框架，通过扩散强迫建模实时用户-头像交互，处理实时多模态输入（用户音频和动作）。引入直接偏好优化方法，利用通过丢弃用户条件构建的合成负样本，实现无标签的表达性互动学习。

Result: 框架实现低延迟实时交互（约500ms），比基线快6.8倍。生成的响应性和表达性头像运动在超过80%的情况下优于基线。

Conclusion: Avatar Forcing框架成功解决了实时互动头像生成的关键挑战，通过扩散强迫和标签自由学习实现了低延迟、表达性强的用户-头像交互。

Abstract: Talking head generation creates lifelike avatars from static portraits for virtual communication and content creation. However, current models do not yet convey the feeling of truly interactive communication, often generating one-way responses that lack emotional engagement. We identify two key challenges toward truly interactive avatars: generating motion in real-time under causal constraints and learning expressive, vibrant reactions without additional labeled data. To address these challenges, we propose Avatar Forcing, a new framework for interactive head avatar generation that models real-time user-avatar interactions through diffusion forcing. This design allows the avatar to process real-time multimodal inputs, including the user's audio and motion, with low latency for instant reactions to both verbal and non-verbal cues such as speech, nods, and laughter. Furthermore, we introduce a direct preference optimization method that leverages synthetic losing samples constructed by dropping user conditions, enabling label-free learning of expressive interaction. Experimental results demonstrate that our framework enables real-time interaction with low latency (approximately 500ms), achieving 6.8X speedup compared to the baseline, and produces reactive and expressive avatar motion, which is preferred over 80% against the baseline.

</details>


### [32] [Online Finetuning Decision Transformers with Pure RL Gradients](https://arxiv.org/abs/2601.00167)
*Junkai Luo,Yinglun Zhu*

Main category: cs.LG

TL;DR: 论文提出了一种基于纯强化学习梯度的在线决策变换器微调方法，解决了现有方法依赖监督序列建模目标的问题，通过改进GRPO算法实现了更稳定的训练和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 决策变换器在离线强化学习中表现出色，但扩展到在线设置时仍主要依赖监督序列建模目标。研究发现后见回报重标注这一标准组件与基于重要性采样的RL算法（如GRPO）存在根本性不兼容，导致训练不稳定，阻碍了纯RL梯度的在线微调。

Method: 提出了基于纯强化学习梯度的在线决策变换器微调算法：1）将GRPO适配到决策变换器；2）引入子轨迹优化以改进信用分配；3）使用序列级似然目标增强稳定性和效率；4）采用主动采样鼓励在不确定区域的探索。

Result: 通过大量实验证明，该方法在多个基准测试中超越了现有的在线决策变换器基线，实现了新的最先进性能，验证了纯RL梯度在线微调决策变换器的有效性。

Conclusion: 该研究成功实现了决策变换器的纯强化学习梯度在线微调，解决了后见回报重标注与重要性采样算法的不兼容问题，为序列决策模型在在线环境中的应用提供了更有效的框架。

Abstract: Decision Transformers (DTs) have emerged as a powerful framework for sequential decision making by formulating offline reinforcement learning (RL) as a sequence modeling problem. However, extending DTs to online settings with pure RL gradients remains largely unexplored, as existing approaches continue to rely heavily on supervised sequence-modeling objectives during online finetuning. We identify hindsight return relabeling -- a standard component in online DTs -- as a critical obstacle to RL-based finetuning: while beneficial for supervised learning, it is fundamentally incompatible with importance sampling-based RL algorithms such as GRPO, leading to unstable training. Building on this insight, we propose new algorithms that enable online finetuning of Decision Transformers using pure reinforcement learning gradients. We adapt GRPO to DTs and introduce several key modifications, including sub-trajectory optimization for improved credit assignment, sequence-level likelihood objectives for enhanced stability and efficiency, and active sampling to encourage exploration in uncertain regions. Through extensive experiments, we demonstrate that our methods outperform existing online DT baselines and achieve new state-of-the-art performance across multiple benchmarks, highlighting the effectiveness of pure-RL-based online finetuning for Decision Transformers.

</details>


### [33] [Sequential Reservoir Computing for Efficient High-Dimensional Spatiotemporal Forecasting](https://arxiv.org/abs/2601.00172)
*Ata Akbari Asanjan,Filip Wudarski,Daniel O'Connor,Shaun Geaney,Elena Strbac,P. Aaron Lott,Davide Venturelli*

Main category: cs.LG

TL;DR: 提出Sequential Reservoir Computing架构，通过将大储层分解为一系列小型互连储层，降低高维时空系统预测的计算成本和内存需求，同时保持长期时间依赖性。


<details>
  <summary>Details</summary>
Motivation: 传统RNN和LSTM在高维时空系统预测中存在梯度训练和内存瓶颈问题，而传统Reservoir Computing虽然通过固定循环层和凸读出优化缓解了这些问题，但在输入维度增加时仍然存在可扩展性问题。

Method: 引入Sequential Reservoir Computing架构，将大型储层分解为一系列小型互连储层，这种设计减少了内存和计算成本，同时保持了长期时间依赖性。在低维混沌系统（Lorenz63）和高维物理模拟（2D涡度和浅水方程）上进行了验证。

Result: 相比LSTM和标准RNN基线，Sequential RC实现了15-25%更长的有效预测范围，20-30%更低的误差指标（SSIM、RMSE），以及高达三个数量级的更低训练成本。

Conclusion: Sequential RC保持了传统RC的简单性和效率，同时在高维动态系统中实现了更优的可扩展性，为科学和工程应用中的实时、节能预测提供了实用路径。

Abstract: Forecasting high-dimensional spatiotemporal systems remains computationally challenging for recurrent neural networks (RNNs) and long short-term memory (LSTM) models due to gradient-based training and memory bottlenecks. Reservoir Computing (RC) mitigates these challenges by replacing backpropagation with fixed recurrent layers and a convex readout optimization, yet conventional RC architectures still scale poorly with input dimensionality. We introduce a Sequential Reservoir Computing (Sequential RC) architecture that decomposes a large reservoir into a series of smaller, interconnected reservoirs. This design reduces memory and computational costs while preserving long-term temporal dependencies. Using both low-dimensional chaotic systems (Lorenz63) and high-dimensional physical simulations (2D vorticity and shallow-water equations), Sequential RC achieves 15-25% longer valid forecast horizons, 20-30% lower error metrics (SSIM, RMSE), and up to three orders of magnitude lower training cost compared to LSTM and standard RNN baselines. The results demonstrate that Sequential RC maintains the simplicity and efficiency of conventional RC while achieving superior scalability for high-dimensional dynamical systems. This approach provides a practical path toward real-time, energy-efficient forecasting in scientific and engineering applications.

</details>


### [34] [Early Prediction of Liver Cirrhosis Up to Three Years in Advance: A Machine Learning Study Benchmarking Against the FIB-4 Score](https://arxiv.org/abs/2601.00175)
*Zhuqi Miao,Sujan Ravi,Abdulaziz Ahmed*

Main category: cs.LG

TL;DR: 基于电子健康记录的机器学习模型在预测肝硬化方面显著优于传统FIB-4评分，能提前1-3年进行更准确的风险分层。


<details>
  <summary>Details</summary>
Motivation: 开发并评估利用常规电子健康记录数据预测肝硬化发病的机器学习模型，并与传统FIB-4评分进行性能比较，旨在实现更早期的肝硬化风险预测和预防管理。

Method: 采用回顾性队列研究，从大型学术医疗系统获取去标识化电子健康记录数据。识别脂肪肝患者并根据ICD-9/10编码分为肝硬化和非肝硬化队列。构建预测场景，使用观察窗口和预测窗口模拟真实临床使用。从观察窗口汇总人口统计学、诊断、实验室结果、生命体征和共病指数。训练XGBoost模型用于1年、2年和3年预测时间点，并在保留测试集上评估性能，使用AUC与FIB-4进行比较。

Result: 最终队列包括1年预测3,043名患者，2年预测1,981名患者，3年预测1,470名患者。在所有预测时间窗口中，机器学习模型始终优于FIB-4。XGBoost模型在1年、2年和3年预测中的AUC分别为0.81、0.73和0.69，而FIB-4的AUC分别为0.71、0.63和0.57。随着预测时间延长，性能优势持续存在，表明早期风险区分能力更强。

Conclusion: 利用常规电子健康记录数据的机器学习模型在早期肝硬化预测方面显著优于传统FIB-4评分。这些模型能够实现更早期、更准确的风险分层，可以作为自动化决策支持工具集成到临床工作流程中，支持主动的肝硬化预防和管理。

Abstract: Objective: Develop and evaluate machine learning (ML) models for predicting incident liver cirrhosis one, two, and three years prior to diagnosis using routinely collected electronic health record (EHR) data, and to benchmark their performance against the FIB-4 score. Methods: We conducted a retrospective cohort study using de-identified EHR data from a large academic health system. Patients with fatty liver disease were identified and categorized into cirrhosis and non-cirrhosis cohorts based on ICD-9/10 codes. Prediction scenarios were constructed using observation and prediction windows to emulate real-world clinical use. Demographics, diagnoses, laboratory results, vital signs, and comorbidity indices were aggregated from the observation window. XGBoost models were trained for 1-, 2-, and 3-year prediction horizons and evaluated on held-out test sets. Model performance was compared with FIB-4 using area under the receiver operating characteristic curve (AUC). Results: Final cohorts included 3,043 patients for the 1-year prediction, 1,981 for the 2-year prediction, and 1,470 for the 3-year prediction. Across all prediction windows, ML models consistently outperformed FIB-4. The XGBoost models achieved AUCs of 0.81, 0.73, and 0.69 for 1-, 2-, and 3-year predictions, respectively, compared with 0.71, 0.63, and 0.57 for FIB-4. Performance gains persisted with longer prediction horizons, indicating improved early risk discrimination. Conclusions: Machine learning models leveraging routine EHR data substantially outperform the traditional FIB-4 score for early prediction of liver cirrhosis. These models enable earlier and more accurate risk stratification and can be integrated into clinical workflows as automated decision-support tools to support proactive cirrhosis prevention and management.

</details>


### [35] [Reinforcement-Learned Unequal Error Protection for Quantized Semantic Embeddings](https://arxiv.org/abs/2601.00186)
*Moirangthem Tiken Singh,Adnan Arif*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应重复编码框架，实现按维度不等错误保护，在有限带宽下提升语义通信质量


<details>
  <summary>Details</summary>
Motivation: 解决带宽受限通信系统中语义信息保持的挑战，传统信道编码（如LDPC或Reed-Solomon）无法实现细粒度语义保护

Method: 基于强化学习的自适应重复编码框架，使用复合语义失真度量（平衡全局嵌入相似性和实体级保持），实现上下文感知的保护分配

Result: 相比均匀保护，在1 dB SNR下获得6.8%更高的chrF分数和9.3%更好的实体保持，统计显著

Conclusion: 代码结构必须与语义粒度对齐，智能分配的简单重复编码能实现细粒度语义保护，适用于边缘计算和物联网场景

Abstract: This paper tackles the pressing challenge of preserving semantic meaning in communication systems constrained by limited bandwidth. We introduce a novel reinforcement learning framework that achieves per-dimension unequal error protection via adaptive repetition coding. Central to our approach is a composite semantic distortion metric that balances global embedding similarity with entity-level preservation, empowering the reinforcement learning agent to allocate protection in a context-aware manner. Experiments show statistically significant gains over uniform protection, achieving 6.8% higher chrF scores and 9.3% better entity preservation at 1 dB SNR. The key innovation of our framework is the demonstration that simple, intelligently allocated repetition coding enables fine-grained semantic protection -- an advantage unattainable with conventional codes such as LDPC or Reed-Solomon. Our findings challenge traditional channel coding paradigms by establishing that code structure must align with semantic granularity. This approach is particularly suited to edge computing and IoT scenarios, where bandwidth is scarce, but semantic fidelity is critical, providing a practical pathway for next-generation semantic-aware networks.

</details>


### [36] [SSI-GAN: Semi-Supervised Swin-Inspired Generative Adversarial Networks for Neuronal Spike Classification](https://arxiv.org/abs/2601.00189)
*Danial Sharifrazi,Nouman Javed,Mojtaba Mohammadi,Seyede Sana Salehi,Roohallah Alizadehsani,Prasad N. Paradkar,U. Rajendra Acharya,Asim Bhatti*

Main category: cs.LG

TL;DR: 提出SSI-GAN半监督学习架构，仅需1-3%标注数据即可实现蚊子神经元尖峰信号的高精度分类，用于检测寨卡病毒、登革热病毒感染，大幅减少人工标注工作量。


<details>
  <summary>Details</summary>
Motivation: 蚊子是虫媒病毒疾病的主要传播媒介，手动分类其神经元尖峰模式非常耗时且昂贵。现有深度学习解决方案需要完全标注的数据集和高度预处理的神经元信号，这在实际野外场景中难以大规模应用。为了解决标注数据稀缺的问题，需要开发更高效的半监督学习方法。

Method: 提出半监督Swin启发式GAN（SSI-GAN）架构，采用基于Transformer的生成器和Swin启发的移位窗口判别器。使用多头自注意力模型在平坦的、基于窗口的Transformer判别器中学习捕捉稀疏的高频尖峰特征。仅使用1-3%的标注数据，训练超过1500万个尖峰样本，分类为寨卡病毒感染、登革热病毒感染或未感染类别。使用贝叶斯Optuna框架优化超参数，并通过五折蒙特卡洛交叉验证验证鲁棒性。

Result: SSI-GAN在感染后第三天仅使用3%标注数据达到99.93%的分类准确率。在仅1%监督的情况下，在所有感染阶段都保持高准确率。相比标准监督方法，在相同性能水平下减少了97-99%的人工标注工作量。提出的移位窗口Transformer设计大幅超越所有基线方法，在基于尖峰的神经元感染分类中创下新的最佳记录。

Conclusion: SSI-GAN架构通过半监督学习方法有效解决了蚊子神经元尖峰信号分类中标注数据稀缺的问题，显著减少了人工标注工作量，为实际野外场景中的大规模应用提供了可行方案，在虫媒病毒疾病监测方面具有重要应用价值。

Abstract: Mosquitos are the main transmissive agents of arboviral diseases. Manual classification of their neuronal spike patterns is very labor-intensive and expensive. Most available deep learning solutions require fully labeled spike datasets and highly preprocessed neuronal signals. This reduces the feasibility of mass adoption in actual field scenarios. To address the scarcity of labeled data problems, we propose a new Generative Adversarial Network (GAN) architecture that we call the Semi-supervised Swin-Inspired GAN (SSI-GAN). The Swin-inspired, shifted-window discriminator, together with a transformer-based generator, is used to classify neuronal spike trains and, consequently, detect viral neurotropism. We use a multi-head self-attention model in a flat, window-based transformer discriminator that learns to capture sparser high-frequency spike features. Using just 1 to 3% labeled data, SSI-GAN was trained with more than 15 million spike samples collected at five-time post-infection and recording classification into Zika-infected, dengue-infected, or uninfected categories. Hyperparameters were optimized using the Bayesian Optuna framework, and performance for robustness was validated under fivefold Monte Carlo cross-validation. SSI-GAN reached 99.93% classification accuracy on the third day post-infection with only 3% labeled data. It maintained high accuracy across all stages of infection with just 1% supervision. This shows a 97-99% reduction in manual labeling effort relative to standard supervised approaches at the same performance level. The shifted-window transformer design proposed here beat all baselines by a wide margin and set new best marks in spike-based neuronal infection classification.

</details>


### [37] [Optimized Hybrid Feature Engineering for Resource-Efficient Arrhythmia Detection in ECG Signals: An Optimization Framework](https://arxiv.org/abs/2601.00192)
*Moirangthem Tiken Singh,Manibhushan Yaikhom*

Main category: cs.LG

TL;DR: 提出一种面向资源受限边缘设备的轻量级心律失常诊断框架，通过特征工程而非复杂模型实现高效分类


<details>
  <summary>Details</summary>
Motivation: 心血管疾病特别是心律失常是全球主要死因，需要IoMT持续监测，但现有深度学习方法计算开销过大，不适合资源受限的边缘设备

Method: 采用数据中心的特征工程方法，结合时频小波分解和图论结构描述符（如PageRank中心性），通过互信息和递归消除优化特征空间，使用超轻量级线性分类器

Result: 在MIT-BIH和INCART数据集上达到98.44%诊断准确率，模型大小仅8.54KB，分类推理延迟0.46μs，每搏处理管道52ms，相比压缩模型KD-Light（25KB，96.32%准确率）有数量级效率提升

Conclusion: 该资源高效框架通过优化特征工程使复杂心律失常数据线性可分，为无电池心脏传感器提供了实时、轻量级的解决方案

Abstract: Cardiovascular diseases, particularly arrhythmias, remain a leading global cause of mortality, necessitating continuous monitoring via the Internet of Medical Things (IoMT). However, state-of-the-art deep learning approaches often impose prohibitive computational overheads, rendering them unsuitable for resource-constrained edge devices. This study proposes a resource-efficient, data-centric framework that prioritizes feature engineering over complexity. Our optimized pipeline makes the complex, high-dimensional arrhythmia data linearly separable. This is achieved by integrating time-frequency wavelet decompositions with graph-theoretic structural descriptors, such as PageRank centrality. This hybrid feature space, combining wavelet decompositions and graph-theoretic descriptors, is then refined using mutual information and recursive elimination, enabling interpretable, ultra-lightweight linear classifiers. Validation on the MIT-BIH and INCART datasets yields 98.44% diagnostic accuracy with an 8.54 KB model footprint. The system achieves 0.46 $μ$s classification inference latency within a 52 ms per-beat pipeline, ensuring real-time operation. These outcomes provide an order-of-magnitude efficiency gain over compressed models, such as KD-Light (25 KB, 96.32% accuracy), advancing battery-less cardiac sensors.

</details>


### [38] [Can Optimal Transport Improve Federated Inverse Reinforcement Learning?](https://arxiv.org/abs/2601.00309)
*David Millard,Ali Baheri*

Main category: cs.LG

TL;DR: 提出基于最优传输的联邦逆强化学习方法，通过Wasserstein重心融合异构智能体的本地奖励函数，获得比传统参数平均更准确的全局奖励估计。


<details>
  <summary>Details</summary>
Motivation: 在机器人多智能体系统中，智能体常在微妙不同的环境中运行，追求共同的高级目标。直接共享数据学习共享奖励函数通常不切实际，因为存在动态差异、隐私约束和通信带宽限制。

Method: 每个客户端首先在本地执行轻量级最大熵逆强化学习，遵守其计算和隐私限制。然后通过Wasserstein重心融合得到的奖励函数，考虑其底层几何结构。

Result: 证明了这种重心融合比联邦学习中传统的参数平均方法能产生更准确的全局奖励估计。

Conclusion: 为异构智能体和环境提供了一个原则性且通信高效的框架，用于推导可泛化的共享奖励函数。

Abstract: In robotics and multi-agent systems, fleets of autonomous agents often operate in subtly different environments while pursuing a common high-level objective. Directly pooling their data to learn a shared reward function is typically impractical due to differences in dynamics, privacy constraints, and limited communication bandwidth. This paper introduces an optimal transport-based approach to federated inverse reinforcement learning (IRL). Each client first performs lightweight Maximum Entropy IRL locally, adhering to its computational and privacy limitations. The resulting reward functions are then fused via a Wasserstein barycenter, which considers their underlying geometric structure. We further prove that this barycentric fusion yields a more faithful global reward estimate than conventional parameter averaging methods in federated learning. Overall, this work provides a principled and communication-efficient framework for deriving a shared reward that generalizes across heterogeneous agents and environments.

</details>


### [39] [Quantum King-Ring Domination in Chess: A QAOA Approach](https://arxiv.org/abs/2601.00318)
*Gerhard Stenzel,Michael Kölle,Tobias Rohe,Julian Hager,Leo Sünkel,Maximilian Zorn,Claudia Linnhoff-Popien*

Main category: cs.LG

TL;DR: 该论文提出了一个基于国际象棋战术的量子基准测试QKRD，用于评估QAOA算法在结构化问题上的表现，发现约束保持混合器、预热策略等设计选择显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前QAOA算法主要在MaxCut、TSP、SAT等随机合成实例上进行基准测试，这些实例缺乏语义结构和人类可解释性，无法反映真实世界问题的性能。需要结构化、可解释的基准来评估QAOA在实际问题上的表现。

Method: 引入Quantum King-Ring Domination (QKRD)基准，基于国际象棋战术位置构建，包含5,000个结构化实例，具有one-hot约束、空间局部性和10-40量子比特规模。该基准结合人类可解释的覆盖度指标和内在验证机制。

Result: 约束保持混合器(XY、domain-wall)比标准混合器收敛快约13步；预热策略减少45步收敛时间，能量改进超过d=8；CVaR优化产生负面结果，能量更差且无覆盖度优势。QAOA比贪婪启发式算法性能高12.6%，比随机选择高80.1%。

Conclusion: 结构化基准能够揭示问题感知的QAOA技术优势，这些优势在随机实例中被掩盖。QKRD为可重复的NISQ算法研究提供了代码、数据和实验工件。

Abstract: The Quantum Approximate Optimization Algorithm (QAOA) is extensively benchmarked on synthetic random instances such as MaxCut, TSP, and SAT problems, but these lack semantic structure and human interpretability, offering limited insight into performance on real-world problems with meaningful constraints. We introduce Quantum King-Ring Domination (QKRD), a NISQ-scale benchmark derived from chess tactical positions that provides 5,000 structured instances with one-hot constraints, spatial locality, and 10--40 qubit scale. The benchmark pairs human-interpretable coverage metrics with intrinsic validation against classical heuristics, enabling algorithmic conclusions without external oracles. Using QKRD, we systematically evaluate QAOA design choices and find that constraint-preserving mixers (XY, domain-wall) converge approximately 13 steps faster than standard mixers (p<10^{-7}, d\approx0.5) while eliminating penalty tuning, warm-start strategies reduce convergence by 45 steps (p<10^{-127}, d=3.35) with energy improvements exceeding d=8, and Conditional Value-at-Risk (CVaR) optimization yields an informative negative result with worse energy (p<10^{-40}, d=1.21) and no coverage benefit. Intrinsic validation shows QAOA outperforms greedy heuristics by 12.6\% and random selection by 80.1\%. Our results demonstrate that structured benchmarks reveal advantages of problem-informed QAOA techniques obscured in random instances. We release all code, data, and experimental artifacts for reproducible NISQ algorithm research.

</details>


### [40] [E-GRPO: High Entropy Steps Drive Effective Reinforcement Learning for Flow Models](https://arxiv.org/abs/2601.00423)
*Shengjun Zhang,Zhang Zhang,Chensheng Dai,Yueqi Duan*

Main category: cs.LG

TL;DR: 提出E-GRPO方法，通过熵感知的组相对策略优化来增强流匹配模型的人类偏好对齐，通过合并低熵步骤形成高熵SDE采样步骤，提高探索效率


<details>
  <summary>Details</summary>
Motivation: 现有方法在多步去噪优化中存在稀疏和模糊的奖励信号问题，高熵步骤能实现更高效探索而低熵步骤导致无区别的轨迹

Method: 提出E-GRPO方法：1)合并连续低熵步骤形成高熵SDE采样步骤，其他步骤使用ODE采样；2)引入多步组归一化优势函数，在共享相同合并SDE去噪步骤的样本内计算组相对优势

Result: 在不同奖励设置下的实验结果证明了该方法的有效性

Conclusion: 通过熵感知的组相对策略优化，能够有效解决流匹配模型中人类偏好对齐的探索效率问题，提高奖励信号的清晰度和训练效果

Abstract: Recent reinforcement learning has enhanced the flow matching models on human preference alignment. While stochastic sampling enables the exploration of denoising directions, existing methods which optimize over multiple denoising steps suffer from sparse and ambiguous reward signals. We observe that the high entropy steps enable more efficient and effective exploration while the low entropy steps result in undistinguished roll-outs. To this end, we propose E-GRPO, an entropy aware Group Relative Policy Optimization to increase the entropy of SDE sampling steps. Since the integration of stochastic differential equations suffer from ambiguous reward signals due to stochasticity from multiple steps, we specifically merge consecutive low entropy steps to formulate one high entropy step for SDE sampling, while applying ODE sampling on other steps. Building upon this, we introduce multi-step group normalized advantage, which computes group-relative advantages within samples sharing the same consolidated SDE denoising step. Experimental results on different reward settings have demonstrated the effectiveness of our methods.

</details>


### [41] [Controllable Concept Bottleneck Models](https://arxiv.org/abs/2601.00451)
*Hongbin Lin,Chenyang Ren,Juangui Xu,Zhengyu Hu,Cheng-Long Wang,Yao Shu,Hui Xiong,Jingfeng Zhang,Di Wang,Lijie Hu*

Main category: cs.LG

TL;DR: 提出可控概念瓶颈模型（CCBMs），支持概念标签级、概念级和数据级三种粒度的模型编辑，无需重新训练即可实现高效编辑。


<details>
  <summary>Details</summary>
Motivation: 现有概念瓶颈模型主要关注静态场景，而实际应用中需要持续维护：删除错误或敏感数据（遗忘）、修正错误标注概念、纳入新样本（增量学习）以适应动态环境，但缺乏高效的可编辑CBMs方法。

Method: 提出可控概念瓶颈模型（CCBMs），基于影响函数推导出数学上严格的闭式近似解，支持三种粒度编辑：概念标签级、概念级和数据级（包括数据删除和添加），无需重新训练。

Result: 实验结果表明CCBMs具有高效性和适应性，验证了其在实现动态可信赖CBMs方面的实用价值。

Conclusion: CCBMs通过数学严谨的闭式近似解决了CBMs在动态环境中的编辑挑战，为大规模应用中的高效模型维护提供了可行方案。

Abstract: Concept Bottleneck Models (CBMs) have garnered much attention for their ability to elucidate the prediction process through a human-understandable concept layer. However, most previous studies focused on static scenarios where the data and concepts are assumed to be fixed and clean. In real-world applications, deployed models require continuous maintenance: we often need to remove erroneous or sensitive data (unlearning), correct mislabeled concepts, or incorporate newly acquired samples (incremental learning) to adapt to evolving environments. Thus, deriving efficient editable CBMs without retraining from scratch remains a significant challenge, particularly in large-scale applications. To address these challenges, we propose Controllable Concept Bottleneck Models (CCBMs). Specifically, CCBMs support three granularities of model editing: concept-label-level, concept-level, and data-level, the latter of which encompasses both data removal and data addition. CCBMs enjoy mathematically rigorous closed-form approximations derived from influence functions that obviate the need for retraining. Experimental results demonstrate the efficiency and adaptability of our CCBMs, affirming their practical value in enabling dynamic and trustworthy CBMs.

</details>


### [42] [Geometric Regularization in Mixture-of-Experts: The Disconnect Between Weights and Activations](https://arxiv.org/abs/2601.00457)
*Hyunjun Kim*

Main category: cs.LG

TL;DR: 正交性损失在MoE模型中无法有效促进专家多样性，反而增加权重空间重叠，对性能影响不一致且不可靠


<details>
  <summary>Details</summary>
Motivation: 研究几何正则化在MoE模型专家专业化中的作用，特别是正交性损失是否能有效促进专家多样性

Method: 在MoE模型中应用正交性损失，通过7种不同的正则化强度进行实验，分析权重空间重叠和激活空间重叠的变化

Result: 正交性损失未能减少权重空间重叠（MSO增加达114%），激活空间重叠保持高位（约0.6），性能影响不一致且高度可变

Conclusion: 权重空间正则化既未实现其几何目标，也未可靠改善性能，不适合用于MoE多样性

Abstract: Mixture-of-Experts (MoE) models achieve efficiency through sparse activation, but the role of geometric regularization in expert specialization remains unclear. We apply orthogonality loss to enforce expert diversity and find it fails on multiple fronts: it does not reduce weight-space overlap (MSO actually increases by up to 114%), activation-space overlap remains high (~0.6) regardless of regularization, and effects on performance are inconsistent -- marginal improvement on WikiText-103 (-0.9%), slight degradation on TinyStories (+0.9%), and highly variable results on PTB (std > 1.0). Our analysis across 7 regularization strengths reveals no significant correlation (r = -0.293, p = 0.523) between weight and activation orthogonality. These findings demonstrate that weight-space regularization neither achieves its geometric goal nor reliably improves performance, making it unsuitable for MoE diversity.

</details>


### [43] [Detecting Spike Wave Discharges (SWD) using 1-dimensional Residual UNet](https://arxiv.org/abs/2601.00459)
*Saurav Sengupta,Scott Kilianski,Suchetha Sharma,Sakina Lashkeri,Ashley McHugh,Mark Beenhakker,Donald E. Brown*

Main category: cs.LG

TL;DR: 本文提出了一种基于1D UNet的数据增强方法（AugUNet1D），用于自动标记脑电图中的棘慢波放电（SWD），相比传统机器学习方法和"Twin Peaks"算法，在961小时小鼠EEG数据上表现更优。


<details>
  <summary>Details</summary>
Motivation: 脑电图（EEG）记录中事件的手动标记非常耗时，特别是对于持续数周至数月的连续记录。棘慢波放电（SWD）作为失神发作的电生理标志，通常需要手动标记。虽然已有研究使用机器学习自动分割和分类EEG信号，但仍有改进空间。

Method: 1. 在961小时C3H/HeJ小鼠EEG记录（包含22,637个标记的SWD）上比较14种机器学习分类器性能；2. 发现1D UNet表现最佳；3. 通过数据增强改进1D UNet，发现缩放增强效果最好；4. 将增强后的AugUNet1D与最近发表的"Twin Peaks"时频算法方法进行比较。

Result: 1D UNet在该数据集上表现最佳；数据增强显著提升性能，其中缩放增强效果最明显；AugUNet1D相比"Twin Peaks"算法表现出更优性能，检测到的事件特征与手动标记的SWD更相似。

Conclusion: AugUNet1D是一种有效的自动标记EEG中SWD的方法，性能优于现有方法。作者公开了在手动注释数据上预训练或未训练的AugUNet1D模型，供其他研究者使用。

Abstract: The manual labeling of events in electroencephalography (EEG) records is time-consuming. This is especially true when EEG recordings are taken continuously over weeks to months. Therefore, a method to automatically label pertinent EEG events reduces the manual workload. Spike wave discharges (SWD), which are the electrographic hallmark of absence seizures, are EEG events that are often labeled manually. While some previous studies have utilized machine learning to automatically segment and classify EEG signals like SWDs, they can be improved. Here we compare the performance of 14 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs. We find that a 1D UNet performs best for labeling SWDs in this dataset. We also improve the 1D UNet by augmenting our training data and determine that scaling showed the greatest benefit of all augmentation procedures applied. We then compare the 1D UNet with data augmentation, AugUNet1D, against a recently published time- and frequency-based algorithmic approach called "Twin Peaks". AugUNet1D showed superior performance and detected events with more similar features to the SWDs labeled manually. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for others users.

</details>


### [44] [Laplacian Kernelized Bandit](https://arxiv.org/abs/2601.00461)
*Shuang Wu,Arash A. Amini*

Main category: cs.LG

TL;DR: 提出了一种基于图结构的多用户上下文赌博机方法，将图拉普拉斯正则化与核化赌博机相结合，通过统一的多用户RKHS核实现结构化探索。


<details>
  <summary>Details</summary>
Motivation: 研究多用户上下文赌博机问题，其中用户通过图结构关联，且奖励函数具有非线性特性和图同质性。现有方法在处理非线性关系和图结构信息方面存在局限。

Method: 引入一个原则性的联合惩罚项，结合基于RKHS距离的图平滑项和个体粗糙度惩罚。证明该惩罚等价于单一多用户RKHS中的平方范数，显式推导其再生核，将图拉普拉斯与基础臂核优雅融合。提出LK-GP-UCB和LK-GP-TS算法，利用高斯过程后验进行探索。

Result: 提供了高概率遗憾界，其缩放与多用户核的有效维度相关，取代了对用户数量或环境维度的依赖。实验表明，在非线性设置中优于强线性和非图感知基线，即使在真实奖励为线性时也保持竞争力。

Conclusion: 提出了一个统一、理论坚实且实用的框架，将拉普拉斯正则化与核化赌博机相结合，用于结构化探索，为图结构多用户上下文学习提供了系统方法。

Abstract: We study multi-user contextual bandits where users are related by a graph and their reward functions exhibit both non-linear behavior and graph homophily. We introduce a principled joint penalty for the collection of user reward functions $\{f_u\}$, combining a graph smoothness term based on RKHS distances with an individual roughness penalty. Our central contribution is proving that this penalty is equivalent to the squared norm within a single, unified \emph{multi-user RKHS}. We explicitly derive its reproducing kernel, which elegantly fuses the graph Laplacian with the base arm kernel. This unification allows us to reframe the problem as learning a single ''lifted'' function, enabling the design of principled algorithms, \texttt{LK-GP-UCB} and \texttt{LK-GP-TS}, that leverage Gaussian Process posteriors over this new kernel for exploration. We provide high-probability regret bounds that scale with an \emph{effective dimension} of the multi-user kernel, replacing dependencies on user count or ambient dimension. Empirically, our methods outperform strong linear and non-graph-aware baselines in non-linear settings and remain competitive even when the true rewards are linear. Our work delivers a unified, theoretically grounded, and practical framework that bridges Laplacian regularization with kernelized bandits for structured exploration.

</details>


### [45] [Trajectory Guard -- A Lightweight, Sequence-Aware Model for Real-Time Anomaly Detection in Agentic AI](https://arxiv.org/abs/2601.00516)
*Laksh Advani*

Main category: cs.LG

TL;DR: Trajectory Guard：一种用于检测LLM智能体多步行动计划异常的Siamese循环自编码器，通过混合损失函数联合学习任务-轨迹对齐和序列有效性，在保持高检测性能的同时实现实时推理。


<details>
  <summary>Details</summary>
Motivation: 现有异常检测方法不适合LLM智能体多步行动计划检测：均值池化嵌入会稀释异常步骤，而仅对比方法忽略序列结构。标准无监督方法在预训练嵌入上F1分数不超过0.69，需要更有效的检测方法。

Method: 提出Trajectory Guard，一种Siamese循环自编码器，采用混合损失函数联合学习：1）通过对比学习实现任务-轨迹对齐，2）通过重构学习序列有效性。这种双重目标能够统一检测"错误的任务计划"和"畸形的计划结构"。

Result: 在合成扰动和真实世界失败案例（RAS-Eval安全审计和Who&When多智能体系统）的基准测试中，在平衡数据集上获得0.88-0.94的F1分数，在不平衡外部基准上获得0.86-0.92的召回率。推理延迟仅32毫秒，比LLM Judge基线快17-27倍。

Conclusion: Trajectory Guard能够有效检测LLM智能体多步行动计划的异常，包括任务不匹配和结构畸形问题，在保持高检测性能的同时实现实时推理，适用于生产部署中的安全验证。

Abstract: Autonomous LLM agents generate multi-step action plans that can fail due to contextual misalignment or structural incoherence. Existing anomaly detection methods are ill-suited for this challenge: mean-pooling embeddings dilutes anomalous steps, while contrastive-only approaches ignore sequential structure. Standard unsupervised methods on pre-trained embeddings achieve F1-scores no higher than 0.69. We introduce Trajectory Guard, a Siamese Recurrent Autoencoder with a hybrid loss function that jointly learns task-trajectory alignment via contrastive learning and sequential validity via reconstruction. This dual objective enables unified detection of both "wrong plan for this task" and "malformed plan structure." On benchmarks spanning synthetic perturbations and real-world failures from security audits (RAS-Eval) and multi-agent systems (Who\&When), we achieve F1-scores of 0.88-0.94 on balanced sets and recall of 0.86-0.92 on imbalanced external benchmarks. At 32 ms inference latency, our approach runs 17-27$\times$ faster than LLM Judge baselines, enabling real-time safety verification in production deployments.

</details>


### [46] [HFedMoE: Resource-aware Heterogeneous Federated Learning with Mixture-of-Experts](https://arxiv.org/abs/2601.00583)
*Zihan Fang,Zheng Lin,Senkang Hu,Yanan Ma,Yihang Tao,Yiqin Deng,Xianhao Chen,Yuguang Fang*

Main category: cs.LG

TL;DR: HFedMoE是一个面向异构客户端的MoE联邦学习框架，通过专家重要性评估、自适应专家选择和稀疏感知聚合，实现资源受限设备上的高效LLM微调。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习中微调大语言模型面临三个主要挑战：1) 缺乏可靠指标评估专家对本地微调性能的影响；2) 客户端异构计算资源限制MoE模型训练；3) 客户端特定的专家子集和路由偏好破坏全局聚合。

Method: 提出HFedMoE框架：1) 基于专家对微调性能的贡献评估专家重要性；2) 从信息瓶颈角度自适应选择专家子集以匹配客户端计算预算；3) 设计稀疏感知模型聚合策略，加权聚合活跃微调的专家和门控参数。

Result: 大量实验表明，HFedMoE在训练准确率和收敛速度方面优于现有最先进的基准方法。

Conclusion: HFedMoE通过定制化专家选择和稀疏感知聚合，有效解决了MoE在联邦学习中的异构计算挑战，实现了资源受限设备上的高效LLM微调。

Abstract: While federated learning (FL) enables fine-tuning of large language models (LLMs) without compromising data privacy, the substantial size of an LLM renders on-device training impractical for resource-constrained clients, such as mobile devices. Thus, Mixture-of-Experts (MoE) models have emerged as a computation-efficient solution, which activates only a sparse subset of experts during model training to reduce computing burden without sacrificing performance. Though integrating MoE into FL fine-tuning holds significant potential, it still encounters three key challenges: i) selecting appropriate experts for clients remains challenging due to the lack of a reliable metric to measure each expert's impact on local fine-tuning performance, ii) the heterogeneous computing resources across clients severely hinder MoE-based LLM fine-tuning, as dynamic expert activations across diverse input samples can overwhelm resource-constrained devices, and iii) client-specific expert subsets and routing preference undermine global aggregation, where misaligned expert updates and inconsistent gating networks in troduce destructive interference. To address these challenges, we propose HFedMoE, a heterogeneous MoE-based FL fine-tuning framework that customizes a subset of experts to each client for computation-efficient LLM fine-tuning. Specifically, HFedMoE identifies the expert importance based on its contributions to fine-tuning performance, and then adaptively selects a subset of experts from an information bottleneck perspective to align with each client' s computing budget. A sparsity-aware model aggregation strategy is also designed to aggregate the actively fine-tuned experts and gating parameters with importance weighted contributions. Extensive experiments demonstrate that HFedMoE outperforms state-of-the-art benchmarks in training accuracy and convergence speed.

</details>


### [47] [Federated Customization of Large Models: Approaches, Experiments, and Insights](https://arxiv.org/abs/2601.00526)
*Yuchuan Ye,Ming Ding,Youjia Chen,Peng Cheng,Dusit Niyato*

Main category: cs.LG

TL;DR: 本文探讨了联邦学习框架下大模型的定制化问题，回顾了多种定制技术，并首次在联邦学习中实验了前缀调优方法，验证了其可行性。


<details>
  <summary>Details</summary>
Motivation: 探索在联邦学习框架下如何实现大模型的定制化，解决联邦学习中模型个性化面临的挑战，特别是如何在保护数据隐私的同时实现有效的模型定制。

Method: 回顾了全微调、高效微调、提示工程、前缀调优、知识蒸馏和检索增强生成等大模型定制技术；重点研究了如何在联邦学习框架下实现这些技术；特别进行了联邦前缀调优的实验，这是首次将前缀调优应用于联邦学习环境。

Result: 联邦前缀调优实验验证了该方法的可行性，性能接近集中式方法；与其他三种联邦定制方法相比，表现出竞争性的性能、令人满意的效率和一致的鲁棒性。

Conclusion: 联邦学习框架下的大模型定制是可行的，前缀调优等方法在联邦环境中具有应用潜力，能够实现接近集中式方法的性能，同时保护数据隐私。

Abstract: In this article, we explore federated customization of large models and highlight the key challenges it poses within the federated learning framework. We review several popular large model customization techniques, including full fine-tuning, efficient fine-tuning, prompt engineering, prefix-tuning, knowledge distillation, and retrieval-augmented generation. Then, we discuss how these techniques can be implemented within the federated learning framework. Moreover, we conduct experiments on federated prefix-tuning, which, to the best of our knowledge, is the first trial to apply prefix-tuning in the federated learning setting. The conducted experiments validate its feasibility with performance close to centralized approaches. Further comparison with three other federated customization methods demonstrated its competitive performance, satisfactory efficiency, and consistent robustness.

</details>


### [48] [Stronger Approximation Guarantees for Non-Monotone γ-Weakly DR-Submodular Maximization](https://arxiv.org/abs/2601.00611)
*Hareshkumar Jadav,Ranveer Singh,Vaneet Aggarwal*

Main category: cs.LG

TL;DR: 该论文提出了一种用于在向下封闭凸体上最大化非负、非单调γ-弱DR-子模函数的近似算法，其保证随γ平滑变化，在γ=1时恢复0.401近似比。


<details>
  <summary>Details</summary>
Motivation: 在机器学习和优化中，约束下的子模目标最大化是一个基本问题。现有研究主要关注单调DR-子模函数，但对于非单调的γ-弱DR-子模函数在向下封闭凸体上的最大化问题，需要更有效的算法和更好的近似保证。

Method: 结合Frank-Wolfe引导的连续贪婪框架与γ感知的双贪婪步骤，构建了一个简单而有效的处理非单调性的算法。这种方法能够优雅地处理γ-弱DR-子模函数的特性。

Result: 算法提供了随γ平滑变化的近似保证：当γ=1（DR-子模情况）时恢复0.401近似因子，对于γ<1的情况，保证优雅地退化，并且在相同约束下改进了先前报道的γ-弱DR-子模最大化边界。

Conclusion: 该研究为在向下封闭凸体上最大化非单调γ-弱DR-子模函数提供了最先进的保证，通过结合连续贪婪框架和双贪婪步骤，实现了简单而有效的算法设计。

Abstract: Maximizing submodular objectives under constraints is a fundamental problem in machine learning and optimization. We study the maximization of a nonnegative, non-monotone $γ$-weakly DR-submodular function over a down-closed convex body. Our main result is an approximation algorithm whose guarantee depends smoothly on $γ$; in particular, when $γ=1$ (the DR-submodular case) our bound recovers the $0.401$ approximation factor, while for $γ<1$ the guarantee degrades gracefully and, it improves upon previously reported bounds for $γ$-weakly DR-submodular maximization under the same constraints. Our approach combines a Frank-Wolfe-guided continuous-greedy framework with a $γ$-aware double-greedy step, yielding a simple yet effective procedure for handling non-monotonicity. This results in state-of-the-art guarantees for non-monotone $γ$-weakly DR-submodular maximization over down-closed convex bodies.

</details>


### [49] [Cloud-Native Generative AI for Automated Planogram Synthesis: A Diffusion Model Approach for Multi-Store Retail Optimization](https://arxiv.org/abs/2601.00527)
*Ravi Teja Pagidoju,Shriya Agarwal*

Main category: cs.LG

TL;DR: 本文提出基于扩散模型的云原生架构，自动生成店铺专用货架图，将设计时间从30小时减少到0.5小时，降低98.3%


<details>
  <summary>Details</summary>
Motivation: 传统货架图设计耗时巨大，每个复杂布局平均需要30小时，零售业需要更高效的自动化解决方案来优化空间布局

Method: 采用云原生架构，结合AWS云训练和边缘部署，使用扩散模型学习多个零售点的成功货架布局，通过改进损失函数整合零售特定约束

Result: 系统将货架图设计时间减少98.3%（30小时降至0.5小时），约束满足率达94.4%，创建成本降低97.5%，盈亏平衡期4.4个月，支持10,000个并发店铺请求

Conclusion: 该研究证明了生成式AI在自动化零售空间优化中的可行性，云原生架构具有线性扩展能力，为零售业提供了高效的货架图生成解决方案

Abstract: Planogram creation is a significant challenge for retail, requiring an average of 30 hours per complex layout. This paper introduces a cloud-native architecture using diffusion models to automatically generate store-specific planograms. Unlike conventional optimization methods that reorganize existing layouts, our system learns from successful shelf arrangements across multiple retail locations to create new planogram configurations. The architecture combines cloud-based model training via AWS with edge deployment for real-time inference. The diffusion model integrates retail-specific constraints through a modified loss function. Simulation-based analysis demonstrates the system reduces planogram design time by 98.3% (from 30 to 0.5 hours) while achieving 94.4% constraint satisfaction. Economic analysis reveals a 97.5% reduction in creation expenses with a 4.4-month break-even period. The cloud-native architecture scales linearly, supporting up to 10,000 concurrent store requests. This work demonstrates the viability of generative AI for automated retail space optimization.

</details>


### [50] [Interpretability-Guided Bi-objective Optimization: Aligning Accuracy and Explainability](https://arxiv.org/abs/2601.00655)
*Kasra Fouladi,Hamta Rahmani*

Main category: cs.LG

TL;DR: IGBO框架通过双目标优化训练可解释模型，利用DAG编码特征重要性层次结构，使用TIG测量特征重要性，并引入最优路径预言机解决OOD问题。


<details>
  <summary>Details</summary>
Motivation: 现有可解释模型训练方法通常缺乏结构化领域知识的有效整合，且特征重要性测量方法存在OOD问题，需要一种既能保持模型准确性又能强制执行领域知识约束的框架。

Method: 提出IGBO框架：1) 使用DAG编码特征重要性层次结构；2) 采用TIG测量特征重要性；3) 引入最优路径预言机解决TIG计算中的OOD问题；4) 通过双目标优化平衡模型准确性和领域知识约束。

Result: 理论分析证明了收敛性和对mini-batch噪声的鲁棒性；在时间序列数据上的实验表明，IGBO能够有效强制执行DAG约束，且准确性损失最小，优于标准正则化基线方法。

Conclusion: IGBO框架成功整合了结构化领域知识到可解释模型训练中，通过双目标优化和最优路径预言机解决了特征重要性测量的OOD问题，为可解释机器学习提供了有效方法。

Abstract: This paper introduces Interpretability-Guided Bi-objective Optimization (IGBO), a framework that trains interpretable models by incorporating structured domain knowledge via a bi-objective formulation. IGBO encodes feature importance hierarchies as a Directed Acyclic Graph (DAG) and uses Temporal Integrated Gradients (TIG) to measure feature importance. To address the Out-of-Distribution (OOD) problem in TIG computation, we propose an Optimal Path Oracle that learns data-manifold-aware integration paths. Theoretical analysis proves convergence properties and robustness to mini-batch noise, while empirical results on time-series data demonstrate IGBO's effectiveness in enforcing DAG constraints with minimal accuracy loss, outperforming standard regularization baselines.

</details>


### [51] [Entropy Production in Machine Learning Under Fokker-Planck Probability Flow](https://arxiv.org/abs/2601.00554)
*Lennon Shikhman*

Main category: cs.LG

TL;DR: 提出基于熵的再训练框架，通过非平衡随机动力学建模数据漂移，使用熵触发机制减少再训练频率同时保持预测性能


<details>
  <summary>Details</summary>
Motivation: 现有漂移检测方法缺乏理论动力学解释，无法指导再训练频率与运营成本之间的平衡。数据漂移导致模型性能下降，需要更原则性的再训练策略。

Method: 将部署时数据漂移建模为福克-普朗克方程控制的概率流，使用时变KL散度量模型-数据不匹配，推导出熵平衡分解，提出基于熵触发的无标签干预策略。

Result: 在受控非平稳分类实验中，熵触发再训练实现与高频再训练相当的预测性能，同时将再训练事件减少一个数量级（相比每日和基于标签的策略）。

Conclusion: 基于非平衡动力学的熵触发再训练框架提供原则性的数据漂移管理方法，能有效平衡性能与运营成本，减少不必要的再训练。

Abstract: Machine learning models deployed in nonstationary environments experience performance degradation due to data drift. While many drift detection heuristics exist, most lack a principled dynamical interpretation and provide limited guidance on how retraining frequency should be balanced against operational cost. In this work, we propose an entropy--based retraining framework grounded in nonequilibrium stochastic dynamics. Modeling deployment--time data drift as probability flow governed by a Fokker--Planck equation, we quantify model--data mismatch using a time--evolving Kullback--Leibler divergence. We show that the time derivative of this mismatch admits an entropy--balance decomposition featuring a nonnegative entropy production term driven by probability currents. This interpretation motivates entropy--triggered retraining as a label--free intervention strategy that responds to accumulated mismatch rather than delayed performance collapse. In a controlled nonstationary classification experiment, entropy--triggered retraining achieves predictive performance comparable to high--frequency retraining while reducing retraining events by an order of magnitude relative to daily and label--based policies.

</details>


### [52] [Adversarial Samples Are Not Created Equal](https://arxiv.org/abs/2601.00577)
*Jennifer Crawford,Amol Khanna,Fred Lu,Amy R. Wagoner,Stella Biderman,Andre T. Nguyen,Edward Raff*

Main category: cs.LG

TL;DR: 论文提出对抗性样本可分为两类：利用非鲁棒特征的和不利用非鲁棒特征的，并开发了基于集成的方法来量化对抗扰动对非鲁棒特征的操纵程度。


<details>
  <summary>Details</summary>
Motivation: 现有非鲁棒特征理论虽然被广泛接受，但忽略了那些不直接利用这些特征的对抗样本。作者认为这两种对抗样本代表了不同类型的对抗弱点，需要在评估对抗鲁棒性时加以区分。

Method: 提出了基于集成的度量方法来量化对抗扰动对非鲁棒特征的操纵程度，并用该度量分析攻击者生成的对抗样本的组成。

Result: 通过新度量方法能够区分不同类型的对抗弱点，并重新审视了多个现象，包括锐度感知最小化对对抗鲁棒性的影响，以及在鲁棒数据集上对抗训练和标准训练之间的鲁棒性差距。

Conclusion: 对抗性弱点应区分为利用非鲁棒特征和不利用非鲁棒特征两种类型，这种新视角有助于更全面地评估对抗鲁棒性并重新理解相关现象。

Abstract: Over the past decade, numerous theories have been proposed to explain the widespread vulnerability of deep neural networks to adversarial evasion attacks. Among these, the theory of non-robust features proposed by Ilyas et al. has been widely accepted, showing that brittle but predictive features of the data distribution can be directly exploited by attackers. However, this theory overlooks adversarial samples that do not directly utilize these features. In this work, we advocate that these two kinds of samples - those which use use brittle but predictive features and those that do not - comprise two types of adversarial weaknesses and should be differentiated when evaluating adversarial robustness. For this purpose, we propose an ensemble-based metric to measure the manipulation of non-robust features by adversarial perturbations and use this metric to analyze the makeup of adversarial samples generated by attackers. This new perspective also allows us to re-examine multiple phenomena, including the impact of sharpness-aware minimization on adversarial robustness and the robustness gap observed between adversarially training and standard training on robust datasets.

</details>


### [53] [IRPO: Scaling the Bradley-Terry Model via Reinforcement Learning](https://arxiv.org/abs/2601.00677)
*Haonan Song,Qingchen Xie,Huan Zhu,Feng Xiao,Luxi Xing,Fuzhen Li,Liu Kang,Feng Jiang,Zhiyong Zheng,Fan Yang*

Main category: cs.LG

TL;DR: 本文提出IRPO框架，通过将Bradley-Terry模型集成到GRPO中，解决了成对生成奖励模型的计算瓶颈问题，实现了高效的点式评分和可解释性。


<details>
  <summary>Details</summary>
Motivation: 成对生成奖励模型（GRMs）虽然具有可解释性和推理时扩展性，但与GRPO等强化学习算法集成时存在计算瓶颈：1）成对比较的O(n²)时间复杂度；2）重复采样或额外思维链推理带来的计算开销。

Method: 提出Intergroup Relative Preference Optimization (IRPO)框架，将Bradley-Terry模型集成到GRPO中，为每个响应生成点式评分，从而在RL训练期间高效评估任意数量的候选响应，同时保持可解释性和细粒度奖励信号。

Result: IRPO在多个基准测试中实现了点式GRMs的最先进性能，性能与当前领先的成对GRMs相当。在训练后评估中，IRPO显著优于成对GRMs。

Conclusion: IRPO通过解决成对GRMs的计算瓶颈，提供了一种高效、可扩展的强化学习框架，在保持可解释性的同时实现了与成对方法相当甚至更优的性能。

Abstract: Generative Reward Models (GRMs) have attracted considerable research interest in reward modeling due to their interpretability, inference-time scalability, and potential for refinement through reinforcement learning (RL). However, widely used pairwise GRMs create a computational bottleneck when integrated with RL algorithms such as Group Relative Policy Optimization (GRPO). This bottleneck arises from two factors: (i) the O(n^2) time complexity of pairwise comparisons required to obtain relative scores, and (ii) the computational overhead of repeated sampling or additional chain-of-thought (CoT) reasoning to improve performance. To address the first factor, we propose Intergroup Relative Preference Optimization (IRPO), a novel RL framework that incorporates the well-established Bradley-Terry model into GRPO. By generating a pointwise score for each response, IRPO enables efficient evaluation of arbitrarily many candidates during RL training while preserving interpretability and fine-grained reward signals. Experimental results demonstrate that IRPO achieves state-of-the-art (SOTA) performance among pointwise GRMs across multiple benchmarks, with performance comparable to that of current leading pairwise GRMs. Furthermore, we show that IRPO significantly outperforms pairwise GRMs in post-training evaluations.

</details>


### [54] [Geometry of Reason: Spectral Signatures of Valid Mathematical Reasoning](https://arxiv.org/abs/2601.00791)
*Valentin Noël*

Main category: cs.LG

TL;DR: 提出一种无需训练的方法，通过注意力矩阵的谱分析检测大语言模型中数学推理的有效性，使用四个可解释的谱诊断指标，在多个模型架构上达到85-95%的分类准确率。


<details>
  <summary>Details</summary>
Motivation: 需要一种无需训练数据、微调或学习分类器的方法来检测大语言模型中数学推理的有效性，以应用于幻觉检测和AI安全监控。

Method: 将注意力矩阵视为token动态图的邻接矩阵，提取四个可解释的谱诊断指标：Fiedler值（代数连通性）、高频能量比（HFER）、图信号平滑度和谱熵，通过单一阈值判断推理有效性。

Result: 在四个独立架构家族的七个transformer模型上，该方法产生高达Cohen's d=3.30的效应量，实现85.0-95.6%的分类准确率，校准阈值在完整数据集上达到93-95%。

Conclusion: 谱图分析为推理验证提供了原则性框架，能够检测逻辑一致性而非编译器接受度，揭示了注意力机制设计影响哪些谱特征捕捉推理有效性。

Abstract: We present a training-free method for detecting valid mathematical reasoning in large language models through spectral analysis of attention patterns. By treating attention matrices as adjacency matrices of dynamic graphs over tokens, we extract four interpretable spectral diagnostics, the Fiedler value (algebraic connectivity), high-frequency energy ratio (HFER), graph signal smoothness, and spectral entropy, that exhibit statistically significant differences between valid and invalid mathematical proofs. Experiments across seven transformer models from four independent architectural families (Meta Llama, Alibaba Qwen, Microsoft Phi, and Mistral AI) demonstrate that this spectral signature produces effect sizes up to Cohen's $d = 3.30$ ($p < 10^{-116}$), enabling 85.0--95.6\% classification accuracy under rigorous evaluation, with calibrated thresholds reaching 93--95\% on the full dataset. The method requires no training data, fine-tuning, or learned classifiers: a single threshold on a spectral metric suffices for high accuracy. Through systematic label correction, we discover that the spectral method detects logical coherence rather than compiler acceptance, identifying mathematically valid proofs that formal verifiers reject due to technical failures. We further identify an architectural dependency: Mistral-7B's Sliding Window Attention shifts the discriminative signal from HFER to late-layer Smoothness ($d = 2.09$, $p_{\text{MW}} = 1.16 \times 10^{-48}$), revealing that attention mechanism design affects which spectral features capture reasoning validity. These findings establish spectral graph analysis as a principled framework for reasoning verification with immediate applications to hallucination detection and AI safety monitoring.

</details>


### [55] [Do Chatbot LLMs Talk Too Much? The YapBench Benchmark](https://arxiv.org/abs/2601.00624)
*Vadim Borisov,Michael Gröger,Mina Mikhael,Richard H. Schreiber*

Main category: cs.LG

TL;DR: YapBench是一个用于量化LLMs在简洁理想提示上过度生成的轻量级基准测试，包含300多个英文提示，通过YapScore和YapIndex评估模型的多余响应长度。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型如ChatGPT、Claude和Gemini作为通用助手时，经常对简单请求给出不必要的冗长回答，包含冗余解释、模棱两可的表述和模板化内容，这增加了认知负担和基于token的推理成本。先前研究表明基于偏好的后训练和LLM评估可能导致系统性的长度偏差，即使质量相当，更长的回答也更容易获得奖励。

Method: 作者引入了YapBench基准测试，每个测试项包含单轮提示、精心策划的最小充分基线答案和类别标签。主要指标YapScore测量超出基线答案的字符数，不依赖特定分词器。通过YapIndex（类别级别中位数YapScore的均匀加权平均值）总结模型性能。基准包含300多个英文提示，涵盖三种简洁理想场景：模糊输入的简短澄清、封闭式事实问题的简短稳定答案、以及单行编码任务的单命令或代码片段。

Result: 评估76个助手型LLM后，观察到中位数多余长度存在数量级差异，并识别出特定类别的失败模式，包括在模糊输入上的"真空填充"和在单行技术请求上的解释或格式化开销。

Conclusion: YapBench为量化LLMs的过度生成行为提供了一个实用的基准测试工具，能够揭示模型在简洁性方面的表现差异和特定失败模式，并发布了基准测试和维护实时排行榜以跟踪模型的冗长行为随时间的变化。

Abstract: Large Language Models (LLMs) such as ChatGPT, Claude, and Gemini increasingly act as general-purpose copilots, yet they often respond with unnecessary length on simple requests, adding redundant explanations, hedging, or boilerplate that increases cognitive load and inflates token-based inference cost. Prior work suggests that preference-based post-training and LLM-judged evaluations can induce systematic length bias, where longer answers are rewarded even at comparable quality.
  We introduce YapBench, a lightweight benchmark for quantifying user-visible over-generation on brevity-ideal prompts. Each item consists of a single-turn prompt, a curated minimal-sufficient baseline answer, and a category label. Our primary metric, YapScore, measures excess response length beyond the baseline in characters, enabling comparisons across models without relying on any specific tokenizer. We summarize model performance via the YapIndex, a uniformly weighted average of category-level median YapScores.
  YapBench contains over three hundred English prompts spanning three common brevity-ideal settings: (A) minimal or ambiguous inputs where the ideal behavior is a short clarification, (B) closed-form factual questions with short stable answers, and (C) one-line coding tasks where a single command or snippet suffices. Evaluating 76 assistant LLMs, we observe an order-of-magnitude spread in median excess length and distinct category-specific failure modes, including vacuum-filling on ambiguous inputs and explanation or formatting overhead on one-line technical requests. We release the benchmark and maintain a live leaderboard for tracking verbosity behavior over time.

</details>


### [56] [TeleDoCTR: Domain-Specific and Contextual Troubleshooting for Telecommunications](https://arxiv.org/abs/2601.00691)
*Mohamed Trabelsi,Huseyin Uzunalioglu*

Main category: cs.LG

TL;DR: TeleDoCTR是一个针对电信领域票务故障排除的端到端系统，通过集成领域特定排序和生成模型，自动化分类、检索和生成任务，显著提升故障排除的准确性和效率。


<details>
  <summary>Details</summary>
Motivation: 电信领域的票务故障排除是一个高度复杂且耗时的任务，需要专家解释票务内容、查阅文档和搜索历史记录。这种人工密集型方法不仅延迟问题解决，还阻碍整体运营效率。

Method: 提出TeleDoCTR系统，这是一个电信相关、领域特定、上下文感知的故障排除系统。系统集成领域特定排序和生成模型，自动化故障排除工作流程的三个关键步骤：票务分类（路由到适当专家团队）、检索（获取上下文和语义相似的历史票务）、生成（创建详细故障分析报告）。

Result: 在真实世界电信基础设施数据集上评估TeleDoCTR，证明其性能优于现有最先进方法，显著提升了故障排除过程的准确性和效率。

Conclusion: TeleDoCTR系统通过自动化电信票务故障排除的关键步骤，有效解决了传统人工方法的效率瓶颈，为电信领域的运营效率提升提供了创新解决方案。

Abstract: Ticket troubleshooting refers to the process of analyzing and resolving problems that are reported through a ticketing system. In large organizations offering a wide range of services, this task is highly complex due to the diversity of submitted tickets and the need for specialized domain knowledge. In particular, troubleshooting in telecommunications (telecom) is a very time-consuming task as it requires experts to interpret ticket content, consult documentation, and search historical records to identify appropriate resolutions. This human-intensive approach not only delays issue resolution but also hinders overall operational efficiency. To enhance the effectiveness and efficiency of ticket troubleshooting in telecom, we propose TeleDoCTR, a novel telecom-related, domain-specific, and contextual troubleshooting system tailored for end-to-end ticket resolution in telecom. TeleDoCTR integrates both domain-specific ranking and generative models to automate key steps of the troubleshooting workflow which are: routing tickets to the appropriate expert team responsible for resolving the ticket (classification task), retrieving contextually and semantically similar historical tickets (retrieval task), and generating a detailed fault analysis report outlining the issue, root cause, and potential solutions (generation task). We evaluate TeleDoCTR on a real-world dataset from a telecom infrastructure and demonstrate that it achieves superior performance over existing state-of-the-art methods, significantly enhancing the accuracy and efficiency of the troubleshooting process.

</details>


### [57] [ARISE: Adaptive Reinforcement Integrated with Swarm Exploration](https://arxiv.org/abs/2601.00693)
*Rajiv Chaitanya M,D R Ramesh Babu*

Main category: cs.LG

TL;DR: ARISE是一个轻量级强化学习框架，通过集成基于群体的探索层来增强标准策略梯度方法，在非平稳奖励和高维策略任务中显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习中的有效探索仍然是一个关键挑战，特别是在非平稳奖励或高维策略的情况下。现有方法在复杂环境中探索不足，需要更有效的探索机制。

Method: ARISE框架将标准策略梯度方法与紧凑的群体探索层结合，将策略动作与基于粒子的提议混合。每个粒子代表在动作空间中采样的候选策略轨迹，并使用奖励方差线索自适应调节探索强度。

Result: 在简单基准测试中改进较小（如CartPole-v1上+0.7%），但在更具挑战性的任务中表现显著提升：LunarLander-v3上+46%，Hopper-v4上+22%，同时在Walker2d和Ant上保持稳定性。在非平稳奖励变化下，ARISE表现出明显的鲁棒性优势，在CartPole上优于PPO +75分，在LunarLander上也有相应改进。

Conclusion: ARISE提供了一个简单、架构无关的途径，在不改变核心算法结构的情况下，创建更具探索性和鲁棒性的强化学习智能体。消融研究证实群体组件和自适应机制都对性能有贡献。

Abstract: Effective exploration remains a key challenge in RL, especially with non-stationary rewards or high-dimensional policies. We introduce ARISE, a lightweight framework that enhances reinforcement learning by augmenting standard policy-gradient methods with a compact swarm-based exploration layer. ARISE blends policy actions with particle-driven proposals, where each particle represents a candidate policy trajectory sampled in the action space, and modulates exploration adaptively using reward-variance cues. While easy benchmarks exhibit only slight improvements (e.g., +0.7% on CartPole-v1), ARISE yields substantial gains on more challenging tasks, including +46% on LunarLander-v3 and +22% on Hopper-v4, while preserving stability on Walker2d and Ant. Under non-stationary reward shifts, ARISE provides marked robustness advantages, outperforming PPO by +75 points on CartPole and improving LunarLander accordingly. Ablation studies confirm that both the swarm component and the adaptive mechanism contribute to the performance. Overall, ARISE offers a simple, architecture-agnostic route to more exploratory and resilient RL agents without altering core algorithmic structures.

</details>


### [58] [Bayesian Inverse Games with High-Dimensional Multi-Modal Observations](https://arxiv.org/abs/2601.00696)
*Yash Jain,Xinjie Liu,Lasse Peters,David Fridovich-Keil,Ufuk Topcu*

Main category: cs.LG

TL;DR: 提出基于贝叶斯推理的逆博弈框架，通过变分自编码器嵌入可微纳什博弈求解器，从交互数据中学习智能体目标的先验和后验分布，提高推断质量并支持更安全的决策。


<details>
  <summary>Details</summary>
Motivation: 现有最大似然逆博弈方法仅提供点估计，无法量化估计不确定性，导致下游规划决策可能过度自信地采取不安全行动。需要能够处理不确定性并支持多模态观测的贝叶斯推理方法。

Method: 提出贝叶斯逆博弈框架：训练结构化变分自编码器，嵌入可微纳什博弈求解器，从交互数据集中学习智能体目标的先验和后验分布，无需真实目标标签，支持多模态观测。

Result: 框架成功学习先验和后验分布，相比最大似然估计方法提高推断质量，实现更安全的下游决策而不牺牲效率。当轨迹信息不充分时，多模态推断进一步减少不确定性。

Conclusion: 贝叶斯逆博弈方法能够有效处理不确定性，通过多模态观测减少推断不确定性，为自主决策提供更安全的规划基础，优于传统点估计方法。

Abstract: Many multi-agent interaction scenarios can be naturally modeled as noncooperative games, where each agent's decisions depend on others' future actions. However, deploying game-theoretic planners for autonomous decision-making requires a specification of all agents' objectives. To circumvent this practical difficulty, recent work develops maximum likelihood techniques for solving inverse games that can identify unknown agent objectives from interaction data. Unfortunately, these methods only infer point estimates and do not quantify estimator uncertainty; correspondingly, downstream planning decisions can overconfidently commit to unsafe actions. We present an approximate Bayesian inference approach for solving the inverse game problem, which can incorporate observation data from multiple modalities and be used to generate samples from the Bayesian posterior over the hidden agent objectives given limited sensor observations in real time. Concretely, the proposed Bayesian inverse game framework trains a structured variational autoencoder with an embedded differentiable Nash game solver on interaction datasets and does not require labels of agents' true objectives. Extensive experiments show that our framework successfully learns prior and posterior distributions, improves inference quality over maximum likelihood estimation-based inverse game approaches, and enables safer downstream decision-making without sacrificing efficiency. When trajectory information is uninformative or unavailable, multimodal inference further reduces uncertainty by exploiting additional observation modalities.

</details>


### [59] [BSAT: B-Spline Adaptive Tokenizer for Long-Term Time Series Forecasting](https://arxiv.org/abs/2601.00698)
*Maximilian Reinwardt,Michael Eichelbeck,Matthias Althoff*

Main category: cs.LG

TL;DR: 提出BSAT方法，使用B样条自适应分词器解决Transformer在长期时间序列预测中的二次复杂度和固定分块问题，结合混合位置编码L-RoPE，在内存受限场景下实现高性能


<details>
  <summary>Details</summary>
Motivation: 传统Transformer在长期时间序列预测中存在两个主要问题：自注意力的二次计算复杂度，以及均匀分块可能无法与数据的语义结构对齐。需要一种更高效、更灵活的方法来处理时间序列数据。

Method: 提出B样条自适应分词器(BSAT)，这是一种无参数方法，通过B样条拟合自适应分割时间序列。在高曲率区域放置token，将变长基函数表示为固定大小的token（包含系数和位置）。同时提出混合位置编码L-RoPE，结合可学习的加性位置编码和具有层间可学习基数的旋转位置嵌入。

Result: 在多个公共基准测试中，该模型在保持强竞争力的同时实现了高压缩率，特别适合内存受限的使用场景。

Conclusion: BSAT方法有效解决了Transformer在长期时间序列预测中的计算效率和语义对齐问题，通过自适应分词和混合位置编码实现了高性能和高压缩率，为内存受限场景提供了实用解决方案。

Abstract: Long-term time series forecasting using transformers is hampered by the quadratic complexity of self-attention and the rigidity of uniform patching, which may be misaligned with the data's semantic structure. In this paper, we introduce the \textit{B-Spline Adaptive Tokenizer (BSAT)}, a novel, parameter-free method that adaptively segments a time series by fitting it with B-splines. BSAT algorithmically places tokens in high-curvature regions and represents each variable-length basis function as a fixed-size token, composed of its coefficient and position. Further, we propose a hybrid positional encoding that combines a additive learnable positional encoding with Rotary Positional Embedding featuring a layer-wise learnable base: L-RoPE. This allows each layer to attend to different temporal dependencies. Our experiments on several public benchmarks show that our model is competitive with strong performance at high compression rates. This makes it particularly well-suited for use cases with strong memory constraints.

</details>


### [60] [Precision Autotuning for Linear Solvers via Contextual Bandit-Based RL](https://arxiv.org/abs/2601.00728)
*Erin Carson,Xinye Chen*

Main category: cs.LG

TL;DR: 提出基于强化学习的自适应精度调优框架，用于线性求解器及其他算法，通过上下文老虎机问题动态选择计算步骤的最优精度配置，平衡精度与计算效率。


<details>
  <summary>Details</summary>
Motivation: 科学计算中混合精度方法需要手动调优精度配置，缺乏自动化工具。传统方法难以动态适应不同计算阶段的需求，需要一种能自动选择最优精度配置的框架。

Method: 将问题建模为上下文老虎机问题，使用离散化状态空间和增量动作价值估计。通过Q表映射离散化特征（如近似条件数和矩阵范数）到精度配置动作，采用epsilon-greedy策略优化多目标奖励函数，平衡精度和计算成本。

Result: 在线性系统迭代求精应用中，该框架能有效选择精度配置，在保持与双精度基准相当精度的同时显著降低计算成本。框架对未见数据集具有良好泛化能力。

Conclusion: 这是首个基于强化学习的精度自动调优工作，为混合精度数值方法在科学计算中的应用提供了新思路，可扩展到其他数值算法。

Abstract: We propose a reinforcement learning (RL) framework for adaptive precision tuning of linear solvers, and can be extended to general algorithms. The framework is formulated as a contextual bandit problem and solved using incremental action-value estimation with a discretized state space to select optimal precision configurations for computational steps, balancing precision and computational efficiency. To verify its effectiveness, we apply the framework to iterative refinement for solving linear systems $Ax = b$. In this application, our approach dynamically chooses precisions based on calculated features from the system. In detail, a Q-table maps discretized features (e.g., approximate condition number and matrix norm)to actions (chosen precision configurations for specific steps), optimized via an epsilon-greedy strategy to maximize a multi-objective reward balancing accuracy and computational cost. Empirical results demonstrate effective precision selection, reducing computational cost while maintaining accuracy comparable to double-precision baselines. The framework generalizes to diverse out-of-sample data and offers insight into utilizing RL precision selection for other numerical algorithms, advancing mixed-precision numerical methods in scientific computing. To the best of our knowledge, this is the first work on precision autotuning with RL and verified on unseen datasets.

</details>


### [61] [The Reasoning-Creativity Trade-off: Toward Creativity-Driven Problem Solving](https://arxiv.org/abs/2601.00747)
*Max Ruiz Luyten,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: 论文分析了当前LLM推理流程中基于正确性的强化学习会导致推理路径分布崩溃，提出了分布创造性推理（DCR）框架，统一了多种训练方法，提供了防止分布崩溃的理论和实用方案。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理流程主要基于正确性优化，通过采样多样化的思维链并强化得分最高的路径，但这种设计会导致模型在推理路径上的分布崩溃，降低语义熵并削弱创造性问题解决能力。

Method: 提出了分布创造性推理（DCR）框架，将训练视为通过解决方案轨迹概率测度的梯度流。该框架统一了STaR、GRPO、DPO等多种方法，以及熵奖励等机制。

Result: 获得了三个核心结果：1）多样性衰减定理，描述了基于正确性的目标如何导致STaR、GRPO、DPO的不同多样性衰减模式；2）确保收敛到稳定且多样化策略的设计，有效防止崩溃；3）实现这一目标的简单实用方案。

Conclusion: DCR为LLM提供了首个既保持正确性又保持创造性的原则性方案，解决了当前推理流程中分布崩溃的问题。

Abstract: State-of-the-art large language model (LLM) pipelines rely on bootstrapped reasoning loops: sampling diverse chains of thought and reinforcing the highest-scoring ones, mainly optimizing correctness. We analyze how this design choice is sensitive to the collapse of the model's distribution over reasoning paths, slashing semantic entropy and undermining creative problem-solving. To analyze this failure, we introduce Distributional Creative Reasoning (DCR), a unified variational objective that casts training as gradient flow through probability measures on solution traces. STaR, GRPO, and DPO, as well as entropy bonuses, and other methods, all constitute special cases of the same loss. The framework delivers three core results: (i) the diversity decay theorem, describing how correctness-based objectives lead to distinct modes of diversity decay for STaR, GRPO, and DPO; (ii) designs that ensure convergence to a stable and diverse policy, effectively preventing collapse; and (iii) simple, actionable recipes to achieve this in practice. DCR thus offers the first principled recipe for LLMs that remain both correct and creative.

</details>


### [62] [A Machine Learning Framework for Off Ball Defensive Role and Performance Evaluation in Football](https://arxiv.org/abs/2601.00748)
*Sean Groom,Shuo Wang,Francisco Belo,Axl Rice,Liam Anderson*

Main category: cs.LG

TL;DR: 提出基于协变量依赖隐马尔可夫模型(CDHMM)的角球防守评估框架，通过球员追踪数据推断盯人和区域防守任务，实现无标签的防守贡献评估和反事实分析。


<details>
  <summary>Details</summary>
Motivation: 足球中无球防守表现评估具有挑战性，传统指标无法捕捉限制对手行动选择的协调运动。现有价值模型主要评估有球动作，反事实方法如ghosting模型依赖缺乏战术背景的"平均"行为模拟。

Method: 针对角球这一高度结构化的比赛环节，提出协变量依赖隐马尔可夫模型(CDHMM)，直接从球员追踪数据推断时间分辨的盯人和区域防守任务分配。基于这些分配提出新的防守贡献归因框架和角色条件ghosting方法。

Result: 模型能够无标签地推断角球防守中的盯人和区域防守任务，提出的框架提供了可解释的防守贡献评估，并能够进行基于上下文的反事实分析。

Conclusion: 该方法为足球无球防守表现评估提供了新的框架，能够基于球员追踪数据进行战术背景感知的防守贡献评估和反事实分析，相比传统方法更具解释性和上下文相关性。

Abstract: Evaluating off-ball defensive performance in football is challenging, as traditional metrics do not capture the nuanced coordinated movements that limit opponent action selection and success probabilities. Although widely used possession value models excel at appraising on-ball actions, their application to defense remains limited. Existing counterfactual methods, such as ghosting models, help extend these analyses but often rely on simulating "average" behavior that lacks tactical context. To address this, we introduce a covariate-dependent Hidden Markov Model (CDHMM) tailored to corner kicks, a highly structured aspect of football games. Our label-free model infers time-resolved man-marking and zonal assignments directly from player tracking data. We leverage these assignments to propose a novel framework for defensive credit attribution and a role-conditioned ghosting method for counterfactual analysis of off-ball defensive performance. We show how these contributions provide a interpretable evaluation of defensive contributions against context-aware baselines.

</details>


### [63] [Memory Bank Compression for Continual Adaptation of Large Language Models](https://arxiv.org/abs/2601.00756)
*Thomas Katraouras,Dimitrios Rafailidis*

Main category: cs.LG

TL;DR: MBC提出了一种通过码本优化策略压缩记忆库的方法，用于大语言模型的持续学习，将记忆库大小减少到最强基线的0.3%，同时保持高精度。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的知识容易过时，持续学习需要更新模型而不遗忘已有知识。现有基于记忆库的方法在现实大规模数据流中会不断增长，存在存储效率低的问题。

Method: 提出MBC模型，通过码本优化策略在线压缩记忆库；引入在线重置机制防止码本崩溃；在注意力层使用Key-Value低秩适配，高效利用压缩后的记忆表示。

Result: 在基准问答数据集上的实验表明，MBC将记忆库大小减少到最强基线的0.3%，同时在在线适应学习中保持高保留准确率。

Conclusion: MBC通过记忆库压缩和码本优化，有效解决了持续学习中记忆库不断增长的问题，实现了高效且稳定的在线学习。

Abstract: Large Language Models (LLMs) have become a mainstay for many everyday applications. However, as data evolve their knowledge quickly becomes outdated. Continual learning aims to update LLMs with new information without erasing previously acquired knowledge. Although methods such as full fine-tuning can incorporate new data, they are computationally expensive and prone to catastrophic forgetting, where prior knowledge is overwritten. Memory-augmented approaches address this by equipping LLMs with a memory bank, that is an external memory module which stores information for future use. However, these methods face a critical limitation, in particular, the memory bank constantly grows in the real-world scenario when large-scale data streams arrive. In this paper, we propose MBC, a model that compresses the memory bank through a codebook optimization strategy during online adaptation learning. To ensure stable learning, we also introduce an online resetting mechanism that prevents codebook collapse. In addition, we employ Key-Value Low-Rank Adaptation in the attention layers of the LLM, enabling efficient utilization of the compressed memory representations. Experiments with benchmark question-answering datasets demonstrate that MBC reduces the memory bank size to 0.3% when compared against the most competitive baseline, while maintaining high retention accuracy during online adaptation learning. Our code is publicly available at https://github.com/Thomkat/MBC.

</details>


### [64] [Categorical Reparameterization with Denoising Diffusion models](https://arxiv.org/abs/2601.00781)
*Samson Gourevitch,Alain Durmus,Eric Moulines,Jimmy Olsson,Yazid Janati*

Main category: cs.LG

TL;DR: 提出了一种基于扩散的软重参数化方法，用于处理分类变量的梯度优化问题，通过高斯噪声过程的去噪器实现高效计算。


<details>
  <summary>Details</summary>
Motivation: 传统分类变量梯度优化方法存在局限性：得分函数估计器虽然无偏但噪声大，而连续松弛方法虽然能获得路径梯度但优化的是有偏的温度依赖目标。需要一种更好的方法来处理分类分布的优化问题。

Method: 引入基于扩散的软重参数化方法，利用高斯噪声过程下分类分布去噪器的闭式解，实现无需训练即可通过扩散采样器进行反向传播。

Result: 实验表明，所提出的重参数化技巧在各种基准测试中取得了有竞争力或改进的优化性能。

Conclusion: 扩散基软重参数化为分类分布优化提供了一种有效的新方法，通过闭式解的去噪器实现高效计算，在多个基准测试中表现出优越性能。

Abstract: Gradient-based optimization with categorical variables typically relies on score-function estimators, which are unbiased but noisy, or on continuous relaxations that replace the discrete distribution with a smooth surrogate admitting a pathwise (reparameterized) gradient, at the cost of optimizing a biased, temperature-dependent objective. In this paper, we extend this family of relaxations by introducing a diffusion-based soft reparameterization for categorical distributions. For these distributions, the denoiser under a Gaussian noising process admits a closed form and can be computed efficiently, yielding a training-free diffusion sampler through which we can backpropagate. Our experiments show that the proposed reparameterization trick yields competitive or improved optimization performance on various benchmarks.

</details>
