<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.HC](#cs.HC) [Total: 7]
- [cs.LG](#cs.LG) [Total: 38]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Mathematical Knowledge Graph-Driven Framework for Equation-Based Predictive and Reliable Additive Manufacturing](https://arxiv.org/abs/2601.05298)
*Yeongbin Cha,Namjung Kim*

Main category: cs.AI

TL;DR: 提出了一种基于本体指导、方程中心的框架，将大语言模型与增材制造数学知识图谱结合，实现可靠知识提取和有原则的外推建模。


<details>
  <summary>Details</summary>
Motivation: 增材制造领域现有数据驱动方法受限于碎片化的知识表示和稀疏数据条件下的不可靠外推，需要更可靠的知识提取和外推建模方法。

Method: 通过形式化本体编码方程、变量、假设及其语义关系，将非结构化文献转化为机器可解释表示；基于知识图谱子图条件化LLM方程生成，确保物理意义；引入置信感知外推评估，整合外推距离、统计稳定性和知识图谱物理一致性。

Result: 本体指导的提取显著提高了提取知识的结构一致性和定量可靠性；子图条件化的方程生成相比无指导的LLM输出产生更稳定和物理一致的外推结果。

Conclusion: 建立了一个统一流程，用于本体驱动的知识表示、方程中心推理和基于置信度的外推评估，展示了知识图谱增强的LLM作为增材制造外推建模可靠工具的潜力。

Abstract: Additive manufacturing (AM) relies critically on understanding and extrapolating process-property relationships; however, existing data-driven approaches remain limited by fragmented knowledge representations and unreliable extrapolation under sparse data conditions. In this study, we propose an ontology-guided, equation-centric framework that tightly integrates large language models (LLMs) with an additive manufacturing mathematical knowledge graph (AM-MKG) to enable reliable knowledge extraction and principled extrapolative modeling. By explicitly encoding equations, variables, assumptions, and their semantic relationships within a formal ontology, unstructured literature is transformed into machine-interpretable representations that support structured querying and reasoning. LLM-based equation generation is further conditioned on MKG-derived subgraphs, enforcing physically meaningful functional forms and mitigating non-physical or unstable extrapolation trends. To assess reliability beyond conventional predictive uncertainty, a confidence-aware extrapolation assessment is introduced, integrating extrapolation distance, statistical stability, and knowledge-graph-based physical consistency into a unified confidence score. Results demonstrate that ontology-guided extraction significantly improves the structural coherence and quantitative reliability of extracted knowledge, while subgraph-conditioned equation generation yields stable and physically consistent extrapolations compared to unguided LLM outputs. Overall, this work establishes a unified pipeline for ontology-driven knowledge representation, equation-centered reasoning, and confidence-based extrapolation assessment, highlighting the potential of knowledge-graph-augmented LLMs as reliable tools for extrapolative modeling in additive manufacturing.

</details>


### [2] [Effects of personality steering on cooperative behavior in Large Language Model agents](https://arxiv.org/abs/2601.05302)
*Mizuki Sakai,Mizuki Yokoyama,Wakaba Tateishi,Genki Ichinose*

Main category: cs.AI

TL;DR: 研究通过重复囚徒困境游戏探索人格引导对LLM智能体合作行为的影响，发现宜人性是促进合作的主导因素，而人格引导更多是行为偏差而非确定性控制机制。


<details>
  <summary>Details</summary>
Motivation: 尽管已有研究表明为LLM分配人格特质可以影响其行为，但人格引导在受控条件下如何影响合作行为仍不清楚。本研究旨在通过重复囚徒困境游戏，系统研究人格引导对LLM智能体合作行为的影响。

Method: 基于大五人格框架，首先使用大五人格量表测量GPT-3.5-turbo、GPT-4o和GPT-5的基本人格特征。然后在基线和人格引导条件下比较行为表现，并进一步分析独立操纵每个人格维度到极端值的效果。

Result: 结果显示：1）宜人性是所有模型中促进合作的主导因素，其他人格特质影响有限；2）明确的人格信息能增加合作，但也可能增加被利用的脆弱性，特别是在早期模型中；3）后期模型表现出更具选择性的合作行为。

Conclusion: 人格引导更多是作为行为偏差而非确定性控制机制。研究揭示了LLM智能体在战略互动中的人格引导效应，对理解AI智能体的社会行为具有重要意义。

Abstract: Large language models (LLMs) are increasingly used as autonomous agents in strategic and social interactions. Although recent studies suggest that assigning personality traits to LLMs can influence their behavior, how personality steering affects cooperation under controlled conditions remains unclear. In this study, we examine the effects of personality steering on cooperative behavior in LLM agents using repeated Prisoner's Dilemma games. Based on the Big Five framework, we first measure basic personality profiles of three models, GPT-3.5-turbo, GPT-4o, and GPT-5, using the Big Five Inventory. We then compare behavior under baseline and personality-informed conditions, and further analyze the effects of independently manipulating each personality dimension to extreme values. Our results show that agreeableness is the dominant factor promoting cooperation across all models, while other personality traits have limited impact. Explicit personality information increases cooperation but can also raise vulnerability to exploitation, particularly in earlier-generation models. In contrast, later-generation models exhibit more selective cooperation. These findings indicate that personality steering acts as a behavioral bias rather than a deterministic control mechanism.

</details>


### [3] [Improving Enzyme Prediction with Chemical Reaction Equations by Hypergraph-Enhanced Knowledge Graph Embeddings](https://arxiv.org/abs/2601.05330)
*Tengwei Song,Long Yin,Zhen Han,Zhiqiang Xu*

Main category: cs.AI

TL;DR: 提出Hyper-Enz模型，通过知识图谱嵌入和超图变换器预测酶-底物相互作用，相比传统方法在酶检索准确率上提升88%，在配对预测上提升30%。


<details>
  <summary>Details</summary>
Motivation: 现有酶-底物相互作用预测方法依赖专家标注的稀疏数据库，训练数据不足导致模型泛化能力差。化学反应方程式数据更易获取且更丰富，但多化合物与酶之间的复杂关系传统模型难以捕捉。

Method: 将化学反应方程式表示为(底物，酶，产物)三元组构建知识图谱，提出Hyper-Enz模型：结合超图变换器和知识图谱嵌入学习涉及多个底物和产物的超边表示，引入多专家范式指导酶-底物相互作用学习。

Result: 实验结果显示显著改进：酶检索平均准确率相对提升88%，配对级别预测提升30%，证明方法的有效性。

Conclusion: 通过知识图谱表示化学反应并利用超图模型捕获复杂关系，有效解决了酶-底物相互作用预测中数据稀疏和关系复杂的问题，显著提升了预测性能。

Abstract: Predicting enzyme-substrate interactions has long been a fundamental problem in biochemistry and metabolic engineering. While existing methods could leverage databases of expert-curated enzyme-substrate pairs for models to learn from known pair interactions, the databases are often sparse, i.e., there are only limited and incomplete examples of such pairs, and also labor-intensive to maintain. This lack of sufficient training data significantly hinders the ability of traditional enzyme prediction models to generalize to unseen interactions. In this work, we try to exploit chemical reaction equations from domain-specific databases, given their easier accessibility and denser, more abundant data. However, interactions of multiple compounds, e.g., educts and products, with the same enzymes create complex relational data patterns that traditional models cannot easily capture. To tackle that, we represent chemical reaction equations as triples of (educt, enzyme, product) within a knowledge graph, such that we can take advantage of knowledge graph embedding (KGE) to infer missing enzyme-substrate pairs for graph completion. Particularly, in order to capture intricate relationships among compounds, we propose our knowledge-enhanced hypergraph model for enzyme prediction, i.e., Hyper-Enz, which integrates a hypergraph transformer with a KGE model to learn representations of the hyper-edges that involve multiple educts and products. Also, a multi-expert paradigm is introduced to guide the learning of enzyme-substrate interactions with both the proposed model and chemical reaction equations. Experimental results show a significant improvement, with up to a 88% relative improvement in average enzyme retrieval accuracy and 30% improvement in pair-level prediction compared to traditional models, demonstrating the effectiveness of our approach.

</details>


### [4] [The Persona Paradox: Medical Personas as Behavioral Priors in Clinical Language Models](https://arxiv.org/abs/2601.05376)
*Tassallah Abdullahi,Shrestha Ghosh,Hamish S Fraser,Daniel León Tramontini,Adeel Abbasi,Ghada Bourjeily,Carsten Eickhoff,Ritambhara Singh*

Main category: cs.AI

TL;DR: 研究发现人物角色设定作为大语言模型的行为先验，在临床决策中产生系统性、情境依赖且非单调的影响：医疗角色在重症监护任务中提升性能，但在初级护理中降低性能，交互风格对风险倾向的影响高度模型依赖。


<details>
  <summary>Details</summary>
Motivation: 虽然人物角色设定通常被认为能赋予大语言模型专业知识和提升安全性，但其对高风险临床决策的具体影响尚不清楚。本研究旨在系统评估基于角色的控制在临床大语言模型中的效果，探究专业角色和交互风格如何影响模型在不同医疗任务中的行为。

Method: 通过多维评估方法，在临床分诊和患者安全任务上测试不同专业角色（如急诊科医生、护士）和交互风格（大胆vs谨慎）对模型行为的影响，评估任务准确性、校准度和安全相关风险行为，并比较LLM判断与人类临床医生的评估。

Result: 医疗角色在重症监护任务中提升性能达+20%的准确性和校准度，但在初级护理环境中降低类似幅度的性能；交互风格调节风险倾向和敏感性，但高度依赖具体模型；人类临床医生在安全合规性上表现出中等一致性，但对推理质量的信心很低。

Conclusion: 人物角色设定作为行为先验引入情境依赖的权衡，而非安全或专业性的保证。在临床大语言模型应用中需要谨慎考虑角色设定的上下文效应，不能简单假设其单调提升安全性或专业性。

Abstract: Persona conditioning can be viewed as a behavioral prior for large language models (LLMs) and is often assumed to confer expertise and improve safety in a monotonic manner. However, its effects on high-stakes clinical decision-making remain poorly characterized. We systematically evaluate persona-based control in clinical LLMs, examining how professional roles (e.g., Emergency Department physician, nurse) and interaction styles (bold vs.\ cautious) influence behavior across models and medical tasks. We assess performance on clinical triage and patient-safety tasks using multidimensional evaluations that capture task accuracy, calibration, and safety-relevant risk behavior. We find systematic, context-dependent, and non-monotonic effects: Medical personas improve performance in critical care tasks, yielding gains of up to $\sim+20\%$ in accuracy and calibration, but degrade performance in primary-care settings by comparable margins. Interaction style modulates risk propensity and sensitivity, but it's highly model-dependent. While aggregated LLM-judge rankings favor medical over non-medical personas in safety-critical cases, we found that human clinicians show moderate agreement on safety compliance (average Cohen's $κ= 0.43$) but indicate a low confidence in 95.9\% of their responses on reasoning quality. Our work shows that personas function as behavioral priors that introduce context-dependent trade-offs rather than guarantees of safety or expertise. The code is available at https://github.com/rsinghlab/Persona\_Paradox.

</details>


### [5] [On the Effect of Cheating in Chess](https://arxiv.org/abs/2601.05386)
*Daniel Keren*

Main category: cs.AI

TL;DR: 该研究评估了在国际象棋比赛中有限次数作弊（使用软件建议）可能带来的性能提升，而非专注于作弊检测。


<details>
  <summary>Details</summary>
Motivation: 国际象棋中使用强大软件作弊已成为严重问题，甚至影响到最高水平的比赛。现有研究大多关注作弊检测，而本研究旨在量化有限次数作弊可能带来的实际性能提升，为防范和检测作弊提供依据。

Method: 开发算法并在常用国际象棋引擎上进行测试，模拟在比赛中有限次数使用软件建议的情况。

Result: 研究结果显示，即使有限次数的作弊也能显著提升比赛表现，具体提升程度取决于作弊次数和时机。

Conclusion: 有限次数的作弊确实能带来显著的性能优势，这一发现对于制定更有效的作弊防范和检测策略至关重要。研究目的不是协助作弊者，而是为了更有效地遏制和检测作弊行为。

Abstract: Cheating in chess, by using advice from powerful software, has become a major problem, reaching the highest levels. As opposed to the large majority of previous work, which concerned {\em detection} of cheating, here we try to evaluate the possible gain in performance, obtained by cheating a limited number of times during a game. Algorithms are developed and tested on a commonly used chess engine (i.e software).\footnote{Needless to say, the goal of this work is not to assist cheaters, but to measure the effectiveness of cheating -- which is crucial as part of the effort to contain and detect it.}

</details>


### [6] [ART: Adaptive Reasoning Trees for Explainable Claim Verification](https://arxiv.org/abs/2601.05455)
*Sahil Wadhwa,Himanshu Kumar,Guanqun Yang,Abbaas Alif Mohamed Nishar,Pranab Mohanty,Swapnil Shinde,Yue Wu*

Main category: cs.AI

TL;DR: ART提出了一种分层论证树方法，通过LLM裁判进行成对比较，实现透明可争议的声明验证，优于传统推理方法。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在复杂决策中具有强大能力，但其不透明性阻碍了在高风险环境中的应用。现有方法缺乏忠实解释和纠错机制，损害了可信度。

Method: 提出自适应推理树（ART）方法：从根声明开始，分支为支持和攻击的子论点。通过LLM裁判对子论点进行成对锦标赛式比较，自底向上确定论点强度，系统化生成透明可争议的最终裁决。

Result: 在多个数据集上实证验证，分析不同论点生成器和比较策略。结果表明ART的结构化推理优于强基线，为可解释声明验证设立了新基准。

Conclusion: ART方法通过分层论证结构和LLM裁判的成对比较，实现了更可靠、透明的决策过程，解决了LLM在高风险应用中缺乏可解释性和可争议性的问题。

Abstract: Large Language Models (LLMs) are powerful candidates for complex decision-making, leveraging vast encoded knowledge and remarkable zero-shot abilities. However, their adoption in high-stakes environments is hindered by their opacity; their outputs lack faithful explanations and cannot be effectively contested to correct errors, undermining trustworthiness. In this paper, we propose ART (Adaptive Reasoning Trees), a hierarchical method for claim verification. The process begins with a root claim, which branches into supporting and attacking child arguments. An argument's strength is determined bottom-up via a pairwise tournament of its children, adjudicated by a judge LLM, allowing a final, transparent and contestable verdict to be systematically derived which is missing in methods like Chain-of-Thought (CoT). We empirically validate ART on multiple datasets, analyzing different argument generators and comparison strategies. Our findings show that ART's structured reasoning outperforms strong baselines, establishing a new benchmark for explainable claim verification which is more reliable and ensures clarity in the overall decision making step.

</details>


### [7] [MMUEChange: A Generalized LLM Agent Framework for Intelligent Multi-Modal Urban Environment Change Analysis](https://arxiv.org/abs/2601.05483)
*Zixuan Xiao,Jun Ma,Siwei Zhang*

Main category: cs.AI

TL;DR: MMUEChange是一个多模态智能体框架，通过模块化工具包和模态控制器整合异构城市数据，实现复杂城市变化场景的鲁棒分析，相比最佳基线任务成功率提升46.7%。


<details>
  <summary>Details</summary>
Motivation: 当前城市环境变化分析方法（特别是遥感变化检测）通常依赖僵化的单模态分析，难以应对复杂的城市变化场景，需要更灵活的多模态整合方法。

Method: 提出MMUEChange多模态智能体框架，包含模块化工具包和核心模块"模态控制器"，实现跨模态和模态内对齐，灵活整合异构城市数据。

Result: 案例研究包括：纽约小型社区公园增加反映本地绿地建设；香港跨区域集中水污染扩散指向协同水管理；深圳露天垃圾场减少，夜间经济活动与不同类型垃圾的关联差异反映城市压力差异。相比最佳基线，任务成功率提升46.7%，有效缓解幻觉问题。

Conclusion: MMUEChange框架能够支持具有现实政策意义的复杂城市变化分析任务，展示了多模态智能体在城市环境变化分析中的潜力。

Abstract: Understanding urban environment change is essential for sustainable development. However, current approaches, particularly remote sensing change detection, often rely on rigid, single-modal analysis. To overcome these limitations, we propose MMUEChange, a multi-modal agent framework that flexibly integrates heterogeneous urban data via a modular toolkit and a core module, Modality Controller for cross- and intra-modal alignment, enabling robust analysis of complex urban change scenarios. Case studies include: a shift toward small, community-focused parks in New York, reflecting local green space efforts; the spread of concentrated water pollution across districts in Hong Kong, pointing to coordinated water management; and a notable decline in open dumpsites in Shenzhen, with contrasting links between nighttime economic activity and waste types, indicating differing urban pressures behind domestic and construction waste. Compared to the best-performing baseline, the MMUEChange agent achieves a 46.7% improvement in task success rate and effectively mitigates hallucination, demonstrating its capacity to support complex urban change analysis tasks with real-world policy implications.

</details>


### [8] [The Evaluation Gap in Medicine, AI and LLMs: Navigating Elusive Ground Truth & Uncertainty via a Probabilistic Paradigm](https://arxiv.org/abs/2601.05500)
*Aparna Elangovan,Lei Xu,Mahsa Elyasi,Ismail Akdulum,Mehmet Aksakal,Enes Gurun,Brian Hur,Saab Mansour,Ravid Shwartz Ziv,Karin Verspoor,Dan Roth*

Main category: cs.AI

TL;DR: 提出概率范式评估AI系统性能，考虑专家标注的不确定性，引入期望准确率和期望F1分数，建议按标注确定性分层评估


<details>
  <summary>Details</summary>
Motivation: 当前AI系统基准测试通常忽略专家标注中的不确定性，这在医学等不确定性普遍存在的领域尤为严重，可能导致误导性结论

Method: 引入概率范式理论分析，提出期望准确率和期望F1分数，建议按专家标注一致性进行分层评估

Result: 理论分析表明高确定性标注对专家获得高分至关重要，当整体性能低于80%时分层评估尤为关键

Conclusion: 应通过分层评估考虑标注不确定性，在确定性高的数据子集中进行可靠性能比较，避免不确定性对评估结果的混淆

Abstract: Benchmarking the relative capabilities of AI systems, including Large Language Models (LLMs) and Vision Models, typically ignores the impact of uncertainty in the underlying ground truth answers from experts. This ambiguity is particularly consequential in medicine where uncertainty is pervasive. In this paper, we introduce a probabilistic paradigm to theoretically explain how high certainty in ground truth answers is almost always necessary for even an expert to achieve high scores, whereas in datasets with high variation in ground truth answers there may be little difference between a random labeller and an expert. Therefore, ignoring uncertainty in ground truth evaluation data can result in the misleading conclusion that a non-expert has similar performance to that of an expert. Using the probabilistic paradigm, we thus bring forth the concepts of expected accuracy and expected F1 to estimate the score an expert human or system can achieve given ground truth answer variability.
  Our work leads to the recommendation that when establishing the capability of a system, results should be stratified by probability of the ground truth answer, typically measured by the agreement rate of ground truth experts. Stratification becomes critical when the overall performance drops below a threshold of 80%. Under stratified evaluation, performance comparison becomes more reliable in high certainty bins, mitigating the effect of the key confounding factor -- uncertainty.

</details>


### [9] [Explainable AI: Learning from the Learners](https://arxiv.org/abs/2601.05525)
*Ricardo Vinuesa,Steven L. Brunton,Gianmarco Mengaldo*

Main category: cs.AI

TL;DR: 本文提出将可解释人工智能（XAI）与因果推理相结合，通过"向学习者学习"的方法，从基础模型中提取因果机制，指导稳健的设计和控制，并支持高风险应用中的信任与问责。


<details>
  <summary>Details</summary>
Motivation: 尽管人工智能在多个科学和工程任务中已超越人类表现，但其内部表征往往不透明。需要开发能够解释AI决策过程的方法，以促进人类与AI在科学和工程领域的协作。

Method: 将可解释人工智能（XAI）与因果推理相结合，通过基础模型和可解释性方法的组合，从AI模型中提取因果机制，指导设计优化和认证过程。

Result: 该方法能够在发现、优化和认证三个关键领域发挥作用：提取因果机制、指导稳健设计和控制、支持高风险应用中的信任与问责。

Conclusion: XAI作为人类-AI协作的统一框架，虽然面临解释的忠实性、泛化性和可用性等挑战，但在科学和工程领域具有重要应用前景。

Abstract: Artificial intelligence now outperforms humans in several scientific and engineering tasks, yet its internal representations often remain opaque. In this Perspective, we argue that explainable artificial intelligence (XAI), combined with causal reasoning, enables {\it learning from the learners}. Focusing on discovery, optimization and certification, we show how the combination of foundation models and explainability methods allows the extraction of causal mechanisms, guides robust design and control, and supports trust and accountability in high-stakes applications. We discuss challenges in faithfulness, generalization and usability of explanations, and propose XAI as a unifying framework for human-AI collaboration in science and engineering.

</details>


### [10] [Safety Not Found (404): Hidden Risks of LLM-Based Robotics Decision Making](https://arxiv.org/abs/2601.05529)
*Jua Han,Jaeyoon Seo,Jungbin Min,Jean Oh,Jihie Kim*

Main category: cs.AI

TL;DR: 论文通过火灾疏散场景评估LLM在安全关键系统中的决策能力，发现即使99%准确率也意味着每百次执行可能造成灾难性后果，当前LLM不适合直接部署于安全关键系统。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型（LLM）在机器人决策中日益重要，其物理风险维度也随之增加。在安全关键环境中，LLM的单个错误指令可能直接危及人类生命安全，因此迫切需要系统评估LLM在微小错误即可能导致灾难性后果的场景中的表现。

Method: 首先通过火灾疏散场景的定性评估识别LLM决策中的关键失败案例。基于这些发现，设计了七项定量评估任务，分为三类：完整信息任务（使用ASCII地图最小化解译歧义）、不完整信息任务（要求模型推断缺失上下文）、安全导向空间推理任务（使用自然语言评估生命威胁情境下的安全决策）。对多种LLM和视觉语言模型进行了基准测试。

Result: 结果揭示了严重漏洞：多个模型在ASCII导航任务中成功率为0%；在模拟火灾演习中，模型指示机器人向危险区域移动而非紧急出口。分析表明，1%的失败率在机器人应用中会升级为灾难性后果，99%的准确率具有误导性，意味着每百次执行可能造成灾难性伤害。

Conclusion: 当前LLM尚未准备好直接部署于安全关键系统。即使最先进的模型也无法保证安全，绝对依赖它们会带来不可接受的风险。在机器人应用中，99%的准确率是危险的，因为这意味着每百次执行中可能有一次导致灾难性伤害。

Abstract: One mistake by an AI system in a safety-critical setting can cost lives. As Large Language Models (LLMs) become integral to robotics decision-making, the physical dimension of risk grows; a single wrong instruction can directly endanger human safety. This paper addresses the urgent need to systematically evaluate LLM performance in scenarios where even minor errors are catastrophic. Through a qualitative evaluation of a fire evacuation scenario, we identified critical failure cases in LLM-based decision-making. Based on these, we designed seven tasks for quantitative assessment, categorized into: Complete Information, Incomplete Information, and Safety-Oriented Spatial Reasoning (SOSR). Complete information tasks utilize ASCII maps to minimize interpretation ambiguity and isolate spatial reasoning from visual processing. Incomplete information tasks require models to infer missing context, testing for spatial continuity versus hallucinations. SOSR tasks use natural language to evaluate safe decision-making in life-threatening contexts. We benchmark various LLMs and Vision-Language Models (VLMs) across these tasks. Beyond aggregate performance, we analyze the implications of a 1% failure rate, highlighting how "rare" errors escalate into catastrophic outcomes. Results reveal serious vulnerabilities: several models achieved a 0% success rate in ASCII navigation, while in a simulated fire drill, models instructed robots to move toward hazardous areas instead of emergency exits. Our findings lead to a sobering conclusion: current LLMs are not ready for direct deployment in safety-critical systems. A 99% accuracy rate is dangerously misleading in robotics, as it implies one out of every hundred executions could result in catastrophic harm. We demonstrate that even state-of-the-art models cannot guarantee safety, and absolute reliance on them creates unacceptable risks.

</details>


### [11] [WildSci: Advancing Scientific Reasoning from In-the-Wild Literature](https://arxiv.org/abs/2601.05567)
*Tengxiao Liu,Deepak Nathani,Zekun Li,Kevin Yang,William Yang Wang*

Main category: cs.AI

TL;DR: WildSci是一个从同行评审文献自动合成的领域特定科学问题数据集，涵盖9个科学学科和26个子领域，通过将复杂科学推理任务转化为多项选择题格式，支持可扩展训练和强化学习微调。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在数学和编程等领域的推理进展迅速，但在医学和材料科学等科学领域进展有限，主要原因是数据集覆盖不足和开放科学问题的复杂性。

Method: 从同行评审文献自动合成领域特定科学问题，将复杂科学推理任务转化为多项选择题格式，应用强化学习对模型进行微调，并分析训练动态。

Result: 在一系列科学基准测试上的实验证明了数据集和方法的有效性，WildSci数据集已公开发布以支持科学推理的可扩展和可持续研究。

Conclusion: WildSci数据集通过提供高质量的科学问题和明确的奖励信号，解决了科学领域LLM推理的数据瓶颈问题，为科学推理研究提供了可扩展的基础设施。

Abstract: Recent progress in large language model (LLM) reasoning has focused on domains like mathematics and coding, where abundant high-quality data and objective evaluation metrics are readily available. In contrast, progress in LLM reasoning models remains limited in scientific domains such as medicine and materials science due to limited dataset coverage and the inherent complexity of open-ended scientific questions. To address these challenges, we introduce WildSci, a new dataset of domain-specific science questions automatically synthesized from peer-reviewed literature, covering 9 scientific disciplines and 26 subdomains. By framing complex scientific reasoning tasks in a multiple-choice format, we enable scalable training with well-defined reward signals. We further apply reinforcement learning to finetune models on these data and analyze the resulting training dynamics, including domain-specific performance changes, response behaviors, and generalization trends. Experiments on a suite of scientific benchmarks demonstrate the effectiveness of our dataset and approach. We release WildSci to enable scalable and sustainable research in scientific reasoning, available at https://huggingface.co/datasets/JustinTX/WildSci.

</details>


### [12] [Reinforcement Learning of Large Language Models for Interpretable Credit Card Fraud Detection](https://arxiv.org/abs/2601.05578)
*Cooper Lin,Yanting Zhang,Maohao Ran,Wei Xue,Hongwei Fan,Yibo Xu,Zhenglin Wan,Sirui Han,Yike Guo,Jun Song*

Main category: cs.AI

TL;DR: 本文提出了一种使用强化学习对轻量级语言模型进行后训练的方法，专门用于电子商务欺诈检测任务，仅使用原始交易数据，在真实数据集上取得了显著的F1分数提升。


<details>
  <summary>Details</summary>
Motivation: 电子商务平台和支付解决方案提供商面临日益复杂的欺诈方案，包括身份盗窃、账户接管和复杂的洗钱操作。尽管大语言模型在理论上具有潜力，但在真实金融欺诈检测中的应用尚未充分探索，其处理领域特定电子商务交易数据的实际效果尚未得到实证验证。

Method: 采用强化学习（RL）对轻量级语言模型进行后训练，专门用于欺诈检测任务。使用Group Sequence Policy Optimization（GSPO）算法结合基于规则的奖励系统，在中国全球支付解决方案公司提供的真实交易数据集上微调不同规模的语言模型。

Result: 实验结果表明该方法有效，后训练的语言模型在保留测试数据上实现了显著的F1分数提升。性能改进主要归因于强化学习固有的探索机制，使模型能够发现传统工程特征之外的新型欺诈指标。

Conclusion: 强化学习框架能够有效挖掘交易文本数据中嵌入的多样信任和风险信号，包括客户信息、运输详情、产品描述和订单历史等模式，为电子商务欺诈检测提供了新的有效方法。

Abstract: E-commerce platforms and payment solution providers face increasingly sophisticated fraud schemes, ranging from identity theft and account takeovers to complex money laundering operations that exploit the speed and anonymity of digital transactions. However, despite their theoretical promise, the application of Large Language Models (LLMs) to fraud detection in real-world financial contexts remains largely unexploited, and their practical effectiveness in handling domain-specific e-commerce transaction data has yet to be empirically validated. To bridge this gap between conventional machine learning limitations and the untapped potential of LLMs in fraud detection, this paper proposes a novel approach that employs Reinforcement Learning (RL) to post-train lightweight language models specifically for fraud detection tasks using only raw transaction data. We utilize the Group Sequence Policy Optimization (GSPO) algorithm combined with a rule-based reward system to fine-tune language models of various sizes on a real-life transaction dataset provided by a Chinese global payment solution company. Through this reinforcement learning framework, the language models are encouraged to explore diverse trust and risk signals embedded within the textual transaction data, including patterns in customer information, shipping details, product descriptions, and order history. Our experimental results demonstrate the effectiveness of this approach, with post-trained language models achieving substantial F1-score improvements on held-out test data. Our findings demonstrate that the observed performance improvements are primarily attributable to the exploration mechanism inherent in reinforcement learning, which allows models to discover novel fraud indicators beyond those captured by traditional engineered features.

</details>


### [13] [A Causal Information-Flow Framework for Unbiased Learning-to-Rank](https://arxiv.org/abs/2601.05590)
*Haoming Gong,Qingyao Ai,Zhihao Tao,Yongfeng Zhang*

Main category: cs.AI

TL;DR: 提出基于因果学习的新型排序框架，通过结构因果模型和信息论工具联合处理点击数据中的多种偏差，使用条件互信息测量偏差泄漏并作为正则化项，结合双重稳健估计器提高风险估计可靠性。


<details>
  <summary>Details</summary>
Motivation: 在网页搜索和推荐系统中，用户点击数据存在严重偏差（位置偏差、选择偏差、信任偏差），现有无偏学习排序方法主要校正位置偏差且依赖倾向性估计，无法测量剩余偏差、提供风险保证或联合处理多种偏差源。

Method: 结合结构因果模型和信息论工具：SCMs指定点击生成过程并识别真实相关性信号；条件互信息测量偏差泄漏到学习相关性估计中的程度；将泄漏测量作为正则化项加入模型训练以减少偏差；结合双重稳健估计器确保更可靠的风险估计。

Result: 在标准学习排序基准测试中，该方法持续减少测量的偏差泄漏并提高排序性能，特别是在位置偏差和信任偏差等多种偏差强烈交互的现实场景中表现更优。

Conclusion: 提出的因果学习排序框架通过联合建模多种偏差源并测量偏差泄漏，能够更有效地从有偏点击数据中学习真实相关性，优于现有无偏学习排序方法。

Abstract: In web search and recommendation systems, user clicks are widely used to train ranking models. However, click data is heavily biased, i.e., users tend to click higher-ranked items (position bias), choose only what was shown to them (selection bias), and trust top results more (trust bias). Without explicitly modeling these biases, the true relevance of ranked items cannot be correctly learned from clicks. Existing Unbiased Learning-to-Rank (ULTR) methods mainly correct position bias and rely on propensity estimation, but they cannot measure remaining bias, provide risk guarantees, or jointly handle multiple bias sources. To overcome these challenges, this paper introduces a novel causal learning-based ranking framework that extends ULTR by combining Structural Causal Models (SCMs) with information-theoretic tools. SCMs specify how clicks are generated and help identify the true relevance signal from click data, while conditional mutual information, measures how much bias leaks into the
  learned relevance estimates. We use this leakage measure to define a rigorous notion of disentanglement and include it as a regularizer during model training to reduce bias. In addition, we incorporate a causal inference estimator, i.e., doubly robust estimator, to ensure more reliable risk estimation. Experiments on standard Learning-to-Rank benchmarks show that our method consistently reduces measured bias leakage and improves ranking performance, especially in realistic scenarios where multiple biases-such as position and trust bias-interact strongly.

</details>


### [14] [Cumulative Path-Level Semantic Reasoning for Inductive Knowledge Graph Completion](https://arxiv.org/abs/2601.05629)
*Jiapu Wang,Xinghe Cheng,Zezheng Wu,Ruiqi Ma,Rui Wang,Zhichao Yan,Haoran Luo,Yuhao Jiang,Kai Sun*

Main category: cs.AI

TL;DR: CPSR框架通过查询相关掩码模块和全局语义评分模块，同时捕捉知识图谱的结构和语义信息，在归纳式知识图谱补全任务中实现了最先进的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识图谱补全方法在处理新兴实体时效果不佳，而现有的归纳式KGC方法虽然能处理新兴实体和关系，但仍面临对噪声结构信息敏感和难以捕捉推理路径中长距离依赖关系的挑战。

Method: 提出CPSR框架：1) 查询相关掩码模块自适应地屏蔽噪声结构信息，保留与目标密切相关的信息；2) 全局语义评分模块评估推理路径中节点的个体贡献和集体影响。

Result: 实验结果表明CPSR在归纳式知识图谱补全任务中实现了最先进的性能。

Conclusion: CPSR框架通过同时捕捉知识图谱的结构和语义信息，有效解决了现有归纳式KGC方法面临的噪声敏感和长距离依赖问题，提升了归纳式知识图谱补全的性能。

Abstract: Conventional Knowledge Graph Completion (KGC) methods aim to infer missing information in incomplete Knowledge Graphs (KGs) by leveraging existing information, which struggle to perform effectively in scenarios involving emerging entities. Inductive KGC methods can handle the emerging entities and relations in KGs, offering greater dynamic adaptability. While existing inductive KGC methods have achieved some success, they also face challenges, such as susceptibility to noisy structural information during reasoning and difficulty in capturing long-range dependencies in reasoning paths. To address these challenges, this paper proposes the Cumulative Path-Level Semantic Reasoning for inductive knowledge graph completion (CPSR) framework, which simultaneously captures both the structural and semantic information of KGs to enhance the inductive KGC task. Specifically, the proposed CPSR employs a query-dependent masking module to adaptively mask noisy structural information while retaining important information closely related to the targets. Additionally, CPSR introduces a global semantic scoring module that evaluates both the individual contributions and the collective impact of nodes along the reasoning path within KGs. The experimental results demonstrate that CPSR achieves state-of-the-art performance.

</details>


### [15] [GenCtrl -- A Formal Controllability Toolkit for Generative Models](https://arxiv.org/abs/2601.05637)
*Emily Cheng,Carmen Amo Alonso,Federico Danieli,Arno Blaas,Luca Zappella,Pau Rodriguez,Xavier Suau*

Main category: cs.AI

TL;DR: 该论文提出了一个理论框架来形式化分析生成模型的可控性，通过对话设置中的可控集估计算法，提供了样本复杂度相关的误差保证，并实证展示了模型可控性的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着生成模型变得无处不在，需要细粒度控制生成过程。然而，尽管从提示到微调的各种控制方法不断涌现，一个基本问题仍未解决：这些模型是否真正可控？

Method: 将人机交互建模为控制过程，提出一种新颖算法来估计对话设置中模型的可控集。该算法提供形式化保证：基于样本复杂度的估计误差边界，无需分布假设，仅需输出有界性假设，适用于任何黑盒非线性控制系统（即任何生成模型）。

Result: 在控制对话过程的不同任务上进行了实证验证，包括语言模型和文本到图像生成。结果显示模型可控性出人意料地脆弱，且高度依赖实验设置。

Conclusion: 需要严格的可控性分析，将重点从单纯尝试控制转向首先理解其基本限制。模型可控性存在根本性限制，需要更系统的方法来评估和确保可控性。

Abstract: As generative models become ubiquitous, there is a critical need for fine-grained control over the generation process. Yet, while controlled generation methods from prompting to fine-tuning proliferate, a fundamental question remains unanswered: are these models truly controllable in the first place? In this work, we provide a theoretical framework to formally answer this question. Framing human-model interaction as a control process, we propose a novel algorithm to estimate the controllable sets of models in a dialogue setting. Notably, we provide formal guarantees on the estimation error as a function of sample complexity: we derive probably-approximately correct bounds for controllable set estimates that are distribution-free, employ no assumptions except for output boundedness, and work for any black-box nonlinear control system (i.e., any generative model). We empirically demonstrate the theoretical framework on different tasks in controlling dialogue processes, for both language models and text-to-image generation. Our results show that model controllability is surprisingly fragile and highly dependent on the experimental setting. This highlights the need for rigorous controllability analysis, shifting the focus from simply attempting control to first understanding its fundamental limits.

</details>


### [16] [HAG: Hierarchical Demographic Tree-based Agent Generation for Topic-Adaptive Simulation](https://arxiv.org/abs/2601.05656)
*Rongxin Chen,Tianyu Wu,Bingbing Xu,Xiucheng Xu,Huawei Shen*

Main category: cs.AI

TL;DR: HAG是一个分层智能体生成框架，通过两阶段决策过程解决智能体初始化问题，实现宏观分布对齐和微观一致性，显著优于现有基线方法。


<details>
  <summary>Details</summary>
Motivation: 高保真的智能体初始化对于基于智能体建模的可信度至关重要。现有方法存在局限性：基于静态数据检索的方法无法适应未见主题，而基于LLM生成的方法缺乏宏观分布意识，导致微观属性与现实不一致。

Method: 提出HAG分层智能体生成框架，将群体生成形式化为两阶段决策过程：1)使用世界知识模型推断分层条件概率构建主题自适应树，实现宏观分布对齐；2)基于真实世界数据进行实例化和智能体增强，确保微观一致性。

Result: 实验表明HAG显著优于代表性基线方法，平均减少群体对齐误差37.7%，提升社会学一致性18.8%。建立了多领域基准和全面的PACE评估框架。

Conclusion: HAG框架有效解决了智能体初始化中的宏观分布对齐和微观一致性问题，为基于智能体建模提供了高质量的主题自适应智能体生成方法。

Abstract: High-fidelity agent initialization is crucial for credible Agent-Based Modeling across diverse domains. A robust framework should be Topic-Adaptive, capturing macro-level joint distributions while ensuring micro-level individual rationality. Existing approaches fall into two categories: static data-based retrieval methods that fail to adapt to unseen topics absent from the data, and LLM-based generation methods that lack macro-level distribution awareness, resulting in inconsistencies between micro-level persona attributes and reality. To address these problems, we propose HAG, a Hierarchical Agent Generation framework that formalizes population generation as a two-stage decision process. Firstly, utilizing a World Knowledge Model to infer hierarchical conditional probabilities to construct the Topic-Adaptive Tree, achieving macro-level distribution alignment. Then, grounded real-world data, instantiation and agentic augmentation are carried out to ensure micro-level consistency. Given the lack of specialized evaluation, we establish a multi-domain benchmark and a comprehensive PACE evaluation framework. Extensive experiments show that HAG significantly outperforms representative baselines, reducing population alignment errors by an average of 37.7% and enhancing sociological consistency by 18.8%.

</details>


### [17] [Circular Reasoning: Understanding Self-Reinforcing Loops in Large Reasoning Models](https://arxiv.org/abs/2601.05693)
*Zenghao Duan,Liang Pang,Zihao Wei,Wenbin Duan,Yuxin Tian,Shicheng Xu,Jingcheng Deng,Zhiyi Yin,Xueqi Cheng*

Main category: cs.AI

TL;DR: 论文提出"循环推理"这一新失败模式，开发LoopBench数据集进行分析，发现推理瓶颈触发循环，提出使用CUSUM算法进行早期预测


<details>
  <summary>Details</summary>
Motivation: 大型推理模型在测试时缩放中经常遇到重复循环，导致计算浪费和推理失败，需要系统分析这种独特的"循环推理"失败模式

Method: 1) 引入LoopBench数据集捕捉数值循环和语句循环两种类型；2) 从机制上分析循环推理作为状态崩溃现象；3) 使用累积和(CUSUM)算法捕捉循环前兆进行早期预测

Result: 实验验证了CUSUM算法在不同大型推理模型中的准确性，阐明了长链推理的稳定性特征

Conclusion: 循环推理是一种独特的自强化陷阱，通过CUSUM算法可以有效预测和避免这种推理失败模式

Abstract: Despite the success of test-time scaling, Large Reasoning Models (LRMs) frequently encounter repetitive loops that lead to computational waste and inference failure. In this paper, we identify a distinct failure mode termed Circular Reasoning. Unlike traditional model degeneration, this phenomenon manifests as a self-reinforcing trap where generated content acts as a logical premise for its own recurrence, compelling the reiteration of preceding text. To systematically analyze this phenomenon, we introduce LoopBench, a dataset designed to capture two distinct loop typologies: numerical loops and statement loops. Mechanistically, we characterize circular reasoning as a state collapse exhibiting distinct boundaries, where semantic repetition precedes textual repetition. We reveal that reasoning impasses trigger the loop onset, which subsequently persists as an inescapable cycle driven by a self-reinforcing V-shaped attention mechanism. Guided by these findings, we employ the Cumulative Sum (CUSUM) algorithm to capture these precursors for early loop prediction. Experiments across diverse LRMs validate its accuracy and elucidate the stability of long-chain reasoning.

</details>


### [18] [Logic-Parametric Neuro-Symbolic NLI: Controlling Logical Formalisms for Verifiable LLM Reasoning](https://arxiv.org/abs/2601.05705)
*Ali Farjami,Luca Redondi,Marco Valentino*

Main category: cs.AI

TL;DR: 提出一个逻辑参数化的神经符号推理框架，将逻辑形式作为可控组件而非固定背景，通过高阶逻辑嵌入多种经典和非经典逻辑，在规范性推理中比较逻辑外部和内部方法，发现逻辑内部策略能提升性能并产生更高效的混合证明。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定的逻辑形式主义，限制了神经符号推理的鲁棒性和适应性。需要将底层逻辑作为可控组件而非静态背景，以支持更灵活、可适应的推理系统。

Method: 采用逻辑参数化框架，使用LogiKEy方法将多种经典和非经典逻辑嵌入高阶逻辑(HOL)中，系统比较推理质量、解释细化和证明行为。特别关注规范性推理，对比逻辑外部方法（通过公理编码规范要求）和逻辑内部方法（规范模式从逻辑内置结构中涌现）。

Result: 实验表明逻辑内部策略能持续提升性能，并为自然语言推理产生更高效的混合证明。逻辑的有效性具有领域依赖性：一阶逻辑在常识推理中表现更好，而道义逻辑和模态逻辑在伦理领域表现更优。

Conclusion: 将逻辑作为神经符号架构中的一等参数化元素，能够实现更鲁棒、模块化和适应性强的推理系统，为可验证的自然语言推理提供更灵活的方法。

Abstract: Large language models (LLMs) and theorem provers (TPs) can be effectively combined for verifiable natural language inference (NLI). However, existing approaches rely on a fixed logical formalism, a feature that limits robustness and adaptability. We propose a logic-parametric framework for neuro-symbolic NLI that treats the underlying logic not as a static background, but as a controllable component. Using the LogiKEy methodology, we embed a range of classical and non-classical formalisms into higher-order logic (HOL), enabling a systematic comparison of inference quality, explanation refinement, and proof behavior. We focus on normative reasoning, where the choice of logic has significant implications. In particular, we compare logic-external approaches, where normative requirements are encoded via axioms, with logic-internal approaches, where normative patterns emerge from the logic's built-in structure. Extensive experiments demonstrate that logic-internal strategies can consistently improve performance and produce more efficient hybrid proofs for NLI. In addition, we show that the effectiveness of a logic is domain-dependent, with first-order logic favouring commonsense reasoning, while deontic and modal logics excel in ethical domains. Our results highlight the value of making logic a first-class, parametric element in neuro-symbolic architectures for more robust, modular, and adaptable reasoning.

</details>


### [19] [Overcoming Joint Intractability with Lossless Hierarchical Speculative Decoding](https://arxiv.org/abs/2601.05724)
*Yuxuan Zhou,Fei Huang,Heng Li,Fengyi Wu,Tianyu Wang,Jianwei Zhang,Junyang Lin,Zhi-Qi Cheng*

Main category: cs.AI

TL;DR: HSD是一种无损验证方法，通过分层概率平衡解决联合不可处理性问题，显著提升推测解码中的接受率，在EAGLE-3上实现超过12%的性能提升。


<details>
  <summary>Details</summary>
Motivation: 推测解码中验证是关键瓶颈，现有序列级验证方法依赖近似或受限于部分信息，难以处理联合不可处理性问题，需要更有效的验证方法。

Method: 提出分层推测解码（HSD），通过在不同可访问分支间平衡过量概率质量和不足概率质量，解决联合不可处理性问题，实现无损验证。

Result: HSD在不同模型家族和基准测试中一致提升接受率，集成到EAGLE-3中实现超过12%的性能增益，在不损失分布保真度的情况下达到最先进的解码效率。

Conclusion: HSD是一种可解释性强、通用性好的无损验证方法，可广泛应用于各种推测解码框架，显著提升解码效率。

Abstract: Verification is a key bottleneck in improving inference speed while maintaining distribution fidelity in Speculative Decoding. Recent work has shown that sequence-level verification leads to a higher number of accepted tokens compared to token-wise verification. However, existing solutions often rely on surrogate approximations or are constrained by partial information, struggling with joint intractability. In this work, we propose Hierarchical Speculative Decoding (HSD), a provably lossless verification method that significantly boosts the expected number of accepted tokens and overcomes joint intractability by balancing excess and deficient probability mass across accessible branches. Our extensive large-scale experiments demonstrate that HSD yields consistent improvements in acceptance rates across diverse model families and benchmarks. Moreover, its strong explainability and generality make it readily integrable into a wide range of speculative decoding frameworks. Notably, integrating HSD into EAGLE-3 yields over a 12% performance gain, establishing state-of-the-art decoding efficiency without compromising distribution fidelity. Code is available at https://github.com/ZhouYuxuanYX/Hierarchical-Speculative-Decoding.

</details>


### [20] [PII-VisBench: Evaluating Personally Identifiable Information Safety in Vision Language Models Along a Continuum of Visibility](https://arxiv.org/abs/2601.05739)
*G M Shahariar,Zabir Al Nazi,Md Olid Hasan Bhuiyan,Zhouxing Shi*

Main category: cs.AI

TL;DR: PII-VisBench基准测试评估视觉语言模型在个人身份信息泄露方面的安全性，发现模型对高在线可见度主体的PII泄露率更高，且存在模型家族异质性和PII类型差异。


<details>
  <summary>Details</summary>
Motivation: 现有VLM隐私评估主要将隐私视为静态提取任务，忽略了主体在线存在（可获取数据量）对隐私对齐的影响。需要建立考虑在线存在连续性的评估基准。

Method: 提出PII-VisBench基准，包含4000个独特探针，将200个主体按在线信息程度分为高、中、低、零四个可见度类别。评估18个开源VLM（0.3B-32B），使用拒绝率和条件PII泄露率两个关键指标。

Result: 模型呈现一致模式：随着主体可见度降低，拒绝率增加，PII泄露率从高可见度的9.10%降至低可见度的5.34%。模型更可能泄露高可见度主体的PII，存在显著的模型家族异质性和PII类型差异。改述和越狱式提示暴露了攻击和模型依赖的失败。

Conclusion: 需要基于可见度的安全性评估和训练干预，以解决VLM在隐私保护方面对高在线可见度主体的偏见和漏洞。

Abstract: Vision Language Models (VLMs) are increasingly integrated into privacy-critical domains, yet existing evaluations of personally identifiable information (PII) leakage largely treat privacy as a static extraction task and ignore how a subject's online presence--the volume of their data available online--influences privacy alignment. We introduce PII-VisBench, a novel benchmark containing 4000 unique probes designed to evaluate VLM safety through the continuum of online presence. The benchmark stratifies 200 subjects into four visibility categories: high, medium, low, and zero--based on the extent and nature of their information available online. We evaluate 18 open-source VLMs (0.3B-32B) based on two key metrics: percentage of PII probing queries refused (Refusal Rate) and the fraction of non-refusal responses flagged for containing PII (Conditional PII Disclosure Rate). Across models, we observe a consistent pattern: refusals increase and PII disclosures decrease (9.10% high to 5.34% low) as subject visibility drops. We identify that models are more likely to disclose PII for high-visibility subjects, alongside substantial model-family heterogeneity and PII-type disparities. Finally, paraphrasing and jailbreak-style prompts expose attack and model-dependent failures, motivating visibility-aware safety evaluation and training interventions.

</details>


### [21] [DynaDebate: Breaking Homogeneity in Multi-Agent Debate with Dynamic Path Generation](https://arxiv.org/abs/2601.05746)
*Zhenghao Li,Zhi Zheng,Wei Chen,Jielun Zhao,Yong Chen,Tong Xu,Enhong Chen*

Main category: cs.AI

TL;DR: DynaDebate提出了一种动态多智能体辩论框架，通过路径生成、过程中心辩论和触发验证机制解决现有方法中智能体推理路径趋同、辩论效果退化为简单投票的问题。


<details>
  <summary>Details</summary>
Motivation: 现有基于大语言模型的多智能体辩论框架存在两个主要问题：1）智能体通常采用相同的初始化推理路径，导致犯相同错误；2）辩论过程容易退化为简单的多数投票，无法有效提升推理质量。

Method: DynaDebate包含三个核心机制：1）动态路径生成与分配：使用专门的路径生成智能体生成多样化、逻辑化的解决方案路径；2）过程中心辩论：从结果投票转向逐步逻辑批判，确保过程正确性；3）触发式验证智能体：在出现分歧时激活，使用外部工具客观解决僵局。

Result: 大量实验表明，DynaDebate在各种基准测试中实现了优越性能，超越了现有的最先进多智能体辩论方法。

Conclusion: DynaDebate通过动态路径生成、过程中心辩论和触发验证机制，有效解决了多智能体辩论中的路径趋同和辩论退化问题，显著提升了协作推理能力。

Abstract: Recent years have witnessed the rapid development of Large Language Model-based Multi-Agent Systems (MAS), which excel at collaborative decision-making and complex problem-solving. Recently, researchers have further investigated Multi-Agent Debate (MAD) frameworks, which enhance the reasoning and collaboration capabilities of MAS through information exchange and debate among multiple agents. However, existing approaches often rely on unguided initialization, causing agents to adopt identical reasoning paths that lead to the same errors. As a result, effective debate among agents is hindered, and the final outcome frequently degenerates into simple majority voting. To solve the above problem, in this paper, we introduce Dynamic Multi-Agent Debate (DynaDebate), which enhances the effectiveness of multi-agent debate through three key mechanisms: (1) Dynamic Path Generation and Allocation, which employs a dedicated Path Generation Agent to generate diverse and logical solution paths with adaptive redundancy; (2) Process-Centric Debate, which shifts the focus from surface-level outcome voting to rigorous step-by-step logic critique to ensure process correctness; (3) A Trigger-Based Verification Agent, which is activated upon disagreement and uses external tools to objectively resolve deadlocks. Extensive experiments demonstrate that DynaDebate achieves superior performance across various benchmarks, surpassing existing state-of-the-art MAD methods.

</details>


### [22] [From Off-Policy to On-Policy: Enhancing GUI Agents via Bi-level Expert-to-Policy Assimilation](https://arxiv.org/abs/2601.05787)
*Zezhou Wang,Ziyun Zhang,Xiaoyi Zhang,Zhuzhong Qian,Yan Lu*

Main category: cs.AI

TL;DR: BEPA方法通过双层专家轨迹同化，将静态专家轨迹转化为策略对齐的指导，显著提升了端到端GUI操作策略在OSWorld-Verified等基准上的性能。


<details>
  <summary>Details</summary>
Motivation: 当前GUI数据集（如OSWorld）存在两个瓶颈：1）仅暴露数百个可交互、可验证的任务和环境；2）专家轨迹必须通过与这些环境交互收集，难以扩展。因此需要研究如何利用少量现有专家轨迹通过可验证奖励的强化学习来训练端到端策略。

Method: 提出BEPA（双层专家到策略同化）方法：LEVEL-1通过基础策略下的自滚动可达轨迹将静态专家轨迹转化为策略对齐的指导；LEVEL-2使用按任务动态更新的缓存进行RLVR（可验证奖励的强化学习）。

Result: 在OSWorld-Verified上，BEPA将UITARS1.5-7B的成功率从22.87%提升到32.13%，在保留集上从5.74%提升到10.30%，在MMBench-GUI和Online-Mind2Web上也有一致的性能提升。

Conclusion: BEPA方法有效解决了专家轨迹与学习者之间的结构不匹配和分布偏移问题，显著提升了端到端GUI操作策略的性能，为小规模专家轨迹的有效利用提供了解决方案。

Abstract: Vision-language models are increasingly deployed as computer-use agents (CUAs) that operate desktops and browsers. Top-performing CUAs are framework-based systems that decompose planning and execution, while end-to-end screenshot-to-action policies are easier to deploy but lag behind on benchmarks such as OSWorld-Verified. GUI datasets like OSWorld pose two bottlenecks: they expose only a few hundred interactive, verifiable tasks and environments, and expert trajectories must be gathered by interacting with these environments, making such data hard to scale. We therefore ask how reinforcement learning from verifiable rewards (RLVR) can best exploit a small pool of exist expert trajectories to train end-to-end policies. Naively mixing these off-policy traces into on-policy RLVR is brittle: even after format conversion, expert trajectories exhibit structural mismatch and distribution shift from the learner. We propose BEPA (Bi-Level Expert-to-Policy Assimilation), which turns static expert traces into policy-aligned guidance via self-rolled reachable trajectories under the base policy (LEVEL-1) and a per-task, dynamically updated cache used in RLVR (LEVEL-2). On OSWorld-Verified, BEPA improves UITARS1.5-7B success from 22.87% to 32.13% and raises a held-out split from 5.74% to 10.30%, with consistent gains on MMBench-GUI and Online-Mind2Web. Our code and data are available at: https://github.com/LEON-gittech/Verl_GUI.git

</details>


### [23] [StackPlanner: A Centralized Hierarchical Multi-Agent System with Task-Experience Memory Management](https://arxiv.org/abs/2601.05890)
*Ruizhe Zhang,Xinke Jiang,Zhibang Yang,Zhixin Zhang,Jiaran Gao,Yuzhen Xiao,Hongbin Lai,Xu Chu,Junfeng Zhao,Yasha Wang*

Main category: cs.AI

TL;DR: StackPlanner是一个具有显式内存控制的分层多智能体框架，通过解耦高层协调与子任务执行来解决长时程协作中的内存管理问题，并利用结构化经验记忆和强化学习重用协调经验。


<details>
  <summary>Details</summary>
Motivation: 基于大语言模型的多智能体系统在处理复杂知识密集型任务时表现出潜力，但中心化架构中的中央智能体由于缺乏内存管理，导致上下文膨胀、错误累积和跨任务泛化能力差，影响了长时程协作的稳定性。

Method: 提出StackPlanner分层多智能体框架，通过主动任务级内存控制将高层协调与子任务执行解耦，同时利用结构化经验记忆和强化学习来检索和利用可重用的协调经验。

Result: 在多个深度搜索和智能体系统基准测试上的实验证明了该方法在实现可靠的长时程多智能体协作方面的有效性。

Conclusion: StackPlanner通过显式内存控制和经验重用机制，有效解决了多智能体系统中任务级内存效率低下和协调经验无法重用的问题，为可靠的长期协作提供了解决方案。

Abstract: Multi-agent systems based on large language models, particularly centralized architectures, have recently shown strong potential for complex and knowledge-intensive tasks. However, central agents often suffer from unstable long-horizon collaboration due to the lack of memory management, leading to context bloat, error accumulation, and poor cross-task generalization. To address both task-level memory inefficiency and the inability to reuse coordination experience, we propose StackPlanner, a hierarchical multi-agent framework with explicit memory control. StackPlanner addresses these challenges by decoupling high-level coordination from subtask execution with active task-level memory control, and by learning to retrieve and exploit reusable coordination experience via structured experience memory and reinforcement learning. Experiments on multiple deep-search and agent system benchmarks demonstrate the effectiveness of our approach in enabling reliable long-horizon multi-agent collaboration.

</details>


### [24] [TowerMind: A Tower Defence Game Learning Environment and Benchmark for LLM as Agents](https://arxiv.org/abs/2601.05899)
*Dawei Wang,Chengming Zhou,Di Zhao,Xinyuan Liu,Marci Chi Ma,Gary Ushaw,Richard Davison*

Main category: cs.AI

TL;DR: TowerMind是一个基于塔防游戏的轻量级多模态环境，用于评估大语言模型在实时策略游戏中的长期规划和决策能力，揭示了LLMs与人类专家在能力和幻觉方面的差距。


<details>
  <summary>Details</summary>
Motivation: 现有RTS游戏环境要么计算需求高，要么缺乏文本观察支持，限制了LLMs在RTS游戏中的评估。塔防游戏作为RTS子类，保留了评估LLMs规划决策能力的关键优势，同时计算需求低且支持多模态观察。

Method: 开发了TowerMind环境，基于塔防游戏，具有低计算需求和多模态观察空间（像素、文本、结构化游戏状态）。设计了五个基准关卡，在不同多模态输入设置下评估多个广泛使用的LLMs，并评估了两种经典强化学习算法（Ape-X DQN和PPO）。

Result: 结果显示LLMs与人类专家在能力和幻觉维度存在明显性能差距。实验突出了LLMs的关键限制：规划验证不足、决策缺乏多终局性、行动使用效率低。TowerMind作为轻量级多模态设计，补充了现有RTS游戏环境。

Conclusion: TowerMind提供了一个轻量级、多模态的RTS游戏环境，能够有效评估LLMs的长期规划和决策能力，揭示了当前LLMs在策略游戏中的局限性，为AI智能体领域引入了新的基准测试。

Abstract: Recent breakthroughs in Large Language Models (LLMs) have positioned them as a promising paradigm for agents, with long-term planning and decision-making emerging as core general-purpose capabilities for adapting to diverse scenarios and tasks. Real-time strategy (RTS) games serve as an ideal testbed for evaluating these two capabilities, as their inherent gameplay requires both macro-level strategic planning and micro-level tactical adaptation and action execution. Existing RTS game-based environments either suffer from relatively high computational demands or lack support for textual observations, which has constrained the use of RTS games for LLM evaluation. Motivated by this, we present TowerMind, a novel environment grounded in the tower defense (TD) subgenre of RTS games. TowerMind preserves the key evaluation strengths of RTS games for assessing LLMs, while featuring low computational demands and a multimodal observation space, including pixel-based, textual, and structured game-state representations. In addition, TowerMind supports the evaluation of model hallucination and provides a high degree of customizability. We design five benchmark levels to evaluate several widely used LLMs under different multimodal input settings. The results reveal a clear performance gap between LLMs and human experts across both capability and hallucination dimensions. The experiments further highlight key limitations in LLM behavior, such as inadequate planning validation, a lack of multifinality in decision-making, and inefficient action use. We also evaluate two classic reinforcement learning algorithms: Ape-X DQN and PPO. By offering a lightweight and multimodal design, TowerMind complements the existing RTS game-based environment landscape and introduces a new benchmark for the AI agent field. The source code is publicly available on GitHub(https://github.com/tb6147877/TowerMind).

</details>


### [25] [Open-Vocabulary 3D Instruction Ambiguity Detection](https://arxiv.org/abs/2601.05991)
*Jiayu Ding,Haoran Tang,Ge Li*

Main category: cs.AI

TL;DR: 提出首个开放词汇3D指令歧义检测任务，构建大规模基准数据集Ambi3D，发现现有3D大语言模型在歧义检测上表现不佳，提出两阶段框架AmbiVer解决该问题。


<details>
  <summary>Details</summary>
Motivation: 在安全关键领域（如手术场景），语言歧义可能导致严重后果，但现有具身AI研究大多忽视这一问题，假设指令清晰且专注于执行而非确认。为填补这一安全空白，需要研究如何在3D场景中检测指令歧义。

Method: 提出AmbiVer两阶段框架：1）从多个视角收集明确的视觉证据；2）使用这些证据引导视觉语言模型判断指令歧义性。同时构建了Ambi3D基准数据集，包含700多个多样3D场景和约22k条指令。

Result: 实验发现最先进的3D大语言模型在可靠判断指令歧义性方面存在困难。AmbiVer框架在歧义检测任务上表现出有效性，为更安全可靠的具身AI铺平道路。

Conclusion: 首次定义了开放词汇3D指令歧义检测任务，揭示了现有模型的局限性，提出的AmbiVer框架为解决这一关键安全问题提供了有效方法，促进了更安全可信的具身AI发展。

Abstract: In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like "Pass me the vial" in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [26] [Protosampling: Enabling Free-Form Convergence of Sampling and Prototyping through Canvas-Driven Visual AI Generation](https://arxiv.org/abs/2601.05401)
*Alicia Guo,David Ledo,George Fitzmaurice,Fraser Anderson*

Main category: cs.HC

TL;DR: 论文提出"protosampling"概念，即利用生成式AI进行原型采样，将思维与创作空间融合，通过Atelier系统实现可视化创作流程。


<details>
  <summary>Details</summary>
Motivation: 传统创意过程依赖现有媒体的采样和原型制作，而生成式AI使从业者能够超越现有媒体，即时生成和重新混合新内容。这种融合需要新的工具和方法来支持创意工作流程。

Method: 提出"protosampling"概念框架，并开发Atelier系统：1）融合思维和创作空间，使参考素材和生成资产共存；2）提供封装的技术工作流程；3）通过交互式可视化、智能搜索和收藏功能支持涌现式导航。

Result: Atelier系统实现了基于多种生成图像和视频模型的视觉创作平台，将protosampling概念操作化，为创意工作提供了新的工具和方法论框架。

Conclusion: Protosampling作为一种视角重新构建了创意工作，强调过程本身以及看似不相关的想法如何紧密交织成最终解决方案，为生成式AI时代的创意实践提供了新范式。

Abstract: As an emergent process, creativity relies on explorations via sampling and prototyping for problem construction. These activities compile knowledge, provide a context enveloping the solution, and answer questions. With Generative AI, practitioners can go beyond sampling existing media towards instantly generating and remixing new ones. We refer to this convergence as 'protosampling'. Using existing literature we ground a definition for protosampling and operationalize it through Atelier, a canvas-like system that leverages a variety of generative image and video models for visual creation. Atelier: (1) blends the spaces for thinking and creation, where both references and generated assets co-exist in one space, (2) provides various encapsulated technical workflows that focus on the activity at hand, and (3) enables navigating emergence through interactive visualizations, smart search, and collections. Protosampling as a lens reframes creative work to emphasize the process itself and how seemingly disjointed thoughts can tightly interweave into a final solution.

</details>


### [27] [Secure Text Entry using a Virtual Radial Keyboard with Dynamically Resized Keys and Non-Intrusive Randomization](https://arxiv.org/abs/2601.05516)
*Yuxuan Huang,Qiao Jin,Tongyu Nie,Victoria Interrante,Evan Suma Rosenberg*

Main category: cs.HC

TL;DR: 本文针对VR文本输入的安全性问题，发现现有安全方法的漏洞，提出了一种新型虚拟径向键盘设计，在安全性和可用性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 随着虚拟现实的普及，安全高效的文本输入需求日益重要。作者发现现有最先进的VR安全文本输入方法存在漏洞，需要设计一种既能保证安全又具有良好可用性的解决方案。

Method: 设计了一种字母顺序排列的圆形布局虚拟径向键盘，通过控制器旋转选择按键，按键动态扩展以方便精确选择。采用随机旋转机制，每次击键后键盘随机旋转，保持相对键位但破坏绝对空间映射，防止推理攻击。

Result: 通过30名参与者的组内研究，与现有安全技术和标准QWERTY键盘对比。结果显示径向键盘显著提高了抗击键预测攻击的能力，但由于非QWERTY布局的陌生性，在输入速度和主观工作量方面存在权衡。定量趋势和定性反馈表明通过练习有很强的性能提升潜力。

Conclusion: 虚拟径向键盘在安全性和可用性之间取得了良好平衡，虽然存在学习曲线，但具有改进潜力。讨论了设计启示、可能的界面改进和未来研究方向，包括布局变化和视觉增强。

Abstract: As virtual reality (VR) becomes more widely adopted, secure and efficient text entry is an increasingly critical need. In this paper, we identify a vulnerability in a state-of-the-art secure VR text entry method and introduce a novel virtual radial keyboard designed to achieve a balance between security with usability. Keys are arranged alphabetically in a circular layout, with each key selected by controller rotation and dynamically expanding to facilitate precise selection. A randomized rotation mechanism shifts the keyboard after each keystroke, preserving relative key positions while disrupting absolute spatial mappings to protect against inference attacks. We conducted a within-subject study (N=30) comparing our method with the prior secure technique and a standard QWERTY keyboard. Results showed that the radial keyboard significantly improves resistance to keystroke prediction attacks while incurring a tradeoff in entry speed and subjective workload due to the unfamiliar non-QWERTY layout. However, both quantitative trends and qualitative feedback indicate strong potential for performance improvements with practice. We also discuss design implications, possible interface refinements, and directions for future work, including layout variations and visual enhancements.

</details>


### [28] [Advancing credit mobility through stakeholder-informed AI design and adoption](https://arxiv.org/abs/2601.05666)
*Yerin Kwak,Siddharth Adelkar,Zachary A. Pardos*

Main category: cs.HC

TL;DR: 该研究应用人工智能技术为社区大学转学到四年制大学的学生提供课程衔接建议，通过与纽约州立大学系统合作，开发了一种监督对齐方法，显著提高了课程匹配准确性，预计可增加12倍的学分转移机会。


<details>
  <summary>Details</summary>
Motivation: 从两年制社区大学转学到四年制大学对学生的社会经济流动至关重要，但学生在学分转移过程中经常面临课程衔接问题，导致学业延误和额外成本。当前建立课程衔接协议的过程耗时耗力，需要人工审核所有可能的课程匹配，而人工智能在这方面的应用仍然有限。

Method: 首先对课程衔接工作人员和教师进行调查，评估基线算法建议的采纳率并收集反馈。基于这些见解，开发了一种监督对齐方法，解决了课程目录描述中的表面匹配和机构偏见问题。该方法与纽约州立大学系统合作实施，这是美国最大的高等教育系统之一。

Result: 新方法比先前方法的准确性提高了5.5倍。根据教师和工作人员平均61%的调查采纳率，这些发现预计能够增加12倍的有效学分转移机会，否则这些机会将无法实现。

Conclusion: 研究表明，基于利益相关者信息设计的人工智能在高等教育管理中的应用，可以扩展学生的学分流动性，并有助于重塑当前机构在课程衔接方面的决策过程。这种方法为解决学分转移挑战提供了可扩展的解决方案。

Abstract: Transferring from a 2-year to a 4-year college is crucial for socioeconomic mobility, yet students often face challenges ensuring their credits are fully recognized, leading to delays in their academic progress and unexpected costs. Determining whether courses at different institutions are equivalent (i.e., articulation) is essential for successful credit transfer, as it minimizes unused credits and increases the likelihood of bachelor's degree completion. However, establishing articulation agreements remains time- and resource-intensive, as all candidate articulations are reviewed manually. Although recent efforts have explored the use of artificial intelligence to support this work, its use in articulation practice remains limited. Given these challenges and the need for scalable support, this study applies artificial intelligence to suggest articulations between institutions in collaboration with the State University of New York system, one of the largest systems of higher education in the US. To develop our methodology, we first surveyed articulation staff and faculty to assess adoption rates of baseline algorithmic recommendations and gather feedback on perceptions and concerns about these recommendations. Building on these insights, we developed a supervised alignment method that addresses superficial matching and institutional biases in catalog descriptions, achieving a 5.5-fold improvement in accuracy over previous methods. Based on articulation predictions of this method and a 61% average surveyed adoption rate among faculty and staff, these findings project a 12-fold increase in valid credit mobility opportunities that would otherwise remain unrealized. This study suggests that stakeholder-informed design of AI in higher education administration can expand student credit mobility and help reshape current institutional decision-making in course articulation.

</details>


### [29] [SAFE: Secure and Accurate Federated Learning for Privacy-Preserving Brain-Computer Interfaces](https://arxiv.org/abs/2601.05789)
*Tianwang Jia,Xiaoqing Chen,Dongrui Wu*

Main category: cs.HC

TL;DR: SAFE是一种基于联邦学习的脑机接口算法，通过本地训练保护隐私，使用批特定归一化提高泛化能力，并通过对抗训练增强鲁棒性，在多个EEG数据集上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 基于EEG的脑机接口解码算法面临泛化能力不足、对抗脆弱性和隐私泄露三大挑战，需要一种能同时解决这些问题的方案。

Method: 提出SAFE联邦学习方法：1）保持数据本地化保护隐私；2）使用本地批特定归一化缓解跨被试特征分布偏移；3）通过联邦对抗训练和对抗权重扰动在输入空间和参数空间引入扰动增强对抗鲁棒性。

Result: 在5个EEG数据集（运动想象和事件相关电位范式）上测试，SAFE在解码准确性和对抗鲁棒性方面均优于14种最先进方法，甚至优于不考虑隐私保护的集中式训练方法。

Conclusion: SAFE是首个无需目标被试校准数据就能同时实现高解码精度、强对抗鲁棒性和可靠隐私保护的算法，非常适合实际脑机接口应用。

Abstract: Electroencephalogram (EEG)-based brain-computer interfaces (BCIs) are widely adopted due to their efficiency and portability; however, their decoding algorithms still face multiple challenges, including inadequate generalization, adversarial vulnerability, and privacy leakage. This paper proposes Secure and Accurate FEderated learning (SAFE), a federated learning-based approach that protects user privacy by keeping data local during model training. SAFE employs local batch-specific normalization to mitigate cross-subject feature distribution shifts and hence improves model generalization. It further enhances adversarial robustness by introducing perturbations in both the input space and the parameter space through federated adversarial training and adversarial weight perturbation. Experiments on five EEG datasets from motor imagery (MI) and event-related potential (ERP) BCI paradigms demonstrated that SAFE consistently outperformed 14 state-of-the-art approaches in both decoding accuracy and adversarial robustness, while ensuring privacy protection. Notably, it even outperformed centralized training approaches that do not consider privacy protection at all. To our knowledge, SAFE is the first algorithm to simultaneously achieve high decoding accuracy, strong adversarial robustness, and reliable privacy protection without using any calibration data from the target subject, making it highly desirable for real-world BCIs.

</details>


### [30] [Decoding Workload and Agreement From EEG During Spoken Dialogue With Conversational AI](https://arxiv.org/abs/2601.05825)
*Lucija Mihić Zidar,Philipp Wicke,Praneel Bhatia,Rosa Lutz,Marius Klug,Thorsten O. Zander*

Main category: cs.HC

TL;DR: 研究验证了被动脑机接口在口语人机对话中的可行性，将脑电图分类器从受控任务迁移到真实对话场景，建立了端到端处理流程。


<details>
  <summary>Details</summary>
Motivation: 被动脑机接口可为大语言模型对齐提供隐式反馈，但现有心理状态解码主要在受控任务中进行，需要验证其在真实口语对话中的适用性。

Method: 引入两种对话范式（拼字游戏和句子补全任务），建立端到端流程进行转录、注释，并将词级对话事件与连续脑电图分类器输出对齐。

Result: 工作量解码在口语互动中显示出可解释的趋势，支持跨范式迁移；隐式协议解码实现了连续应用和精确时间对齐，但也发现了构造迁移和异步应用的限制。

Conclusion: 研究确立了将被动脑机接口信号整合到对话AI系统中的可行性和约束条件，为未来应用奠定了基础。

Abstract: Passive brain-computer interfaces offer a potential source of implicit feedback for alignment of large language models, but most mental state decoding has been done in controlled tasks. This paper investigates whether established EEG classifiers for mental workload and implicit agreement can be transferred to spoken human-AI dialogue. We introduce two conversational paradigms - a Spelling Bee task and a sentence completion task- and an end-to-end pipeline for transcribing, annotating, and aligning word-level conversational events with continuous EEG classifier output. In a pilot study, workload decoding showed interpretable trends during spoken interaction, supporting cross-paradigm transfer. For implicit agreement, we demonstrate continuous application and precise temporal alignment to conversational events, while identifying limitations related to construct transfer and asynchronous application of event-based classifiers. Overall, the results establish feasibility and constraints for integrating passive BCI signals into conversational AI systems.

</details>


### [31] [How to Analyse Interviews: A Documentary Method of Interpretation](https://arxiv.org/abs/2601.05871)
*Andy Crabtree*

Main category: cs.HC

TL;DR: 提出一种新的纪录片解释方法，用于分析访谈转录本中的内生主题，该方法不需要分析者具备定性分析专业知识，仅需掌握自然语言能力


<details>
  <summary>Details</summary>
Motivation: HCI领域访谈普遍，但现有定性分析方法需要分析者具备专业知识和理论背景，限制了非专业人士参与分析

Method: 提出纪录片解释方法，专注于分析转录本集合中的内生主题，这些主题源于参与者集体推理，与内容分析、扎根理论、解释现象学分析和主题分析等传统方法形成对比

Result: DMI方法不需要分析者精通定性分析或具备理论基础，是一种成员方法而非社会科学方法，依赖大多数人具备的自然语言能力

Conclusion: 纪录片解释方法为HCI访谈分析提供了更易访问的替代方案，使非专业人士也能有效分析转录本中的内生主题和集体推理

Abstract: Interviews are commonplace in HCI. This paper presents a novel documentary method of interpretation that supports analysis of the topics contained within a collection of transcripts, topics that are endogenous to it and which elaborate participants collective reasoning about issues of relevance to research. We contrast endogenous topic analysis with established qualitative approaches, including content analysis, grounded theory, interpretative phenomenological analysis, and thematic analysis, to draw out the distinctive character of the documentary method of interpretation. Unlike established methods, the DMI does not require that the analyst be proficient in qualitative analysis, or have sound knowledge of underlying theories and methods. The DMI is a members method, not a social science method, that relies on mastery of natural language; a competence most people possess.

</details>


### [32] [A Framework for Optimizing Human-Machine Interaction in Classification Systems](https://arxiv.org/abs/2601.05974)
*Goran Muric,Steven Minton*

Main category: cs.HC

TL;DR: 提出基于双阈值策略优化人机协同分类系统的框架，通过设定上下阈值自动处理置信案例，将模糊案例路由给人工审核，平衡系统准确性与人工工作量。


<details>
  <summary>Details</summary>
Motivation: 自动化决策系统在不确定情况下需要人工监督来确保准确性，但如何有效平衡系统自动化与人工干预的工作量是一个实际问题。

Method: 采用双阈值策略：设定下阈值和上阈值，置信度高的案例自动接受，置信度低的自动拒绝，中间模糊案例路由给人工审核。将该问题形式化为优化任务，通过蒙特卡洛模拟验证行为。

Result: 量化了不同概率分数分布对人工干预效率的影响，识别了边际收益递减区域（额外审核带来的收益最小）。

Conclusion: 该框架为需要选择性人工验证的任何决策流程提供了通用、可重复的方法，可应用于实体解析、欺诈检测、医疗分诊和内容审核等领域。

Abstract: Automated decision systems increasingly rely on human oversight to ensure accuracy in uncertain cases. This paper presents a practical framework for optimizing such human-in-the-loop classification systems using a double-threshold policy. Instead of relying on a single decision cutoff, the system defines two thresholds (a lower and an upper) to automatically accept or reject confident cases while routing ambiguous ones for human review. We formalize this problem as an optimization task that balances system accuracy against human review workload and demonstrate its behavior through extensive Monte Carlo simulations. Our results quantify how different probability score distributions affect the efficiency of human intervention and identify the regions of diminishing returns where additional review yields minimal benefit. The framework provides a general, reproducible method for improving reliability in any decision pipeline requiring selective human validation, including applications in entity resolution, fraud detection, medical triage, and content moderation.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [33] [MoEBlaze: Breaking the Memory Wall for Efficient MoE Training on Modern GPUs](https://arxiv.org/abs/2601.05296)
*Jiyuan Zhang,Yining Liu,Siqi Yan,Lisen Deng,Jennifer Cao,Shuqi Yang,Min Ni,Bi Xue,Shen Li*

Main category: cs.LG

TL;DR: MoEBlaze是一个内存高效的MoE训练框架，通过协同设计的系统方法解决MoE架构中的内存瓶颈问题，实现4倍加速和50%内存节省。


<details>
  <summary>Details</summary>
Motivation: 现代大规模Mixture-of-Experts架构中的"内存墙"瓶颈被显著放大。MoE固有的架构稀疏性导致稀疏算术计算，同时引入了大量的激活内存开销，包括大型令牌路由缓冲区和需要实例化和缓冲中间张量。这种内存压力限制了GPU上可以容纳的最大批处理大小和序列长度，并导致过多的数据移动，阻碍了性能和高效的模型扩展。

Method: MoEBlaze采用协同设计的系统方法：1）端到端的令牌调度和MoE训练方法，使用优化的数据结构消除中间缓冲区和激活实例化；2）协同设计的内核与智能激活检查点，在减少内存占用的同时实现更好的性能。

Result: MoEBlaze相比现有的MoE框架可以实现超过4倍的加速和超过50%的内存节省。

Conclusion: MoEBlaze通过协同设计的系统方法有效解决了MoE训练中的内存瓶颈问题，显著提升了训练效率和可扩展性。

Abstract: The pervasive "memory wall" bottleneck is significantly amplified in modern large-scale Mixture-of-Experts (MoE) architectures. MoE's inherent architectural sparsity leads to sparse arithmetic compute and also introduces substantial activation memory overheads -- driven by large token routing buffers and the need to materialize and buffer intermediate tensors. This memory pressure limits the maximum batch size and sequence length that can fit on GPUs, and also results in excessive data movements that hinders performance and efficient model scaling. We present MoEBlaze, a memory-efficient MoE training framework that addresses these issues through a co-designed system approach: (i) an end-to-end token dispatch and MoE training method with optimized data structures to eliminate intermediate buffers and activation materializing, and (ii) co-designed kernels with smart activation checkpoint to mitigate memory footprint while simultaneously achieving better performance. We demonstrate that MoEBlaze can achieve over 4x speedups and over 50% memory savings compared to existing MoE frameworks.

</details>


### [34] [TIME: Temporally Intelligent Meta-reasoning Engine for Context Triggered Explicit Reasoning](https://arxiv.org/abs/2601.05300)
*Susmit Das*

Main category: cs.LG

TL;DR: TIME框架通过引入时间标签和短思考块，让大语言模型能够根据对话和时间线索进行上下文敏感推理，显著减少推理标记并提升时间感知能力


<details>
  <summary>Details</summary>
Motivation: 现有推理导向的大语言模型存在三个主要问题：1）推理过程通常作为冗长的全局痕迹出现在回复开头，成本高昂且模糊了声明级别的可审计性；2）一旦模型开始呈现结果，就无法重新触发显式推理；3）对话模型对时间结构不敏感，除非在文本中明确说明时间

Method: 引入TIME（Temporally Intelligent Meta-reasoning Engine）框架，通过添加可选的ISO 8601时间标签、代表静默间隔的tick turns以及可在回复中任意位置出现的短思考块。采用四阶段课程训练，包括一个小型但最大多样化的全批次对齐步骤，训练Qwen3密集模型进行简短、就地推理爆发，并保持面向用户的文本紧凑

Result: 在4B到32B规模上，TIME在TIMEBench基准测试中（涵盖时间顺序、间隔和偏移下的常识、异常检测和连续性）的表现优于基础Qwen3模型，同时将推理标记减少约一个数量级

Conclusion: TIME框架成功地将显式推理转化为上下文敏感的资源，通过时间感知的对话增强，显著提高了模型的时间智能和推理效率，为更自然、更高效的对话系统提供了新方向

Abstract: Reasoning oriented large language models often expose explicit "thinking" as long, turn-global traces at the start of every response, either always on or toggled externally at inference time. While useful for arithmetic, programming, and problem solving, this design is costly, blurs claim level auditability, and cannot re-trigger explicit reasoning once the model begins presenting. Dialogue models are also largely blind to temporal structure, treating replies after seconds and replies after weeks as equivalent unless time is stated in text. We introduce TIME, the Temporally Intelligent Meta-reasoning Engine, a behavioral alignment framework that treats explicit reasoning as a context sensitive resource driven by discourse and temporal cues. TIME augments dialogue with optional ISO 8601 <time> tags, tick turns that represent silent gaps, and short <think> blocks that can appear anywhere in a reply. A four-phase curriculum including a small, maximally diverse full-batch alignment step trains Qwen3 dense models to invoke brief, in-place reasoning bursts and keep user facing text compact. We evaluate with TIMEBench, a temporally grounded dialogue benchmark probing chronology, commonsense under gaps and offsets, anomaly detection, and continuity. Across 4B to 32B scales, TIME improves TIMEBench scores over base Qwen3 in both thinking and no-thinking modes while reducing reasoning tokens by about an order of magnitude. Our training data and code are available at https://github.com/The-Coherence-Initiative/TIME and TIMEBench is available at https://github.com/The-Coherence-Initiative/TIMEBench

</details>


### [35] [When the Server Steps In: Calibrated Updates for Fair Federated Learning](https://arxiv.org/abs/2601.05352)
*Tianrun Yu,Kaixiang Zhao,Cheng Zhang,Anjun Gao,Yueyang Quan,Zhuqing Liu,Minghong Fang*

Main category: cs.LG

TL;DR: EquFL是一种新颖的服务器端去偏方法，通过生成校准更新并与聚合的客户端更新结合来减少联邦学习系统中的偏见，同时保持与FedAvg相同的收敛性。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然具有分布式学习的优势，但在确保不同人口群体间的公平性方面面临挑战。现有的公平性去偏方法要么需要修改客户端训练协议，要么缺乏聚合策略的灵活性。

Method: EquFL是一种服务器端去偏方法，允许服务器在接收客户端模型更新后生成单个校准更新，然后将该校准更新与聚合的客户端更新结合，生成减少偏见的调整后全局模型。

Result: 理论上证明EquFL能够收敛到FedAvg达到的最优全局模型，并有效减少训练轮次中的公平性损失。实证结果表明EquFL显著减轻了系统中的偏见。

Conclusion: EquFL提供了一种有效的服务器端去偏解决方案，能够在保持联邦学习收敛性的同时显著减少系统偏见，解决了现有方法在协议修改和灵活性方面的限制。

Abstract: Federated learning (FL) has emerged as a transformative distributed learning paradigm, enabling multiple clients to collaboratively train a global model under the coordination of a central server without sharing their raw training data. While FL offers notable advantages, it faces critical challenges in ensuring fairness across diverse demographic groups. To address these fairness concerns, various fairness-aware debiasing methods have been proposed. However, many of these approaches either require modifications to clients' training protocols or lack flexibility in their aggregation strategies. In this work, we address these limitations by introducing EquFL, a novel server-side debiasing method designed to mitigate bias in FL systems. EquFL operates by allowing the server to generate a single calibrated update after receiving model updates from the clients. This calibrated update is then integrated with the aggregated client updates to produce an adjusted global model that reduces bias. Theoretically, we establish that EquFL converges to the optimal global model achieved by FedAvg and effectively reduces fairness loss over training rounds. Empirically, we demonstrate that EquFL significantly mitigates bias within the system, showcasing its practical effectiveness.

</details>


### [36] [The Kernel Manifold: A Geometric Approach to Gaussian Process Model Selection](https://arxiv.org/abs/2601.05371)
*Md Shafiqul Islam,Shakti Prasad Padhy,Douglas Allaire,Raymundo Arróyave*

Main category: cs.LG

TL;DR: 提出基于核-核几何的贝叶斯优化框架，通过预期散度距离在多维标度嵌入中构建连续核空间，实现高效核选择，在合成基准、时间序列和增材制造案例中优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 高斯过程回归的性能严重依赖于协方差核的选择，但核选择是概率建模中最具挑战性和计算成本最高的步骤之一，需要一种更高效的核搜索方法。

Method: 基于核-核几何的贝叶斯优化框架：1) 使用预期散度距离度量高斯过程先验之间的差异；2) 通过多维标度(MDS)将离散核库嵌入连续欧几里得流形；3) 将核组合作为输入空间，对数边际似然作为目标函数，MDS坐标作为特征化表示；4) 当散度产生有效度量时，嵌入保持几何结构并产生稳定的BO景观。

Result: 在合成基准测试、真实世界时间序列数据集和增材制造案例研究（预测熔池几何形状）中，该方法相比基线（包括LLM引导搜索）实现了更优的预测准确性和不确定性校准。

Conclusion: 该框架为核搜索建立了可重用的概率几何结构，对高斯过程建模和深度核学习具有直接相关性，提供了一种高效且几何感知的核选择方法。

Abstract: Gaussian Process (GP) regression is a powerful nonparametric Bayesian framework, but its performance depends critically on the choice of covariance kernel. Selecting an appropriate kernel is therefore central to model quality, yet remains one of the most challenging and computationally expensive steps in probabilistic modeling. We present a Bayesian optimization framework built on kernel-of-kernels geometry, using expected divergence-based distances between GP priors to explore kernel space efficiently. A multidimensional scaling (MDS) embedding of this distance matrix maps a discrete kernel library into a continuous Euclidean manifold, enabling smooth BO. In this formulation, the input space comprises kernel compositions, the objective is the log marginal likelihood, and featurization is given by the MDS coordinates. When the divergence yields a valid metric, the embedding preserves geometry and produces a stable BO landscape. We demonstrate the approach on synthetic benchmarks, real-world time-series datasets, and an additive manufacturing case study predicting melt-pool geometry, achieving superior predictive accuracy and uncertainty calibration relative to baselines including Large Language Model (LLM)-guided search. This framework establishes a reusable probabilistic geometry for kernel search, with direct relevance to GP modeling and deep kernel learning.

</details>


### [37] [Imitation Learning for Combinatorial Optimisation under Uncertainty](https://arxiv.org/abs/2601.05383)
*Prakash Gawas,Antoine Legrain,Louis-Martin Rousseau*

Main category: cs.LG

TL;DR: 该论文系统研究了模仿学习中专家构建的分类框架，提出了基于不确定性处理、最优性水平和交互模式的三维分类法，并在动态医生-患者分配问题上验证了随机专家优于确定性专家，交互式学习能减少所需演示数量。


<details>
  <summary>Details</summary>
Motivation: 现有模仿学习研究在组合优化问题中使用了各种专家构建方法，但缺乏统一的框架来系统分析不同专家类型的建模假设、计算特性和学习效果影响。这导致难以理解专家选择如何影响学习性能，也无法为特定问题选择最合适的专家类型。

Method: 1. 提出三维专家分类法：不确定性处理（近视、确定性、全信息、两阶段随机、多阶段随机）、最优性水平（任务最优与近似专家）、交互模式（一次性监督到迭代交互方案）
2. 基于此分类法提出广义的Dataset Aggregation (DAgger)算法，支持多专家查询、专家聚合和灵活交互策略
3. 在动态医生-患者分配问题（具有随机到达和容量约束）上进行实验评估

Result: 1. 从随机专家学习的策略持续优于从确定性或全信息专家学习的策略
2. 交互式学习使用更少的专家演示就能提高解的质量
3. 当随机优化计算困难时，聚合的确定性专家提供了有效的替代方案
4. 计算实验比较了不同专家类型和交互机制下的学习结果

Conclusion: 该研究为模仿学习中的专家构建提供了系统化的分类框架，证明了随机专家在不确定环境下的优越性，交互式学习的高效性，以及聚合确定性专家在计算受限时的实用价值。该框架有助于指导实际应用中专家类型的选择和算法设计。

Abstract: Imitation learning (IL) provides a data-driven framework for approximating policies for large-scale combinatorial optimisation problems formulated as sequential decision problems (SDPs), where exact solution methods are computationally intractable. A central but underexplored aspect of IL in this context is the role of the \emph{expert} that generates training demonstrations. Existing studies employ a wide range of expert constructions, yet lack a unifying framework to characterise their modelling assumptions, computational properties, and impact on learning performance.
  This paper introduces a systematic taxonomy of experts for IL in combinatorial optimisation under uncertainty. Experts are classified along three dimensions: (i) their treatment of uncertainty, including myopic, deterministic, full-information, two-stage stochastic, and multi-stage stochastic formulations; (ii) their level of optimality, distinguishing task-optimal and approximate experts; and (iii) their interaction mode with the learner, ranging from one-shot supervision to iterative, interactive schemes. Building on this taxonomy, we propose a generalised Dataset Aggregation (DAgger) algorithm that supports multiple expert queries, expert aggregation, and flexible interaction strategies.
  The proposed framework is evaluated on a dynamic physician-to-patient assignment problem with stochastic arrivals and capacity constraints. Computational experiments compare learning outcomes across expert types and interaction regimes. The results show that policies learned from stochastic experts consistently outperform those learned from deterministic or full-information experts, while interactive learning improves solution quality using fewer expert demonstrations. Aggregated deterministic experts provide an effective alternative when stochastic optimisation becomes computationally challenging.

</details>


### [38] [Interactive Distillation for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2601.05407)
*Minwoo Cho,Batuhan Altundas,Matthew Gombolay*

Main category: cs.LG

TL;DR: HINT是一个用于多智能体强化学习的知识蒸馏框架，通过分层交互式教师机制解决传统方法在复杂领域、分布外状态和观察空间不匹配方面的瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法在多智能体强化学习中面临三个主要瓶颈：1）在复杂领域难以合成高性能教学策略；2）教师需要在分布外状态进行推理；3）去中心化学生与中心化教师的观察空间不匹配。

Method: 提出HINT框架：1）利用分层强化学习构建可扩展的高性能教师；2）引入伪离策略强化学习，使教师策略能够同时使用教师和学生经验进行更新，改善分布外适应；3）应用基于性能的过滤机制，仅保留结果相关的指导，减少观察不匹配。

Result: 在具有挑战性的合作领域（如资源分配的FireCommander和战术作战的MARINE）进行评估，HINT显著优于基线方法，成功率提升60%到165%。

Conclusion: HINT框架通过分层交互式教师机制有效解决了多智能体强化学习中知识蒸馏的关键瓶颈，在复杂合作任务中实现了显著性能提升。

Abstract: Knowledge distillation (KD) has the potential to accelerate MARL by employing a centralized teacher for decentralized students but faces key bottlenecks. Specifically, there are (1) challenges in synthesizing high-performing teaching policies in complex domains, (2) difficulties when teachers must reason in out-of-distribution (OOD) states, and (3) mismatches between the decentralized students' and the centralized teacher's observation spaces. To address these limitations, we propose HINT (Hierarchical INteractive Teacher-based transfer), a novel KD framework for MARL in a centralized training, decentralized execution setup. By leveraging hierarchical RL, HINT provides a scalable, high-performing teacher. Our key innovation, pseudo off-policy RL, enables the teacher policy to be updated using both teacher and student experience, thereby improving OOD adaptation. HINT also applies performance-based filtering to retain only outcome-relevant guidance, reducing observation mismatches. We evaluate HINT on challenging cooperative domains (e.g., FireCommander for resource allocation, MARINE for tactical combat). Across these benchmarks, HINT outperforms baselines, achieving improvements of 60% to 165% in success rate.

</details>


### [39] [Efficient Inference for Noisy LLM-as-a-Judge Evaluation](https://arxiv.org/abs/2601.05420)
*Yiqun T Chen,Sizhu Lu,Sijia Li,Moran Guo,Shengyi Li*

Main category: cs.LG

TL;DR: 本文系统研究并统一了两种纠正LLM作为评估者偏差的方法：直接测量误差校正和预测驱动推断，通过半参数效率理论推导出高效估计量，并比较了它们的渐近方差性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为生成式AI输出的自动评估者存在系统性、非随机性误差，需要有效的方法来纠正这些偏差，以准确估计平均参数如基准分数或成对胜率。

Method: 使用半参数效率理论工具，统一了两种偏差校正方法：基于误分类模型的直接测量误差校正（如Rogan-Gladen式估计器）和基于小规模人工标注校准预测残差的预测驱动推断方法，推导出高效影响函数为基础的高效估计量。

Result: 理论分析表明，在某些条件下，预测驱动推断式估计器比测量误差校正方法具有更小的渐近方差。仿真实验验证了理论结果，并在真实数据示例中展示了方法的有效性。

Conclusion: 本文为LLM作为评估者的偏差校正提供了统一的理论框架和实用方法，明确了不同方法的最优使用条件，有助于更准确地评估生成式AI系统的性能。

Abstract: Large language models (LLMs) are increasingly used as automatic evaluators of generative AI outputs, a paradigm often referred to as "LLM-as-a-judge." In practice, LLM judges are imperfect predictions for the underlying truth and can exhibit systematic, non-random errors. Two main approaches have recently been proposed to address this issue: (i) direct measurementerror correction based on misclassification models such as Rogan-Gladen-style estimators, and (ii) surrogate-outcome approaches such as prediction-powered inference (PPI), which correct bias by calibrating prediction residuals on a small set of gold-standard human labels. In this paper, we systematically study the performance of these two approaches for estimating mean parameters (e.g., average benchmark scores or pairwise win rates). Leveraging tools from semiparametric efficiency theory, we unify the two classes of estimators by deriving explicit forms of efficient influence function (EIF)-based efficient estimators and characterize conditions under which PPI-style estimators attain strictly smaller asymptotic variance than measurement-error corrections. We verify our theoretical results in simulations and demonstrate the methods on real-data examples. We provide an implementation of the benchmarked methods and comparison utilities at https://github.com/yiqunchen/debias-llm-as-a-judge.

</details>


### [40] [Prediction of Fault Slip Tendency in CO${_2}$ Storage using Data-space Inversion](https://arxiv.org/abs/2601.05431)
*Xiaowen He,Su Jiang,Louis J. Durlofsky*

Main category: cs.LG

TL;DR: 本文提出了一种基于变分自编码器(VAE)的数据空间反演(DSI)框架，用于预测CO₂封存项目中的压力、应力、应变场和断层滑移趋势，避免了传统基于模型的历史匹配方法中生成后验地质模型的挑战。


<details>
  <summary>Details</summary>
Motivation: 在涉及断层的耦合流动-地质力学问题中，传统的基于模型的历史匹配方法（需要生成校准到观测数据的后验地质模型）应用困难。需要一种更有效的方法来准确评估断层滑移潜力，特别是在CO₂封存等地下作业中。

Method: 开发了基于变分自编码器(VAE)的数据空间反演(DSI)框架。首先使用地质统计学软件生成O(1000)个先验地质模型，每个模型包含异质渗透率和孔隙度场，并从先验分布中采样不确定的地质力学和断层参数。使用GEOS进行耦合流动-地质力学模拟。训练具有堆叠卷积长短期记忆层的VAE，用先验模拟结果表示压力、应变、有效正应力和剪应力场。将VAE参数化与DSI结合，利用监测井提供的观测压力和应变数据进行后验预测。

Result: 对于合成真实模型的后验结果表明，DSI-VAE框架能够准确预测压力、应变和应力场以及断层滑移趋势。该框架还显著减少了关键地质力学和断层参数的不确定性。

Conclusion: DSI-VAE框架为涉及断层的耦合流动-地质力学问题提供了一种有效的数据空间反演方法，避免了传统后验地质模型生成的复杂性，同时能够准确预测关键地质力学参数并减少不确定性，在CO₂封存等地下作业中具有重要应用价值。

Abstract: Accurately assessing the potential for fault slip is essential in many subsurface operations. Conventional model-based history matching methods, which entail the generation of posterior geomodels calibrated to observed data, can be challenging to apply in coupled flow-geomechanics problems with faults. In this work, we implement a variational autoencoder (VAE)-based data-space inversion (DSI) framework to predict pressure, stress and strain fields, and fault slip tendency, in CO${_2}$ storage projects. The main computations required by the DSI workflow entail the simulation of O(1000) prior geomodels. The posterior distributions for quantities of interest are then inferred directly from prior simulation results and observed data, without the need to generate posterior geomodels. The model used here involves a synthetic 3D system with two faults. Realizations of heterogeneous permeability and porosity fields are generated using geostatistical software, and uncertain geomechanical and fault parameters are sampled for each realization from prior distributions. Coupled flow-geomechanics simulations for these geomodels are conducted using GEOS. A VAE with stacked convolutional long short-term memory layers is trained, using the prior simulation results, to represent pressure, strain, effective normal stress and shear stress fields in terms of latent variables. The VAE parameterization is used with DSI for posterior predictions, with monitoring wells providing observed pressure and strain data. Posterior results for synthetic true models demonstrate that the DSI-VAE framework gives accurate predictions for pressure, strain, and stress fields and for fault slip tendency. The framework is also shown to reduce uncertainty in key geomechanical and fault parameters.

</details>


### [41] [RingSQL: Generating Synthetic Data with Schema-Independent Templates for Text-to-SQL Reasoning Models](https://arxiv.org/abs/2601.05451)
*Marko Sterbentz,Kevin Cushing,Cameron Barrie,Kristian J. Hammond*

Main category: cs.LG

TL;DR: RingSQL是一个混合数据生成框架，结合了模式无关的查询模板和基于LLM的自然语言问题改写，用于生成高质量的文本到SQL训练数据。


<details>
  <summary>Details</summary>
Motivation: 当前文本到SQL系统受限于高质量训练数据的稀缺性。手动创建数据成本高昂，现有合成方法在可靠性和可扩展性之间存在权衡：模板方法能确保SQL正确性但需要特定模式模板，而LLM生成方法易于扩展但缺乏质量和正确性保证。

Method: RingSQL采用混合数据生成框架，结合了模式无关的查询模板（确保SQL正确性）和基于大语言模型的自然语言问题改写（提供语言多样性）。这种方法能在不同数据库模式中保持SQL正确性的同时，提供广泛的语言变化。

Result: 实验表明，使用RingSQL生成的数据训练的模型在六个文本到SQL基准测试中平均准确率提升了+2.3%，相比使用其他合成数据训练的模型。

Conclusion: RingSQL框架通过结合模板方法的正确性保证和LLM方法的可扩展性，有效解决了文本到SQL训练数据生成中可靠性与可扩展性的权衡问题，显著提升了模型性能。

Abstract: Recent advances in text-to-SQL systems have been driven by larger models and improved datasets, yet progress is still limited by the scarcity of high-quality training data. Manual data creation is expensive, and existing synthetic methods trade off reliability and scalability. Template-based approaches ensure correct SQL but require schema-specific templates, while LLM-based generation scales easily but lacks quality and correctness guarantees. We introduce RingSQL, a hybrid data generation framework that combines schema-independent query templates with LLM-based paraphrasing of natural language questions. This approach preserves SQL correctness across diverse schemas while providing broad linguistic variety. In our experiments, we find that models trained using data produced by RingSQL achieve an average gain in accuracy of +2.3% across six text-to-SQL benchmarks when compared to models trained on other synthetic data. We make our code available at https://github.com/nu-c3lab/RingSQL.

</details>


### [42] [Efficient Differentiable Causal Discovery via Reliable Super-Structure Learning](https://arxiv.org/abs/2601.05474)
*Pingchuan Ma,Qixin Zhang,Shuai Wang,Dacheng Tao*

Main category: cs.LG

TL;DR: ALVGL是一种增强可微分因果发现流程的新方法，通过稀疏低秩分解学习数据的精度矩阵，构建包含真实因果图的超结构，从而缩小搜索空间并提高优化效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 现有可微分因果发现方法在处理高维数据或存在潜在混杂因素的数据时面临挑战：搜索空间巨大、目标函数复杂、图论约束非平凡。虽然利用超结构指导优化过程受到关注，但学习适当粒度的超结构在不同设置下效率低下。

Method: ALVGL采用稀疏低秩分解学习数据的精度矩阵，设计ADMM优化过程识别与底层因果结构最相关的成分，将这些成分组合构建可证明包含真实因果图的超结构，然后用该超结构初始化标准可微分因果发现方法。

Result: 在合成和真实数据集上的广泛实验表明，ALVGL不仅达到最先进的准确性，还显著提高了优化效率，适用于高斯和非高斯设置，无论是否存在未测量的混杂因素。

Conclusion: ALVGL为可微分因果发现提供了一种可靠有效的解决方案，通过构建合适的超结构来指导优化过程，在保持准确性的同时显著提升计算效率。

Abstract: Recently, differentiable causal discovery has emerged as a promising approach to improve the accuracy and efficiency of existing methods. However, when applied to high-dimensional data or data with latent confounders, these methods, often based on off-the-shelf continuous optimization algorithms, struggle with the vast search space, the complexity of the objective function, and the nontrivial nature of graph-theoretical constraints. As a result, there has been a surge of interest in leveraging super-structures to guide the optimization process. Nonetheless, learning an appropriate super-structure at the right level of granularity, and doing so efficiently across various settings, presents significant challenges.
  In this paper, we propose ALVGL, a novel and general enhancement to the differentiable causal discovery pipeline. ALVGL employs a sparse and low-rank decomposition to learn the precision matrix of the data. We design an ADMM procedure to optimize this decomposition, identifying components in the precision matrix that are most relevant to the underlying causal structure. These components are then combined to construct a super-structure that is provably a superset of the true causal graph. This super-structure is used to initialize a standard differentiable causal discovery method with a more focused search space, thereby improving both optimization efficiency and accuracy.
  We demonstrate the versatility of ALVGL by instantiating it across a range of structural causal models, including both Gaussian and non-Gaussian settings, with and without unmeasured confounders. Extensive experiments on synthetic and real-world datasets show that ALVGL not only achieves state-of-the-art accuracy but also significantly improves optimization efficiency, making it a reliable and effective solution for differentiable causal discovery.

</details>


### [43] [MaxCode: A Max-Reward Reinforcement Learning Framework for Automated Code Optimization](https://arxiv.org/abs/2601.05475)
*Jiefu Ou,Sapana Chaudhary,Kaj Bostrom,Nathaniel Weir,Shuai Zhang,Huzefa Rangwala,George Karypis*

Main category: cs.LG

TL;DR: MaxCode提出了一种基于最大奖励强化学习框架的推理时搜索算法，通过执行反馈引导LLM迭代优化代码，在CUDA和C++优化基准测试中显著提升了代码性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在通用编码任务中表现出色，但在优化代码时面临两大挑战：1) 编写优化代码（如高性能CUDA内核和竞赛级CPU代码）需要系统、算法和特定语言的专业知识；2) 需要解释性能指标（如计时和设备利用率），而不仅仅是二进制正确性。

Method: MaxCode采用最大奖励强化学习框架统一现有搜索方法，使观察和动作价值函数模块化可修改。通过集成自然语言批判模型将原始执行反馈转换为诊断见解，并使用生成式奖励到目标模型对潜在解决方案进行重排序以改进搜索探索。

Result: 在KernelBench（CUDA）和PIE（C++）优化基准测试中，MaxCode相比基线方法在绝对加速值和相对加速排名上分别实现了20.3%和10.1%的相对改进。

Conclusion: MaxCode通过推理时搜索算法和强化学习框架，有效解决了LLM在代码优化中的挑战，显著提升了优化代码的性能表现。

Abstract: Large Language Models (LLMs) demonstrate strong capabilities in general coding tasks but encounter two key challenges when optimizing code: (i) the complexity of writing optimized code (such as performant CUDA kernels and competition-level CPU code) requires expertise in systems, algorithms and specific languages and (ii) requires interpretation of performance metrics like timing and device utilization beyond binary correctness. In this work, we explore inference-time search algorithms that guide the LLM to discover better solutions through iterative refinement based on execution feedback. Our approach, called MaxCode unifies existing search methods under a max-reward reinforcement learning framework, making the observation and action-value functions modular for modification. To enhance the observation space, we integrate a natural language critique model that converts raw execution feedback into diagnostic insights about errors and performance bottlenecks, and the best-discounted reward seen so far. Together, these provide richer input to the code proposal function. To improve exploration during search, we train a generative reward-to-go model using action values from rollouts to rerank potential solutions. Testing on the KernelBench (CUDA) and PIE (C++) optimization benchmarks shows that MaxCode improves optimized code performance compared to baselines, achieving 20.3% and 10.1% relative improvements in absolute speedup value and relative speedup ranking, respectively.

</details>


### [44] [Hi-ZFO: Hierarchical Zeroth- and First-Order LLM Fine-Tuning via Importance-Guided Tensor Selection](https://arxiv.org/abs/2601.05501)
*Feihu Jin,Ying Tan*

Main category: cs.LG

TL;DR: Hi-ZFO是一种分层混合优化框架，结合了零阶和一阶优化方法的优势，用于大语言模型微调，在保持性能的同时显著减少训练时间。


<details>
  <summary>Details</summary>
Motivation: 传统一阶优化方法容易使训练陷入尖锐、泛化性差的局部最小值，而零阶方法虽然探索能力强但收敛慢且方差大，特别是在生成任务中输出和搜索空间巨大时问题更加严重。

Method: 提出Hi-ZFO分层混合优化框架：通过层重要性分析自适应划分模型，对关键层使用精确的一阶梯度更新，对不敏感层使用零阶优化，后者不仅节省内存还作为"有益随机性"帮助模型逃离局部最小值。

Result: 在多种生成、数学和代码推理任务上的验证表明，Hi-ZFO始终实现更优性能，同时显著减少训练时间。

Conclusion: 分层混合优化是LLM微调的有效方法，能够协同利用一阶优化的精确性和零阶优化的探索能力。

Abstract: Fine-tuning large language models (LLMs) using standard first-order (FO) optimization often drives training toward sharp, poorly generalizing minima. Conversely, zeroth-order (ZO) methods offer stronger exploratory behavior without relying on explicit gradients, yet suffer from slow convergence. More critically, our analysis reveals that in generative tasks, the vast output and search space significantly amplify estimation variance, rendering ZO methods both noisy and inefficient. To address these challenges, we propose \textbf{Hi-ZFO} (\textbf{Hi}erarchical \textbf{Z}eroth- and \textbf{F}irst-\textbf{O}rder optimization), a hybrid framework designed to synergize the precision of FO gradients with the exploratory capability of ZO estimation. Hi-ZFO adaptively partitions the model through layer-wise importance profiling, applying precise FO updates to critical layers while leveraging ZO optimization for less sensitive ones. Notably, ZO in Hi-ZFO is not merely a memory-saving surrogate; it is intentionally introduced as a source of "beneficial stochasticity" to help the model escape the local minima where pure FO optimization tends to stagnate. Validated across diverse generative, mathematical, and code reasoning tasks, Hi-ZFO consistently achieves superior performance while significantly reducing the training time. These results demonstrate the effectiveness of hierarchical hybrid optimization for LLM fine-tuning.

</details>


### [45] [Over-Searching in Search-Augmented Large Language Models](https://arxiv.org/abs/2601.05503)
*Roy Xie,Deepak Gopinath,David Qiu,Dong Lin,Haitian Sun,Saloni Potdar,Bhuwan Dhingra*

Main category: cs.LG

TL;DR: 该论文系统评估了搜索增强大语言模型中的过度搜索问题，提出了量化指标TPC，并发布了OverSearchQA数据集以促进高效搜索增强LLMs的研究。


<details>
  <summary>Details</summary>
Motivation: 搜索增强大语言模型在知识密集型任务中表现出色，但存在过度搜索问题——即使搜索不会提高响应质量也频繁调用搜索工具，导致计算效率低下和因引入无关上下文而产生的幻觉。

Method: 从多个维度系统评估过度搜索现象，包括查询类型、模型类别、检索条件和多轮对话；引入Tokens Per Correctness (TPC)指标量化性能-成本权衡；在查询和检索层面研究缓解方法；发布OverSearchQA数据集。

Result: 研究发现：(1)搜索通常提高可回答查询的准确性，但会损害不可回答查询的弃权能力；(2)过度搜索在复杂推理模型和深度研究系统中更明显，受噪声检索加剧，并在多轮对话中累积；(3)检索证据的组成很关键，负面证据的存在能改善弃权。

Conclusion: 过度搜索是搜索增强LLMs的重要效率问题，需要系统评估和缓解策略；TPC指标能有效量化性能-成本权衡；OverSearchQA数据集将促进高效搜索增强LLMs的持续研究。

Abstract: Search-augmented large language models (LLMs) excel at knowledge-intensive tasks by integrating external retrieval. However, they often over-search -- unnecessarily invoking search tool even when it does not improve response quality, which leads to computational inefficiency and hallucinations by incorporating irrelevant context. In this work, we conduct a systematic evaluation of over-searching across multiple dimensions, including query types, model categories, retrieval conditions, and multi-turn conversations. Our finding shows: (i) search generally improves answer accuracy on answerable queries but harms abstention on unanswerable ones; (ii) over-searching is more pronounced in complex reasoning models and deep research systems, is exacerbated by noisy retrieval, and compounds across turns in multi-turn conversations; and (iii) the composition of retrieved evidence is crucial, as the presence of negative evidence improves abstention. To quantify over-searching, we introduce Tokens Per Correctness (TPC), an evaluation metric that captures the performance-cost trade-off for search-augmented LLMs. Lastly, we investigate mitigation approaches at both the query and retrieval levels and release the OverSearchQA to foster continued research into efficient search-augmented LLMs.

</details>


### [46] [Toward an Integrated Cross-Urban Accident Prevention System: A Multi-Task Spatial-Temporal Learning Framework for Urban Safety Management](https://arxiv.org/abs/2601.05521)
*Jiayu Fang,Zhiqi Shao,Haoning Xi,Boris Choy,Junbin Gao*

Main category: cs.LG

TL;DR: MLA-STNet是一个跨城市事故预防系统，通过多任务学习框架处理异构城市数据，结合时空地理和语义注意力模块，在纽约和芝加哥数据集上表现出优越性能。


<details>
  <summary>Details</summary>
Motivation: 城市事故数据存在异构性、报告不一致、稀疏性、周期性和噪声等问题，加上碎片化治理和不兼容的报告标准，阻碍了跨城市事故预防系统的开发。

Method: 提出MLA-STNet系统，将事故风险预测构建为跨城市多任务学习问题。包含两个互补模块：STG-MA（时空地理Mamba注意力）抑制不稳定时空波动并增强长期时间依赖；STS-MA（时空语义Mamba注意力）通过共享参数设计缓解跨城市异构性，同时保留个体语义表示空间。

Result: 在纽约和芝加哥真实数据集上进行75个实验，在全天和高频事故时段两种预测场景下，相比最先进基线方法，MLA-STNet实现RMSE降低6%、召回率提高8%、MAP提高5%，在50%输入噪声下性能变化小于1%。

Conclusion: MLA-STNet有效统一了异构城市数据集，构建了可扩展、鲁棒且可解释的跨城市事故预防系统，为协调和数据驱动的城市安全管理铺平了道路。

Abstract: The development of a cross-city accident prevention system is particularly challenging due to the heterogeneity, inconsistent reporting, and inherently clustered, sparse, cyclical, and noisy nature of urban accident data. These intrinsic data properties, combined with fragmented governance and incompatible reporting standards, have long hindered the creation of an integrated, cross-city accident prevention framework. To address this gap, we propose the Mamba Local-ttention Spatial-Temporal Network MLA-STNet, a unified system that formulates accident risk prediction as a multi-task learning problem across multiple cities. MLA-STNet integrates two complementary modules: (i)the Spatio-Temporal Geographical Mamba-Attention (STG-MA), which suppresses unstable spatio-temporal fluctuations and strengthens long-range temporal dependencies; and (ii) the Spatio-Temporal Semantic Mamba-Attention (STS-MA), which mitigates cross-city heterogeneity through a shared-parameter design that jointly trains all cities while preserving individual semantic representation spaces. We validate the proposed framework through 75 experiments under two forecasting scenarios, full-day and high-frequency accident periods, using real-world datasets from New York City and Chicago. Compared with the state-of-the-art baselines, MLA-STNet achieves up to 6% lower RMSE, 8% higher Recall, and 5% higher MAP, while maintaining less than 1% performance variation under 50% input noise. These results demonstrate that MLA-STNet effectively unifies heterogeneous urban datasets within a scalable, robust, and interpretable Cross-City Accident Prevention System, paving the way for coordinated and data-driven urban safety management.

</details>


### [47] [Buffered AUC maximization for scoring systems via mixed-integer optimization](https://arxiv.org/abs/2601.05544)
*Moe Shiina,Shunnosuke Ikeda,Yuichi Takano*

Main category: cs.LG

TL;DR: 本文提出了一种基于混合整数线性优化的评分系统构建方法，直接最大化缓冲AUC（bAUC）作为AUC的最紧凹下界，相比传统正则化和逐步回归方法能获得更好的AUC性能。


<details>
  <summary>Details</summary>
Motivation: 评分系统作为高度可解释的线性分类器，在医疗诊断等领域有重要应用。现有基于混合整数优化的方法未直接最大化AUC这一评分系统关键评估指标，需要建立直接优化AUC的有效框架。

Method: 建立混合整数线性优化（MILO）框架，最大化缓冲AUC（bAUC）作为AUC的最紧凹下界，同时通过组稀疏约束限制评分系统中的问题数量，确保模型简洁可解释。

Result: 在公开真实数据集上的计算实验表明，相比基于正则化和逐步回归的基线方法，本文的MILO方法能构建具有更优AUC值的评分系统。

Conclusion: 本研究推进了混合整数优化技术在开发高度可解释分类模型方面的应用，为直接优化AUC的评分系统构建提供了有效框架。

Abstract: A scoring system is a linear classifier composed of a small number of explanatory variables, each assigned a small integer coefficient. This system is highly interpretable and allows predictions to be made with simple manual calculations without the need for a calculator. Several previous studies have used mixed-integer optimization (MIO) techniques to develop scoring systems for binary classification; however, they have not focused on directly maximizing AUC (i.e., area under the receiver operating characteristic curve), even though AUC is recognized as an essential evaluation metric for scoring systems. Our goal herein is to establish an effective MIO framework for constructing scoring systems that directly maximize the buffered AUC (bAUC) as the tightest concave lower bound on AUC. Our optimization model is formulated as a mixed-integer linear optimization (MILO) problem that maximizes bAUC subject to a group sparsity constraint for limiting the number of questions in the scoring system. Computational experiments using publicly available real-world datasets demonstrate that our MILO method can build scoring systems with superior AUC values compared to the baseline methods based on regularization and stepwise regression. This research contributes to the advancement of MIO techniques for developing highly interpretable classification models.

</details>


### [48] [Learn to Evolve: Self-supervised Neural JKO Operator for Wasserstein Gradient Flow](https://arxiv.org/abs/2601.05583)
*Xue Feng,Li Wang,Deanna Needell,Rongjie Lai*

Main category: cs.LG

TL;DR: 提出了一种自监督学习方法，无需数值求解JKO子问题即可学习JKO解算子，通过Learn-to-Evolve算法联合学习算子和轨迹生成，显著提高了泛化能力。


<details>
  <summary>Details</summary>
Motivation: JKO方案为计算Wasserstein梯度流提供了稳定的变分框架，但其实际应用受到重复求解JKO子问题的高计算成本限制。需要一种更高效的方法来生成梯度流演化。

Method: 提出自监督方法学习JKO解算子，该算子将输入密度直接映射到对应JKO子问题的最小化解。引入Learn-to-Evolve算法，通过交替进行轨迹生成和算子更新来联合学习JKO算子及其诱导的轨迹。

Result: 数值实验表明，该方法在各种能量选择和初始条件下都表现出准确性、稳定性和鲁棒性。生成的轨迹逐渐逼近真实的JKO轨迹，同时Learn-to-Evolve策略作为数据增强形式显著提高了学习算子的泛化能力。

Conclusion: 该方法成功克服了传统JKO方案的高计算成本限制，通过学习JKO解算子实现了高效的梯度流演化计算，为Wasserstein梯度流计算提供了实用的解决方案。

Abstract: The Jordan-Kinderlehrer-Otto (JKO) scheme provides a stable variational framework for computing Wasserstein gradient flows, but its practical use is often limited by the high computational cost of repeatedly solving the JKO subproblems. We propose a self-supervised approach for learning a JKO solution operator without requiring numerical solutions of any JKO trajectories. The learned operator maps an input density directly to the minimizer of the corresponding JKO subproblem, and can be iteratively applied to efficiently generate the gradient-flow evolution. A key challenge is that only a number of initial densities are typically available for training. To address this, we introduce a Learn-to-Evolve algorithm that jointly learns the JKO operator and its induced trajectories by alternating between trajectory generation and operator updates. As training progresses, the generated data increasingly approximates true JKO trajectories. Meanwhile, this Learn-to-Evolve strategy serves as a natural form of data augmentation, significantly enhancing the generalization ability of the learned operator. Numerical experiments demonstrate the accuracy, stability, and robustness of the proposed method across various choices of energies and initial conditions.

</details>


### [49] [PaCoRe: Learning to Scale Test-Time Compute with Parallel Coordinated Reasoning](https://arxiv.org/abs/2601.05593)
*Jingcheng Hu,Yinmin Zhang,Shijie Shang,Xiaobo Yang,Yue Peng,Zhewei Huang,Hebin Zhou,Xin Wu,Jie Cheng,Fanqi Wan,Xiangwen Kong,Chengyuan Yao,Kaiwen Yan,Ailin Huang,Hongyu Zhou,Qi Han,Zheng Ge,Daxin Jiang,Xiangyu Zhang,Heung-Yeung Shum*

Main category: cs.LG

TL;DR: PaCoRe是一个训练-推理框架，通过并行协调推理突破语言模型测试时计算无法超越固定上下文窗口的限制，使用消息传递架构进行多轮并行探索，显著提升数学推理等任务性能。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型存在核心限制：无法将测试时计算（TTC）扩展到远超固定上下文窗口下的顺序推理。传统顺序推理范式限制了模型利用更多计算资源进行复杂推理的能力。

Method: 提出并行协调推理（PaCoRe）框架，采用消息传递架构进行多轮并行探索。每轮启动多个并行推理轨迹，将发现压缩到上下文受限的消息中，综合这些消息指导下一轮并最终产生答案。通过大规模基于结果的强化学习进行端到端训练。

Result: PaCoRe能够将有效TTC扩展到数百万token而不超过上下文限制。在数学推理领域表现突出：8B模型在HMMT 2025上达到94.5%，超过GPT-5的93.2%，有效TTC约两百万token。在多个领域都有显著改进。

Conclusion: PaCoRe通过并行协调推理架构成功突破了语言模型测试时计算受限于上下文窗口的问题，实现了计算资源的有效扩展，显著提升了复杂推理任务的性能，特别是在数学领域超越了前沿系统。

Abstract: We introduce Parallel Coordinated Reasoning (PaCoRe), a training-and-inference framework designed to overcome a central limitation of contemporary language models: their inability to scale test-time compute (TTC) far beyond sequential reasoning under a fixed context window. PaCoRe departs from the traditional sequential paradigm by driving TTC through massive parallel exploration coordinated via a message-passing architecture in multiple rounds. Each round launches many parallel reasoning trajectories, compacts their findings into context-bounded messages, and synthesizes these messages to guide the next round and ultimately produce the final answer. Trained end-to-end with large-scale, outcome-based reinforcement learning, the model masters the synthesis abilities required by PaCoRe and scales to multi-million-token effective TTC without exceeding context limits. The approach yields strong improvements across diverse domains, and notably pushes reasoning beyond frontier systems in mathematics: an 8B model reaches 94.5% on HMMT 2025, surpassing GPT-5's 93.2% by scaling effective TTC to roughly two million tokens. We open-source model checkpoints, training data, and the full inference pipeline to accelerate follow-up work.

</details>


### [50] [Good Allocations from Bad Estimates](https://arxiv.org/abs/2601.05597)
*Sílvia Casacuberta,Moritz Hardt*

Main category: cs.LG

TL;DR: 本文提出了一种比传统CATE估计更高效的样本分配方法，仅需O(M/ε)样本而非O(M/ε²)即可实现接近最优的治疗分配效果


<details>
  <summary>Details</summary>
Motivation: 传统条件平均治疗效果(CATE)估计需要大量样本(O(M/ε²))来精确估计治疗效果，但治疗分配的实际目标不是精确估计所有治疗效果，而是找到最优的治疗分配方案。本文旨在减少样本需求，实现更高效的治疗分配

Method: 提出新算法，利用粗粒度估计即可实现接近最优的治疗分配，关键在于认识到治疗分配不需要精确的个体治疗效果估计。同时利用预算灵活性进一步降低样本复杂度

Result: 在多种真实世界RCT数据集上的评估显示，该算法能以极少的样本找到接近最优的治疗分配方案，验证了治疗分配比治疗效果估计需要更少样本的理论发现

Conclusion: 本文揭示了治疗效果估计与治疗分配之间的根本区别：后者所需样本量远少于前者。粗粒度估计足以实现接近最优的治疗分配，这为实际医疗资源分配提供了更高效的解决方案

Abstract: Conditional average treatment effect (CATE) estimation is the de facto gold standard for targeting a treatment to a heterogeneous population. The method estimates treatment effects up to an error $ε> 0$ in each of $M$ different strata of the population, targeting individuals in decreasing order of estimated treatment effect until the budget runs out. In general, this method requires $O(M/ε^2)$ samples. This is best possible if the goal is to estimate all treatment effects up to an $ε$ error. In this work, we show how to achieve the same total treatment effect as CATE with only $O(M/ε)$ samples for natural distributions of treatment effects. The key insight is that coarse estimates suffice for near-optimal treatment allocations. In addition, we show that budget flexibility can further reduce the sample complexity of allocation. Finally, we evaluate our algorithm on various real-world RCT datasets. In all cases, it finds nearly optimal treatment allocations with surprisingly few samples. Our work highlights the fundamental distinction between treatment effect estimation and treatment allocation: the latter requires far fewer samples.

</details>


### [51] [PiXTime: A Model for Federated Time Series Forecasting with Heterogeneous Data Structures Across Nodes](https://arxiv.org/abs/2601.05613)
*Yiming Zhou,Mingyue Cheng,Hao Wang,Enhong Chen*

Main category: cs.LG

TL;DR: PiXTime是一个用于联邦学习的时间序列预测模型，能够处理多粒度时间序列和异构变量集，通过个性化补丁嵌入和全局变量嵌入表实现跨节点有效预测。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据价值高但难以跨节点共享，不同节点的采样标准导致时间粒度和变量集存在差异，这使得传统的联邦学习方法难以有效应用。

Method: 1. 使用个性化补丁嵌入将节点特定粒度的时间序列映射到统一维度的令牌序列；2. 使用全局变量嵌入表对齐跨节点的变量类别语义；3. 基于Transformer的共享模型捕获任意数量变量的辅助序列表示，并通过交叉注意力增强目标序列预测。

Result: 实验表明PiXTime在联邦学习设置下达到了最先进的性能，并在八个广泛使用的真实世界传统基准测试中表现出优越性能。

Conclusion: PiXTime通过创新的个性化嵌入和全局对齐机制，成功解决了联邦学习中多粒度时间序列和异构变量集的挑战，为分布式时间序列预测提供了有效解决方案。

Abstract: Time series are highly valuable and rarely shareable across nodes, making federated learning a promising paradigm to leverage distributed temporal data. However, different sampling standards lead to diverse time granularities and variable sets across nodes, hindering classical federated learning. We propose PiXTime, a novel time series forecasting model designed for federated learning that enables effective prediction across nodes with multi-granularity and heterogeneous variable sets. PiXTime employs a personalized Patch Embedding to map node-specific granularity time series into token sequences of a unified dimension for processing by a subsequent shared model, and uses a global VE Table to align variable category semantics across nodes, thereby enhancing cross-node transferability. With a transformer-based shared model, PiXTime captures representations of auxiliary series with arbitrary numbers of variables and uses cross-attention to enhance the prediction of the target series. Experiments show PiXTime achieves state-of-the-art performance in federated settings and demonstrates superior performance on eight widely used real-world traditional benchmarks.

</details>


### [52] [Transformer Is Inherently a Causal Learner](https://arxiv.org/abs/2601.05647)
*Xinyue Wang,Stephen Wang,Biwei Huang*

Main category: cs.LG

TL;DR: Transformer模型在自回归训练中自然学习到时延因果结构，其梯度敏感性可直接恢复因果图，无需显式因果目标


<details>
  <summary>Details</summary>
Motivation: 探索Transformer在时间序列预测中是否自然学习到因果结构，以及如何利用这种特性进行因果发现

Method: 通过分析Transformer输出的梯度敏感性来提取因果结构，开发基于聚合梯度归因的实用提取方法

Result: 该方法在非线性动态、长期依赖和非平稳系统等挑战性场景中显著优于现有因果发现算法，且性能随数据量和异质性增加而提升

Conclusion: 为未来范式奠定基础：通过基础模型视角进行因果发现，同时通过因果视角增强基础模型的可解释性和性能

Abstract: We reveal that transformers trained in an autoregressive manner naturally encode time-delayed causal structures in their learned representations. When predicting future values in multivariate time series, the gradient sensitivities of transformer outputs with respect to past inputs directly recover the underlying causal graph, without any explicit causal objectives or structural constraints. We prove this connection theoretically under standard identifiability conditions and develop a practical extraction method using aggregated gradient attributions. On challenging cases such as nonlinear dynamics, long-term dependencies, and non-stationary systems, this approach greatly surpasses the performance of state-of-the-art discovery algorithms, especially as data heterogeneity increases, exhibiting scaling potential where causal accuracy improves with data volume and heterogeneity, a property traditional methods lack. This unifying view lays the groundwork for a future paradigm where causal discovery operates through the lens of foundation models, and foundation models gain interpretability and enhancement through the lens of causality.

</details>


### [53] [From Global to Local: Cluster-Aware Learning for Wi-Fi Fingerprinting Indoor Localisation](https://arxiv.org/abs/2601.05650)
*Miguel Matey-Sanz,Joaquín Torres-Sospedra,Joaquín Huerta,Sergio Trilles*

Main category: cs.LG

TL;DR: 该论文提出了一种基于聚类的Wi-Fi指纹室内定位方法，通过先对指纹数据集进行结构化分组，再在选定簇内进行定位，从而提升定位精度。


<details>
  <summary>Details</summary>
Motivation: Wi-Fi指纹定位在实际应用中面临数据集规模与异质性、信号强度波动、大空间多楼层环境模糊性等挑战，这些因素显著降低了定位精度，特别是当全局模型不考虑结构约束时。

Method: 提出聚类方法在定位前对指纹数据集进行结构化：基于空间或无线电特征对指纹分组，可在建筑或楼层级别应用聚类。定位阶段，基于最强接入点的聚类估计程序将未见指纹分配到最相关簇，然后在选定簇内进行定位，使学习模型在更小、更一致的数据子集上运行。

Result: 在三个公共数据集和多个机器学习模型上评估，结果显示定位误差持续减少，特别是在建筑级策略下，但代价是降低了楼层检测精度。

Conclusion: 通过聚类显式结构化数据集是室内定位可扩展的有效灵活方法。

Abstract: Wi-Fi fingerprinting remains one of the most practical solutions for indoor positioning, however, its performance is often limited by the size and heterogeneity of fingerprint datasets, strong Received Signal Strength Indicator variability, and the ambiguity introduced in large and multi-floor environments. These factors significantly degrade localisation accuracy, particularly when global models are applied without considering structural constraints. This paper introduces a clustering-based method that structures the fingerprint dataset prior to localisation. Fingerprints are grouped using either spatial or radio features, and clustering can be applied at the building or floor level. In the localisation phase, a clustering estimation procedure based on the strongest access points assigns unseen fingerprints to the most relevant cluster. Localisation is then performed only within the selected clusters, allowing learning models to operate on reduced and more coherent subsets of data. The effectiveness of the method is evaluated on three public datasets and several machine learning models. Results show a consistent reduction in localisation errors, particularly under building-level strategies, but at the cost of reducing the floor detection accuracy. These results demonstrate that explicitly structuring datasets through clustering is an effective and flexible approach for scalable indoor positioning.

</details>


### [54] [Do Sparse Autoencoders Identify Reasoning Features in Language Models?](https://arxiv.org/abs/2601.05679)
*George Ma,Zhongyuan Liang,Irene Y. Chen,Somayeh Sojoudi*

Main category: cs.LG

TL;DR: 研究发现稀疏自编码器（SAEs）通过对比激活方法识别的"推理特征"实际上主要捕捉的是推理的语言相关性而非真正的推理计算过程。


<details>
  <summary>Details</summary>
Motivation: 研究动机是验证稀疏自编码器（SAEs）是否真的能识别大型语言模型（LLMs）中真正的推理特征，还是仅仅捕捉到与推理相关的表面语言模式。

Method: 采用证伪导向框架，结合因果标记注入实验和LLM引导的证伪方法。通过向非推理文本注入少量特征相关标记来测试特征激活是否反映推理过程，并在20种不同配置（跨多个模型家族、层和推理数据集）中进行验证。

Result: 研究发现：1）59%至94%的特征对标记级干预高度敏感，注入少量特征相关标记即可在非推理文本中引发强激活；2）剩余特征通过LLM引导证伪也能找到非推理输入激活特征而推理输入不激活的情况；3）没有分析的特征满足真正推理行为的标准；4）操纵这些特征对基准性能影响很小或略有下降。

Conclusion: 结论表明，通过对比方法识别的SAE特征主要捕捉的是推理的语言相关性，而非底层的推理计算本身，这对当前基于稀疏自编码器的可解释性方法提出了重要质疑。

Abstract: We investigate whether sparse autoencoders (SAEs) identify genuine reasoning features in large language models (LLMs). Starting from features selected using standard contrastive activation methods, we introduce a falsification-oriented framework that combines causal token injection experiments and LLM-guided falsification to test whether feature activation reflects reasoning processes or superficial linguistic correlates. Across 20 configurations spanning multiple model families, layers, and reasoning datasets, we find that identified reasoning features are highly sensitive to token-level interventions. Injecting a small number of feature-associated tokens into non-reasoning text is sufficient to elicit strong activation for 59% to 94% of features, indicating reliance on lexical artifacts. For the remaining features that are not explained by simple token triggers, LLM-guided falsification consistently produces non-reasoning inputs that activate the feature and reasoning inputs that do not, with no analyzed feature satisfying our criteria for genuine reasoning behavior. Steering these features yields minimal changes or slight degradations in benchmark performance. Together, these results suggest that SAE features identified by contrastive approaches primarily capture linguistic correlates of reasoning rather than the underlying reasoning computations themselves.

</details>


### [55] [AGDC: Autoregressive Generation of Variable-Length Sequences with Joint Discrete and Continuous Spaces](https://arxiv.org/abs/2601.05680)
*Yeonsang Shin,Insoo Kim,Bongkeun Kim,Keonwoo Bae,Bohyung Han*

Main category: cs.LG

TL;DR: 提出AGDC框架，联合建模离散和连续值，解决Transformer自回归模型在生成高精度混合序列时的精度限制问题，特别针对半导体电路设计等高精度领域。


<details>
  <summary>Details</summary>
Motivation: Transformer自回归模型依赖离散化token，难以精确表示连续值，在高精度领域（如半导体电路设计）中精度损失会导致功能失效。现有离散化方法在生成混合离散-连续序列时存在可扩展性限制。

Method: 提出AGDC统一框架，结合分类预测（离散值）和基于扩散的建模（连续值）。关键技术包括：基于MLP的动态EOS对数调整机制，根据序列上下文调整EOS token对数；以及集成到损失函数中的长度正则化项。同时提出ContLayNet大规模基准数据集。

Result: 在半导体布局（ContLayNet）、图形布局和SVG上的实验表明，AGDC在生成高保真混合向量表示方面优于离散化方法和固定模式基线，实现了跨领域可扩展的高精度生成。

Conclusion: AGDC框架有效解决了Transformer模型在高精度混合序列生成中的精度限制问题，通过联合建模离散和连续值，实现了跨领域的高精度可扩展生成，特别适用于半导体电路设计等对精度要求极高的应用场景。

Abstract: Transformer-based autoregressive models excel in data generation but are inherently constrained by their reliance on discretized tokens, which limits their ability to represent continuous values with high precision. We analyze the scalability limitations of existing discretization-based approaches for generating hybrid discrete-continuous sequences, particularly in high-precision domains such as semiconductor circuit designs, where precision loss can lead to functional failure. To address the challenge, we propose AGDC, a novel unified framework that jointly models discrete and continuous values for variable-length sequences. AGDC employs a hybrid approach that combines categorical prediction for discrete values with diffusion-based modeling for continuous values, incorporating two key technical components: an end-of-sequence (EOS) logit adjustment mechanism that uses an MLP to dynamically adjust EOS token logits based on sequence context, and a length regularization term integrated into the loss function. Additionally, we present ContLayNet, a large-scale benchmark comprising 334K high-precision semiconductor layout samples with specialized evaluation metrics that capture functional correctness where precision errors significantly impact performance. Experiments on semiconductor layouts (ContLayNet), graphic layouts, and SVGs demonstrate AGDC's superior performance in generating high-fidelity hybrid vector representations compared to discretization-based and fixed-schema baselines, achieving scalable high-precision generation across diverse domains.

</details>


### [56] [FLRQ: Faster LLM Quantization with Flexible Low-Rank Matrix Sketching](https://arxiv.org/abs/2601.05684)
*Hongyaoxing Gul,Lijuan Hu,Shuzi Niu,Fangfang Liu*

Main category: cs.LG

TL;DR: FLRQ是一种灵活的低秩量化方法，通过快速识别各层最优秩并聚合，实现最小存储组合，在量化质量和算法效率上达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有低秩PTQ方法需要昂贵的微调来确定不同数据和层的最佳秩，无法充分利用模型潜力，且基于SVD的低秩近似计算开销大。

Method: 提出FLRQ方法，包含两个核心组件：1) R1-FLR使用R1-Sketch和高斯投影进行快速低秩近似，实现异常值感知的秩提取；2) BLC通过迭代方法最小化缩放和裁剪策略下的低秩量化误差。

Result: FLRQ在综合实验中表现出强大的有效性和鲁棒性，在量化质量和算法效率方面均达到最先进的性能。

Conclusion: FLRQ通过灵活的低秩量化策略，解决了传统PTQ方法在秩选择和计算效率方面的局限性，为大语言模型的高效量化提供了有效解决方案。

Abstract: Traditional post-training quantization (PTQ) is considered an effective approach to reduce model size and accelerate inference of large-scale language models (LLMs). However, existing low-rank PTQ methods require costly fine-tuning to determine a compromise rank for diverse data and layers in large models, failing to exploit their full potential. Additionally, the current SVD-based low-rank approximation compounds the computational overhead. In this work, we thoroughly analyze the varying effectiveness of low-rank approximation across different layers in representative models. Accordingly, we introduce \underline{F}lexible \underline{L}ow-\underline{R}ank \underline{Q}uantization (FLRQ), a novel solution designed to quickly identify the accuracy-optimal ranks and aggregate them to achieve minimal storage combinations. FLRQ comprises two powerful components, Rank1-Sketch-based Flexible Rank Selection (R1-FLR) and Best Low-rank Approximation under Clipping (BLC). R1-FLR applies the R1-Sketch with Gaussian projection for the fast low-rank approximation, enabling outlier-aware rank extraction for each layer. Meanwhile, BLC aims at minimizing the low-rank quantization error under the scaling and clipping strategy through an iterative method. FLRQ demonstrates strong effectiveness and robustness in comprehensive experiments, achieving state-of-the-art performance in both quantization quality and algorithm efficiency.

</details>


### [57] [Variational Autoencoders for P-wave Detection on Strong Motion Earthquake Spectrograms](https://arxiv.org/abs/2601.05759)
*Turkan Simge Ispak,Salih Tileylioglu,Erdem Akagunduz*

Main category: cs.LG

TL;DR: 该研究将P波检测重构为自监督异常检测任务，通过492种VAE配置对比发现，注意力机制优于跳跃连接，在近源区域AUC达0.91，更适合地震预警应用。


<details>
  <summary>Details</summary>
Motivation: 强震记录中的P波检测面临高噪声、标记数据有限和复杂波形特征的挑战，传统方法效果有限，需要探索自监督学习的新途径。

Method: 将P波到达检测重构为自监督异常检测任务，通过492种变分自编码器配置的网格搜索，对比跳跃连接和注意力机制等不同架构对重建保真度和异常判别能力的影响。

Result: 跳跃连接虽然最小化重建误差（MAE约0.0012），但导致"过度泛化"，会重建噪声而掩盖检测信号；注意力机制优先考虑全局上下文而非局部细节，获得最高检测性能（AUC 0.875），在0-40公里近源范围内AUC达0.91。

Conclusion: 优先考虑全局上下文而非像素级完美重建的架构约束对于稳健的自监督P波检测至关重要，注意力机制VAE在近源区域表现优异，非常适合即时地震预警应用。

Abstract: Accurate P-wave detection is critical for earthquake early warning, yet strong-motion records pose challenges due to high noise levels, limited labeled data, and complex waveform characteristics. This study reframes P-wave arrival detection as a self-supervised anomaly detection task to evaluate how architectural variations regulate the trade-off between reconstruction fidelity and anomaly discrimination. Through a comprehensive grid search of 492 Variational Autoencoder configurations, we show that while skip connections minimize reconstruction error (Mean Absolute Error approximately 0.0012), they induce "overgeneralization", allowing the model to reconstruct noise and masking the detection signal. In contrast, attention mechanisms prioritize global context over local detail and yield the highest detection performance with an area-under-the-curve of 0.875. The attention-based Variational Autoencoder achieves an area-under-the-curve of 0.91 in the 0 to 40-kilometer near-source range, demonstrating high suitability for immediate early warning applications. These findings establish that architectural constraints favoring global context over pixel-perfect reconstruction are essential for robust, self-supervised P-wave detection.

</details>


### [58] [Weights to Code: Extracting Interpretable Algorithms from the Discrete Transformer](https://arxiv.org/abs/2601.05770)
*Yifan Zhang,Wei Bi,Kechi Zhang,Dongming Jin,Jie Fu,Zhi Jin*

Main category: cs.LG

TL;DR: Discrete Transformer通过强制功能解耦和温度退火采样，从连续表示中提取可读程序，实现无演示算法发现和Transformer可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统算法提取方法在Transformer上受到叠加效应的阻碍，即纠缠特征编码在重叠方向中阻碍符号表达式的提取，需要解决连续表示与离散符号逻辑之间的鸿沟。

Method: 提出Discrete Transformer架构，强制功能解耦：数值注意力仅用于信息路由，数值MLP仅用于元素级算术运算，并采用温度退火采样方法。

Result: Discrete Transformer性能与RNN基线相当，但关键扩展了连续变量域的可解释性；退火过程显示从探索到利用的清晰相变；可通过归纳偏置对合成程序进行细粒度控制。

Conclusion: Discrete Transformer为无演示算法发现提供了稳健框架，为Transformer可解释性提供了严格途径，建立了连续表示与离散符号逻辑之间的桥梁。

Abstract: Algorithm extraction aims to synthesize executable programs directly from models trained on specific algorithmic tasks, enabling de novo algorithm discovery without relying on human-written code. However, extending this paradigm to Transformer is hindered by superposition, where entangled features encoded in overlapping directions obstruct the extraction of symbolic expressions. In this work, we propose the Discrete Transformer, an architecture explicitly engineered to bridge the gap between continuous representations and discrete symbolic logic. By enforcing a strict functional disentanglement, which constrains Numerical Attention to information routing and Numerical MLP to element-wise arithmetic, and employing temperature-annealed sampling, our method effectively facilitates the extraction of human-readable programs. Empirically, the Discrete Transformer not only achieves performance comparable to RNN-based baselines but crucially extends interpretability to continuous variable domains. Moreover, our analysis of the annealing process shows that the efficient discrete search undergoes a clear phase transition from exploration to exploitation. We further demonstrate that our method enables fine-grained control over synthesized programs by imposing inductive biases. Collectively, these findings establish the Discrete Transformer as a robust framework for demonstration-free algorithm discovery, offering a rigorous pathway toward Transformer interpretability.

</details>


### [59] [Tensor-DTI: Enhancing Biomolecular Interaction Prediction with Contrastive Embedding Learning](https://arxiv.org/abs/2601.05792)
*Manel Gil-Sorribes,Júlia Vilalta-Mor,Isaac Filella-Mercè,Robert Soliva,Álvaro Ciudad,Víctor Guallar,Alexis Molina*

Main category: cs.LG

TL;DR: Tensor-DTI是一个基于对比学习的多模态药物-靶点相互作用预测框架，整合分子图、蛋白质语言模型和结合位点预测信息，在多个基准测试中优于现有方法，并在大规模虚拟筛选中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有药物-靶点相互作用预测模型通常依赖单模态预定义分子描述符或序列嵌入，代表性有限，需要更全面的多模态信息整合来提升预测准确性。

Method: 提出Tensor-DTI对比学习框架，采用孪生双编码器架构，整合分子图、蛋白质语言模型和结合位点预测的多模态嵌入，捕捉化学和结构相互作用特征，区分相互作用与非相互作用对。

Result: 在多个DTI基准测试中优于现有序列基和图基模型；在大规模CDK2化学库筛选中产生化学合理的命中分布；在严格家族保留分割下，对家族外靶点的高亲和力配体筛选预算有所改善；可扩展应用于蛋白质-RNA和肽-蛋白质相互作用预测。

Conclusion: 整合多模态信息与对比学习目标能显著提升相互作用预测准确性，为虚拟筛选提供更具可解释性和可靠性感知的模型，展示了多模态方法在计算药物发现中的优势。

Abstract: Accurate drug-target interaction (DTI) prediction is essential for computational drug discovery, yet existing models often rely on single-modality predefined molecular descriptors or sequence-based embeddings with limited representativeness. We propose Tensor-DTI, a contrastive learning framework that integrates multimodal embeddings from molecular graphs, protein language models, and binding-site predictions to improve interaction modeling. Tensor-DTI employs a siamese dual-encoder architecture, enabling it to capture both chemical and structural interaction features while distinguishing interacting from non-interacting pairs. Evaluations on multiple DTI benchmarks demonstrate that Tensor-DTI outperforms existing sequence-based and graph-based models. We also conduct large-scale inference experiments on CDK2 across billion-scale chemical libraries, where Tensor-DTI produces chemically plausible hit distributions even when CDK2 is withheld from training. In enrichment studies against Glide docking and Boltz-2 co-folder, Tensor-DTI remains competitive on CDK2 and improves the screening budget required to recover moderate fractions of high-affinity ligands on out-of-family targets under strict family-holdout splits. Additionally, we explore its applicability to protein-RNA and peptide-protein interactions. Our findings highlight the benefits of integrating multimodal information with contrastive objectives to enhance interaction-prediction accuracy and to provide more interpretable and reliability-aware models for virtual screening.

</details>


### [60] [Fusion Matters: Length-Aware Analysis of Positional-Encoding Fusion in Transformers](https://arxiv.org/abs/2601.05807)
*Mohamed Amine Hallam,Kuo-Kun Tseng*

Main category: cs.LG

TL;DR: 研究位置编码融合机制对Transformer性能的影响，发现融合策略在长序列任务中显著影响性能，而在短文本中影响可忽略


<details>
  <summary>Details</summary>
Motivation: 大多数先前工作专注于设计新的位置编码，而忽略了位置信息如何与词嵌入融合的机制。本文研究融合机制本身是否影响性能，特别是在长序列设置中

Method: 在相同的Transformer架构、数据划分和随机种子下，对三种经典融合策略进行控制性实证研究：元素级加法、带投影的拼接和标量门控融合。在三个文本分类数据集上进行实验，涵盖短序列（AG News）、中序列（IMDB）和长序列（ArXiv）。使用配对种子分析和跨数据集比较验证结果的稳定性

Result: 融合选择对短文本影响可忽略，但在长文档中产生一致的性能提升。在ArXiv数据集上的额外实验表明，可学习的融合机制在多个位置编码家族中都能泛化。轻量级卷积门控机制在融合层面引入局部归纳偏置，在长文档评估中表现良好

Conclusion: 位置编码融合是长序列Transformer中一个重要的设计选择，应被视为明确的建模决策而非固定默认选项

Abstract: Transformers require positional encodings to represent sequence order, yet most prior work focuses on designing new positional encodings rather than examining how positional information is fused with token embeddings. In this paper, we study whether the fusion mechanism itself affects performance, particularly in long-sequence settings. We conduct a controlled empirical study comparing three canonical fusion strategies--element-wise addition, concatenation with projection, and scalar gated fusion--under identical Transformer architectures, data splits, and random seeds. Experiments on three text classification datasets spanning short (AG News), medium (IMDB), and long (ArXiv) sequences show that fusion choice has negligible impact on short texts but produces consistent gains on long documents. To verify that these gains are structural rather than stochastic, we perform paired-seed analysis and cross-dataset comparison across sequence-length regimes. Additional experiments on the ArXiv dataset indicate that the benefit of learnable fusion generalizes across multiple positional encoding families. Finally, we explore a lightweight convolutional gating mechanism that introduces local inductive bias at the fusion level, evaluated on long documents only. Our results indicate that positional-encoding fusion is a non-trivial design choice for long-sequence Transformers and should be treated as an explicit modeling decision rather than a fixed default.

</details>


### [61] [Detecting Autism Spectrum Disorder with Deep Eye Movement Features](https://arxiv.org/abs/2601.05812)
*Zhanpei Huang,Taochen chen,Fangqing Gu,Yiqun Zhang*

Main category: cs.LG

TL;DR: 论文提出DSTS框架，利用眼动数据的离散短期特性进行自闭症谱系障碍诊断，相比传统方法和复杂深度学习模型表现更优。


<details>
  <summary>Details</summary>
Motivation: 眼动数据具有离散性和短期时间依赖特性，能反映自闭症谱系障碍的细微行为标记，但传统的Transformer全局注意力机制对此类数据效果有限，需要专门针对眼动数据特性的建模方法。

Method: 设计了离散短期序列建模框架DSTS，包含类感知表示和失衡感知机制，专门针对眼动数据的离散注视点和短期依赖特性进行建模。

Result: 在多个眼动数据集上的实验表明，DSTS框架优于传统机器学习方法和更复杂的深度学习模型。

Conclusion: 针对眼动数据的离散短期特性设计的专门建模框架能更有效地捕捉自闭症谱系障碍的细微行为模式，为ASD诊断提供了更有效的工具。

Abstract: Autism Spectrum Disorder (ASD) is a neurodevelopmental disorder characterized by deficits in social communication and behavioral patterns. Eye movement data offers a non-invasive diagnostic tool for ASD detection, as it is inherently discrete and exhibits short-term temporal dependencies, reflecting localized gaze focus between fixation points. These characteristics enable the data to provide deeper insights into subtle behavioral markers, distinguishing ASD-related patterns from typical development. Eye movement signals mainly contain short-term and localized dependencies. However, despite the widespread application of stacked attention layers in Transformer-based models for capturing long-range dependencies, our experimental results indicate that this approach yields only limited benefits when applied to eye movement data. This may be because discrete fixation points and short-term dependencies in gaze focus reduce the utility of global attention mechanisms, making them less efficient than architectures focusing on local temporal patterns. To efficiently capture subtle and complex eye movement patterns, distinguishing ASD from typically developing (TD) individuals, a discrete short-term sequential (DSTS) modeling framework is designed with Class-aware Representation and Imbalance-aware Mechanisms. Through extensive experiments on several eye movement datasets, DSTS outperforms both traditional machine learning techniques and more sophisticated deep learning models.

</details>


### [62] [A New Family of Poisson Non-negative Matrix Factorization Methods Using the Shifted Log Link](https://arxiv.org/abs/2601.05845)
*Eric Weine,Peter Carbonetto,Rafael A. Irizarry,Matthew Stephens*

Main category: cs.LG

TL;DR: 提出了一种使用移位对数连接函数的泊松非负矩阵分解方法，通过调整参数在加性组合和乘性组合之间灵活变化，改进了传统泊松NMF的加性假设限制。


<details>
  <summary>Details</summary>
Motivation: 传统泊松非负矩阵分解假设分解的"部分"以加性方式组合，这种假设在某些场景下可能不自然。需要一种更灵活的模型来适应不同数据组合模式。

Method: 引入移位对数连接函数，通过单个调优参数控制组合方式从加性到乘性的变化。提供了最大似然估计算法，以及针对大型稀疏数据集的近似计算方法。

Result: 在多个真实数据集上验证了新方法的有效性。结果表明连接函数的选择对分解结果有实质性影响，移位对数连接函数在某些情况下能提高结果的解释性。

Conclusion: 移位对数连接函数为泊松NMF提供了更灵活的建模框架，突破了传统加性组合的限制，能够更好地适应不同数据特性并提高模型解释性。

Abstract: Poisson non-negative matrix factorization (NMF) is a widely used method to find interpretable "parts-based" decompositions of count data. While many variants of Poisson NMF exist, existing methods assume that the "parts" in the decomposition combine additively. This assumption may be natural in some settings, but not in others. Here we introduce Poisson NMF with the shifted-log link function to relax this assumption. The shifted-log link function has a single tuning parameter, and as this parameter varies the model changes from assuming that parts combine additively (i.e., standard Poisson NMF) to assuming that parts combine more multiplicatively. We provide an algorithm to fit this model by maximum likelihood, and also an approximation that substantially reduces computation time for large, sparse datasets (computations scale with the number of non-zero entries in the data matrix). We illustrate these new methods on a variety of real datasets. Our examples show how the choice of link function in Poisson NMF can substantively impact the results, and how in some settings the use of a shifted-log link function may improve interpretability compared with the standard, additive link.

</details>


### [63] [IIB-LPO: Latent Policy Optimization via Iterative Information Bottleneck](https://arxiv.org/abs/2601.05870)
*Huilin Deng,Hongchen Luo,Yue Zhu,Long Li,Zhuoyue Chen,Xinghao Zhao,Ming Li,Jihai Zhang,Mengchang Wang,Yang Cao,Yu Kang*

Main category: cs.LG

TL;DR: IIB-LPO通过潜在策略优化和信息瓶颈原则解决RLVR中探索崩溃问题，在数学推理基准上实现SOTA性能


<details>
  <summary>Details</summary>
Motivation: 现有强化学习可验证奖励方法面临探索崩溃问题，随机rollout的语义同质性导致模型陷入狭窄、过度优化的行为。全局熵正则化容易受到奖励攻击，而局部token选择性更新难以克服预训练模型的强归纳偏置。

Method: 提出IIB-LPO方法，将探索从token分布的统计扰动转向推理轨迹的拓扑分支。在高熵状态触发潜在分支以多样化推理路径，并采用信息瓶颈原则作为轨迹过滤器和自奖励机制，确保简洁且信息丰富的探索。

Result: 在四个数学推理基准上的实验结果表明，IIB-LPO实现了最先进的性能，在准确率上比先前方法高出5.3%，在多样性指标上高出7.4%。

Conclusion: IIB-LPO通过拓扑分支和信息瓶颈原则有效解决了RLVR中的探索崩溃问题，为LLM推理提供了更有效的探索策略。

Abstract: Recent advances in Reinforcement Learning with Verifiable Rewards (RLVR) for Large Language Model (LLM) reasoning have been hindered by a persistent challenge: exploration collapse. The semantic homogeneity of random rollouts often traps models in narrow, over-optimized behaviors. While existing methods leverage policy entropy to encourage exploration, they face inherent limitations. Global entropy regularization is susceptible to reward hacking, which can induce meaningless verbosity, whereas local token-selective updates struggle with the strong inductive bias of pre-trained models. To address this, we propose Latent Policy Optimization via Iterative Information Bottleneck (IIB-LPO), a novel approach that shifts exploration from statistical perturbation of token distributions to topological branching of reasoning trajectories. IIB-LPO triggers latent branching at high-entropy states to diversify reasoning paths and employs the Information Bottleneck principle both as a trajectory filter and a self-reward mechanism, ensuring concise and informative exploration. Empirical results across four mathematical reasoning benchmarks demonstrate that IIB-LPO achieves state-of-the-art performance, surpassing prior methods by margins of up to 5.3% in accuracy and 7.4% in diversity metrics.

</details>


### [64] [Towards Realistic Guarantees: A Probabilistic Certificate for SmoothLLM](https://arxiv.org/abs/2511.18721)
*Adarsh Kumarappan,Ayushi Mehrotra*

Main category: cs.LG

TL;DR: 针对SmoothLLM防御的局限性，提出了更现实的(k,ε)-不稳定框架，为对抗越狱攻击提供更可信的安全认证。


<details>
  <summary>Details</summary>
Motivation: SmoothLLM防御虽然提供对抗越狱攻击的认证保证，但其依赖的严格k-不稳定假设在实践中很少成立，限制了安全证书的可信度。

Method: 引入更现实的概率框架(k,ε)-不稳定，通过结合攻击成功的经验模型，推导出SmoothLLM防御概率的新数据驱动下界。

Result: 新框架为从业者提供了可操作的安全保证，能够设置更符合LLM实际行为的认证阈值，增强对抗越狱攻击的防御能力。

Conclusion: 这项工作为安全AI部署提供了实用且理论基础的机制，使LLM更能抵抗对其安全对齐的利用，解决了安全部署中的关键挑战。

Abstract: The SmoothLLM defense provides a certification guarantee against jailbreaking attacks, but it relies on a strict `k-unstable' assumption that rarely holds in practice. This strong assumption can limit the trustworthiness of the provided safety certificate. In this work, we address this limitation by introducing a more realistic probabilistic framework, `(k, $\varepsilon$)-unstable,' to certify defenses against diverse jailbreaking attacks, from gradient-based (GCG) to semantic (PAIR). We derive a new, data-informed lower bound on SmoothLLM's defense probability by incorporating empirical models of attack success, providing a more trustworthy and practical safety certificate. By introducing the notion of (k, $\varepsilon$)-unstable, our framework provides practitioners with actionable safety guarantees, enabling them to set certification thresholds that better reflect the real-world behavior of LLMs. Ultimately, this work contributes a practical and theoretically-grounded mechanism to make LLMs more resistant to the exploitation of their safety alignments, a critical challenge in secure AI deployment.

</details>


### [65] [Auditing Fairness under Model Updates: Fundamental Complexity and Property-Preserving Updates](https://arxiv.org/abs/2601.05909)
*Ayoub Ajarra,Debabrota Basu*

Main category: cs.LG

TL;DR: 提出一个在模型自适应更新下进行群体公平性审计的通用框架，通过SP维度量化允许更新的复杂性，实现样本高效的PAC审计


<details>
  <summary>Details</summary>
Motivation: 现实世界中机器学习模型会自适应更新，这改变了底层模型类别但可能保持某些审计属性不变，需要研究在这种变化下如何进行可靠的公平性审计

Method: 提出基于经验属性优化(EPO)oracle的通用PAC审计框架，针对统计公平性建立了由SP维度（一种新的组合度量）表征的分布无关审计边界

Result: 建立了统计公平性审计的理论边界，SP维度量化了允许策略更新的复杂性，框架可扩展到其他审计目标如预测误差和鲁棒风险

Conclusion: 该框架为模型自适应更新环境下的公平性审计提供了理论基础和实用方法，能够用最少标注样本有效估计审计属性

Abstract: As machine learning models become increasingly embedded in societal infrastructure, auditing them for bias is of growing importance. However, in real-world deployments, auditing is complicated by the fact that model owners may adaptively update their models in response to changing environments, such as financial markets. These updates can alter the underlying model class while preserving certain properties of interest, raising fundamental questions about what can be reliably audited under such shifts.
  In this work, we study group fairness auditing under arbitrary updates. We consider general shifts that modify the pre-audit model class while maintaining invariance of the audited property. Our goals are two-fold: (i) to characterize the information complexity of allowable updates, by identifying which strategic changes preserve the property under audit; and (ii) to efficiently estimate auditing properties, such as group fairness, using a minimal number of labeled samples.
  We propose a generic framework for PAC auditing based on an Empirical Property Optimization (EPO) oracle. For statistical parity, we establish distribution-free auditing bounds characterized by the SP dimension, a novel combinatorial measure that captures the complexity of admissible strategic updates. Finally, we demonstrate that our framework naturally extends to other auditing objectives, including prediction error and robust risk.

</details>


### [66] [Automating Deception: Scalable Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2511.19517)
*Adarsh Kumarappan,Ananya Mujoo*

Main category: cs.LG

TL;DR: 本文提出了一种自动化生成大规模、基于心理学原理的多轮越狱数据集的方法，评估了不同LLM对基于"得寸进尺"心理技巧的多轮对话攻击的防御能力，发现GPT系列模型对此类攻击特别脆弱，而Gemini 2.5 Flash表现出卓越的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 多轮对话攻击利用"得寸进尺"等心理学原理绕过LLM的安全对齐机制，构成持续威胁。现有防御进展受限于依赖手动、难以扩展的数据集创建方法，需要自动化、可扩展的数据生成方案来推动防御研究。

Method: 提出了一种新颖的自动化流水线，用于生成大规模、基于心理学原理的多轮越狱数据集。系统地将"得寸进尺"技术转化为可复现的模板，创建了包含1,500个场景的基准数据集，涵盖非法活动和冒犯性内容。评估了来自三个主要LLM家族的七个模型，分别在多轮（带历史）和单轮（无历史）条件下进行测试。

Result: GPT系列模型对对话历史表现出显著脆弱性，攻击成功率（ASR）最多增加32个百分点。Google的Gemini 2.5 Flash表现出卓越的抵抗力，几乎免疫于这些攻击。Anthropic的Claude 3 Haiku显示出强大但不完美的抵抗力。不同模型在安全架构处理对话上下文方面存在关键差异。

Conclusion: 当前安全架构在处理对话上下文方面存在显著差异，突显了需要能够抵抗基于叙事操纵的防御机制。自动化数据集生成方法为评估和改进LLM对多轮心理操纵攻击的防御能力提供了重要工具。

Abstract: Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.

</details>


### [67] [Distilling Lightweight Domain Experts from Large ML Models by Identifying Relevant Subspaces](https://arxiv.org/abs/2601.05913)
*Pattarawat Chormai,Ali Hashemi,Klaus-Robert Müller,Grégoire Montavon*

Main category: cs.LG

TL;DR: SubDistill是一种新的知识蒸馏算法，专注于只蒸馏教师模型中与特定子任务相关的组件，在计算资源有限的环境中实现更高效的知识迁移。


<details>
  <summary>Details</summary>
Motivation: 实际应用中通常只需要蒸馏少数相关类别及其关联的中间概念，但现有蒸馏方法很少明确关注相关子任务，导致资源浪费和效率低下。

Method: 提出SubDistill算法，改进数值特性，在每一层只蒸馏教师模型的相关组件，而不是整个模型的所有信息。

Result: 在CIFAR-100和ImageNet数据集上，使用卷积和Transformer模型的实验表明，SubDistill在代表性子任务集上优于现有的逐层蒸馏技术。可解释AI分析显示，蒸馏后的学生模型更接近原始教师模型的决策结构。

Conclusion: SubDistill通过专注于相关子任务的蒸馏，提高了知识迁移的效率和效果，为计算资源有限环境下的模型部署提供了更优解决方案。

Abstract: Knowledge distillation involves transferring the predictive capabilities of large, high-performing AI models (teachers) to smaller models (students) that can operate in environments with limited computing power. In this paper, we address the scenario in which only a few classes and their associated intermediate concepts are relevant to distill. This scenario is common in practice, yet few existing distillation methods explicitly focus on the relevant subtask. To address this gap, we introduce 'SubDistill', a new distillation algorithm with improved numerical properties that only distills the relevant components of the teacher model at each layer. Experiments on CIFAR-100 and ImageNet with Convolutional and Transformer models demonstrate that SubDistill outperforms existing layer-wise distillation techniques on a representative set of subtasks. Our benchmark evaluations are complemented by Explainable AI analyses showing that our distilled student models more closely match the decision structure of the original teacher model.

</details>


### [68] [Tiny Recursive Models on ARC-AGI-1: Inductive Biases, Identity Conditioning, and Test-Time Compute](https://arxiv.org/abs/2512.11847)
*Antonio Roye-Azar,Santiago Vargas-Naranjo,Dhruv Ghai,Nithin Balamurugan,Rayan Amir*

Main category: cs.LG

TL;DR: TRM在ARC-AGI-1上的表现主要来自测试时增强、多数投票集成、任务标识符依赖和浅层递归，而非深度内部推理能力


<details>
  <summary>Details</summary>
Motivation: 分析Tiny Recursive Models在ARC任务中表现的真实原因，探究其性能是来自架构优势、测试时计算还是任务特定先验

Method: 对ARC Prize TRM检查点在ARC-AGI-1上进行实证分析：测试时增强和多数投票影响评估、任务标识符消融实验、递归轨迹分析、不同增强策略的训练实验、与Llama 3 8B QLoRA微调的效率比较

Result: 1) 1000样本投票管道比单次推理提升11个百分点；2) 替换正确谜题ID导致零准确率；3) 大部分准确率在第一步递归获得，递归深度浅；4) 强增强策略扩展候选解分布；5) TRM比Llama 3 8B QLoRA吞吐量高、内存使用低

Conclusion: TRM在ARC-AGI-1上的性能主要源于效率优势、任务特定条件设置和激进的测试时计算，而非深度内部推理能力

Abstract: Tiny Recursive Models (TRM) were proposed as a parameter-efficient alternative to large language models for solving Abstraction and Reasoning Corpus (ARC) style tasks. The original work reports strong performance and suggests that recursive latent updates enable non-trivial reasoning, but it remains unclear how much of this performance stems from architecture, test-time compute, or task-specific priors. In this technical note, we empirically analyze the ARC Prize TRM checkpoint on ARC-AGI-1 and report four behavioral findings and an efficiency comparison. First, we show that test-time augmentation and majority-vote ensembling account for a substantial fraction of reported performance: the 1000-sample voting pipeline improves Pass@1 by about 11 percentage points over single-pass canonical inference. Second, a puzzle-identity ablation reveals strict dependence on task identifiers: replacing the correct puzzle ID with a blank or random token yields zero accuracy. Third, a recursion trajectory analysis shows that most of the final accuracy is achieved at the first recursion step and that performance saturates after few latent updates, indicating shallow effective recursion. Fourth, early-stage training experiments under canonical versus heavy augmentation regimes suggest that heavy augmentation broadens the distribution of candidate solutions and improves multi-sample success. Finally, we compare TRM with a naive QLoRA fine-tune of Llama 3 8B on canonical ARC-AGI-1, finding that TRM's non-autoregressive design achieves much higher throughput and substantially lower memory usage in this setting. Overall, TRM's ARC-AGI-1 performance appears to arise from an interaction between efficiency, task-specific conditioning, and aggressive test-time compute rather than deep internal reasoning.

</details>


### [69] [Prophet as a Repro ducible Forecasting Framework: A Methodological Guide for Business and Financial Analytics](https://arxiv.org/abs/2601.05929)
*Sidney Shapiro,Burhanuddin Panvelwala*

Main category: cs.LG

TL;DR: 该论文评估了Meta开发的Prophet开源预测框架如何通过其可解释的加性结构、标准化工作流程和开源实现来提升预测研究的可重复性，并与ARIMA和随机森林进行了对比分析。


<details>
  <summary>Details</summary>
Motivation: 预测研究在商业和金融分析中存在可重复性挑战，传统方法需要大量手动调参且难以在专有环境中复制，机器学习方法虽然灵活但存在可解释性和随机训练过程的问题。需要一种能平衡可解释性、标准化工作流程和可访问性的解决方案。

Method: 使用公开可用的金融和零售数据集，在受控且完全记录实验设计的条件下，比较Prophet与多种ARIMA规格（自动选择、手动指定和季节性变体）以及随机森林的性能和可解释性。通过具体Python示例展示Prophet如何促进高效预测工作流程。

Result: Prophet通过其加性结构、开源实现和标准化工作流程，为透明和可复制的预测实践做出了贡献。研究提供了Prophet在可重复性方面的优势评估，并展示了其作为支持验证、可审计性和方法严谨性的方法论构建模块的作用。

Conclusion: Prophet框架在预测研究中扮演着促进可重复性的重要角色，为基于Python的研究工作流程提供了实用的参考框架，支持验证、可审计性和方法严谨性，有助于解决预测研究中的可重复性挑战。

Abstract: Reproducibility remains a persistent challenge in forecasting research and practice, particularly in business and financial analytics where forecasts inform high-stakes decisions. Traditional forecasting methods, while theoretically interpretable, often require extensive manual tuning and are difficult to replicate in proprietary environments. Machine learning approaches offer predictive flexibility but introduce challenges related to interpretability, stochastic training procedures, and cross-environment reproducibility. This paper examines Prophet, an open-source forecasting framework developed by Meta, as a reproducibility-enabling solution that balances interpretability, standardized workflows, and accessibility. Rather than proposing a new algorithm, this study evaluates how Prophet's additive structure, open-source implementation, and standardized workflow contribute to transparent and replicable forecasting practice. Using publicly available financial and retail datasets, we compare Prophet's performance and interpretability with multiple ARIMA specifications (auto-selected, manually specified, and seasonal variants) and Random Forest under a controlled and fully documented experimental design. This multi-model comparison provides a robust assessment of Prophet's relative performance and reproducibility advantages. Through concrete Python examples, we demonstrate how Prophet facilitates efficient forecasting workflows and integration with analytical pipelines. The study positions Prophet within the broader context of reproducible research. It highlights Prophet's role as a methodological building block that supports verification, auditability, and methodological rigor. This work provides researchers and practitioners with a practical reference framework for reproducible forecasting in Python-based research workflows.

</details>


### [70] [LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection](https://arxiv.org/abs/2601.06016)
*Þór Sverrisson,Steinn Guðmundsson*

Main category: cs.LG

TL;DR: LookAroundNet是一种基于Transformer的癫痫发作检测器，通过使用更宽的EEG时间窗口来建模癫痫活动，利用发作前后信号上下文提高检测性能，在多种临床和家庭监测条件下表现优异。


<details>
  <summary>Details</summary>
Motivation: 由于癫痫发作动态在不同患者、记录条件和临床环境中存在巨大差异，自动癫痫检测仍然具有挑战性。需要开发能够适应这种多样性并在真实临床环境中实用的检测方法。

Method: 提出LookAroundNet，一种基于Transformer的癫痫检测器，使用更宽的EEG时间窗口（包括感兴趣片段前后的信号），模拟临床医生解读EEG时使用上下文的方式。在多个EEG数据集上进行评估，包括临床常规EEG和长期动态记录，涵盖不同数据分布。

Result: LookAroundNet在多个数据集上表现出色，能够很好地泛化到未见过的记录条件，计算成本适合真实临床部署。结果表明扩展时间上下文、增加训练数据多样性和模型集成是提高性能的关键因素。

Conclusion: 这项工作推动了自动癫痫检测模型向临床可行解决方案的发展，表明利用更广泛的时间上下文和多样化的训练数据可以显著提高模型在不同临床环境中的性能。

Abstract: Automated seizure detection from electroencephalography (EEG) remains difficult due to the large variability of seizure dynamics across patients, recording conditions, and clinical settings. We introduce LookAroundNet, a transformer-based seizure detector that uses a wider temporal window of EEG data to model seizure activity. The seizure detector incorporates EEG signals before and after the segment of interest, reflecting how clinicians use surrounding context when interpreting EEG recordings. We evaluate the proposed method on multiple EEG datasets spanning diverse clinical environments, patient populations, and recording modalities, including routine clinical EEG and long-term ambulatory recordings, in order to study performance across varying data distributions. The evaluation includes publicly available datasets as well as a large proprietary collection of home EEG recordings, providing complementary views of controlled clinical data and unconstrained home-monitoring conditions. Our results show that LookAroundNet achieves strong performance across datasets, generalizes well to previously unseen recording conditions, and operates with computational costs compatible with real-world clinical deployment. The results indicate that extended temporal context, increased training data diversity, and model ensembling are key factors for improving performance. This work contributes to moving automatic seizure detection models toward clinically viable solutions.

</details>
