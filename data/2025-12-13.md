<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 9]
- [cs.AI](#cs.AI) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [UrbanAI 2025 Challenge: Linear vs Transformer Models for Long-Horizon Exogenous Temperature Forecasting](https://arxiv.org/abs/2512.10866)
*Ruslan Gokhman*

Main category: cs.LG

TL;DR: 线性模型在仅使用历史室内温度数据的长期外生预测任务中优于Transformer架构，DLinear表现最佳


<details>
  <summary>Details</summary>
Motivation: 研究仅使用历史室内温度值进行长期预测的挑战性单变量设置，比较线性和Transformer家族模型在此任务上的表现

Method: 使用Linear、NLinear、DLinear、Transformer、Informer和Autoformer模型，在标准化的训练、验证和测试分割下评估长期外生温度预测性能

Result: 线性基线模型（Linear、NLinear、DLinear）持续优于更复杂的Transformer架构，其中DLinear在所有分割上获得最佳整体准确率

Conclusion: 精心设计的线性模型在仅使用外生变量的挑战性时间序列预测场景中仍然是强大的基线方法

Abstract: We study long-horizon exogenous-only temperature forecasting - a challenging univariate setting where only the past values of the indoor temperature are used for prediction - using linear and Transformer-family models. We evaluate Linear, NLinear, DLinear, Transformer, Informer, and Autoformer under standardized train, validation, and test splits. Results show that linear baselines (Linear, NLinear, DLinear) consistently outperform more complex Transformer-family architectures, with DLinear achieving the best overall accuracy across all splits. These findings highlight that carefully designed linear models remain strong baselines for time series forecasting in challenging exogenous-only settings.

</details>


### [2] [Guided Transfer Learning for Discrete Diffusion Models](https://arxiv.org/abs/2512.10877)
*Julian Kleutgens,Claudio Battiloro,Lingkai Kong,Benjamin Grewe,Francesca Dominici,Mauricio Tec*

Main category: cs.LG

TL;DR: 提出GTL方法，实现离散扩散模型的引导式迁移学习，无需微调预训练去噪器，通过高效采样器降低计算成本


<details>
  <summary>Details</summary>
Motivation: 离散扩散模型在语言等离散领域表现优异，但需要大量训练数据，迁移学习时微调成本高且不实用，需要更高效的适应方法

Method: 基于连续扩散的比率迁移学习，提出GTL引导式迁移学习，适用于离散时间扩散和连续时间得分离散扩散，并开发高效引导采样器，聚焦规划选择位置和候选词

Result: GTL能在不修改预训练去噪器的情况下从目标分布采样，高效采样器显著降低大词汇量和长序列的采样时间和计算量

Conclusion: GTL为离散扩散模型提供了实用的迁移学习方法，使引导式语言建模在大词汇量和长序列场景下变得可行

Abstract: Discrete diffusion models achieve strong performance across language and other discrete domains, providing a powerful alternative to autoregressive models. However, their strong performance relies on large training datasets, which are costly or risky to obtain, especially when adapting to new domains. Transfer learning is the natural way to adapt pretrained discrete diffusion models, but current methods require fine-tuning large diffusion models, which is computationally expensive and often impractical. Building on ratio-based transfer learning for continuous diffusion, we provide Guided Transfer Learning for discrete diffusion models (GTL). This enables sampling from a target distribution without modifying the pretrained denoiser. The same guidance formulation applies to both discrete-time diffusion and continuous-time score-based discrete diffusion, yielding a unified treatment. Guided discrete diffusion often requires many forward passes of the guidance network, which becomes impractical for large vocabularies and long sequences. To address this, we further present an efficient guided sampler that concentrates evaluations on planner-selected positions and top candidate tokens, thus lowering sampling time and computation. This makes guided language modeling practical at scale for large vocabularies and long sequences. We evaluate GTL on sequential data, including synthetic Markov chains and language modeling, and provide empirical analyses of its behavior.

</details>


### [3] [SparseSwaps: Tractable LLM Pruning Mask Refinement at Scale](https://arxiv.org/abs/2512.10922)
*Max Zimmer,Christophe Roux,Moritz Wagner,Deborah Hendrych,Sebastian Pokutta*

Main category: cs.LG

TL;DR: 提出一种针对大语言模型的高效剪枝方法，通过行级稀疏约束和1-swap优化算法，显著降低剪枝误差并提升模型性能


<details>
  <summary>Details</summary>
Motivation: 传统剪枝方法在大语言模型上存在局限性：完全重训练成本过高，全局幅度剪枝对Transformer架构效果不佳，现有方法依赖近似或启发式算法，无法精确求解剪枝掩码选择问题

Method: 1. 通过强制每行相等稀疏度来解耦行间依赖；2. 推导出可高效计算的Gram矩阵最优1-swap（交换一个保留权重和一个剪枝权重）；3. 提出基于GPU可扩展的1-swap算法，从任意剪枝掩码开始，无需超参数调优

Result: 相比Wanda方法，将每层剪枝误差降低高达60%；在先进的GPT架构上，持续改善困惑度和零样本准确率

Conclusion: 该方法使大语言模型规模的剪枝掩码选择问题变得可行，提供了一种高效、可扩展且超参数自由的剪枝解决方案

Abstract: The resource requirements of Neural Networks can be significantly reduced through pruning -- the removal of seemingly less important parameters. However, with the rise of Large Language Models (LLMs), full retraining to recover pruning-induced performance degradation is often prohibitive and classical approaches such as global magnitude pruning are suboptimal on Transformer architectures. State-of-the-art methods hence solve a layer-wise mask selection problem, the problem of finding a pruning mask which minimizes the per-layer pruning error on a small set of calibration data. Exactly solving this problem to optimality using Integer Programming (IP) solvers is computationally infeasible due to its combinatorial nature and the size of the search space, and existing approaches therefore rely on approximations or heuristics. In this work, we demonstrate that the mask selection problem can be made drastically more tractable at LLM scale. To that end, we decouple the rows by enforcing equal sparsity levels per row. This allows us to derive optimal 1-swaps (exchanging one kept and one pruned weight) that can be computed efficiently using the Gram matrix of the calibration data. Using these observations, we propose a tractable and simple 1-swap algorithm that warm starts from any pruning mask, runs efficiently on GPUs at LLM scale, and is essentially hyperparameter-free. We demonstrate that our approach reduces per-layer pruning error by up to 60% over Wanda (Sun et al., 2023) and consistently improves perplexity and zero-shot accuracy across state-of-the-art GPT architectures.

</details>


### [4] [Digital Twin Supervised Reinforcement Learning Framework for Autonomous Underwater Navigation](https://arxiv.org/abs/2512.10925)
*Zamirddine Mari,Mohamad Motasem Nawaf,Pierre Drap*

Main category: cs.LG

TL;DR: 本文提出基于PPO算法的深度强化学习方法，用于BlueROV2水下机器人的自主导航，在复杂障碍环境中优于传统DWA方法，并验证了从仿真到实物的可迁移性。


<details>
  <summary>Details</summary>
Motivation: 水下环境自主导航面临GPS缺失、能见度差和障碍物等挑战，需要开发能够适应复杂水下环境的鲁棒导航方法。

Method: 采用基于PPO的深度强化学习算法，结合目标导向导航信息、虚拟占用网格和操作区域边界的射线投射作为观测空间，在仿真环境中训练策略。

Result: PPO策略在高度杂乱环境中持续优于DWA基准方法，表现出更好的局部适应性和更少的碰撞，成功实现了从仿真到真实BlueROV2的迁移。

Conclusion: 深度强化学习适用于水下机器人自主导航，PPO方法在复杂障碍环境中表现优异，仿真到实物的迁移验证了该方法的实用性和有效性。

Abstract: Autonomous navigation in underwater environments remains a major challenge due to the absence of GPS, degraded visibility, and the presence of submerged obstacles. This article investigates these issues through the case of the BlueROV2, an open platform widely used for scientific experimentation. We propose a deep reinforcement learning approach based on the Proximal Policy Optimization (PPO) algorithm, using an observation space that combines target-oriented navigation information, a virtual occupancy grid, and ray-casting along the boundaries of the operational area. The learned policy is compared against a reference deterministic kinematic planner, the Dynamic Window Approach (DWA), commonly employed as a robust baseline for obstacle avoidance. The evaluation is conducted in a realistic simulation environment and complemented by validation on a physical BlueROV2 supervised by a 3D digital twin of the test site, helping to reduce risks associated with real-world experimentation. The results show that the PPO policy consistently outperforms DWA in highly cluttered environments, notably thanks to better local adaptation and reduced collisions. Finally, the experiments demonstrate the transferability of the learned behavior from simulation to the real world, confirming the relevance of deep RL for autonomous navigation in underwater robotics.

</details>


### [5] [Decoupled Q-Chunking](https://arxiv.org/abs/2512.10926)
*Qiyang Li,Seohong Park,Sergey Levine*

Main category: cs.LG

TL;DR: 提出一种新算法，通过解耦评论家与策略的动作块长度，让策略在较短动作块上操作，同时保留多步价值传播优势，解决传统块状评论家策略提取的开放环路次优问题。


<details>
  <summary>Details</summary>
Motivation: 时间差分方法通过自举机制高效学习状态和动作价值，但容易产生自举偏差。块状评论家通过估计短动作序列的价值来加速价值备份，但从中提取策略存在挑战：策略必须开放环路输出整个动作块，这在需要策略反应性的环境中可能次优，且当块长度增长时建模困难。

Method: 提出解耦评论家与策略动作块长度的算法，通过优化策略对抗蒸馏评论家来估计部分动作块的价值。蒸馏评论家通过乐观地从原始块状评论家回溯，近似部分动作块扩展为完整块时可实现的最大价值。

Result: 在具有挑战性的长时域离线目标条件任务上评估该方法，结果显示它可靠地优于先前方法。

Conclusion: 该方法既保留了多步价值传播的优势，又避免了开放环路次优性和学习长动作块策略的困难，在复杂任务中表现出优越性能。

Abstract: Temporal-difference (TD) methods learn state and action values efficiently by bootstrapping from their own future value predictions, but such a self-bootstrapping mechanism is prone to bootstrapping bias, where the errors in the value targets accumulate across steps and result in biased value estimates. Recent work has proposed to use chunked critics, which estimate the value of short action sequences ("chunks") rather than individual actions, speeding up value backup. However, extracting policies from chunked critics is challenging: policies must output the entire action chunk open-loop, which can be sub-optimal for environments that require policy reactivity and also challenging to model especially when the chunk length grows. Our key insight is to decouple the chunk length of the critic from that of the policy, allowing the policy to operate over shorter action chunks. We propose a novel algorithm that achieves this by optimizing the policy against a distilled critic for partial action chunks, constructed by optimistically backing up from the original chunked critic to approximate the maximum value achievable when a partial action chunk is extended to a complete one. This design retains the benefits of multi-step value propagation while sidestepping both the open-loop sub-optimality and the difficulty of learning action chunking policies for long action chunks. We evaluate our method on challenging, long-horizon offline goal-conditioned tasks and show that it reliably outperforms prior methods. Code: github.com/ColinQiyangLi/dqc.

</details>


### [6] [Asynchronous Reasoning: Training-Free Interactive Thinking LLMs](https://arxiv.org/abs/2512.10931)
*George Yakushev,Nataliia Babina,Masoud Vahid Dastgerdi,Vyacheslav Zhdanovskiy,Alina Shutova,Denis Kuznedelev*

Main category: cs.LG

TL;DR: 该研究提出了一种让LLM能够同时思考、倾听和生成输出的方法，通过利用旋转嵌入的特性，使原本需要顺序交互的LLM能够实现异步推理，显著减少实时响应延迟。


<details>
  <summary>Details</summary>
Motivation: 当前LLM的推理过程需要顺序交互（先思考再回答），这限制了其在需要实时响应的应用场景（如语音助手）中的实用性。人类可以异步地倾听、思考和行动，而现有LLM无法做到这一点。

Method: 利用旋转嵌入的特性，在不需额外训练的情况下，使原本设计用于顺序交互的LLM能够同时进行思考、倾听和生成输出，实现异步推理能力。

Result: 在数学推理、常识推理和安全推理任务上，该方法能够实时生成准确的思考增强答案，将首个非思考token的生成时间从几分钟减少到≤5秒，整体实时延迟降低了6-11倍。

Conclusion: 通过利用旋转嵌入的特性，可以使现有LLM在不需额外训练的情况下实现异步推理，显著提高实时响应能力，为LLM在需要实时交互的应用场景中提供了新的可能性。

Abstract: Many state-of-the-art LLMs are trained to think before giving their answer. Reasoning can greatly improve language model capabilities and safety, but it also makes them less interactive: given a new input, a model must stop thinking before it can respond. Real-world use cases such as voice-based or embedded assistants require an LLM agent to respond and adapt to additional information in real time, which is incompatible with sequential interactions. In contrast, humans can listen, think, and act asynchronously: we begin thinking about the problem while reading it and continue thinking while formulating the answer. In this work, we augment LLMs capable of reasoning to operate in a similar way without additional training. Our method uses the properties of rotary embeddings to enable LLMs built for sequential interactions to simultaneously think, listen, and generate outputs. We evaluate our approach on math, commonsense, and safety reasoning and find that it can generate accurate thinking-augmented answers in real time, reducing time to first non-thinking token from minutes to <= 5s. and the overall real-time delays by 6-11x.

</details>


### [7] [Stronger Normalization-Free Transformers](https://arxiv.org/abs/2512.10938)
*Mingzhi Chen,Taiming Lu,Jiachen Zhu,Mingjie Sun,Zhuang Liu*

Main category: cs.LG

TL;DR: 本文提出Derf函数作为归一化层的替代方案，通过大规模搜索发现erf函数变体在多个领域超越传统归一化方法


<details>
  <summary>Details</summary>
Motivation: 尽管归一化层长期以来被视为深度学习架构不可或缺的组成部分，但最近提出的Dynamic Tanh (DyT)表明存在替代方案。本研究旨在寻找能够超越DyT性能的函数设计，探索点函数的内在特性如何影响训练和性能。

Method: 首先研究点函数的内在特性对训练和性能的影响，然后基于这些发现进行大规模搜索以寻找更有效的函数设计。最终引入Derf(x) = erf(αx + s)函数，其中erf(x)是重新缩放的高斯累积分布函数。

Result: Derf在多个领域超越了LayerNorm、RMSNorm和DyT，包括视觉（图像识别和生成）、语音表示和DNA序列建模。Derf的性能提升主要源于其改进的泛化能力而非更强的拟合能力。

Conclusion: Derf的简单性和更强的性能使其成为无归一化Transformer架构的实用选择，为深度学习架构设计提供了新的方向。

Abstract: Although normalization layers have long been viewed as indispensable components of deep learning architectures, the recent introduction of Dynamic Tanh (DyT) has demonstrated that alternatives are possible. The point-wise function DyT constrains extreme values for stable convergence and reaches normalization-level performance; this work seeks further for function designs that can surpass it. We first study how the intrinsic properties of point-wise functions influence training and performance. Building on these findings, we conduct a large-scale search for a more effective function design. Through this exploration, we introduce $\mathrm{Derf}(x) = \mathrm{erf}(αx + s)$, where $\mathrm{erf}(x)$ is the rescaled Gaussian cumulative distribution function, and identify it as the most performant design. Derf outperforms LayerNorm, RMSNorm, and DyT across a wide range of domains, including vision (image recognition and generation), speech representation, and DNA sequence modeling. Our findings suggest that the performance gains of Derf largely stem from its improved generalization rather than stronger fitting capacity. Its simplicity and stronger performance make Derf a practical choice for normalization-free Transformer architectures.

</details>


### [8] [Hierarchical Dataset Selection for High-Quality Data Sharing](https://arxiv.org/abs/2512.10952)
*Xiaona Zhou,Yingyan Zeng,Ran Jin,Ismini Lourentzou*

Main category: cs.LG

TL;DR: DaSH方法通过层次化建模数据集和组级效用，在资源约束下选择整个数据集来提升下游性能，比现有方法准确率提升高达26.2%


<details>
  <summary>Details</summary>
Motivation: 现实世界中数据通常以离散数据集形式组织，不同数据集在相关性、质量和效用上存在差异。现有方法通常选择单个样本并假设所有数据同等相关，忽略了数据集及其来源之间的差异，因此需要开发能够选择整个数据集的方法来优化模型训练

Method: 提出DaSH（Dataset Selection via Hierarchies）方法，在数据集和组（如集合、机构）两个层次上建模效用，通过层次化建模实现从有限观察中的高效泛化

Result: 在两个公共基准测试（Digit-Five和DomainNet）上，DaSH比最先进的数据选择基线方法在准确率上提升高达26.2%，同时需要显著更少的探索步骤。消融实验表明DaSH对低资源设置和缺乏相关数据集具有鲁棒性

Conclusion: DaSH方法适用于实际多源学习工作流中的可扩展和自适应数据集选择，能够有效处理异构数据集池中的选择问题，在资源约束下提升下游性能

Abstract: The success of modern machine learning hinges on access to high-quality training data. In many real-world scenarios, such as acquiring data from public repositories or sharing across institutions, data is naturally organized into discrete datasets that vary in relevance, quality, and utility. Selecting which repositories or institutions to search for useful datasets, and which datasets to incorporate into model training are therefore critical decisions, yet most existing methods select individual samples and treat all data as equally relevant, ignoring differences between datasets and their sources. In this work, we formalize the task of dataset selection: selecting entire datasets from a large, heterogeneous pool to improve downstream performance under resource constraints. We propose Dataset Selection via Hierarchies (DaSH), a dataset selection method that models utility at both dataset and group (e.g., collections, institutions) levels, enabling efficient generalization from limited observations. Across two public benchmarks (Digit-Five and DomainNet), DaSH outperforms state-of-the-art data selection baselines by up to 26.2% in accuracy, while requiring significantly fewer exploration steps. Ablations show DaSH is robust to low-resource settings and lack of relevant datasets, making it suitable for scalable and adaptive dataset selection in practical multi-source learning workflows.

</details>


### [9] [Bidirectional Normalizing Flow: From Data to Noise and Back](https://arxiv.org/abs/2512.10953)
*Yiyang Lu,Qiao Sun,Xianbang Wang,Zhicheng Jiang,Hanhong Zhao,Kaiming He*

Main category: cs.LG

TL;DR: BiFlow框架通过移除精确解析逆变换的需求，使用近似逆映射的逆向模型，在保持生成质量的同时将采样速度提升两个数量级


<details>
  <summary>Details</summary>
Motivation: 传统归一化流需要精确的解析逆变换，这限制了模型架构的灵活性。最近TARFlow等变体虽然结合了Transformer和自回归流，但因果解码成为主要瓶颈。需要一种更灵活的框架来克服这些限制。

Method: 提出双向归一化流(BiFlow)框架，移除对精确解析逆变换的需求。BiFlow学习一个逆向模型来近似底层的噪声到数据的逆映射，从而支持更灵活的损失函数和架构设计。

Result: 在ImageNet上的实验表明，与因果解码对应方法相比，BiFlow在提高生成质量的同时，将采样速度提升最多两个数量级。在基于NF的方法中达到最先进水平，在单评估方法中具有竞争力。

Conclusion: BiFlow通过移除精确逆变换约束，为归一化流提供了更灵活的框架，显著提升了采样效率，有望进一步推动这一经典范式的发展。

Abstract: Normalizing Flows (NFs) have been established as a principled framework for generative modeling. Standard NFs consist of a forward process and a reverse process: the forward process maps data to noise, while the reverse process generates samples by inverting it. Typical NF forward transformations are constrained by explicit invertibility, ensuring that the reverse process can serve as their exact analytic inverse. Recent developments in TARFlow and its variants have revitalized NF methods by combining Transformers and autoregressive flows, but have also exposed causal decoding as a major bottleneck. In this work, we introduce Bidirectional Normalizing Flow ($\textbf{BiFlow}$), a framework that removes the need for an exact analytic inverse. BiFlow learns a reverse model that approximates the underlying noise-to-data inverse mapping, enabling more flexible loss functions and architectures. Experiments on ImageNet demonstrate that BiFlow, compared to its causal decoding counterpart, improves generation quality while accelerating sampling by up to two orders of magnitude. BiFlow yields state-of-the-art results among NF-based methods and competitive performance among single-evaluation ("1-NFE") methods. Following recent encouraging progress on NFs, we hope our work will draw further attention to this classical paradigm.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [10] [LLMs Can Assist with Proposal Selection at Large User Facilities](https://arxiv.org/abs/2512.10895)
*Lijie Ding,Janell Thomson,Jon Taylor,Changwoo Do*

Main category: cs.AI

TL;DR: LLMs可显著改善大型用户设施的提案选择流程，提供比传统人工评审更可扩展、一致且经济高效的替代方案，同时能识别高发表潜力的提案并支持高级分析。


<details>
  <summary>Details</summary>
Motivation: 传统人工评审存在提案间相关性弱、评审者偏见和不一致的问题，而基于成对偏好的方法虽然逻辑上更优越，但工作量呈二次方增长，对人类评审者不切实际。

Method: 利用LLMs进行提案选择，基于来自橡树岭国家实验室散裂中子源三个光束线的精心策划的提案和发表记录，采用成对偏好方法进行提案排名。

Result: LLM排名与人类排名有强相关性（Spearman ρ≈0.2-0.8，去除10%异常值后≥0.5），在识别高发表潜力提案方面不逊于人类评审，且成本降低两个数量级以上。

Conclusion: LLMs为提案选择提供了可扩展、一致且经济高效的解决方案，不仅能有效排名，还能支持如通过嵌入模型定量评估提案相似性等人类难以完成的高级分析。

Abstract: We explore how large language models (LLMs) can enhance the proposal selection process at large user facilities, offering a scalable, consistent, and cost-effective alternative to traditional human review. Proposal selection depends on assessing the relative strength among submitted proposals; however, traditional human scoring often suffers from weak inter-proposal correlations and is subject to reviewer bias and inconsistency. A pairwise preference-based approach is logically superior, providing a more rigorous and internally consistent basis for ranking, but its quadratic workload makes it impractical for human reviewers. We address this limitation using LLMs. Leveraging the uniquely well-curated proposals and publication records from three beamlines at the Spallation Neutron Source (SNS), Oak Ridge National Laboratory (ORNL), we show that the LLM rankings correlate strongly with the human rankings (Spearman $ρ\simeq 0.2-0.8$, improving to $\geq 0.5$ after 10\% outlier removal). Moreover, LLM performance is no worse than that of human reviewers in identifying proposals with high publication potential, while costing over two orders of magnitude less. Beyond ranking, LLMs enable advanced analyses that are challenging for humans, such as quantitative assessment of proposal similarity via embedding models, which provides information crucial for review committees.

</details>


### [11] [Multi-Granular Node Pruning for Circuit Discovery](https://arxiv.org/abs/2512.10903)
*Muhammad Umair Haider,Hammad Rizwan,Hassan Sajjad,A. B. Siddique*

Main category: cs.AI

TL;DR: 提出了一种用于电路发现的节点级剪枝框架，解决了现有方法在可扩展性和粒度方面的限制，通过多粒度可学习掩码和特定粒度稀疏性惩罚，在单次微调中实现全面压缩。


<details>
  <summary>Details</summary>
Motivation: 现有电路发现方法主要依赖迭代边缘剪枝，计算成本高且仅限于粗粒度单元（如注意力头或MLP块），忽略了更细粒度的结构如单个神经元。

Method: 提出了节点级剪枝框架，引入跨多个粒度级别（从整个块到单个神经元）的可学习掩码，使用统一的优化目标和粒度特定的稀疏性惩罚来指导剪枝过程。

Result: 该方法发现的电路节点数比先前方法更少；证明许多被粗粒度方法认为重要的神经元实际上无关紧要，同时仍能保持任务性能；内存占用显著降低5-10倍，因为不需要在内存中保存中间激活。

Conclusion: 提出的节点级剪枝框架在电路发现中解决了可扩展性和粒度限制，能够识别更小、更精确的电路，同时大幅降低计算和内存需求。

Abstract: Circuit discovery aims to identify minimal subnetworks that are responsible for specific behaviors in large language models (LLMs). Existing approaches primarily rely on iterative edge pruning, which is computationally expensive and limited to coarse-grained units such as attention heads or MLP blocks, overlooking finer structures like individual neurons. We propose a node-level pruning framework for circuit discovery that addresses both scalability and granularity limitations. Our method introduces learnable masks across multiple levels of granularity, from entire blocks to individual neurons, within a unified optimization objective. Granularity-specific sparsity penalties guide the pruning process, allowing a comprehensive compression in a single fine-tuning run. Empirically, our approach identifies circuits that are smaller in nodes than those discovered by prior methods; moreover, we demonstrate that many neurons deemed important by coarse methods are actually irrelevant, while still maintaining task performance. Furthermore, our method has a significantly lower memory footprint, 5-10x, as it does not require keeping intermediate activations in the memory to work.

</details>


### [12] [On Decision-Making Agents and Higher-Order Causal Processes](https://arxiv.org/abs/2512.10937)
*Matt Wilson*

Main category: cs.AI

TL;DR: 该论文建立了部分可观测马尔可夫决策过程（POMDP）中的智能体决策与单输入过程函数（高阶量子操作的经典极限）之间的精确对应关系。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能中的决策理论与物理学中量子操作理论之间的深层联系，为理解智能体与环境交互提供新的理论框架。

Method: 通过将智能体的策略和记忆更新结合成过程函数w，使用链接积与POMDP环境交互，建立POMDP智能体与单输入过程函数之间的对应关系。

Result: 成功建立了POMDP智能体与过程函数之间的精确对应，并提出了双重解释：从物理学视角看，过程函数作为环境；从AI视角看，过程函数编码智能体。同时将这一视角扩展到多智能体系统。

Conclusion: 该研究揭示了人工智能决策理论与量子操作理论之间的深刻联系，为理解智能体-环境交互提供了统一的数学框架，并展示了如何将这一框架扩展到多智能体系统。

Abstract: We establish a precise correspondence between decision-making agents in partially observable Markov decision processes (POMDPs) and one-input process functions, the classical limit of higher-order quantum operations. In this identification an agent's policy and memory update combine into a process function w that interacts with a POMDP environment via the link product. This suggests a dual interpretation: in the physics view, the process function acts as the environment into which local operations (agent interventions) are inserted, whereas in the AI view it encodes the agent and the inserted functions represent environments. We extend this perspective to multi-agent systems by identifying observation-independent decentralized POMDPs as natural domains for multi-input process functions.

</details>
