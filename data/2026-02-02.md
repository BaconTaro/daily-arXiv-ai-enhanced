<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 126]
- [cs.HC](#cs.HC) [Total: 13]
- [cs.AI](#cs.AI) [Total: 38]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Attention Isn't All You Need for Emotion Recognition:Domain Features Outperform Transformers on the EAV Dataset](https://arxiv.org/abs/2601.22161)
*Anmol Guragain*

Main category: cs.LG

TL;DR: 在多模态情感识别研究中，复杂注意力机制在小数据集上表现不佳，而简单的领域适应性改进（如添加delta特征）能显著提升性能


<details>
  <summary>Details</summary>
Motivation: 研究复杂注意力机制是否能在小规模多模态情感识别数据集（EAV）上提升性能，探索在小数据集场景下的有效方法

Method: 使用EAV数据集，实现三类模型：基线Transformer（M1）、新型因子化注意力机制（M2）、改进的CNN基线（M3），并进行系统比较

Result: 复杂注意力机制（M2）在小数据集上表现不佳，比基线低5-13个百分点；而简单的领域适应性改进效果显著：音频CNN添加delta MFCCs提升3.66pp至65.56%，EEG频域特征提升7.62pp至67.62%，视觉Transformer基线达到75.30%超过原论文结果

Conclusion: 对于小规模情感识别任务，领域知识和适当实现比架构复杂性更重要，简单的领域适应性特征改进比复杂注意力机制更有效

Abstract: We present a systematic study of multimodal emotion recognition using the EAV dataset, investigating whether complex attention mechanisms improve performance on small datasets. We implement three model categories: baseline transformers (M1), novel factorized attention mechanisms (M2), and improved CNN baselines (M3). Our experiments show that sophisticated attention mechanisms consistently underperform on small datasets. M2 models achieved 5 to 13 percentage points below baselines due to overfitting and destruction of pretrained features. In contrast, simple domain-appropriate modifications proved effective: adding delta MFCCs to the audio CNN improved accuracy from 61.9\% to \textbf{65.56\%} (+3.66pp), while frequency-domain features for EEG achieved \textbf{67.62\%} (+7.62pp over the paper baseline). Our vision transformer baseline (M1) reached \textbf{75.30\%}, exceeding the paper's ViViT result (74.5\%) through domain-specific pretraining, and vision delta features achieved \textbf{72.68\%} (+1.28pp over the paper CNN). These findings demonstrate that for small-scale emotion recognition, domain knowledge and proper implementation outperform architectural complexity.

</details>


### [2] [Multitask Learning for Earth Observation Data Classification with Hybrid Quantum Network](https://arxiv.org/abs/2601.22195)
*Fan Fan,Yilei Shi,Tobias Guggemos,Xiao Xiang Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种结合多任务学习和量子卷积操作的混合模型，用于地球观测数据的分类，旨在利用量子计算解决大数据时代地球观测分析的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 地球观测已进入大数据时代，使用复杂深度学习模型分析大量地球观测数据面临计算需求瓶颈。量子机器学习作为潜在解决方案受到关注，尽管当前量子设备存在限制，但作者希望探索量子计算在地球观测数据分类中的优势。

Method: 提出了一种混合模型：1）结合多任务学习以辅助高效数据编码；2）采用带有量子卷积操作的位置权重模块来提取有效分类特征。

Result: 使用多个地球观测基准数据集验证了所提模型的有效性，通过实验探索了模型的泛化能力，并研究了其优势来源因素。

Conclusion: 研究结果突显了量子机器学习在地球观测数据分析中的潜力，为利用量子计算解决地球观测大数据分析的计算挑战提供了新思路。

Abstract: Quantum machine learning (QML) has gained increasing attention as a potential solution to address the challenges of computation requirements in the future. Earth observation (EO) has entered the era of Big Data, and the computational demands for effectively analyzing large EO data with complex deep learning models have become a bottleneck. Motivated by this, we aim to leverage quantum computing for EO data classification and explore its advantages despite the current limitations of quantum devices. This paper presents a hybrid model that incorporates multitask learning to assist efficient data encoding and employs a location weight module with quantum convolution operations to extract valid features for classification. The validity of our proposed model was evaluated using multiple EO benchmarks. Additionally, we experimentally explored the generalizability of our model and investigated the factors contributing to its advantage, highlighting the potential of QML in EO data analysis.

</details>


### [3] [Neural Signals Generate Clinical Notes in the Wild](https://arxiv.org/abs/2601.22197)
*Jathurshan Pradeepkumar,Zheng Chen,Jimeng Sun*

Main category: cs.LG

TL;DR: CELM是首个临床EEG到语言的基座模型，能够总结长时间、变长的EEG记录，并生成多尺度的临床报告，包括记录描述、背景活动、癫痫样异常、事件/癫痫发作和印象。


<details>
  <summary>Details</summary>
Motivation: 从长期EEG记录中生成总结异常模式、诊断发现和临床解释的临床报告仍然非常耗时费力，需要自动化解决方案。

Method: 构建大规模临床EEG数据集（9,922份报告，约11,000小时EEG记录，9,048名患者），开发CELM模型，整合预训练的EEG基座模型和语言模型，支持多尺度临床报告生成。

Result: 在有患者病史监督的情况下，生成指标（如ROUGE-1和METEOR）相对改进70%-95%，从0.2-0.3提升到0.4-0.6；在零样本设置下，CELM得分为0.43-0.52，基线为0.17-0.26。

Conclusion: CELM是首个临床EEG到语言的基座模型，能够有效生成多尺度临床报告，显著优于基线方法，并已开源模型和基准构建流程。

Abstract: Generating clinical reports that summarize abnormal patterns, diagnostic findings, and clinical interpretations from long-term EEG recordings remains labor-intensive. We curate a large-scale clinical EEG dataset with $9{,}922$ reports paired with approximately $11{,}000$ hours of EEG recordings from $9{,}048$ patients. We therefore develop CELM, the first clinical EEG-to-Language foundation model capable of summarizing long-duration, variable-length EEG recordings and performing end-to-end clinical report generation at multiple scales, including recording description, background activity, epileptiform abnormalities, events/seizures, and impressions. Experimental results show that, with patient history supervision, our method achieves $70\%$--$95\%$ average relative improvements in standard generation metrics (e.g., ROUGE-1 and METEOR) from $0.2$--$0.3$ to $0.4$--$0.6$. In the zero-shot setting without patient history, CELM attains generation scores in the range of $0.43$--$0.52$, compared to baselines of $0.17$--$0.26$. CELM integrates pretrained EEG foundation models with language models to enable scalable multimodal learning. We release our model and benchmark construction pipeline at [URL].

</details>


### [4] [FedAdaVR: Adaptive Variance Reduction for Robust Federated Learning under Limited Client Participation](https://arxiv.org/abs/2601.22204)
*S M Ruhul Kabir Howlader,Xiao Chen,Yifei Xie,Lu Liu*

Main category: cs.LG

TL;DR: FedAdaVR是一种新颖的联邦学习算法，通过自适应优化器和方差缩减技术解决客户端异构性问题，特别是部分客户端参与问题，其量化版本FedAdaVR-Quant能显著减少内存需求。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临异构性挑战，导致梯度噪声、客户端漂移和部分客户端参与误差，其中部分客户端参与问题最为普遍但现有研究解决不足。

Method: 提出FedAdaVR算法，结合自适应优化器和方差缩减技术，利用最近存储的客户端更新来模拟缺席客户端的参与；进一步提出FedAdaVR-Quant，通过量化存储客户端更新来减少内存需求。

Result: FedAdaVR-Quant能将内存需求减少50%、75%和87.5%的同时保持同等模型性能；在非凸条件下证明了收敛性，并能消除部分客户端参与误差；在多个数据集上的实验表明，FedAdaVR在IID和非IID设置下均优于现有基线方法。

Conclusion: FedAdaVR有效解决了联邦学习中的异构性问题，特别是部分客户端参与问题，其量化版本在保持性能的同时显著降低了内存需求，为实际部署提供了可行性。

Abstract: Federated learning (FL) encounters substantial challenges due to heterogeneity, leading to gradient noise, client drift, and partial client participation errors, the last of which is the most pervasive but remains insufficiently addressed in current literature. In this paper, we propose FedAdaVR, a novel FL algorithm aimed at solving heterogeneity issues caused by sporadic client participation by incorporating an adaptive optimiser with a variance reduction technique. This method takes advantage of the most recent stored updates from clients, even when they are absent from the current training round, thereby emulating their presence. Furthermore, we propose FedAdaVR-Quant, which stores client updates in quantised form, significantly reducing the memory requirements (by 50%, 75%, and 87.5%) of FedAdaVR while maintaining equivalent model performance. We analyse the convergence behaviour of FedAdaVR under general nonconvex conditions and prove that our proposed algorithm can eliminate partial client participation error. Extensive experiments conducted on multiple datasets, under both independent and identically distributed (IID) and non-IID settings, demonstrate that FedAdaVR consistently outperforms state-of-the-art baseline methods.

</details>


### [5] [DAJ: Data-Reweighted LLM Judge for Test-Time Scaling in Code Generation](https://arxiv.org/abs/2601.22230)
*Peijia Qin,Ruiyi Zhang,Qi Cao,Pengtao Xie*

Main category: cs.LG

TL;DR: DAJ：基于推理的LLM评判器，采用双层数据重加权学习框架，通过可验证奖励训练，优化代码生成中的测试时扩展性能


<details>
  <summary>Details</summary>
Motivation: 当前代码生成中的测试时扩展主要依赖Best-of-N选择，但训练可靠的LLM评判器面临严重分布偏移挑战，包括简单与困难问题不平衡、训练任务与评估基准不匹配、以及训练数据与推理模型行为差异等问题

Method: 提出DAJ方法，采用基于推理的LLM评判器，在双层数据重加权学习框架下使用可验证奖励进行训练。该框架学习数据重要性权重（域级或实例级），以优化与目标基准对齐的保留元集上的泛化性能

Result: DAJ在LiveCodeBench和BigCodeBench上实现了最先进的性能，超越了强大的测试时扩展基线以及领先的专有模型

Conclusion: 这是首次将数据重加权应用于LLM-as-a-Judge训练以进行测试时扩展，该方法能自动强调困难问题、分布内样本和轨迹对齐数据，无需依赖手工启发式方法

Abstract: Test-time scaling for code generation commonly relies on Best-of-N selection, in which multiple candidate solutions are sampled from a base model, and the best one is selected by an LLM judge. However, training reliable LLM judges is challenging due to severe distribution shifts, including imbalances between easy and hard problems, mismatches between training tasks and evaluation benchmarks, and trajectory mismatch arising from training data generated by cheaper models whose behavior differs from that of inference-time models. We propose DAJ, a reasoning-based LLM judge trained with verifiable rewards under a bi-level data-reweighted learning framework. The proposed framework learns data-importance weights (either domain-level or instance-level) to optimize generalization performance on a held-out meta set aligned with target benchmarks. To the best of our knowledge, this is the first application of data reweighting to LLM-as-a-Judge training for test-time scaling. Our approach automatically emphasizes hard problems, in-distribution samples, and trajectory-aligned data, without relying on hand-crafted heuristics. Empirically, DAJ achieves state-of-the-art performance on LiveCodeBench and BigCodeBench, outperforming strong test-time scaling baselines as well as leading proprietary models.

</details>


### [6] [FunPRM: Function-as-Step Process Reward Model with Meta Reward Correction for Code Generation](https://arxiv.org/abs/2601.22249)
*Ruiyi Zhang,Peijia Qin,Qi Cao,Eric Xue,Pengtao Xie*

Main category: cs.LG

TL;DR: FunPRM是一种针对代码生成的测试时扩展方法，通过函数化模块化代码生成和元学习奖励校正机制，显著提升大语言模型在复杂编程任务上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在复杂代码生成任务上仍经常失败，而基于过程奖励模型的测试时扩展方法在数学推理中表现良好但在代码生成中效果有限，主要原因是代码缺乏有意义的步骤分解和蒙特卡洛估计的部分解决方案正确性分数存在噪声。

Method: FunPRM包含两个核心创新：1）通过提示大语言模型生成模块化的函数化代码，将函数作为过程奖励模型的推理步骤；2）引入基于元学习的奖励校正机制，利用单元测试评估系统获得的干净最终解决方案奖励来净化噪声的部分解决方案奖励。

Result: 在LiveCodeBench和BigCodeBench上的实验表明，FunPRM在五个基础大语言模型上始终优于现有的测试时扩展方法，特别是与O4-mini结合时在LiveCodeBench上实现了最先进的性能。此外，FunPRM生成的代码对开发者来说更具可读性和可重用性。

Conclusion: FunPRM通过函数化模块化代码生成和元学习奖励校正，有效解决了代码生成中过程奖励模型的两个关键挑战，显著提升了代码生成质量，为开发人员提供了更实用、可维护的代码。

Abstract: Code generation is a core application of large language models (LLMs), yet LLMs still frequently fail on complex programming tasks. Given its success in mathematical reasoning, test-time scaling approaches such as Process Reward Model (PRM)-based Best-of-N selection offer a promising way to improve performance. However, existing PRMs remain ineffective for code generation due to the lack of meaningful step decomposition in code and the noise of Monte Carlo-estimated partial-solution correctness scores (rewards). To address these challenges, we propose FunPRM. FunPRM prompts LLMs to encourage modular code generation organized into functions, with functions treated as PRM reasoning steps. Furthermore, FunPRM introduces a novel meta-learning-based reward correction mechanism that leverages clean final-solution rewards obtained via a unit-test-based evaluation system to purify noisy partial-solution rewards. Experiments on LiveCodeBench and BigCodeBench demonstrate that FunPRM consistently outperforms existing test-time scaling methods across five base LLMs, notably achieving state-of-the-art performance on LiveCodeBench when combined with O4-mini. Furthermore, FunPRM produces code that is more readable and reusable for developers.

</details>


### [7] [Symmetry Breaking in Transformers for Efficient and Interpretable Training](https://arxiv.org/abs/2601.22257)
*Eva Silverstein,Daniel Kunin,Vasudev Shyam*

Main category: cs.LG

TL;DR: 论文提出一种打破注意力机制中冗余旋转自由度的对称性协议，通过批量采样的无学习查询和值偏置来改善优化器性能并增强模型可解释性。


<details>
  <summary>Details</summary>
Motivation: 标准注意力机制中存在不影响模型激活或输出的冗余旋转自由度，这些自由度在计算中被保留但没有实际作用。研究者希望利用这些自由度来改善模型性能。

Method: 引入对称性打破协议，通过批量采样的无学习查询和值偏置在旋转空间中插入首选方向，利用原本冗余的旋转自由度。

Result: 1. 显著提升简单内存高效优化器的性能，缩小与复杂自适应方法的差距；2. 实现可解释性，能选择性放大语义上有意义的token类别；3. 在124M参数transformer模型预训练中验证了有效性。

Conclusion: 最小化、有原则的架构改变可以同时提升模型性能和可解释性，为注意力机制设计提供了新思路。

Abstract: The attention mechanism in its standard implementation contains extraneous rotational degrees of freedom that are carried through computation but do not affect model activations or outputs. We introduce a simple symmetry-breaking protocol that inserts a preferred direction into this rotational space through batchwise-sampled, unlearned query and value biases. This modification has two theoretically motivated and empirically validated consequences. First, it can substantially improve the performance of simple, memory-efficient optimizers, narrowing -- and in some cases closing -- the gap to successful but more complex memory-intensive adaptive methods. We demonstrate this by pretraining 124M parameter transformer models with four optimization algorithms (AdamW, SOAP, SGDM, and Energy Conserving Descent(ECD)) and evaluating both validation loss and downstream logical reasoning. Second, it enables an interpretable use of otherwise redundant rotational degrees of freedom, selectively amplifying semantically meaningful token classes within individual attention heads. Overall, our results show that minimal, principled architectural changes can simultaneously improve performance and interpretability.

</details>


### [8] [Tabular Foundation Models Can Do Survival Analysis](https://arxiv.org/abs/2601.22259)
*Da In Kim,Wei Siang Lai,Kelly W. Zhang*

Main category: cs.LG

TL;DR: 将生存分析转化为分类问题，利用表格基础模型通过上下文学习处理右删失数据，无需显式训练


<details>
  <summary>Details</summary>
Motivation: 表格基础模型在分类和回归任务上表现出色，但难以直接应用于生存分析，因为生存数据存在右删失问题（事件可能在观察期结束前未发生）

Method: 开发了一个基于分类的框架，通过离散化事件时间将静态和动态生存分析转化为一系列二分类问题。删失观测被自然地处理为在某些时间点缺少标签的样本。这种分类公式使现有表格基础模型能够通过上下文学习进行生存分析，无需显式训练

Result: 在标准删失假设下，最小化二分类损失能够恢复真实的生存概率。在53个真实世界数据集上的评估表明，使用这种分类公式的现成表格基础模型在多个生存指标上平均优于经典和深度学习基线方法

Conclusion: 该研究提出了一种创新的方法，将生存分析转化为分类问题，使强大的表格基础模型能够有效处理右删失数据，为生存分析提供了新的有效工具

Abstract: While tabular foundation models have achieved remarkable success in classification and regression, adapting them to model time-to-event outcomes for survival analysis is non-trivial due to right-censoring, where data observations may end before the event occurs. We develop a classification-based framework that reformulates both static and dynamic survival analysis as a series of binary classification problems by discretizing event times. Censored observations are naturally handled as examples with missing labels at certain time points. This classification formulation enables existing tabular foundation models to perform survival analysis through in-context learning without explicit training. We prove that under standard censoring assumptions, minimizing our binary classification loss recovers the true survival probabilities as the training set size increases. We demonstrate through evaluation across $53$ real-world datasets that off-the-shelf tabular foundation models with this classification formulation outperform classical and deep learning baselines on average over multiple survival metrics.

</details>


### [9] [Privacy-Preserving Sensor-Based Human Activity Recognition for Low-Resource Healthcare Using Classical Machine Learning](https://arxiv.org/abs/2601.22265)
*Ramakant Kumar,Pravin Kumar*

Main category: cs.LG

TL;DR: 提出基于可穿戴惯性传感器和机器学习的低成本自动化人体活动识别框架，用于远程医疗和老年人护理，支持张量机模型显著优于传统分类器。


<details>
  <summary>Details</summary>
Motivation: 医疗基础设施有限导致老年人和弱势患者依赖家庭护理，常出现忽视和治疗性锻炼（如瑜伽或物理治疗）依从性差的问题。需要低成本自动化解决方案来监测活动。

Method: 使用加速度计和陀螺仪测量收集活动数据（行走、上下楼梯、坐、站、躺）。评估四种经典分类器（逻辑回归、随机森林、SVM、k-NN）并与提出的支持张量机（STM）进行比较。STM利用张量表示保留时空运动动态。

Result: SVM准确率为93.33%，逻辑回归、随机森林和k-NN为91.11%。STM显著优于这些模型，测试准确率达到96.67%，交叉验证准确率最高达98.50%。

Conclusion: 提出的框架在远程医疗、老年人辅助、儿童活动监测、瑜伽反馈和智能家居健康方面具有强大潜力，为低资源和农村医疗环境提供了可扩展的解决方案。

Abstract: Limited access to medical infrastructure forces elderly and vulnerable patients to rely on home-based care, often leading to neglect and poor adherence to therapeutic exercises such as yoga or physiotherapy. To address this gap, we propose a low-cost and automated human activity recognition (HAR) framework based on wearable inertial sensors and machine learning. Activity data, including walking, walking upstairs, walking downstairs, sitting, standing, and lying, were collected using accelerometer and gyroscope measurements. Four classical classifiers, Logistic Regression, Random Forest, Support Vector Machine (SVM), and k-Nearest Neighbors (k-NN), were evaluated and compared with the proposed Support Tensor Machine (STM). Experimental results show that SVM achieved an accuracy of 93.33 percent, while Logistic Regression, Random Forest, and k-NN achieved 91.11 percent. In contrast, STM significantly outperformed these models, achieving a test accuracy of 96.67 percent and the highest cross-validation accuracy of 98.50 percent. Unlike conventional methods, STM leverages tensor representations to preserve spatio-temporal motion dynamics, resulting in robust classification across diverse activities. The proposed framework demonstrates strong potential for remote healthcare, elderly assistance, child activity monitoring, yoga feedback, and smart home wellness, offering a scalable solution for low-resource and rural healthcare settings.

</details>


### [10] [Task-Uniform Convergence and Backward Transfer in Federated Domain-Incremental Learning with Partial Participation](https://arxiv.org/abs/2601.22274)
*Longtao Xu,Jian Li*

Main category: cs.LG

TL;DR: SPECIAL算法为联邦域增量学习提供了一种简单、无内存的方法，通过服务器端锚点控制累积漂移，实现了向后知识转移保证和跨任务收敛率。


<details>
  <summary>Details</summary>
Motivation: 现实世界联邦系统面临动态数据分布漂移和隐私限制，现有联邦域增量学习缺乏向后知识转移的理论保证和跨任务收敛率分析。

Method: 提出SPECIAL算法，在FedAvg基础上添加服务器端"锚点"项，通过轻量级近端项将参与客户端更新推向先前全局模型，无需重放缓冲区或任务特定头。

Result: 理论证明SPECIAL能保持先前任务性能（BKT边界），并获得O((E/NT)^(1/2))的通信效率收敛率，实验验证了其有效性。

Conclusion: SPECIAL为联邦域增量学习提供了理论保证和实用算法，在保持通信和模型大小不变的同时，有效处理数据分布漂移问题。

Abstract: Real-world federated systems seldom operate on static data: input distributions drift while privacy rules forbid raw-data sharing. We study this setting as Federated Domain-Incremental Learning (FDIL), where (i) clients are heterogeneous, (ii) tasks arrive sequentially with shifting domains, yet (iii) the label space remains fixed. Two theoretical pillars remain missing for FDIL under realistic deployment: a guarantee of backward knowledge transfer (BKT) and a convergence rate that holds across the sequence of all tasks with partial participation. We introduce SPECIAL (Server-Proximal Efficient Continual Aggregation for Learning), a simple, memory-free FDIL algorithm that adds a single server-side ``anchor'' to vanilla FedAvg: in each round, the server nudges the uniformly sampled participated clients update toward the previous global model with a lightweight proximal term. This anchor curbs cumulative drift without replay buffers, synthetic data, or task-specific heads, keeping communication and model size unchanged. Our theory shows that SPECIAL (i) preserves earlier tasks: a BKT bound caps any increase in prior-task loss by a drift-controlled term that shrinks with more rounds, local epochs, and participating clients; and (ii) learns efficiently across all tasks: the first communication-efficient non-convex convergence rate for FDIL with partial participation, O((E/NT)^(1/2)), with E local epochs, T communication rounds, and N participated clients per round, matching single-task FedAvg while explicitly separating optimization variance from inter-task drift. Experimental results further demonstrate the effectiveness of SPECIAL.

</details>


### [11] [SurrogateSHAP: Training-Free Contributor Attribution for Text-to-Image (T2I) Models](https://arxiv.org/abs/2601.22276)
*Mingyu Lu,Soham Gadgil,Chris Lin,Chanwoo Kim,Su-In Lee*

Main category: cs.LG

TL;DR: SurrogateSHAP：一个无需重新训练的框架，通过预训练模型推理来近似Shapley值，解决了T2I扩散模型中数据贡献者价值评估的计算瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 随着文本到图像扩散模型在现实创意工作流中的广泛应用，需要建立公平的数据贡献者价值评估框架。虽然Shapley值提供了理论上的归因方法，但面临双重计算瓶颈：1）为每个数据子集重新训练模型的成本过高；2）由于贡献者交互需要评估的组合子集数量巨大。

Method: 提出SurrogateSHAP框架：1）通过预训练模型推理来近似昂贵的重新训练游戏；2）使用梯度提升树近似效用函数，并从树模型中解析推导Shapley值。

Result: 在三个不同的归因任务中评估：DDPM-CFG在CIFAR-20上的图像质量、Stable Diffusion在后印象派艺术作品上的美学质量、FLUX.1在时尚产品数据上的产品多样性。SurrogateSHAP优于现有方法，显著降低计算开销，能一致识别多个效用指标中的有影响力贡献者，并能有效定位临床图像中虚假相关的数据源。

Conclusion: SurrogateSHAP为审计安全关键生成模型提供了可扩展的路径，解决了T2I扩散模型中数据贡献者价值评估的计算挑战，实现了公平补偿和可持续数据市场。

Abstract: As Text-to-Image (T2I) diffusion models are increasingly used in real-world creative workflows, a principled framework for valuing contributors who provide a collection of data is essential for fair compensation and sustainable data marketplaces. While the Shapley value offers a theoretically grounded approach to attribution, it faces a dual computational bottleneck: (i) the prohibitive cost of exhaustive model retraining for each sampled subset of players (i.e., data contributors) and (ii) the combinatorial number of subsets needed to estimate marginal contributions due to contributor interactions. To this end, we propose SurrogateSHAP, a retraining-free framework that approximates the expensive retraining game through inference from a pretrained model. To further improve efficiency, we employ a gradient-boosted tree to approximate the utility function and derive Shapley values analytically from the tree-based model. We evaluate SurrogateSHAP across three diverse attribution tasks: (i) image quality for DDPM-CFG on CIFAR-20, (ii) aesthetics for Stable Diffusion on Post-Impressionist artworks, and (iii) product diversity for FLUX.1 on Fashion-Product data. Across settings, SurrogateSHAP outperforms prior methods while substantially reducing computational overhead, consistently identifying influential contributors across multiple utility metrics. Finally, we demonstrate that SurrogateSHAP effectively localizes data sources responsible for spurious correlations in clinical images, providing a scalable path toward auditing safety-critical generative models.

</details>


### [12] [Riemannian Lyapunov Optimizer: A Unified Framework for Optimization](https://arxiv.org/abs/2601.22284)
*Yixuan Wang,Omkar Sudhir Patil,Warren E. Dixon*

Main category: cs.LG

TL;DR: Riemannian Lyapunov Optimizers (RLOs) 是一个基于控制理论的优化算法框架，将经典优化器统一在几何框架下，通过构造严格Lyapunov函数保证收敛，并在大规模基准测试中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有优化器的改进大多是启发式的，缺乏系统性理论框架。作者希望建立控制理论与机器学习优化之间的桥梁，提供一个统一的语言和系统化工具包来设计稳定有效的优化器。

Method: 将优化问题重新解释为黎曼参数流形上的扩展状态离散时间受控动力系统。核心是识别通常吸引不变流形(NAIM)，将训练动态组织为两个阶段：速度状态快速对齐到目标图，然后在其中受控演化。通过构造严格Lyapunov函数来证明收敛到目标流形，并基于此构建"优化器生成器"。

Result: RLOs不仅能够恢复经典算法，还能原则性地设计新的优化器。通过几何诊断验证理论，并在大规模基准测试中展示了基于控制理论的优化器设计能够达到最先进的性能。

Conclusion: RLOs桥接了控制理论与现代机器学习优化，提供了一个统一的语言和系统化工具包来设计稳定、有效的优化器，为优化器设计提供了新的理论基础。

Abstract: We introduce Riemannian Lyapunov Optimizers (RLOs), a family of optimization algorithms that unifies classic optimizers within one geometric framework. Unlike heuristic improvements to existing optimizers, RLOs are systematically derived from a novel control-theoretic framework that reinterprets optimization as an extended state discrete-time controlled dynamical system on a Riemannian parameter manifold. Central to this framework is the identification of a Normally Attracting Invariant Manifold (NAIM), which organizes training dynamics into two distinct stages: rapid alignment of the speed state to a target graph, followed by controlled evolution within it. We formalize this by constructing a strict Lyapunov function that certifies convergence to a target manifold. This perspective yields a constructive ``optimizer generator" that not only recovers classic algorithms but enables the principled design of RLOs. We validate our theory via geometric diagnostics and demonstrate that grounding optimizer design in control theory yields state-of-the-art performance in large-scale benchmarks. Overall, RLOs bridge control theory and modern machine learning optimization, providing a unified language and a systematic toolkit for designing stable, effective optimizers.

</details>


### [13] [Demystifying Mergeability: Interpretable Properties to Predict Model Merging Success](https://arxiv.org/abs/2601.22285)
*Luca Zhou,Bo Zhao,Rose Yu,Emanuele Rodolà*

Main category: cs.LG

TL;DR: 模型合并的成功因素不仅取决于模型本身，还依赖于合并方法和任务特性，研究发现梯度对齐和子空间重叠是方法无关的兼容性基础条件。


<details>
  <summary>Details</summary>
Motivation: 虽然模型合并能够整合不同微调模型的知识，但其成功因素仍不明确。现有研究将可合并性视为内在属性，但本文旨在揭示合并成功实际上取决于合并方法和任务特性。

Method: 采用架构无关的框架，通过线性优化一组可解释的成对指标（如梯度L2距离），分析四种合并方法，探索与合并后性能相关的属性。

Result: 研究发现成功驱动因素存在显著差异（46.7%指标重叠；55.3%符号一致性），揭示了方法特定的"指纹"。但子空间重叠和梯度对齐指标始终作为方法无关的兼容性基础条件出现。

Conclusion: 这些发现为理解可合并性提供了诊断基础，并激励未来开发能够明确促进这些属性的微调策略。

Abstract: Model merging combines knowledge from separately fine-tuned models, yet success factors remain poorly understood. While recent work treats mergeability as an intrinsic property, we show with an architecture-agnostic framework that it fundamentally depends on both the merging method and the partner tasks. Using linear optimization over a set of interpretable pairwise metrics (e.g., gradient L2 distance), we uncover properties correlating with post-merge performance across four merging methods. We find substantial variation in success drivers (46.7% metric overlap; 55.3% sign agreement), revealing method-specific "fingerprints". Crucially, however, subspace overlap and gradient alignment metrics consistently emerge as foundational, method-agnostic prerequisites for compatibility. These findings provide a diagnostic foundation for understanding mergeability and motivate future fine-tuning strategies that explicitly encourage these properties.

</details>


### [14] [Conformal Prediction for Generative Models via Adaptive Cluster-Based Density Estimation](https://arxiv.org/abs/2601.22298)
*Qidong Yang,Qianyu Julie Zhu,Jonathan Giezendanner,Youssef Marzouk,Stephen Bates,Sherrie Wang*

Main category: cs.LG

TL;DR: CP4Gen：一种针对条件生成模型的新型系统化共形预测方法，通过聚类密度估计构建预测集，在异常值敏感性、可解释性和结构复杂度方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 条件生成模型缺乏校准的不确定性估计，这在高风险应用中削弱了对单个输出的信任，需要一种可靠的不确定性量化方法。

Method: 提出CP4Gen方法，利用模型生成样本的密度估计，采用基于聚类的密度估计技术构建预测集，减少对异常值的敏感性。

Result: 在合成数据集和实际应用（包括气候模拟任务）上的广泛实验表明，CP4Gen在预测集体积和结构简洁性方面始终优于现有方法。

Conclusion: CP4Gen为条件生成模型提供了强大的不确定性估计工具，特别适用于需要严格且可解释预测集的场景。

Abstract: Conditional generative models map input variables to complex, high-dimensional distributions, enabling realistic sample generation in a diverse set of domains. A critical challenge with these models is the absence of calibrated uncertainty, which undermines trust in individual outputs for high-stakes applications. To address this issue, we propose a systematic conformal prediction approach tailored to conditional generative models, leveraging density estimation on model-generated samples. We introduce a novel method called CP4Gen, which utilizes clustering-based density estimation to construct prediction sets that are less sensitive to outliers, more interpretable, and of lower structural complexity than existing methods. Extensive experiments on synthetic datasets and real-world applications, including climate emulation tasks, demonstrate that CP4Gen consistently achieves superior performance in terms of prediction set volume and structural simplicity. Our approach offers practitioners a powerful tool for uncertainty estimation associated with conditional generative models, particularly in scenarios demanding rigorous and interpretable prediction sets.

</details>


### [15] [ZK-HybridFL: Zero-Knowledge Proof-Enhanced Hybrid Ledger for Federated Learning](https://arxiv.org/abs/2601.22302)
*Amirhossein Taherpour,Xiaodong Wang*

Main category: cs.LG

TL;DR: ZK-HybridFL是一个安全的去中心化联邦学习框架，结合DAG账本、侧链和零知识证明，实现隐私保护的模型验证和对抗行为检测。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在保护数据隐私的同时进行协作模型训练，但集中式和去中心化方法都面临可扩展性、安全性和更新验证的挑战。

Method: 提出ZK-HybridFL框架，集成有向无环图（DAG）账本、专用侧链和零知识证明（ZKPs），使用事件驱动智能合约和预言机辅助侧链验证本地模型更新，内置挑战机制检测对抗行为。

Result: 在图像分类和语言建模任务实验中，ZK-HybridFL相比Blade-FL和ChainFL实现了更快的收敛速度、更高的准确率、更低的困惑度和更低的延迟。框架对大量对抗节点和空闲节点保持鲁棒性，支持亚秒级链上验证和高效gas使用，防止无效更新和孤儿式攻击。

Conclusion: ZK-HybridFL为跨不同环境的去中心化联邦学习提供了一个可扩展且安全的解决方案。

Abstract: Federated learning (FL) enables collaborative model training while preserving data privacy, yet both centralized and decentralized approaches face challenges in scalability, security, and update validation. We propose ZK-HybridFL, a secure decentralized FL framework that integrates a directed acyclic graph (DAG) ledger with dedicated sidechains and zero-knowledge proofs (ZKPs) for privacy-preserving model validation. The framework uses event-driven smart contracts and an oracle-assisted sidechain to verify local model updates without exposing sensitive data. A built-in challenge mechanism efficiently detects adversarial behavior. In experiments on image classification and language modeling tasks, ZK-HybridFL achieves faster convergence, higher accuracy, lower perplexity, and reduced latency compared to Blade-FL and ChainFL. It remains robust against substantial fractions of adversarial and idle nodes, supports sub-second on-chain verification with efficient gas usage, and prevents invalid updates and orphanage-style attacks. This makes ZK-HybridFL a scalable and secure solution for decentralized FL across diverse environments.

</details>


### [16] [BayesFlow: A Probability Inference Framework for Meta-Agent Assisted Workflow Generation](https://arxiv.org/abs/2601.22305)
*Bo Yuan,Yun Zhou,Zhichao Xu,Kiran Ramnath,Aosong Feng,Balasubramaniam Srinivasan*

Main category: cs.LG

TL;DR: 本文提出了一种基于贝叶斯推理的工作流生成方法BWG，通过重要性加权和序列优化改进工作流生成效果，在多个基准数据集上显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有自动工作流生成方法大多基于优化问题，缺乏理论基础。本文旨在建立一个有理论保证的贝叶斯推理框架，提升工作流生成的准确性和可靠性。

Method: 提出贝叶斯工作流生成(BWG)框架，将工作流生成建模为贝叶斯后验分布推理。使用并行前瞻回滚进行重要性加权，结合序列循环优化器进行整体改进。具体实现为无需训练的BayesFlow算法。

Result: 在六个基准数据集上，BayesFlow相比最先进的工作流生成基线方法准确率提升高达9个百分点，相比零样本提示提升高达65个百分点。理论证明无优化器时加权经验分布收敛到目标后验。

Conclusion: BWG为基于搜索的工作流设计提供了有理论基础的升级方案，通过贝叶斯推理框架显著提升了工作流生成性能，代码将开源供研究使用。

Abstract: Automatic workflow generation is the process of automatically synthesizing sequences of LLM calls, tool invocations, and post-processing steps for complex end-to-end tasks. Most prior methods cast this task as an optimization problem with limited theoretical grounding. We propose to cast workflow generation as Bayesian inference over a posterior distribution on workflows, and introduce \textbf{Bayesian Workflow Generation (BWG)}, a sampling framework that builds workflows step-by-step using parallel look-ahead rollouts for importance weighting and a sequential in-loop refiner for pool-wide improvements. We prove that, without the refiner, the weighted empirical distribution converges to the target posterior. We instantiate BWG as \textbf{BayesFlow}, a training-free algorithm for workflow construction. Across six benchmark datasets, BayesFlow improves accuracy by up to 9 percentage points over SOTA workflow generation baselines and by up to 65 percentage points over zero-shot prompting, establishing BWG as a principled upgrade to search-based workflow design. Code will be available on https://github.com/BoYuanVisionary/BayesFlow.

</details>


### [17] [Stealthy Poisoning Attacks Bypass Defenses in Regression Settings](https://arxiv.org/abs/2601.22308)
*Javier Carnerero-Cano,Luis Muñoz-González,Phillippa Spencer,Emil C. Lupu*

Main category: cs.LG

TL;DR: 论文提出了一种新的最优隐蔽攻击方法，能够绕过现有防御，并开发了新的评估方法和防御系统BayesClean


<details>
  <summary>Details</summary>
Motivation: 回归模型在工业过程、工程和自然科学中广泛应用，但其对投毒攻击的鲁棒性研究不足，现有研究往往基于不现实的威胁模型，实用性有限

Method: 提出了一种考虑不同可检测程度的最优隐蔽攻击公式；开发了基于目标归一化的新评估方法来权衡攻击效果和可检测性；设计了针对隐蔽攻击的新防御方法BayesClean

Result: 提出的最优隐蔽攻击能够绕过最先进的防御系统；BayesClean在攻击具有隐蔽性且投毒点数量较多时，相比之前的防御方法表现更好

Conclusion: 该研究填补了回归模型投毒攻击鲁棒性研究的空白，提出了更实用的攻击和防御方法，为实际应用中的安全防护提供了新思路

Abstract: Regression models are widely used in industrial processes, engineering and in natural and physical sciences, yet their robustness to poisoning has received less attention. When it has, studies often assume unrealistic threat models and are thus less useful in practice. In this paper, we propose a novel optimal stealthy attack formulation that considers different degrees of detectability and show that it bypasses state-of-the-art defenses. We further propose a new methodology based on normalization of objectives to evaluate different trade-offs between effectiveness and detectability. Finally, we develop a novel defense (BayesClean) against stealthy attacks. BayesClean improves on previous defenses when attacks are stealthy and the number of poisoning points is significant.

</details>


### [18] [SCALAR: Quantifying Structural Hallucination, Consistency, and Reasoning Gaps in Materials Foundation Models](https://arxiv.org/abs/2601.22312)
*Can Polat,Erchin Serpedin,Mustafa Kurban,Hasan Kurban*

Main category: cs.LG

TL;DR: SCALAR是一个用于评估材料基础模型在几何尺度泛化能力的基准测试，包含三个任务：CIF到属性预测、基于物理推理的思维链变体、以及给定目标属性的逆向检索任务。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在材料科学推理中的应用日益增多，但它们在物理结构化分布偏移下的行为仍不清楚。需要评估几何尺度泛化能力及其与结构幻觉、一致性和推理能力的关系。

Method: 引入SCALAR基准，使用从DFT验证的晶胞通过超胞扩展和几何截断得到的纳米颗粒结构，涵盖从几个原子到超过18,000个原子的长度尺度。包含三个任务：CIF到属性预测、基于物理推理的思维链变体、逆向检索。使用结构化指标评估数值误差、幻觉、跨提示一致性、单调推理、输出有效性和检索遗憾。

Result: 实验显示不同基础模型在显式推理下存在显著的模型依赖性偏移，通常减少幻觉和误差，但经常破坏一致性或有效性。几何尺度泛化不能仅从准确性推断。

Conclusion: SCALAR基准揭示了材料基础模型在几何尺度泛化方面的复杂行为，强调了需要结构化评估指标来全面理解模型性能，而不仅仅是准确性。

Abstract: Large language models are increasingly applied to materials science reasoning, yet their behavior under physically structured distribution shifts remains poorly understood. We introduce SCALAR (Structural Consistency And Logic Across Regimes), a benchmark for evaluating geometric scale generalization and its connection to structural hallucination, consistency, and reasoning in materials foundation models. Given canonical crystal representations, models must reason about derived nanoparticle structures obtained through supercell expansion and geometric truncation across length scales spanning a few atoms to over 18,000 atoms, totaling $\approx$100,000 structures from DFT-validated unit cells. SCALAR defines three tasks. (i) CIF to property prediction. (ii) A Chain-of-Thought variant with explicit physics-grounded reasoning. (iii) Inverse retrieval identifying crystals from candidates given target properties. Outputs are evaluated via structured metrics capturing numeric error, hallucination, cross-prompt consistency, monotonic reasoning, output validity, and retrieval regret. Experiments across diverse foundation models reveal large, model-dependent shifts under explicit reasoning, often reducing hallucination and error, but frequently destabilizing consistency or validity. These results demonstrate that geometric scale generalization cannot be inferred from accuracy alone. Supplementary materials are available at https://github.com/KurbanIntelligenceLab/SCALAR.

</details>


### [19] [Hair-Trigger Alignment: Black-Box Evaluation Cannot Guarantee Post-Update Alignment](https://arxiv.org/abs/2601.22313)
*Yavuz Bakman,Duygu Nur Yaldiz,Salman Avestimehr,Sai Praneeth Karimireddy*

Main category: cs.LG

TL;DR: 研究发现静态黑盒评估无法保证大语言模型在更新后的对齐性，理论上证明过参数化模型即使通过静态测试，也可能在单次良性更新后出现严重失准行为，且这种隐藏的对抗行为随模型规模增大而增强。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型频繁更新，但现有对齐研究通常假设初始模型已对齐，仅基于静态黑盒评估（对固定查询集无不良响应）。然而实践中发现，原本"对齐"的模型在微调后可能出现失准行为，如忘记越狱安全特性或重新浮现本应遗忘的知识。这揭示了静态评估的局限性，需要研究模型更新后的对齐保证问题。

Method: 1）理论分析：形式化模型在静态和更新后的对齐定义，证明由于过参数化，静态对齐无法保证更新后的对齐，且静态黑盒探测无法区分真正更新后鲁棒的模型与隐藏任意数量对抗行为的模型；2）实证验证：在大语言模型的三个核心对齐领域（隐私、越狱安全、行为诚实性）进行实验，展示存在通过所有标准黑盒对齐测试但在单次良性更新后严重失准的模型；3）规模分析：验证隐藏对抗行为的能力随模型规模增加而增强。

Result: 理论证明：静态对齐对任何更新数据集都无法保证更新后的对齐；静态黑盒探测无法检测隐藏的对抗行为。实证结果：发现通过所有标准测试的LLM在单次良性更新后出现严重失准；模型隐藏对抗行为的能力随参数数量增加而增强，证实了理论预测的更新后失准随模型规模增长的趋势。

Conclusion: 静态评估协议存在根本性不足，无法保证模型在更新后的对齐性。研究结果强调了迫切需要开发更新后鲁棒的对齐评估方法，特别是考虑到模型规模越大，隐藏对抗行为的能力越强，更新后失准风险越高。

Abstract: Large Language Models (LLMs) are rarely static and are frequently updated in practice. A growing body of alignment research has shown that models initially deemed "aligned" can exhibit misaligned behavior after fine-tuning, such as forgetting jailbreak safety features or re-surfacing knowledge that was intended to be forgotten. These works typically assume that the initial model is aligned based on static black-box evaluation, i.e., the absence of undesired responses to a fixed set of queries. In contrast, we formalize model alignment in both the static and post-update settings and uncover a fundamental limitation of black-box evaluation. We theoretically show that, due to overparameterization, static alignment provides no guarantee of post-update alignment for any update dataset. Moreover, we prove that static black-box probing cannot distinguish a model that is genuinely post-update robust from one that conceals an arbitrary amount of adversarial behavior which can be activated by even a single benign gradient update. We further validate these findings empirically in LLMs across three core alignment domains: privacy, jailbreak safety, and behavioral honesty. We demonstrate the existence of LLMs that pass all standard black-box alignment tests, yet become severely misaligned after a single benign update. Finally, we show that the capacity to hide such latent adversarial behavior increases with model scale, confirming our theoretical prediction that post-update misalignment grows with the number of parameters. Together, our results highlight the inadequacy of static evaluation protocols and emphasize the urgent need for post-update-robust alignment evaluation.

</details>


### [20] [Gaussian Process Bandit Optimization with Machine Learning Predictions and Application to Hypothesis Generation](https://arxiv.org/abs/2601.22315)
*Xin Jennifer Chen,Yunjin Tong*

Main category: cs.LG

TL;DR: PA-GP-UCB：一种利用廉价预测模型和离线数据提升昂贵真值查询样本效率的贝叶斯优化算法


<details>
  <summary>Details</summary>
Motivation: 现实优化问题中常存在昂贵的真值评估（如人工评估、物理实验）和廉价的低保真度预测（如机器学习模型、模拟），同时存在大量离线数据可用于预训练预测模型和提供先验信息。需要开发能同时利用两种评估源和离线数据的方法来提升样本效率。

Method: 提出预测增强高斯过程上置信界算法（PA-GP-UCB），使用联合高斯过程后验推导的控制变量估计器来校正预测偏差和减少不确定性，通过离线数据预训练预测模型，并在贝叶斯优化框架中整合廉价预测和昂贵真值查询。

Result: 理论证明PA-GP-UCB保持了GP-UCB的标准遗憾率，同时获得了严格更小的主导常数，该常数由预测质量和离线数据覆盖度明确控制。实证表明在合成基准测试和基于人类行为数据的真实世界假设评估任务中，PA-GP-UCB比Vanilla GP-UCB和朴素预测增强基线收敛更快。

Conclusion: PA-GP-UCB为昂贵反馈下的假设生成提供了一个通用且样本高效的框架，通过有效整合廉价预测、离线数据和昂贵真值查询，显著提升了优化效率。

Abstract: Many real-world optimization problems involve an expensive ground-truth oracle (e.g., human evaluation, physical experiments) and a cheap, low-fidelity prediction oracle (e.g., machine learning models, simulations). Meanwhile, abundant offline data (e.g., past experiments and predictions) are often available and can be used to pretrain powerful predictive models, as well as to provide an informative prior. We propose Prediction-Augmented Gaussian Process Upper Confidence Bound (PA-GP-UCB), a novel Bayesian optimization algorithm that leverages both oracles and offline data to achieve provable gains in sample efficiency for the ground-truth oracle queries. PA-GP-UCB employs a control-variates estimator derived from a joint Gaussian process posterior to correct prediction bias and reduce uncertainty. We prove that PA-GP-UCB preserves the standard regret rate of GP-UCB while achieving a strictly smaller leading constant that is explicitly controlled by prediction quality and offline data coverage. Empirically, PA-GP-UCB converges faster than Vanilla GP-UCB and naive prediction-augmented GP-UCB baselines on synthetic benchmarks and on a real-world hypothesis evaluation task grounded in human behavioral data, where predictions are provided by large language models. These results establish PA-GP-UCB as a general and sample-efficient framework for hypothesis generation under expensive feedback.

</details>


### [21] [Federate the Router: Learning Language Model Routers with Sparse and Decentralized Evaluations](https://arxiv.org/abs/2601.22318)
*Baris Askin,Shivam Patel,Anupam Nayak,Andrea Vigano,Jiin Woo,Gauri Joshi,Carlee Joe-Wong*

Main category: cs.LG

TL;DR: 论文提出了首个用于大语言模型路由的联邦学习框架，解决客户端数据隐私敏感且分布不均的问题，通过联邦协作提升路由策略的质量和成本效益。


<details>
  <summary>Details</summary>
Motivation: 大语言模型作为远程服务被广泛使用，但不同模型在能力和价格上差异很大，需要智能路由来平衡质量和成本。现有路由方法需要集中化的查询-模型评估数据，但这些数据分散在客户端且具有隐私敏感性，无法集中处理。同时，单个客户端训练路由策略效果有限，因为本地数据覆盖范围有限且存在偏差。

Method: 提出了首个用于LLM路由的联邦学习框架，支持参数化的多层感知机路由器和非参数的K-means路由器。该框架能够在异构客户端查询分布和非均匀模型覆盖的情况下，让客户端从本地离线查询-模型评估数据中学习共享的路由策略。

Result: 在两个基准测试中，联邦协作相比客户端本地路由器在准确率-成本边界上表现更好，既通过增加有效模型覆盖，也通过更好的查询泛化能力。理论结果也验证了联邦训练能够减少路由次优性。

Conclusion: 联邦学习框架为LLM路由提供了一种有效的解决方案，能够在保护数据隐私的同时，通过跨客户端协作提升路由策略的性能，解决数据碎片化和隐私限制的问题。

Abstract: Large language models (LLMs) are increasingly accessed as remotely hosted services by edge and enterprise clients that cannot run frontier models locally. Since models vary widely in capability and price, routing queries to models that balance quality and inference cost is essential. Existing router approaches assume access to centralized query-model evaluation data. However, these data are often fragmented across clients, such as end users and organizations, and are privacy-sensitive, which makes centralizing data infeasible. Additionally, per-client router training is ineffective since local evaluation data is limited and covers only a restricted query distribution and a biased subset of model evaluations. We introduce the first federated framework for LLM routing, enabling clients to learn a shared routing policy from local offline query-model evaluation data. Our framework supports both parametric multilayer perceptron router and nonparametric K-means router under heterogeneous client query distributions and non-uniform model coverage. Across two benchmarks, federated collaboration improves the accuracy-cost frontier over client-local routers, both via increased effective model coverage and better query generalization. Our theoretical results also validate that federated training reduces routing suboptimality.

</details>


### [22] [Matrix Factorization for Practical Continual Mean Estimation Under User-Level Differential Privacy](https://arxiv.org/abs/2601.22320)
*Nikita P. Kalinin,Ali Najar,Valentin Roth,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 论文研究持续均值估计问题，在用户级差分隐私下采用近似差分隐私方法，提出新的矩阵分解机制，相比纯差分隐私显著降低误差。


<details>
  <summary>Details</summary>
Motivation: 现有持续均值估计研究主要关注纯差分隐私，但这种方法会产生过多噪声，限制了实际应用。需要研究在近似差分隐私下的解决方案，以平衡隐私保护和估计准确性。

Method: 采用近似差分隐私框架，基于矩阵分解机制的最新进展，提出了一种专门针对均值估计的新型矩阵分解方法，该方法既高效又准确。

Result: 提出的方法在用户级差分隐私下的持续均值估计中，实现了渐进更低的均方误差界限，显著优于纯差分隐私方法。

Conclusion: 近似差分隐私框架下的新型矩阵分解机制为持续均值估计提供了更实用的解决方案，在保护用户隐私的同时显著提高了估计精度。

Abstract: We study continual mean estimation, where data vectors arrive sequentially and the goal is to maintain accurate estimates of the running mean. We address this problem under user-level differential privacy, which protects each user's entire dataset even when they contribute multiple data points. Previous work on this problem has focused on pure differential privacy. While important, this approach limits applicability, as it leads to overly noisy estimates. In contrast, we analyze the problem under approximate differential privacy, adopting recent advances in the Matrix Factorization mechanism. We introduce a novel mean estimation specific factorization, which is both efficient and accurate, achieving asymptotically lower mean-squared error bounds in continual mean estimation under user-level differential privacy.

</details>


### [23] [Models Under SCOPE: Scalable and Controllable Routing via Pre-hoc Reasoning](https://arxiv.org/abs/2601.22323)
*Qi Cao,Shuhao Zhang,Ruizhe Zhou,Ruiyi Zhang,Peijia Qin,Pengtao Xie*

Main category: cs.LG

TL;DR: SCOPE是一个可扩展可控的路由框架，通过预测模型成本和性能实现智能路由，能适应新模型和动态预算约束，在准确性和成本间灵活权衡。


<details>
  <summary>Details</summary>
Motivation: 现有模型路由方法通常将路由视为在固定小模型集合中的选择，难以适应新模型或变化的预算约束，缺乏灵活性和可扩展性。

Method: 提出SCOPE框架，通过强化学习训练，基于检索类似问题上的模型行为进行推理预测，而非依赖固定模型名称，能预测模型的准确性和成本，将路由转化为动态决策问题。

Result: SCOPE不仅能节省成本，还能灵活适应用户需求：当性能优先时，准确率可提升25.7%；当效率优先时，成本可降低95.1%。

Conclusion: SCOPE超越了传统模型选择，通过预测成本和性能实现智能路由，提供了灵活的可控性，能适应新模型和动态预算约束，在准确性和成本间实现有效权衡。

Abstract: Model routing chooses which language model to use for each query. By sending easy queries to cheaper models and hard queries to stronger ones, it can significantly reduce inference cost while maintaining high accuracy. However, most existing routers treat this as a fixed choice among a small set of models, which makes them hard to adapt to new models or changing budget constraints. In this paper, we propose SCOPE (Scalable and Controllable Outcome Performance Estimator), a routing framework that goes beyond model selection by predicting their cost and performance. Trained with reinforcement learning, SCOPE makes reasoning-based predictions by retrieving how models behave on similar problems, rather than relying on fixed model names, enabling it to work with new, unseen models. Moreover, by explicitly predicting how accurate and how expensive a model will be, it turns routing into a dynamic decision problem, allowing users to easily control the trade-off between accuracy and cost. Experiments show that SCOPE is more than just a cost-saving tool. It flexibly adapts to user needs: it can boost accuracy by up to 25.7% when performance is the priority, or cut costs by up to 95.1% when efficiency matters most.

</details>


### [24] [AgentScore: Autoformulation of Deployable Clinical Scoring Systems](https://arxiv.org/abs/2601.22324)
*Silas Ruhrberg Estévez,Christopher Chiu,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: AgentScore使用LLM生成候选规则，通过确定性验证选择循环创建可部署的临床评分系统，在多个临床预测任务中表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型虽然预测性能强，但往往不符合临床工作流程约束（如可记忆性、可审计性、床边执行），导致难以转化为常规临床使用。问题根源在于模型类别与指南部署要求不兼容。

Method: AgentScore采用语义引导优化方法：使用大型语言模型（LLMs）提出候选决策规则，通过确定性的、基于数据的验证和选择循环来确保统计有效性和可部署性约束，从而在指数级大的离散规则空间中进行搜索。

Result: 在八个临床预测任务中，AgentScore优于现有的评分生成方法，并在更强的结构约束下实现了与更灵活的可解释模型相当的AUC。在两个额外的外部验证任务中，AgentScore比已建立的基于指南的评分具有更高的区分度。

Conclusion: AgentScore通过结合LLM的语义理解和数据驱动的验证，能够生成既符合临床部署约束又具有良好预测性能的评分系统，弥合了机器学习模型与临床实践需求之间的差距。

Abstract: Modern clinical practice relies on evidence-based guidelines implemented as compact scoring systems composed of a small number of interpretable decision rules. While machine-learning models achieve strong performance, many fail to translate into routine clinical use due to misalignment with workflow constraints such as memorability, auditability, and bedside execution. We argue that this gap arises not from insufficient predictive power, but from optimizing over model classes that are incompatible with guideline deployment. Deployable guidelines often take the form of unit-weighted clinical checklists, formed by thresholding the sum of binary rules, but learning such scores requires searching an exponentially large discrete space of possible rule sets. We introduce AgentScore, which performs semantically guided optimization in this space by using LLMs to propose candidate rules and a deterministic, data-grounded verification-and-selection loop to enforce statistical validity and deployability constraints. Across eight clinical prediction tasks, AgentScore outperforms existing score-generation methods and achieves AUC comparable to more flexible interpretable models despite operating under stronger structural constraints. On two additional externally validated tasks, AgentScore achieves higher discrimination than established guideline-based scores.

</details>


### [25] [Knowledge-Informed Kernel State Reconstruction for Interpretable Dynamical System Discovery](https://arxiv.org/abs/2601.22328)
*Luca Muscarnera,Silas Ruhrberg Estévez,Samuel Holt,Evgeny Saveliev,Mihaela van der Schaar*

Main category: cs.LG

TL;DR: MAAT框架通过知识引导的核状态重构，从噪声、不完整观测数据中恢复物理系统控制方程，结合结构先验知识提升符号发现能力


<details>
  <summary>Details</summary>
Motivation: 现有方法在噪声、部分观测条件下容易失效，或依赖黑盒潜在动力学模型而缺乏机制解释性，需要一种能处理不完整数据并保持物理一致性的符号发现框架

Method: MAAT框架基于知识引导的核状态重构，在再生核希尔伯特空间中构建状态重构，直接融入结构先验（如非负性、守恒律、领域特定观测模型），处理异构采样和测量粒度，生成平滑、物理一致的状态估计和解析时间导数

Result: 在12个不同科学基准测试和多种噪声机制下，MAAT显著降低了状态估计的均方误差，为下游符号回归提供了更准确的轨迹和导数数据

Conclusion: MAAT为碎片化传感器数据与符号回归之间提供了原则性接口，通过知识引导的状态重构提升了从噪声、不完整观测中恢复控制方程的能力

Abstract: Recovering governing equations from data is central to scientific discovery, yet existing methods often break down under noisy, partial observations, or rely on black-box latent dynamics that obscure mechanism. We introduce MAAT (Model Aware Approximation of Trajectories), a framework for symbolic discovery built on knowledge-informed Kernel State Reconstruction. MAAT formulates state reconstruction in a reproducing kernel Hilbert space and directly incorporates structural and semantic priors such as non-negativity, conservation laws, and domain-specific observation models into the reconstruction objective, while accommodating heterogeneous sampling and measurement granularity. This yields smooth, physically consistent state estimates with analytic time derivatives, providing a principled interface between fragmented sensor data and symbolic regression. Across twelve diverse scientific benchmarks and multiple noise regimes, MAAT substantially reduces state-estimation MSE for trajectories and derivatives used by downstream symbolic regression relative to strong baselines.

</details>


### [26] [Scalable Batch Correction for Cell Painting via Batch-Dependent Kernels and Adaptive Sampling](https://arxiv.org/abs/2601.22331)
*Aditya Narayan Ravi,Snehal Vadvalkar,Abhishek Pandey,Ilan Shomorony*

Main category: cs.LG

TL;DR: BALANS是一种可扩展的批次校正方法，通过局部亲和度和子采样来对齐跨批次样本，适用于大规模细胞绘画数据分析。


<details>
  <summary>Details</summary>
Motivation: 细胞绘画数据在大规模分析中受到实验室、仪器和协议差异引起的批次效应影响，这会掩盖生物信号，需要可扩展的批次校正方法。

Method: BALANS通过构建平滑的亲和度矩阵来对齐批次样本：1) 使用批次感知的局部尺度计算高斯核亲和度；2) 采用自适应采样策略优先选择邻居覆盖度低的行，仅保留每行最强的亲和度，形成稀疏但信息丰富的近似矩阵。

Result: BALANS在样本复杂度方面达到最优阶，提供近似保证，运行时间接近线性。在真实细胞绘画数据集和合成基准测试中，BALANS能够扩展到大规模数据集，在保持校正质量的同时提高运行速度。

Conclusion: BALANS是一种高效、可扩展的批次校正方法，能够处理大规模细胞绘画数据中的批次效应问题，为药物发现提供可靠的数据分析支持。

Abstract: Cell Painting is a microscopy-based, high-content imaging assay that produces rich morphological profiles of cells and can support drug discovery by quantifying cellular responses to chemical perturbations. At scale, however, Cell Painting data is strongly affected by batch effects arising from differences in laboratories, instruments, and protocols, which can obscure biological signal. We present BALANS (Batch Alignment via Local Affinities and Subsampling), a scalable batch-correction method that aligns samples across batches by constructing a smoothed affinity matrix from pairwise distances. Given $n$ data points, BALANS builds a sparse affinity matrix $A \in \mathbb{R}^{n \times n}$ using two ideas. (i) For points $i$ and $j$, it sets a local scale using the distance from $i$ to its $k$-th nearest neighbor within the batch of $j$, then computes $A_{ij}$ via a Gaussian kernel calibrated by these batch-aware local scales. (ii) Rather than forming all $n^2$ entries, BALANS uses an adaptive sampling procedure that prioritizes rows with low cumulative neighbor coverage and retains only the strongest affinities per row, yielding a sparse but informative approximation of $A$. We prove that this sampling strategy is order-optimal in sample complexity and provides an approximation guarantee, and we show that BALANS runs in nearly linear time in $n$. Experiments on diverse real-world Cell Painting datasets and controlled large-scale synthetic benchmarks demonstrate that BALANS scales to large collections while improving runtime over native implementations of widely used batch-correction methods, without sacrificing correction quality.

</details>


### [27] [DP-$λ$CGD: Efficient Noise Correlation for Differentially Private Model Training](https://arxiv.org/abs/2601.22334)
*Nikita P. Kalinin,Ryan McKenna,Rasmus Pagh,Christoph H. Lampert*

Main category: cs.LG

TL;DR: 提出一种新的DP-SGD噪声相关策略，仅与前一次迭代相关并抵消部分噪声，使用伪随机噪声生成器无需存储历史噪声，内存开销与标准DP-SGD相同


<details>
  <summary>Details</summary>
Motivation: 现有DP-SGD扩展方法（如矩阵分解机制）通过跨多个训练迭代引入相关噪声来提高准确性，但需要存储先前添加的噪声向量，导致显著的内存开销

Method: 提出新的噪声相关策略：1）噪声仅与前一迭代相关；2）抵消受控部分的噪声；3）使用伪随机噪声生成器重新生成噪声，无需存储历史噪声

Result: 方法无需额外内存（与标准DP-SGD相同），计算开销最小，实证显示比DP-SGD准确性更高

Conclusion: 提出了一种内存高效的DP-SGD改进方法，通过有限的相关噪声策略在保持隐私保护的同时提高模型准确性

Abstract: Differentially private stochastic gradient descent (DP-SGD) is the gold standard for training machine learning models with formal differential privacy guarantees. Several recent extensions improve its accuracy by introducing correlated noise across training iterations. Matrix factorization mechanisms are a prominent example, but they correlate noise across many iterations and require storing previously added noise vectors, leading to substantial memory overhead in some settings. In this work, we propose a new noise correlation strategy that correlates noise only with the immediately preceding iteration and cancels a controlled portion of it. Our method relies on noise regeneration using a pseudorandom noise generator, eliminating the need to store past noise. As a result, it requires no additional memory beyond standard DP-SGD. We show that the computational overhead is minimal and empirically demonstrate improved accuracy over DP-SGD.

</details>


### [28] [Knowledge Gradient for Preference Learning](https://arxiv.org/abs/2601.22335)
*Kaiwen Wu,Jacob R. Gardner*

Main category: cs.LG

TL;DR: 本文提出了针对偏好贝叶斯优化的精确知识梯度方法，解决了传统方法在计算非高斯后验时的计算挑战，并在基准测试中表现出色。


<details>
  <summary>Details</summary>
Motivation: 许多实际场景只允许成对比较查询（偏好贝叶斯优化问题），而无法直接评估函数值。将知识梯度扩展到偏好贝叶斯优化面临计算挑战，因为前瞻步骤需要计算非高斯后验，之前被认为难以处理。

Method: 推导出精确且解析的偏好贝叶斯优化知识梯度方法，解决了非高斯后验的计算难题。

Result: 精确知识梯度在一系列基准问题上表现强劲，通常优于现有的采集函数。同时，研究也展示了知识梯度在某些场景下的局限性。

Conclusion: 成功解决了偏好贝叶斯优化中知识梯度的计算挑战，提出了有效的精确方法，同时指出了该方法在某些情况下的局限性。

Abstract: The knowledge gradient is a popular acquisition function in Bayesian optimization (BO) for optimizing black-box objectives with noisy function evaluations. Many practical settings, however, allow only pairwise comparison queries, yielding a preferential BO problem where direct function evaluations are unavailable. Extending the knowledge gradient to preferential BO is hindered by its computational challenge. At its core, the look-ahead step in the preferential setting requires computing a non-Gaussian posterior, which was previously considered intractable. In this paper, we address this challenge by deriving an exact and analytical knowledge gradient for preferential BO. We show that the exact knowledge gradient performs strongly on a suite of benchmark problems, often outperforming existing acquisition functions. In addition, we also present a case study illustrating the limitation of the knowledge gradient in certain scenarios.

</details>


### [29] [Failing to Explore: Language Models on Interactive Tasks](https://arxiv.org/abs/2601.22345)
*Mahdi JafariRaviz,Keivan Rezaei,Arshia Soltani Moakhar,Zahra Sodagar,Yize Cheng,Soheil Feizi*

Main category: cs.LG

TL;DR: 该论文评估了语言模型在有限交互预算下探索交互环境的能力，发现当前最先进模型存在系统性探索不足和次优解问题，性能甚至不如简单的探索-利用启发式基线方法。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型在有限交互预算下探索交互环境的能力，了解当前模型在探索任务中的表现和局限性。

Method: 引入了三个参数可调的探索难度任务（涵盖连续和离散环境），评估了最先进的语言模型，并与简单的探索-利用启发式基线进行比较。研究了两种轻量级干预措施：将固定预算分割为并行执行，以及定期总结交互历史。

Result: 发现当前最先进模型存在系统性探索不足和次优解问题，性能往往显著低于简单基线方法，且随着预算增加性能提升有限。令人惊讶的是，将预算分割为并行执行能提高性能（尽管理论分析显示不应有增益），而定期总结交互历史能保留关键发现并进一步改善探索。

Conclusion: 当前语言模型在探索交互环境方面存在显著局限性，需要开发更好的探索策略。轻量级干预措施如并行执行和定期总结能有效改善模型探索能力，为未来研究提供了方向。

Abstract: We evaluate language models on their ability to explore interactive environments under a limited interaction budget. We introduce three parametric tasks with controllable exploration difficulty, spanning continuous and discrete environments. Across state-of-the-art models, we find systematic under-exploration and suboptimal solutions, with performance often significantly worse than simple explore--exploit heuristic baselines and scaling weakly as the budget increases. Finally, we study two lightweight interventions: splitting a fixed budget into parallel executions, which surprisingly improves performance despite a no-gain theoretical result for our tasks, and periodically summarizing the interaction history, which preserves key discoveries and further improves exploration.

</details>


### [30] [MixQuant: Pushing the Limits of Block Rotations in Post-Training Quantization](https://arxiv.org/abs/2601.22347)
*Sai Sanjeet,Ian Colbert,Pablo Monteagudo-Lago,Giuseppe Franco,Yaman Umuroglu,Nicholas J. Fraser*

Main category: cs.LG

TL;DR: MixQuant：一种基于块旋转感知的后训练量化框架，通过排列重新分布激活质量，改善异常值抑制，在INT4量化中显著提升精度


<details>
  <summary>Details</summary>
Motivation: 现有后训练量化方法使用块旋转来扩散异常值，但块结构对异常值抑制的影响尚不清楚。需要系统分析块Hadamard旋转的异常值抑制机制，并开发更有效的量化方法。

Method: 1. 对块Hadamard旋转进行非渐近分析，发现异常值抑制受输入向量几何结构限制；2. 提出MixQuant框架，通过排列重新分布激活质量；3. 设计贪心质量扩散算法校准排列；4. 识别transformer中的排列等变区域，将排列合并到模型权重中以避免推理开销。

Result: MixQuant在所有块大小上一致提高精度，在Llama3 1B的INT4量化中，块大小为16时恢复了90%的全向量旋转困惑度（相比无排列的46%）。

Conclusion: 块旋转的异常值抑制效果受输入向量几何结构限制，通过排列重新分布激活质量可以显著改善量化性能。MixQuant框架在不增加推理开销的情况下，有效提升了低精度量化的准确性。

Abstract: Recent post-training quantization (PTQ) methods have adopted block rotations to diffuse outliers prior to rounding. While this reduces the overhead of full-vector rotations, the effect of block structure on outlier suppression remains poorly understood. To fill this gap, we present the first systematic, non-asymptotic analysis of outlier suppression for block Hadamard rotations. Our analysis reveals that outlier suppression is fundamentally limited by the geometry of the input vector. In particular, post-rotation outliers are deterministically minimized when the pre-rotation $\ell_1$ norm mass is evenly distributed across blocks. Guided by these insights, we introduce MixQuant, a block rotation-aware PTQ framework that redistributes activation mass via permutations prior to rotation. We propose a greedy mass diffusion algorithm to calibrate permutations by equalizing the expected blockwise $\ell_1$ norms. To avoid adding inference overhead, we identify permutation-equivariant regions in transformer architectures to merge the resulting permutations into model weights before deployment. Experiments show that MixQuant consistently improves accuracy across all block sizes, recovering up to 90% of the full-vector rotation perplexity when quantizing Llama3 1B to INT4 with block size 16, compared to 46% without permutations.

</details>


### [31] [Learning Policy Representations for Steerable Behavior Synthesis](https://arxiv.org/abs/2601.22350)
*Beiming Li,Sergio Rozada,Alejandro Ribeiro*

Main category: cs.LG

TL;DR: 提出一种基于集合架构的MDP策略表示学习方法，通过变分生成和对比学习构建平滑潜在空间，支持基于梯度的策略优化和行为引导


<details>
  <summary>Details</summary>
Motivation: 为了在测试时方便地进行行为引导，需要学习一系列策略的表示。由于MDP策略由其占用测度唯一确定，因此考虑将策略表示建模为状态-动作特征映射相对于占用测度的期望

Method: 使用基于集合的架构近似策略表示，将状态-动作样本集编码为潜在嵌入，从中解码策略及其对应多个奖励的价值函数。采用变分生成方法构建平滑潜在空间，并通过对比学习使其几何结构与价值函数差异对齐

Result: 构建的潜在空间允许基于梯度的优化，能够解决新颖的行为合成任务：在无需额外训练的情况下，引导策略满足先前未见过的价值函数约束

Conclusion: 提出的方法能够学习可解释的策略表示，支持在潜在空间中进行梯度优化，实现了对未见约束的行为引导能力

Abstract: Given a Markov decision process (MDP), we seek to learn representations for a range of policies to facilitate behavior steering at test time. As policies of an MDP are uniquely determined by their occupancy measures, we propose modeling policy representations as expectations of state-action feature maps with respect to occupancy measures. We show that these representations can be approximated uniformly for a range of policies using a set-based architecture. Our model encodes a set of state-action samples into a latent embedding, from which we decode both the policy and its value functions corresponding to multiple rewards. We use variational generative approach to induce a smooth latent space, and further shape it with contrastive learning so that latent distances align with differences in value functions. This geometry permits gradient-based optimization directly in the latent space. Leveraging this capability, we solve a novel behavior synthesis task, where policies are steered to satisfy previously unseen value function constraints without additional training.

</details>


### [32] [Relative Wasserstein Angle and the Problem of the $W_2$-Nearest Gaussian Distribution](https://arxiv.org/abs/2601.22355)
*Binshuai Wang,Peng Wei*

Main category: cs.LG

TL;DR: 该论文提出基于最优传输框架的几何方法量化经验分布与高斯分布之间的偏差，引入相对Wasserstein角和正交投影距离作为非高斯性度量，并证明在高维情况下矩匹配高斯并非W_2最近邻高斯。


<details>
  <summary>Details</summary>
Motivation: 现有方法在量化经验分布与高斯分布偏差时存在局限性，需要更稳健的几何度量来评估非高斯性，特别是在高维数据中。

Method: 利用相对平移不变二次Wasserstein空间的锥几何结构，引入相对Wasserstein角和正交投影距离；在一维情况下推导闭式表达式，在高维情况下开发基于半离散对偶公式的随机流形优化算法。

Result: 证明该空间中任意两条射线生成的填充锥是平坦的，确保角度、投影和内积严格定义；实验表明相对Wasserstein角比Wasserstein距离更稳健，提出的最近高斯在FID评分评估中优于矩匹配方法。

Conclusion: 该几何视角将高斯近似重构为高斯锥上的投影问题，揭示了矩匹配高斯并非W_2最近邻高斯，为量化非高斯性提供了更稳健的几何框架。

Abstract: We study the problem of quantifying how far an empirical distribution deviates from Gaussianity under the framework of optimal transport. By exploiting the cone geometry of the relative translation invariant quadratic Wasserstein space, we introduce two novel geometric quantities, the relative Wasserstein angle and the orthogonal projection distance, which provide meaningful measures of non-Gaussianity. We prove that the filling cone generated by any two rays in this space is flat, ensuring that angles, projections, and inner products are rigorously well-defined. This geometric viewpoint recasts Gaussian approximation as a projection problem onto the Gaussian cone and reveals that the commonly used moment-matching Gaussian can \emph{not} be the \(W_2\)-nearest Gaussian for a given empirical distribution. In one dimension, we derive closed-form expressions for the proposed quantities and extend them to several classical distribution families, including uniform, Laplace, and logistic distributions; while in high dimensions, we develop an efficient stochastic manifold optimization algorithm based on a semi-discrete dual formulation. Experiments on synthetic data and real-world feature distributions demonstrate that the relative Wasserstein angle is more robust than the Wasserstein distance and that the proposed nearest Gaussian provides a better approximation than moment matching in the evaluation of Fréchet Inception Distance (FID) scores.

</details>


### [33] [PoSafeNet: Safe Learning with Poset-Structured Neural Nets](https://arxiv.org/abs/2601.22356)
*Kiwan Wong,Wei Xiao,Daniela Rus*

Main category: cs.LG

TL;DR: PoSafeNet：一种基于偏序集结构安全约束的神经安全层，通过顺序闭式投影自适应执行安全策略，提高可行性和鲁棒性


<details>
  <summary>Details</summary>
Motivation: 现有安全学习方法通常对多个安全约束采用统一或固定优先级执行，导致不可行性和脆弱行为。实际应用中安全需求具有异质性，只存在部分优先级关系，有些约束可比而有些不可比。

Method: 将安全约束形式化为偏序集结构，将安全组合视为策略类的结构属性。提出PoSafeNet可微分神经安全层，通过偏序一致约束排序下的顺序闭式投影强制执行安全，自适应选择或混合有效安全执行，同时保持优先级语义。

Result: 在多障碍物导航、受限机器人操作和视觉自动驾驶实验中，相比非结构化和基于可微分二次规划的安全层，显示出更好的可行性、鲁棒性和可扩展性。

Conclusion: 偏序集结构安全建模能更好地处理异质安全约束，PoSafeNet通过自适应安全执行机制在实际机器人系统中提供了更可行和鲁棒的安全保障。

Abstract: Safe learning is essential for deploying learningbased controllers in safety-critical robotic systems, yet existing approaches often enforce multiple safety constraints uniformly or via fixed priority orders, leading to infeasibility and brittle behavior. In practice, safety requirements are heterogeneous and admit only partial priority relations, where some constraints are comparable while others are inherently incomparable. We formalize this setting as poset-structured safety, modeling safety constraints as a partially ordered set and treating safety composition as a structural property of the policy class. Building on this formulation, we propose PoSafeNet, a differentiable neural safety layer that enforces safety via sequential closed-form projection under poset-consistent constraint orderings, enabling adaptive selection or mixing of valid safety executions while preserving priority semantics by construction. Experiments on multi-obstacle navigation, constrained robot manipulation, and vision-based autonomous driving demonstrate improved feasibility, robustness, and scalability over unstructured and differentiable quadratic program-based safety layers.

</details>


### [34] [Understanding Efficiency: Quantization, Batching, and Serving Strategies in LLM Energy Use](https://arxiv.org/abs/2601.22362)
*Julien Delavande,Regis Pierrard,Sasha Luccioni*

Main category: cs.LG

TL;DR: 本文通过实证研究发现，大语言模型推理的能耗不仅取决于模型本身，还受到系统级设计选择（如数值精度、批处理策略、请求调度）的显著影响，这些因素可导致能耗数量级差异。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型在生产环境中的部署增加，计算资源和能源需求从训练转向推理。现有研究主要关注每个提示或每个令牌的推理能耗，但系统级设计选择对能耗的影响尚未得到充分研究。

Method: 在NVIDIA H100 GPU上进行详细的LLM推理能耗和延迟实证研究，分析量化、批处理大小和服务配置（如Hugging Face的文本生成推理服务器）的影响。

Result: 研究发现：1）低精度格式仅在计算受限场景下带来能耗收益；2）批处理提高能效，尤其在解码等内存受限阶段；3）结构化请求时序（到达整形）可将每个请求的能耗降低高达100倍。

Conclusion: 可持续的LLM部署不仅取决于模型内部设计，还依赖于服务堆栈的编排。研究结果支持基于阶段的能耗分析和系统级优化，以实现更环保的AI服务。

Abstract: Large Language Models (LLMs) are increasingly deployed in production, contributing towards shifting the burden in terms of computational resources and energy demands from training to inference. While prior work has examined the energy cost of inference per prompt or per token, we highlight how \emph{system-level design choices} - such as numerical precision, batching strategy, and request scheduling - can lead to orders-of-magnitude differences in energy consumption for the same model. We perform a detailed empirical study of LLM inference energy and latency on NVIDIA H100 GPUs, analyzing the impact of quantization, batch size, and serving configuration (e.g., with Hugging Face's Text Generation Inference server). Our results reveal that lower-precision formats only yield energy gains in compute-bound regimes; that batching improves energy efficiency, especially in memory-bound phases like decoding; and that structured request timing (arrival shaping) can reduce per-request energy by up to 100 times. We argue that sustainable LLM deployment depends not only on model internals, but also on the orchestration of the serving stack. Our findings motivate phase-aware energy profiling and system-level optimizations for greener AI services.

</details>


### [35] [FIRE: Multi-fidelity Regression with Distribution-conditioned In-context Learning using Tabular Foundation Models](https://arxiv.org/abs/2601.22371)
*Rosen Ting-Ying Yu,Nicholas Sung,Faez Ahmed*

Main category: cs.LG

TL;DR: FIRE是一个免训练的多保真度回归框架，使用表格基础模型进行零样本上下文贝叶斯推断，通过高低保真模型的后验预测分布实现跨保真度信息传递，在31个基准问题上优于7种最先进方法。


<details>
  <summary>Details</summary>
Motivation: 传统高斯过程在多保真度回归中存在立方计算复杂度问题，容易对稀疏的高保真度观测过拟合，限制了实际应用中的效率和泛化能力。

Method: FIRE框架将表格基础模型与高保真度校正模型耦合，通过低保真度模型的后验预测分布进行条件化，实现零样本上下文贝叶斯推断，捕捉异方差误差，无需模型重新训练。

Result: 在31个基准问题（包括DrivAerNet、LCBench等合成和真实任务）上，FIRE在性能-时间权衡方面优于7种最先进的GP或深度学习多保真度回归方法，在准确性和不确定性量化方面排名最高，具有运行时优势。

Conclusion: FIRE提供了一个有效的免训练多保真度回归框架，通过跨保真度信息传递实现鲁棒的残差学习，但存在上下文窗口限制和依赖预训练表格基础模型质量的局限性。

Abstract: Multi-fidelity (MF) regression often operates in regimes of extreme data imbalance, where the commonly-used Gaussian-process (GP) surrogates struggle with cubic scaling costs and overfit to sparse high-fidelity observations, limiting efficiency and generalization in real-world applications. We introduce FIRE, a training-free MF framework that couples tabular foundation models (TFMs) to perform zero-shot in-context Bayesian inference via a high-fidelity correction model conditioned on the low-fidelity model's posterior predictive distributions. This cross-fidelity information transfer via distributional summaries captures heteroscedastic errors, enabling robust residual learning without model retraining. Across 31 benchmark problems spanning synthetic and real-world tasks (e.g., DrivAerNet, LCBench), FIRE delivers a stronger performance-time trade-off than seven state-of-the-art GP-based or deep learning MF regression methods, ranking highest in accuracy and uncertainty quantification with runtime advantages. Limitations include context window constraints and dependence on the quality of the pre-trained TFM's.

</details>


### [36] [Graph is a Substrate Across Data Modalities](https://arxiv.org/abs/2601.22384)
*Ziming Li,Xiaoming Wu,Zehong Wang,Jiazheng Li,Yijun Tian,Jinhe Bi,Yunpu Ma,Yanfang Ye,Chuxu Zhang*

Main category: cs.LG

TL;DR: G-Substrate框架将图结构作为跨模态和任务共享的结构基板，通过统一结构模式和角色交错训练策略，实现图表示在不同学习场景中的持续积累和复用。


<details>
  <summary>Details</summary>
Motivation: 当前图结构学习通常以模态和任务孤立的方式进行，图表示在单个任务上下文中构建后即被丢弃，导致跨模态和任务的结构规律需要重复重建，无法在中间图表示层面进行积累。

Method: 提出G-Substrate图基板框架，包含两个互补机制：1）统一结构模式确保跨异构模态和任务的图表示兼容性；2）角色交错训练策略让同一图结构在训练过程中承担多种功能角色。

Result: 在多个领域、模态和任务上的实验表明，G-Substrate优于任务孤立学习和朴素多任务学习方法。

Conclusion: 将图结构视为跨学习上下文持久存在的结构基板，通过G-Substrate框架组织学习，能够有效积累和复用图结构知识，提升跨模态和任务的表示学习效果。

Abstract: Graphs provide a natural representation of relational structure that arises across diverse domains. Despite this ubiquity, graph structure is typically learned in a modality- and task-isolated manner, where graph representations are constructed within individual task contexts and discarded thereafter. As a result, structural regularities across modalities and tasks are repeatedly reconstructed rather than accumulated at the level of intermediate graph representations. This motivates a representation-learning question: how should graph structure be organized so that it can persist and accumulate across heterogeneous modalities and tasks? We adopt a representation-centric perspective in which graph structure is treated as a structural substrate that persists across learning contexts. To instantiate this perspective, we propose G-Substrate, a graph substrate framework that organizes learning around shared graph structures. G-Substrate comprises two complementary mechanisms: a unified structural schema that ensures compatibility among graph representations across heterogeneous modalities and tasks, and an interleaved role-based training strategy that exposes the same graph structure to multiple functional roles during learning. Experiments across multiple domains, modalities, and tasks show that G-Substrate outperforms task-isolated and naive multi-task learning methods.

</details>


### [37] [SAIR: Cost-Efficient Multi-Stage ML Pipeline Autoscaling via In-Context Reinforcement Learning](https://arxiv.org/abs/2601.22397)
*Jianchang Su,Yifan Zhang,Shengkai Lin,Shizhen Zhao,Yusheng Zheng,Yiwei Yang,Wei Zhang*

Main category: cs.LG

TL;DR: SAIR是一个多阶段ML推理管道的自动扩缩框架，使用LLM作为上下文强化学习控制器，无需梯度更新即可在线改进策略，显著改善延迟和资源成本。


<details>
  <summary>Details</summary>
Motivation: 多阶段ML推理管道由于异构资源、跨阶段耦合和动态瓶颈迁移而难以实现自动扩缩，需要一种能够适应动态变化的高效扩缩解决方案。

Method: 使用LLM作为上下文强化学习控制器，结合帕累托支配奖励塑造与可证明分离边界、基于惊奇度的经验检索以提高上下文效率，以及通过用户空间CUDA拦截实现细粒度GPU速率控制。

Result: 在四种ML服务管道和三种工作负载模式下，SAIR实现了最佳或并列最佳的P99延迟和有效资源成本，P99延迟提升高达50%，有效成本降低高达97%，瓶颈检测准确率达到86%，且无需离线训练。

Conclusion: SAIR通过LLM驱动的上下文强化学习控制器，有效解决了多阶段ML推理管道的自动扩缩挑战，在延迟、成本和瓶颈检测方面表现出色，为ML服务系统提供了实用的自动扩缩解决方案。

Abstract: Multi-stage ML inference pipelines are difficult to autoscale due to heterogeneous resources, cross-stage coupling, and dynamic bottleneck migration. We present SAIR, an autoscaling framework that uses an LLM as an in-context reinforcement learning controller, improving its policy online from reward-labeled interaction histories without gradient updates. SAIR combines Pareto-dominance reward shaping with a provable separation margin, surprisal-guided experience retrieval for context efficiency, and fine-grained GPU rate control via user-space CUDA interception. We provide regret analysis decomposing error into retrieval coverage and LLM selection components. On four ML serving pipelines under three workload patterns, SAIR achieves the best or tied-best P99 latency and effective resource cost among deployed baselines, improving P99 by up to 50% and reducing effective cost by up to 97% (under GPU rate-control assumptions), with 86% bottleneck detection accuracy and no offline training.

</details>


### [38] [Score-based Integrated Gradient for Root Cause Explanations of Outliers](https://arxiv.org/abs/2601.22399)
*Phuoc Nguyen,Truyen Tran,Sunil Gupta,Svetha Venkatesh*

Main category: cs.LG

TL;DR: SIREN是一种新颖的异常根因分析方法，通过估计数据似然的得分函数，使用积分梯度从异常点向正常数据分布累积得分贡献，满足Shapley值公理，在非线性高维因果模型中实现可扩展的根因归因。


<details>
  <summary>Details</summary>
Motivation: 传统基于启发式或反事实推理的异常根因分析方法在不确定性和高维依赖下效果不佳，需要一种能够处理非线性、高维、异方差因果模型的可扩展方法。

Method: SIREN通过估计数据似然的得分函数，使用积分梯度方法沿着从异常点到正常数据分布的路径累积得分贡献，实现根因归因。该方法满足Shapley值的三个公理（虚拟性、效率性、线性性）以及基于因果结构的不对称性公理。

Result: 在合成随机图和真实世界云服务及供应链数据集上的实验表明，SIREN在归因准确性和计算效率方面均优于现有最先进基线方法。

Conclusion: SIREN提供了一种直接操作于得分函数的可扩展方法，能够在非线性、高维、异方差因果模型中实现可处理且具有不确定性感知的异常根因归因，显著优于传统方法。

Abstract: Identifying the root causes of outliers is a fundamental problem in causal inference and anomaly detection. Traditional approaches based on heuristics or counterfactual reasoning often struggle under uncertainty and high-dimensional dependencies. We introduce SIREN, a novel and scalable method that attributes the root causes of outliers by estimating the score functions of the data likelihood. Attribution is computed via integrated gradients that accumulate score contributions along paths from the outlier toward the normal data distribution. Our method satisfies three of the four classic Shapley value axioms - dummy, efficiency, and linearity - as well as an asymmetry axiom derived from the underlying causal structure. Unlike prior work, SIREN operates directly on the score function, enabling tractable and uncertainty-aware root cause attribution in nonlinear, high-dimensional, and heteroscedastic causal models. Extensive experiments on synthetic random graphs and real-world cloud service and supply chain datasets show that SIREN outperforms state-of-the-art baselines in both attribution accuracy and computational efficiency.

</details>


### [39] [MM-OpenFGL: A Comprehensive Benchmark for Multimodal Federated Graph Learning](https://arxiv.org/abs/2601.22416)
*Xunkai Li,Yuming Ai,Yinlin Zhu,Haodong Lu,Yi Zhang,Guohao Fu,Bowen Fan,Qiangqiang Dai,Rong-Hua Li,Guoren Wang*

Main category: cs.LG

TL;DR: MM-OpenFGL是首个多模态联邦图学习基准，包含19个数据集、8种模拟策略、6个下游任务和57种SOTA方法，系统评估多模态联邦图学习的必要性、有效性、鲁棒性和效率。


<details>
  <summary>Details</summary>
Motivation: 现实中的多模态属性图常分布在隔离平台，因隐私或商业限制无法共享，而现有联邦图学习研究主要关注单模态图，缺乏针对多模态联邦图学习的系统评估框架。

Method: 提出MM-OpenFGL基准，系统形式化多模态联邦图学习范式，包含：19个多模态数据集覆盖7个应用领域，8种模拟策略捕捉模态和拓扑变化，6个下游任务，57种SOTA方法通过模块化API实现。

Result: 通过大量实验从必要性、有效性、鲁棒性和效率四个维度深入研究了多模态联邦图学习，为未来研究提供了有价值的见解。

Conclusion: MM-OpenFGL填补了多模态联邦图学习评估的空白，为这一新兴领域提供了首个全面基准，促进了多模态联邦图学习的系统研究和未来发展。

Abstract: Multimodal-attributed graphs (MMAGs) provide a unified framework for modeling complex relational data by integrating heterogeneous modalities with graph structures. While centralized learning has shown promising performance, MMAGs in real-world applications are often distributed across isolated platforms and cannot be shared due to privacy concerns or commercial constraints. Federated graph learning (FGL) offers a natural solution for collaborative training under such settings; however, existing studies largely focus on single-modality graphs and do not adequately address the challenges unique to multimodal federated graph learning (MMFGL). To bridge this gap, we present MM-OpenFGL, the first comprehensive benchmark that systematically formalizes the MMFGL paradigm and enables rigorous evaluation. MM-OpenFGL comprises 19 multimodal datasets spanning 7 application domains, 8 simulation strategies capturing modality and topology variations, 6 downstream tasks, and 57 state-of-the-art methods implemented through a modular API. Extensive experiments investigate MMFGL from the perspectives of necessity, effectiveness, robustness, and efficiency, offering valuable insights for future research on MMFGL.

</details>


### [40] [MetaLead: A Comprehensive Human-Curated Leaderboard Dataset for Transparent Reporting of Machine Learning Experiments](https://arxiv.org/abs/2601.22420)
*Roelien C. Timmer,Necva Bölücü,Stephen Wan*

Main category: cs.LG

TL;DR: MetaLead是一个完全人工标注的机器学习排行榜数据集，旨在解决现有数据集仅捕获最佳结果和元数据有限的问题，提供更透明和细致的评估资源。


<details>
  <summary>Details</summary>
Motivation: 机器学习排行榜对于基准测试和跟踪进展至关重要，但传统创建方式需要大量人工努力。现有自动化排行榜生成的数据集存在局限性：仅捕获每篇论文的最佳结果，元数据有限，缺乏结果透明度和细致比较能力。

Method: 提出MetaLead数据集，这是一个完全人工标注的机器学习排行榜数据集。它捕获所有实验结果以确保结果透明度，包含额外元数据如实验类型（基线、提出方法或提出方法变体），并明确分离训练和测试数据集以支持跨领域评估。

Result: MetaLead通过其丰富的结构，包括所有实验结果、详细的实验类型元数据和明确的数据集分离，成为一个支持更透明和细致评估的强大资源。

Conclusion: MetaLead解决了现有排行榜数据集的局限性，通过提供完全人工标注、包含所有实验结果和丰富元数据的结构，为机器学习研究提供了更透明和细致的评估能力，支持实验类型引导的比较和跨领域评估。

Abstract: Leaderboards are crucial in the machine learning (ML) domain for benchmarking and tracking progress. However, creating leaderboards traditionally demands significant manual effort. In recent years, efforts have been made to automate leaderboard generation, but existing datasets for this purpose are limited by capturing only the best results from each paper and limited metadata. We present MetaLead, a fully human-annotated ML Leaderboard dataset that captures all experimental results for result transparency and contains extra metadata, such as the result experimental type: baseline, proposed method, or variation of proposed method for experiment-type guided comparisons, and explicitly separates train and test dataset for cross-domain assessment. This enriched structure makes MetaLead a powerful resource for more transparent and nuanced evaluations across ML research.

</details>


### [41] [ReNCE: Learning to Reason by Noise Contrastive Estimation](https://arxiv.org/abs/2601.22432)
*Wenzheng Zhang,Karl Stratos*

Main category: cs.LG

TL;DR: 提出了一种基于显式对比学习的LLM推理训练方法，替代传统的GRPO优势估计方法


<details>
  <summary>Details</summary>
Motivation: GRPO方法虽然有效，但其改进（如非对称裁剪和零方差数据过滤）需要大量经验洞察且难以识别，因此需要更简洁的替代方案

Method: 将K个结果分为正负两组，最大化正结果的似然，可视为在线多标签噪声对比估计在LLM推理中的应用

Result: 在具有挑战性的数学基准测试中，与DAPO和在线DPO等强基线相比表现出有竞争力的性能

Conclusion: 提出的显式对比学习方法为LLM推理训练提供了更简洁有效的替代方案，避免了传统方法中需要复杂经验调整的问题

Abstract: GRPO is a standard approach to endowing pretrained LLMs with reasoning capabilities. It estimates the advantage of an outcome from a group of $K$ outcomes, and promotes those with positive advantages inside a trust region. Since GRPO discriminates between good and bad outcomes softly, it benefits from additional refinements such as asymmetric clipping and zero-variance data filtering. While effective, these refinements require significant empirical insight and can be challenging to identify. We instead propose an explicit contrastive learning approach. Instead of estimating advantages, we bifurcate $K$ outcomes into positive and negative sets, then maximize the likelihood of positive outcomes. Our approach can be viewed as an online instantiation of (multi-label) noise contrastive estimation for LLM reasoning. We validate our method by demonstrating competitive performance on a suite of challenging math benchmarks against strong baselines such as DAPO and online DPO.

</details>


### [42] [Weak Diffusion Priors Can Still Achieve Strong Inverse-Problem Performance](https://arxiv.org/abs/2601.22443)
*Jing Jia,Wei Yuan,Sifan Liu,Liyue Shen,Guanyang Wang*

Main category: cs.LG

TL;DR: 扩散模型在逆问题中作为先验时，即使训练数据与目标信号不匹配（弱先验），在某些条件下仍能有效恢复信号


<details>
  <summary>Details</summary>
Motivation: 研究扩散模型作为逆问题先验时的鲁棒性问题。实际应用中常使用不匹配或低保真的扩散先验（弱先验），但这些弱先验往往表现接近完整强度的域内基线，需要理解何时及为何逆求解器对弱扩散先验具有鲁棒性

Method: 通过大量实验研究弱先验的成功与失败机制，基于贝叶斯一致性理论，给出高维测量使后验集中于真实信号附近的条件

Result: 发现弱先验在测量信息丰富时（如观测像素多）能成功，识别了其失败的机制。理论分析提供了弱扩散先验可靠使用的原则性依据

Conclusion: 扩散模型作为逆问题先验时，即使训练数据与目标信号不匹配，在测量信息充分的条件下仍能可靠使用，这为实际应用中弱扩散先验的使用提供了理论支持

Abstract: Can a diffusion model trained on bedrooms recover human faces? Diffusion models are widely used as priors for inverse problems, but standard approaches usually assume a high-fidelity model trained on data that closely match the unknown signal. In practice, one often must use a mismatched or low-fidelity diffusion prior. Surprisingly, these weak priors often perform nearly as well as full-strength, in-domain baselines. We study when and why inverse solvers are robust to weak diffusion priors. Through extensive experiments, we find that weak priors succeed when measurements are highly informative (e.g., many observed pixels), and we identify regimes where they fail. Our theory, based on Bayesian consistency, gives conditions under which high-dimensional measurements make the posterior concentrate near the true signal. These results provide a principled justification on when weak diffusion priors can be used reliably.

</details>


### [43] [Beyond Activation Patterns: A Weight-Based Out-of-Context Explanation of Sparse Autoencoder Features](https://arxiv.org/abs/2601.22447)
*Yiting Liu,Zhi-Hong Deng*

Main category: cs.LG

TL;DR: 本文提出了一种基于权重的稀疏自编码器特征解释框架，通过直接分析权重交互来测量功能效应，无需激活数据，揭示了SAE特征在语言模型中的计算角色。


<details>
  <summary>Details</summary>
Motivation: 当前稀疏自编码器特征解释方法主要基于激活模式推断语义，但忽略了这些特征被训练来重建在前向传播中具有计算作用的激活。需要一种能够捕捉特征功能角色的解释框架。

Method: 引入基于权重的解释框架，通过直接分析权重交互来测量功能效应，无需激活数据。在Gemma-2和Llama-3.1模型上进行了三个实验。

Result: 实验发现：(1) 1/4的特征直接预测输出token；(2) 特征在注意力机制中积极参与，具有深度依赖的结构；(3) 语义和非语义特征群体在注意力电路中表现出不同的分布特征。

Conclusion: 该分析提供了SAE特征可解释性中缺失的"上下文外"部分，通过权重交互揭示了特征的计算功能，补充了基于激活的语义解释方法。

Abstract: Sparse autoencoders (SAEs) have emerged as a powerful technique for decomposing language model representations into interpretable features. Current interpretation methods infer feature semantics from activation patterns, but overlook that features are trained to reconstruct activations that serve computational roles in the forward pass. We introduce a novel weight-based interpretation framework that measures functional effects through direct weight interactions, requiring no activation data. Through three experiments on Gemma-2 and Llama-3.1 models, we demonstrate that (1) 1/4 of features directly predict output tokens, (2) features actively participate in attention mechanisms with depth-dependent structure, and (3) semantic and non-semantic feature populations exhibit distinct distribution profiles in attention circuits. Our analysis provides the missing out-of-context half of SAE feature interpretability.

</details>


### [44] [HeaPA: Difficulty-Aware Heap Sampling and On-Policy Query Augmentation for LLM Reinforcement Learning](https://arxiv.org/abs/2601.22448)
*Weiqi Wang,Xin Liu,Binxuan Huang,Hejie Cui,Rongzhi Zhang,Changlong Yu,Shuowei Jin,Jingfeng Yang,Qingyu Yin,Zhengyang Wang,Zheng Li,Yifan Gao,Priyanka Nigam,Bing Yin,Lihong Li,Yangqiu Song*

Main category: cs.LG

TL;DR: HeaPA是一种用于RLVR训练的高效采样方法，通过边界采样和在线查询增强来优化提示池管理，减少计算成本的同时保持性能


<details>
  <summary>Details</summary>
Motivation: 当前RLVR训练中，提示池通常是静态的或与模型学习进度松散关联，均匀采样无法适应能力边界的变化，导致在已解决或无法解决的提示上浪费计算资源

Method: HeaPA维护有界演化池，使用堆基边界采样跟踪能力边界，通过轻量级异步验证进行在线查询增强，并通过拓扑感知统计重估计和控制重插入来稳定相关查询

Result: 在两个训练语料库、两种训练方案和七个基准测试中，HeaPA持续提高准确性，以更少的计算达到目标性能，同时保持实际训练时间相当

Conclusion: HeaPA通过边界聚焦采样和在线池增长提高了RLVR训练效率，随着模型规模增大，其优势更加明显，为大规模语言模型推理训练提供了有效的采样策略

Abstract: RLVR is now a standard way to train LLMs on reasoning tasks with verifiable outcomes, but when rollout generation dominates the cost, efficiency depends heavily on which prompts you sample and when. In practice, prompt pools are often static or only loosely tied to the model's learning progress, so uniform sampling can't keep up with the shifting capability frontier and ends up wasting rollouts on prompts that are already solved or still out of reach. Existing approaches improve efficiency through filtering, curricula, adaptive rollout allocation, or teacher guidance, but they typically assume a fixed pool-which makes it hard to support stable on-policy pool growth-or they add extra teacher cost and latency. We introduce HeaPA (Heap Sampling and On-Policy Query Augmentation), which maintains a bounded, evolving pool, tracks the frontier using heap-based boundary sampling, expands the pool via on-policy augmentation with lightweight asynchronous validation, and stabilizes correlated queries through topology-aware re-estimation of pool statistics and controlled reinsertion. Across two training corpora, two training recipes, and seven benchmarks, HeaPA consistently improves accuracy and reaches target performance with fewer computations while keeping wall-clock time comparable. Our analyses suggest these gains come from frontier-focused sampling and on-policy pool growth, with the benefits becoming larger as model scale increases. Our code is available at https://github.com/horizon-rl/HeaPA.

</details>


### [45] [Machine Unlearning in Low-Dimensional Feature Subspace](https://arxiv.org/abs/2601.22456)
*Kun Fang,Qinghua Tao,Junxu Liu,Yaxin Xiao,Qingqing Ye,Jian Sun,Haibo Hu*

Main category: cs.LG

TL;DR: LOFT提出了一种基于低维特征子空间的机器遗忘方法，通过优化投影矩阵在特征空间中分离保留数据和遗忘数据，显著降低了计算开销并提升了遗忘性能。


<details>
  <summary>Details</summary>
Motivation: 现有机器遗忘方法存在两大关键问题：1）大量数据重新加载带来的隐私泄露风险；2）对整个预训练模型进行更新的低效率。需要一种更高效、更安全的遗忘方法。

Method: LOFT方法在预训练模型的低维特征子空间中进行遗忘操作，通过主成分投影优化投影矩阵，最大化保留数据信息同时最小化遗忘数据信息。只需优化小型投影矩阵，一次性获取特征而不需要重复访问原始数据。

Result: 大量实验验证了LOFT在多种模型、数据集、任务和应用中具有显著更低的计算开销和优越的遗忘性能。

Conclusion: LOFT提供了一种高效、安全的机器遗忘新视角，通过在低维特征子空间中分离保留和遗忘数据，解决了现有方法的主要缺陷，具有实际应用价值。

Abstract: Machine Unlearning (MU) aims at removing the influence of specific data from a pretrained model while preserving performance on the remaining data. In this work, a novel perspective for MU is presented upon low-dimensional feature subspaces, which gives rise to the potentials of separating the remaining and forgetting data herein. This separability motivates our LOFT, a method that proceeds unlearning in a LOw-dimensional FeaTure subspace from the pretrained model skithrough principal projections, which are optimized to maximally capture the information of the remaining data and meanwhile diminish that of the forgetting data. In training, LOFT simply optimizes a small-size projection matrix flexibly plugged into the pretrained model, and only requires one-shot feature fetching from the pretrained backbone instead of repetitively accessing the raw data. Hence, LOFT mitigates two critical issues in mainstream MU methods, i.e., the privacy leakage risk from massive data reload and the inefficiency of updates to the entire pretrained model. Extensive experiments validate the significantly lower computational overhead and superior unlearning performance of LOFT across diverse models, datasets, tasks, and applications. Code is anonymously available at https://anonymous.4open.science/r/4352/.

</details>


### [46] [EvoEGF-Mol: Evolving Exponential Geodesic Flow for Structure-based Drug Design](https://arxiv.org/abs/2601.22466)
*Yaowei Jin,Junjie Wang,Cheng Cao,Penglei Wang,Duo An,Qian Shi*

Main category: cs.LG

TL;DR: EvoEGF-Mol：基于信息几何的SBDD新方法，通过指数测地线流生成分子，避免传统方法在欧几里得空间和概率空间不匹配的问题，在几何精度和相互作用保真度上达到参考水平。


<details>
  <summary>Details</summary>
Motivation: 传统基于结构的药物设计方法分别在欧几里得空间和概率空间中构建概率路径，导致与底层统计流形不匹配。需要从信息几何角度解决这一问题。

Method: 将分子建模为复合指数族分布，在Fisher-Rao度量下定义沿指数测地线的生成流。为避免直接以狄拉克分布为目标导致的瞬时轨迹崩溃，提出EvoEGF-Mol，用动态集中分布替代静态狄拉克目标，通过渐进参数细化架构确保稳定训练。

Result: 在CrossDock上达到93.4%的参考级PoseBusters通过率，表现出卓越的几何精度和相互作用保真度。在MolGenBench任务中超越基线，能够恢复生物活性支架并生成符合MedChem过滤器的候选分子。

Conclusion: EvoEGF-Mol从信息几何角度解决了SBDD中的流形不匹配问题，通过指数测地线流实现了稳定高效的分子生成，在几何精度和药物化学性质方面均表现出色。

Abstract: Structure-Based Drug Design (SBDD) aims to discover bioactive ligands. Conventional approaches construct probability paths separately in Euclidean and probabilistic spaces for continuous atomic coordinates and discrete chemical categories, leading to a mismatch with the underlying statistical manifolds. We address this issue from an information-geometric perspective by modeling molecules as composite exponential-family distributions and defining generative flows along exponential geodesics under the Fisher-Rao metric. To avoid the instantaneous trajectory collapse induced by geodesics directly targeting Dirac distributions, we propose Evolving Exponential Geodesic Flow for SBDD (EvoEGF-Mol), which replaces static Dirac targets with dynamically concentrating distributions, ensuring stable training via a progressive-parameter-refinement architecture. Our model approaches a reference-level PoseBusters passing rate (93.4%) on CrossDock, demonstrating remarkable geometric precision and interaction fidelity, while outperforming baselines on real-world MolGenBench tasks by recovering bioactive scaffolds and generating candidates that meet established MedChem filters.

</details>


### [47] [Unrewarded Exploration in Large Language Models Reveals Latent Learning from Psychology](https://arxiv.org/abs/2601.22474)
*Jian Xiong,Jingbo Zhou,Zihan Zhou,Yixiong Xiao,Le Zhang,Jingyong Ye,Rui Qian,Yang Zhou,Dejing Dou*

Main category: cs.LG

TL;DR: 研究发现大型语言模型（LLMs）在无奖励探索阶段表现出潜在学习动态，这种心理学现象使模型在引入奖励后获得更好性能。


<details>
  <summary>Details</summary>
Motivation: 传统奖励学习过度依赖外部反馈，限制了灵活性和泛化能力。虽然LLMs在推理能力上取得突破，但仍主要依赖奖励中心的强化学习范式。心理学中已确立的潜在学习现象是否能在LLMs训练中出现或为其提供启示，尚待探索。

Method: 采用两阶段探索机制：第一阶段进行无奖励探索，让LLMs在没有奖励驱动偏见的情况下组织任务相关知识；第二阶段引入奖励。在多个模型家族和多样化任务领域进行广泛实验，并提供理论分析解释无奖励探索为何能带来性能提升。

Result: LLMs在无奖励探索阶段表现出适度的性能改进，一旦引入奖励后性能进一步增强。采用这种两阶段探索机制后训练的LLMs最终比全程使用基于奖励的强化学习训练的模型获得更高的能力。

Conclusion: LLMs确实表现出潜在学习动态，无奖励探索阶段有助于模型组织相关知识而不受奖励驱动偏见的限制。这种心理学现象可以为LLMs训练提供新的启示，提高模型的适应能力和性能。

Abstract: Latent learning, classically theorized by Tolman, shows that biological agents (e.g., rats) can acquire internal representations of their environment without rewards, enabling rapid adaptation once rewards are introduced. In contrast, from a cognitive science perspective, reward learning remains overly dependent on external feedback, limiting flexibility and generalization. Although recent advances in the reasoning capabilities of large language models (LLMs), such as OpenAI-o1 and DeepSeek-R1, mark a significant breakthrough, these models still rely primarily on reward-centric reinforcement learning paradigms. Whether and how the well-established phenomenon of latent learning in psychology can inform or emerge within LLMs' training remains largely unexplored. In this work, we present novel findings from our experiments that LLMs also exhibit the latent learning dynamics. During an initial phase of unrewarded exploration, LLMs display modest performance improvements, as this phase allows LLMs to organize task-relevant knowledge without being constrained by reward-driven biases, and performance is further enhanced once rewards are introduced. LLMs post-trained under this two-stage exploration regime ultimately achieve higher competence than those post-trained with reward-based reinforcement learning throughout. Beyond these empirical observations, we also provide theoretical analyses for our experiments explaining why unrewarded exploration yields performance gains, offering a mechanistic account of these dynamics. Specifically, we conducted extensive experiments across multiple model families and diverse task domains to establish the existence of the latent learning dynamics in LLMs.

</details>


### [48] [Continual Policy Distillation from Distributed Reinforcement Learning Teachers](https://arxiv.org/abs/2601.22475)
*Yuxuan Li,Qijun He,Mingqi Yuan,Wen-Tse Chen,Jeff Schneider,Jiayu Chen*

Main category: cs.LG

TL;DR: 提出教师-学生框架，将持续强化学习解耦为两个独立过程：通过分布式RL训练单任务教师模型，并持续蒸馏到中央通用模型中，结合MoE架构和回放机制提升性能。


<details>
  <summary>Details</summary>
Motivation: 持续强化学习需要在多样化任务中持续学习同时避免灾难性遗忘，但直接应用RL到序列任务流中仍具挑战性。观察到RL擅长解决单任务，而策略蒸馏作为相对稳定的监督学习过程更适合大规模基础模型和多任务学习。

Method: 1) 教师-学生框架：将CRL解耦为两个独立过程；2) 分布式RL训练单任务教师模型；3) 持续蒸馏到中央通用学生模型；4) 采用混合专家(MoE)架构增强可塑性；5) 使用回放机制提升稳定性。

Result: 在Meta-World基准测试中，框架能够恢复超过85%的教师性能，同时将任务间遗忘控制在10%以内，实现了高效的持续强化学习。

Conclusion: 提出的教师-学生框架通过解耦单任务RL训练和持续策略蒸馏，有效解决了持续强化学习中的稳定性-可塑性权衡问题，为大规模基础模型的持续学习提供了可行方案。

Abstract: Continual Reinforcement Learning (CRL) aims to develop lifelong learning agents to continuously acquire knowledge across diverse tasks while mitigating catastrophic forgetting. This requires efficiently managing the stability-plasticity dilemma and leveraging prior experience to rapidly generalize to novel tasks. While various enhancement strategies for both aspects have been proposed, achieving scalable performance by directly applying RL to sequential task streams remains challenging. In this paper, we propose a novel teacher-student framework that decouples CRL into two independent processes: training single-task teacher models through distributed RL and continually distilling them into a central generalist model. This design is motivated by the observation that RL excels at solving single tasks, while policy distillation -- a relatively stable supervised learning process -- is well aligned with large foundation models and multi-task learning. Moreover, a mixture-of-experts (MoE) architecture and a replay-based approach are employed to enhance the plasticity and stability of the continual policy distillation process. Extensive experiments on the Meta-World benchmark demonstrate that our framework enables efficient continual RL, recovering over 85% of teacher performance while constraining task-wise forgetting to within 10%.

</details>


### [49] [Transform-Augmented GRPO Improves Pass@k](https://arxiv.org/abs/2601.22478)
*Khiem Le,Youssef Mroueh,Phuc Nguyen,Chi-Heng Lin,Shangqian Gao,Ting Hua,Nitesh V. Chawla*

Main category: cs.LG

TL;DR: TA-GRPO通过生成语义等价的变换问题变体，解决了GRPO中的多样性崩溃和梯度消失问题，在数学推理基准上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于下一个token预测的大语言模型本质上是模式匹配器，对表面表述变化敏感。GRPO虽然旨在改进推理，但存在两个失败模式：多样性崩溃（训练放大单一解决策略）和梯度消失（大量问题产生零梯度）。

Method: 提出TA-GRPO（变换增强的GRPO），通过转述、变量重命名和格式变化生成语义等价的变换问题变体，并通过跨整个组池化奖励来计算优势，确保混合奖励并促进多种解决策略。

Result: 在数学推理基准上获得一致的Pass@k改进，在竞赛数学（AMC12，AIME24）上提升高达9.84分，在分布外科学推理（GPQA-Diamond）上提升5.05分。

Conclusion: TA-GRPO通过减少零梯度概率和通过降低训练-测试分布偏移来改善泛化，为语言模型推理训练提供了更稳健的方法。

Abstract: Large language models trained via next-token prediction are fundamentally pattern-matchers: sensitive to superficial phrasing variations even when the underlying problem is identical. Group Relative Policy Optimization (GRPO) was designed to improve reasoning, but in fact it worsens this situation through two failure modes: diversity collapse, where training amplifies a single solution strategy while ignoring alternatives of gradient signal, and gradient diminishing, where a large portion of questions yield zero gradients because all rollouts receive identical rewards. We propose TA-GRPO (Transform-Augmented GRPO), which generates semantically equivalent transformed variants of each question (via paraphrasing, variable renaming, and format changes) and computes advantages by pooling rewards across the entire group. This pooled computation ensures mixed rewards even when the original question is too easy or too hard, while training on diverse phrasings promotes multiple solution strategies. We provide theoretical justification showing that TA-GRPO reduces zero-gradient probability and improves generalization via reduced train-test distribution shift. Experiments on mathematical reasoning benchmarks show consistent Pass@k improvements, with gains up to 9.84 points on competition math (AMC12, AIME24) and 5.05 points on out-of-distribution scientific reasoning (GPQA-Diamond).

</details>


### [50] [Mitigating Cognitive Inertia in Large Reasoning Models via Latent Spike Steering](https://arxiv.org/abs/2601.22484)
*Seojin Lee,ByeongJeong Kim,Hwanhee Lee*

Main category: cs.LG

TL;DR: STARS框架通过监测隐藏状态的L2距离尖峰来检测推理过程中的认知惯性，并实时注入语言提示来引导模型，无需额外训练即可优化大型推理模型的推理过程。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然通过扩展测试时计算取得了显著性能，但经常遭受认知惯性的困扰，表现为过度思考或推理僵化。现有检测方法通常依赖表面文本启发式方法，无法捕捉模型未表达的内部冲突。

Method: 提出STARS（尖峰触发自适应推理引导）框架：1）通过检测隐藏状态的L2距离尖峰来识别认知枢轴（推理转换的关键时刻）；2）使用几何轨迹分析诊断转换的结构性质；3）注入状态感知的语言提示来实时引导模型。

Result: 在多个基准测试上的实验证实，STARS能有效减少冗余循环，同时通过自适应纠正错误轨迹来提高准确性。该框架无需额外微调即可优化大型推理模型的推理过程。

Conclusion: STARS提供了一个强大、无监督的机制，可以在不需要额外微调的情况下优化大型推理模型的推理过程，有效解决了认知惯性问题。

Abstract: While Large Reasoning Models (LRMs) have achieved remarkable performance by scaling test-time compute, they frequently suffer from Cognitive Inertia, a failure pattern manifesting as either overthinking (inertia of motion) or reasoning rigidity (inertia of direction). Existing detection methods, typically relying on superficial textual heuristics like self-correction tokens, often fail to capture the model's unvoiced internal conflicts. To address this, we propose STARS (Spike-Triggered Adaptive Reasoning Steering), a training-free framework designed to rectify cognitive inertia by monitoring latent dynamics. STARS identifies Cognitive Pivots-critical moments of reasoning transition-by detecting distinct L2 distance spikes in the hidden states. Upon detection, the framework employs geometric trajectory analysis to diagnose the structural nature of the transition and injects state-aware language cues to steer the model in real-time. Our experiments across diverse benchmarks confirm that STARS efficiently curtails redundant loops while improving accuracy through the adaptive correction of erroneous trajectories. STARS offers a robust, unsupervised mechanism to optimize the reasoning process of LRMs without requiring additional fine-tuning.

</details>


### [51] [Gradual Fine-Tuning for Flow Matching Models](https://arxiv.org/abs/2601.22495)
*Gudrun Thorkelsdottir,Arindam Banerjee*

Main category: cs.LG

TL;DR: GFT是一种用于流匹配模型微调的理论框架，通过温度控制的中间目标平滑过渡预训练和目标分布，提高收敛稳定性和推理速度


<details>
  <summary>Details</summary>
Motivation: 在数据有限、分布变化或效率要求严格的场景中，传统微调方法会损害预训练获得的准确性和效率优势，现有方法对漂移结构或训练技术有限制

Method: 提出渐进微调(GFT)框架，为随机流定义温度控制的中间目标序列，平滑插值预训练和目标漂移，支持使用合适的耦合（如最优传输）

Result: GFT提高了收敛稳定性，缩短了概率路径，实现了更快的推理速度，同时保持了与标准微调相当的生成质量

Conclusion: GFT为流匹配模型在分布偏移下的可扩展适应提供了理论基础和实践有效的替代方案

Abstract: Fine-tuning flow matching models is a central challenge in settings with limited data, evolving distributions, or strict efficiency demands, where unconstrained fine-tuning can erode the accuracy and efficiency gains learned during pretraining. Prior work has produced theoretical guarantees and empirical advances for reward-based fine-tuning formulations, but these methods often impose restrictions on permissible drift structure or training techniques. In this work, we propose Gradual Fine-Tuning (GFT), a principled framework for fine-tuning flow-based generative models when samples from the target distribution are available. For stochastic flows, GFT defines a temperature-controlled sequence of intermediate objectives that smoothly interpolate between the pretrained and target drifts, approaching the true target as the temperature approaches zero. We prove convergence results for both marginal and conditional GFT objectives, enabling the use of suitable (e.g., optimal transport) couplings during GFT while preserving correctness. Empirically, GFT improves convergence stability and shortens probability paths, resulting in faster inference, while maintaining generation quality comparable to standard fine-tuning. Our results position GFT as a theoretically grounded and practically effective alternative for scalable adaptation of flow matching models under distribution shift.

</details>


### [52] [Action-Sufficient Goal Representations](https://arxiv.org/abs/2601.22496)
*Jinu Hyeon,Woobin Park,Hongjoon Ahn,Taesup Moon*

Main category: cs.LG

TL;DR: 离线目标条件强化学习中，传统基于价值函数的目标表示可能无法区分需要不同动作的目标状态，本文提出动作充分性框架，证明基于策略学习的目标表示优于基于价值估计的表示。


<details>
  <summary>Details</summary>
Motivation: 现有分层策略架构中，目标表示通常在学习价值函数时推导，隐含假设保留价值估计所需信息就足以实现最优控制。但本文发现这种假设可能失败，即使价值估计准确，这类表示可能将需要不同动作的目标状态混为一谈。

Method: 引入信息论框架定义动作充分性条件，证明价值充分性不蕴含动作充分性。提出通过标准对数损失训练低级策略自然诱导动作充分表示的方法，并在流行基准上进行实验验证。

Result: 实验证明动作充分性与控制成功更密切相关，基于策略学习的目标表示在性能上一致优于基于价值估计的表示。

Conclusion: 在离线目标条件强化学习中，为确保最优控制，目标表示需要满足动作充分性而非仅价值充分性，基于策略学习的方法能更有效地实现这一目标。

Abstract: Hierarchical policies in offline goal-conditioned reinforcement learning (GCRL) addresses long-horizon tasks by decomposing control into high-level subgoal planning and low-level action execution. A critical design choice in such architectures is the goal representation-the compressed encoding of goals that serves as the interface between these levels. Existing approaches commonly derive goal representations while learning value functions, implicitly assuming that preserving information sufficient for value estimation is adequate for optimal control. We show that this assumption can fail, even when the value estimation is exact, as such representations may collapse goal states that need to be differentiated for action learning. To address this, we introduce an information-theoretic framework that defines action sufficiency, a condition on goal representations necessary for optimal action selection. We prove that value sufficiency does not imply action sufficiency and empirically verify that the latter is more strongly associated with control success in a discrete environment. We further demonstrate that standard log-loss training of low-level policies naturally induces action-sufficient representations. Our experimental results a popular benchmark demonstrate that our actor-derived representations consistently outperform representations learned via value estimation.

</details>


### [53] [Shattered Compositionality: Counterintuitive Learning Dynamics of Transformers for Arithmetic](https://arxiv.org/abs/2601.22510)
*Xingyu Zhao,Darsh Sharma,Rheeya Uppaal,Yiqiao Zhong*

Main category: cs.LG

TL;DR: 研究发现Transformer模型在技能组合学习上存在"破碎组合性"问题，其学习动态与人类顺序规则不符，导致分布偏移时出现意外错误，且无法通过模型缩放或思维链缓解。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型经常表现出意外的错误或非预期行为，即使在大规模训练下也是如此。虽然近期研究揭示了LLMs与人类在技能组合上的差异，但技能组合的学习动态及其非人类行为的根本原因仍不清楚。

Method: 通过在合成算术任务上训练Transformer模型，进行广泛的消融实验和细粒度诊断指标分析，研究学习动态机制。

Result: 发现Transformer模型不按照人类顺序规则可靠地构建技能组合，而是经常以相反顺序或并行方式获取技能，导致在分布偏移下出现意外的混合错误（即"破碎组合性"）。证据表明训练数据的相关性匹配而非因果或程序性组合塑造了学习动态。这种现象在现代LLMs中持续存在，且无法通过纯模型缩放或思维链推理缓解。

Conclusion: 模型的学习行为与期望的技能组合之间存在根本性不匹配，这对推理可靠性、分布外鲁棒性和对齐具有重要影响。

Abstract: Large language models (LLMs) often exhibit unexpected errors or unintended behavior, even at scale. While recent work reveals the discrepancy between LLMs and humans in skill compositions, the learning dynamics of skill compositions and the underlying cause of non-human behavior remain elusive. In this study, we investigate the mechanism of learning dynamics by training transformers on synthetic arithmetic tasks. Through extensive ablations and fine-grained diagnostic metrics, we discover that transformers do not reliably build skill compositions according to human-like sequential rules. Instead, they often acquire skills in reverse order or in parallel, which leads to unexpected mixing errors especially under distribution shifts--a phenomenon we refer to as shattered compositionality. To explain these behaviors, we provide evidence that correlational matching to the training data, rather than causal or procedural composition, shapes learning dynamics. We further show that shattered compositionality persists in modern LLMs and is not mitigated by pure model scaling or scratchpad-based reasoning. Our results reveal a fundamental mismatch between a model's learning behavior and desired skill compositions, with implications for reasoning reliability, out-of-distribution robustness, and alignment.

</details>


### [54] [DRL-Enabled Trajectory Planing for UAV-Assisted VLC: Optimal Altitude and Reward Design](https://arxiv.org/abs/2601.22512)
*Tian-Tian Lin,Yi Liu,Xiao-Wei Tang,Yunmei Shi,Yi Huang,Zhongxiang Wei,Qingqing Wu,Yuhan Dong*

Main category: cs.LG

TL;DR: 该论文研究无人机辅助可见光通信系统中的三维轨迹规划，通过优化飞行高度和水平轨迹来最小化无人机飞行距离，提高数据收集效率。


<details>
  <summary>Details</summary>
Motivation: 无人机与可见光通信技术的结合为灵活通信和高效照明提供了有前景的解决方案。需要开发轨迹规划框架来最小化无人机飞行距离，从而提高数据收集效率。

Method: 首先推导了特定可见光信道增益阈值下的闭式最优飞行高度，然后通过将新型信息素驱动奖励机制与双延迟深度确定性策略梯度算法相结合来优化无人机水平轨迹。

Result: 仿真结果表明，推导的最优高度相比基线方法可减少高达35%的飞行距离，提出的奖励机制可将收敛步数缩短约50%，在无人机辅助可见光数据收集中表现出显著的效率提升。

Conclusion: 该研究提出的三维轨迹规划框架有效解决了无人机辅助可见光通信系统中的轨迹优化问题，通过高度优化和自适应运动策略显著提高了数据收集效率。

Abstract: Recently, the integration of unmanned aerial vehicle (UAV) and visible light communication (VLC) technologies has emerged as a promising solution to offer flexible communication and efficient lighting. This letter investigates the three-dimensional trajectory planning in a UAV-assisted VLC system, where a UAV is dispatched to collect data from ground users (GUs). The core objective is to develop a trajectory planning framework that minimizes UAV flight distance, which is equivalent to maximizing the data collection efficiency. This issue is formulated as a challenging mixed-integer non-convex optimization problem. To tackle it, we first derive a closed-form optimal flight altitude under specific VLC channel gain threshold. Subsequently, we optimize the UAV horizontal trajectory by integrating a novel pheromone-driven reward mechanism with the twin delayed deep deterministic policy gradient algorithm, which enables adaptive UAV motion strategy in complex environments. Simulation results validate that the derived optimal altitude effectively reduces the flight distance by up to 35% compared to baseline methods. Additionally, the proposed reward mechanism significantly shortens the convergence steps by approximately 50%, demonstrating notable efficiency gains in the context of UAV-assisted VLC data collection.

</details>


### [55] [SCOPE-PD: Explainable AI on Subjective and Clinical Objective Measurements of Parkinson's Disease for Precision Decision-Making](https://arxiv.org/abs/2601.22516)
*Md Mezbahul Islam,John Michael Templeton,Masrur Sobhan,Christian Poellabauer,Ananda Mohan Mondal*

Main category: cs.LG

TL;DR: 该研究提出了SCOPE-PD框架，通过整合主观和客观评估数据，使用机器学习预测帕金森病，随机森林模型达到98.66%准确率，并通过SHAP分析识别关键预测特征。


<details>
  <summary>Details</summary>
Motivation: 帕金森病诊断面临主观性挑战，传统诊断方法存在延迟问题。现有机器学习方法主要依赖主观报告，缺乏可解释性和个性化风险评估能力。

Method: 提出SCOPE-PD可解释AI预测框架，整合主观和客观临床评估数据。从PPMI研究中收集多模态数据，应用多种机器学习技术，选择最佳模型并使用SHAP进行结果解释。

Result: 随机森林算法在结合主观和客观测试数据特征时达到最高98.66%准确率。SHAP分析识别出震颤、运动迟缓和面部表情是MDS-UPDRS测试中对PD预测贡献最大的三个特征。

Conclusion: SCOPE-PD框架通过整合多模态数据和可解释AI技术，能够提供个性化的帕金森病风险评估，有助于早期诊断和临床决策支持。

Abstract: Parkinson's disease (PD) is a chronic and complex neurodegenerative disorder influenced by genetic, clinical, and lifestyle factors. Predicting this disease early is challenging because it depends on traditional diagnostic methods that face issues of subjectivity, which commonly delay diagnosis. Several objective analyses are currently in practice to help overcome the challenges of subjectivity; however, a proper explanation of these analyses is still lacking. While machine learning (ML) has demonstrated potential in supporting PD diagnosis, existing approaches often rely on subjective reports only and lack interpretability for individualized risk estimation. This study proposes SCOPE-PD, an explainable AI-based prediction framework, by integrating subjective and objective assessments to provide personalized health decisions. Subjective and objective clinical assessment data are collected from the Parkinson's Progression Markers Initiative (PPMI) study to construct a multimodal prediction framework. Several ML techniques are applied to these data, and the best ML model is selected to interpret the results. Model interpretability is examined using SHAP-based analysis. The Random Forest algorithm achieves the highest accuracy of 98.66 percent using combined features from both subjective and objective test data. Tremor, bradykinesia, and facial expression are identified as the top three contributing features from the MDS-UPDRS test in the prediction of PD.

</details>


### [56] [Demystifying Design Choices of Reinforcement Fine-tuning: A Batched Contextual Bandit Learning Perspective](https://arxiv.org/abs/2601.22532)
*Hong Xie,Xiao Hu,Tao Tan,Haoran Gu,Xin Li,Jianyu Han,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 本文系统分析了强化学习微调中的设计选择，通过构建简约基线方法解耦各因素影响，实验揭示了不同设计选择在学习和泛化中的作用，并识别出关键因素。


<details>
  <summary>Details</summary>
Motivation: 强化学习微调领域虽然论文爆炸式增长，但性能提升的结论常常不一致，进展显得虚幻。缺乏对两个基本问题的原则性答案：1）每个设计选择的作用是什么？2）哪些是关键因素？

Method: 首先构建简约基线方法解耦因素：每轮查询一次rollout，使用结果奖励作为训练信号（无优势技巧），批量大小为32。该基线连接到批量上下文bandit学习，便于实验分析。围绕此基线设计实验流程，检查优势、rollout数量等因素的边际增益。

Result: 在三个基础模型和两个数据集上的实验不仅揭示了各种设计选择对学习和泛化动态的新理解，还识别出值得更多努力的关键因素。

Conclusion: 通过系统解耦分析强化学习微调中的设计选择，本文为理解各因素作用提供了新见解，并指出了未来研究应关注的关键方向。

Abstract: The reinforcement fine-tuning area is undergoing an explosion papers largely on optimizing design choices. Though performance gains are often claimed, inconsistent conclusions also arise from time to time, making the progress illusive. Reflecting on this illusion, we still lack principled answers to two fundamental questions: 1) what is the role of each design choice? 2) which ones are critical? This paper aims to shed light on them. The underlying challenge is that design choices are entangled together, making their contribution to learning and generalization difficult to attribute. To address this challenge, we first construct a minimalist baseline for disentangling factors: one rollout per query in each round, the outcome reward serving as the training signal without any advantage trick, and a batch size of thirty-two. This baseline connects to batched contextual bandit learning, which facilitates experimental analysis. Centering around this baseline, we design an experiment pipeline, examining the marginal gains of factors like advantage, number of rollouts, etc. Experiments on three base models and two datasets, not only reveal new understanding on the role of various design choices on learning and generalization dynamics, but also identify critical ones that deserve more effort.

</details>


### [57] [Learning to Defer in Non-Stationary Time Series via Switching State-Space Models](https://arxiv.org/abs/2601.22538)
*Yannis Montreuil,Letian Yu,Axel Carlier,Lai Xing Ng,Wei Tsang Ooi*

Main category: cs.LG

TL;DR: 提出了一种用于非平稳时间序列的延迟学习框架，通过因子化切换线性高斯状态空间模型建模专家残差，支持动态专家注册和基于信息导向的专家选择策略


<details>
  <summary>Details</summary>
Motivation: 解决非平稳时间序列中专家可用性随时间变化、只能获得部分反馈的延迟学习问题，需要处理专家动态加入和退出的情况

Method: 使用L2D-SLDS模型（因子化切换线性高斯状态空间模型）建模专家残差，包含上下文相关的状态切换、共享全局因子实现跨专家信息传递、专家特定状态，支持动态专家注册。基于一步预测信念提出信息导向的专家选择策略，平衡预测成本与潜在状态信息获取

Result: 实验表明该方法优于上下文多臂老虎机基线方法，且优于没有共享因子的消融版本

Conclusion: 提出的L2D-SLDS模型和信息导向专家选择策略能有效处理非平稳时间序列中的延迟学习问题，支持动态专家管理并实现跨专家信息共享

Abstract: We study Learning to Defer for non-stationary time series with partial feedback and time-varying expert availability. At each time step, the router selects an available expert, observes the target, and sees only the queried expert's prediction. We model signed expert residuals using L2D-SLDS, a factorized switching linear-Gaussian state-space model with context-dependent regime transitions, a shared global factor enabling cross-expert information transfer, and per-expert idiosyncratic states. The model supports expert entry and pruning via a dynamic registry. Using one-step-ahead predictive beliefs, we propose an IDS-inspired routing rule that trades off predicted cost against information gained about the latent regime and shared factor. Experiments show improvements over contextual-bandit baselines and a no-shared-factor ablation.

</details>


### [58] [Neural-Inspired Posterior Approximation (NIPA)](https://arxiv.org/abs/2601.22539)
*Babak Shahbaba,Zahra Moslemi*

Main category: cs.LG

TL;DR: 提出一种受人类多系统学习启发的贝叶斯采样算法，结合模型导向、无模型和情景控制模块，实现高效的后验分布探索，应用于大规模贝叶斯深度学习


<details>
  <summary>Details</summary>
Motivation: 人类通过多个相互作用的神经系统的互补控制（模型导向规划、无模型习惯反应、情景记忆学习）实现高效学习。作者旨在阐明这种生物效率的计算原理，并将其转化为可扩展贝叶斯推理的采样算法

Method: 提出三模块算法：1) 模型导向模块使用目标分布进行引导但计算缓慢的采样；2) 无模型模块利用先前样本学习参数空间模式，实现快速反射式采样而无需直接评估昂贵的目标分布；3) 情景控制模块通过回忆特定过去事件（样本）支持快速采样

Result: 该方法推进了贝叶斯方法，并促进其在大规模统计机器学习问题中的应用。特别应用于贝叶斯深度学习，强调适当和原则性的不确定性量化

Conclusion: 受人类多系统学习启发的三模块采样算法为可扩展贝叶斯推理提供了有效解决方案，特别适用于需要不确定性量化的大规模贝叶斯深度学习问题

Abstract: Humans learn efficiently from their environment by engaging multiple interacting neural systems that support distinct yet complementary forms of control, including model-based (goal-directed) planning, model-free (habitual) responding, and episodic memory-based learning. Model-based mechanisms compute prospective action values using an internal model of the environment, supporting flexible but computationally costly planning; model-free mechanisms cache value estimates and build heuristics that enable fast, efficient habitual responding; and memory-based mechanisms allow rapid adaptation from individual experience. In this work, we aim to elucidate the computational principles underlying this biological efficiency and translate them into a sampling algorithm for scalable Bayesian inference through effective exploration of the posterior distribution. More specifically, our proposed algorithm comprises three components: a model-based module that uses the target distribution for guided but computationally slow sampling; a model-free module that uses previous samples to learn patterns in the parameter space, enabling fast, reflexive sampling without directly evaluating the expensive target distribution; and an episodic-control module that supports rapid sampling by recalling specific past events (i.e., samples). We show that this approach advances Bayesian methods and facilitates their application to large-scale statistical machine learning problems. In particular, we apply our proposed framework to Bayesian deep learning, with an emphasis on proper and principled uncertainty quantification.

</details>


### [59] [Benchmarking Long Roll-outs of Auto-regressive Neural Operators for the Compressible Navier-Stokes Equations with Conserved Quantity Correction](https://arxiv.org/abs/2601.22541)
*Sean Current,Chandan Kumar,Datta Gaitonde,Srinivasan Parthasarathy*

Main category: cs.LG

TL;DR: 提出一种模型无关的守恒量校正技术，用于在深度学习模型中融入物理守恒准则，改善神经算子模型在长期预测中的稳定性。


<details>
  <summary>Details</summary>
Motivation: 深度学习用于PDE数值解时，在长期预测中表现不佳，主要原因是自回归误差累积以及模型无法守恒物理量。

Method: 提出守恒量校正技术，这是一种模型无关的方法，将物理守恒准则整合到深度学习模型中。

Result: 该方法显著提高了自回归神经算子模型的长期稳定性，且不依赖于具体模型架构。同时从谱域分析揭示了现有架构在高频分量处理上的局限性。

Conclusion: 未来工作需要设计能更好处理高频分量的架构，这对于理解和模拟湍流等复杂流动至关重要。

Abstract: Deep learning has been proposed as an efficient alternative for the numerical approximation of PDE solutions, offering fast, iterative simulation of PDEs through the approximation of solution operators. However, deep learning solutions have struggle to perform well over long prediction durations due to the accumulation of auto-regressive error, which is compounded by the inability of models to conserve physical quantities. In this work, we present conserved quantity correction, a model-agnostic technique for incorporation physical conservation criteria within deep learning models. Our results demonstrate consistent improvement in the long-term stability of auto-regressive neural operator models, regardless of the model architecture. Furthermore, we analyze the performance of neural operators from the spectral domain, highlighting significant limitations of present architectures. These results highlight the need for future work to consider architectures that place specific emphasis on high frequency components, which are integral to the understanding and modeling of turbulent flows.

</details>


### [60] [FedDis: A Causal Disentanglement Framework for Federated Traffic Prediction](https://arxiv.org/abs/2601.22578)
*Chengyang Zhou,Zijian Zhang,Chunxu Zhang,Hao Miao,Yulin Zhang,Kedi Lyu,Juncheng Hu*

Main category: cs.LG

TL;DR: FedDis：首个利用因果解耦的联邦时空预测框架，通过分离客户端特定因素和全局模式来应对非IID交通数据挑战


<details>
  <summary>Details</summary>
Motivation: 现有联邦学习方法在处理去中心化交通数据的非独立同分布特性时表现不佳，通常将全局共享模式与客户端特定局部动态纠缠在单一表示中。作者认为这种异质性源于两个不同生成源的纠缠：客户端特定的局部动态和跨客户端的全局时空模式。

Method: FedDis采用双分支架构设计：个性化银行学习捕获客户端特定因素，全局模式银行提炼共同知识。通过互信息最小化目标强制两个分支之间的信息正交性，确保有效的解耦。

Result: 在四个真实世界基准数据集上的综合实验表明，FedDis始终实现最先进的性能，具有有前景的效率和卓越的可扩展性。

Conclusion: FedDis是首个利用因果解耦进行联邦时空预测的框架，通过分离客户端特定因素和全局模式，实现了鲁棒的跨客户端知识转移，同时保持对独特本地环境的高度适应性。

Abstract: Federated learning offers a promising paradigm for privacy-preserving traffic prediction, yet its performance is often challenged by the non-identically and independently distributed (non-IID) nature of decentralized traffic data. Existing federated methods frequently struggle with this data heterogeneity, typically entangling globally shared patterns with client-specific local dynamics within a single representation. In this work, we postulate that this heterogeneity stems from the entanglement of two distinct generative sources: client-specific localized dynamics and cross-client global spatial-temporal patterns. Motivated by this perspective, we introduce FedDis, a novel framework that, to the best of our knowledge, is the first to leverage causal disentanglement for federated spatial-temporal prediction. Architecturally, FedDis comprises a dual-branch design wherein a Personalized Bank learns to capture client-specific factors, while a Global Pattern Bank distills common knowledge. This separation enables robust cross-client knowledge transfer while preserving high adaptability to unique local environments. Crucially, a mutual information minimization objective is employed to enforce informational orthogonality between the two branches, thereby ensuring effective disentanglement. Comprehensive experiments conducted on four real-world benchmark datasets demonstrate that FedDis consistently achieves state-of-the-art performance, promising efficiency, and superior expandability.

</details>


### [61] [MC-GRPO: Median-Centered Group Relative Policy Optimization for Small-Rollout Reinforcement Learning](https://arxiv.org/abs/2601.22582)
*Youngeun Kim*

Main category: cs.LG

TL;DR: MC-GRPO提出用中位数基线替代均值基线来解决小样本训练中的优势符号翻转问题，在资源受限的小样本设置下显著提升稳定性和准确性。


<details>
  <summary>Details</summary>
Motivation: 在资源受限的小样本训练设置中，基于组相对策略优化的方法由于共享均值基线中的噪声会导致优势符号翻转问题，即某些样本获得错误的正负优势符号，从而反向更新梯度，导致准确性下降。

Method: 提出中位数中心化组相对策略优化（MC-GRPO），用中位数基线替代均值基线，中位数对异常奖励值不敏感。生成G+1个样本用于中位数参考，使用组中位数计算优势值。当组大小为奇数时，恰好有一个样本是中位数并获得零优势，将该枢轴样本从反向传播中排除，保持每个提示的梯度贡献样本数为G，维持标准G样本训练的核心更新成本。

Result: 在各种GRPO系列方法和不同规模模型上的实验表明，中位数中心化训练在小样本机制下持续提高稳定性和最终准确性，将G=2和G=8之间的性能差距缩小到1%以内。

Conclusion: MC-GRPO通过简单而有效的中位数基线替代均值基线，解决了小样本训练中的优势符号翻转问题，在资源受限的设置下显著提升了训练稳定性和模型性能。

Abstract: Group-relative policy optimization methods train language models by generating multiple rollouts per prompt and normalizing rewards with a shared mean reward baseline. In resource-constrained settings where the rollout budget is small, accuracy often degrades. We find that noise in the shared baseline induces advantage sign flips, where some rollouts receive an incorrect advantage sign, and the update direction is reversed. To address this, we propose Median-Centered Group Relative Policy Optimization (MC-GRPO), a simple and effective solution for small-rollout training. Our main idea is to replace the mean baseline with a median baseline: the median is far less sensitive to outlier rewards than the mean, mitigating the sign flips under small rollout size (G). We generate one additional rollout for median reference (G+1), and compute advantages by using the group median. With an odd-sized group, exactly one completion is the median and receives zero advantage, we exclude this pivot rollout from backpropagation so the number of gradient-contributing samples per prompt remains G, preserving the core update cost of standard G-rollout training. Across various GRPO-family methods and a wide range of models and scales, this median-centered training consistently improves stability and final accuracy in the low-rollout regime, reducing the gap between G=2 and G=8 to within 1%. Code is available at https://github.com/lotusroot-kim/MC-GRPO

</details>


### [62] [Heterogeneous Graph Alignment for Joint Reasoning and Interpretability](https://arxiv.org/abs/2601.22593)
*Zahra Moslemi,Ziyi Liang,Norbert Fortin,Babak Shahbaba*

Main category: cs.LG

TL;DR: MGMT是一个用于多图学习的统一框架，通过图Transformer编码器将不同图映射到共享潜在空间，构建元图实现跨图推理，在保持可解释性的同时提升预测性能。


<details>
  <summary>Details</summary>
Motivation: 多图学习面临的主要挑战是如何有效整合具有不同拓扑结构、规模和语义的异构图信息，特别是在缺乏共享节点标识的情况下。现有方法难以统一处理这些差异化的图数据。

Method: MGMT首先使用图Transformer编码器将每个图的结构和属性映射到共享潜在空间，然后通过注意力机制选择任务相关的超节点，基于潜在空间相似性构建连接跨图功能对齐超节点的元图，最后在元图上应用额外的图Transformer层进行联合推理。

Result: 在合成数据集和真实世界神经科学应用上的评估表明，MGMT在图级预测任务中持续优于现有最先进模型，同时提供了可解释的表示，有助于科学发现。

Conclusion: MGMT建立了一个统一的结构化多图学习框架，在图数据起核心作用的领域中推进了表示学习技术，为跨图推理提供了可扩展且可解释的解决方案。

Abstract: Multi-graph learning is crucial for extracting meaningful signals from collections of heterogeneous graphs. However, effectively integrating information across graphs with differing topologies, scales, and semantics, often in the absence of shared node identities, remains a significant challenge. We present the Multi-Graph Meta-Transformer (MGMT), a unified, scalable, and interpretable framework for cross-graph learning. MGMT first applies Graph Transformer encoders to each graph, mapping structure and attributes into a shared latent space. It then selects task-relevant supernodes via attention and builds a meta-graph that connects functionally aligned supernodes across graphs using similarity in the latent space. Additional Graph Transformer layers on this meta-graph enable joint reasoning over intra- and inter-graph structure. The meta-graph provides built-in interpretability: supernodes and superedges highlight influential substructures and cross-graph alignments. Evaluating MGMT on both synthetic datasets and real-world neuroscience applications, we show that MGMT consistently outperforms existing state-of-the-art models in graph-level prediction tasks while offering interpretable representations that facilitate scientific discoveries. Our work establishes MGMT as a unified framework for structured multi-graph learning, advancing representation techniques in domains where graph-based data plays a central role.

</details>


### [63] [Lethe:Adapter-Augmented Dual-Stream Update for Persistent Knowledge Erasure in Federated Unlearning](https://arxiv.org/abs/2601.22601)
*Hanwei Tan,Wentai Hu,Ligang He,Yijun Quan*

Main category: cs.LG

TL;DR: 联邦遗忘学习（FU）旨在从全局模型中删除指定客户端、类别或样本的知识。现有研究通常假设协作以遗忘操作为结束，忽略了联邦训练在剩余数据上继续的情况。研究发现存在"知识重现"问题，即继续训练会重新激活已遗忘的知识。为此提出Lethe方法，通过解耦待遗忘与待保留知识，确保在继续训练中持久删除。


<details>
  <summary>Details</summary>
Motivation: 现有联邦遗忘学习研究存在局限性，它们假设遗忘操作后协作即结束，而忽略了实际场景中联邦训练可能在剩余数据上继续的情况。研究发现这种继续训练会导致"知识重现"问题，即已删除的知识会重新出现在全局模型中，这使得现有方法的遗忘效果不持久。

Method: 提出Lethe方法，采用Reshape-Rectify-Restore三阶段流水线：1) 使用梯度上升在遗忘数据上训练临时适配器获得放大更新；2) 将这些更新作为校正信号，在两个流中对剩余更新进行分层校正；3) 移除适配器并在保留数据上进行短期恢复训练。该方法通过解耦待遗忘与待保留知识，确保持久删除。

Result: 实验表明Lethe能以统一方式支持联邦系统中所有级别的遗忘学习，在大多数情况下保持卓越的持久性（重现率<1%），即使在多轮后续训练后也能维持遗忘效果。

Conclusion: Lethe方法有效解决了联邦遗忘学习中的知识重现问题，通过解耦知识并采用三阶段流水线，实现了在继续训练场景下的持久遗忘，为联邦学习系统的隐私保护提供了更可靠的解决方案。

Abstract: Federated unlearning (FU) aims to erase designated client-level, class-level, or sample-level knowledge from a global model. Existing studies commonly assume that the collaboration ends up with the unlearning operation, overlooking the follow-up situation where the federated training continues over the remaining data.We identify a critical failure mode, termed Knowledge resurfacing, by revealing that continued training can re-activate unlearned knowledge and cause the removed influence to resurface in the global model. To address this, we propose Lethe, a novel federated unlearning method that de-correlates knowledge to be unlearned from knowledge to be retained, ensuring persistent erasure during continued training.Lethe follows a Reshape--Rectify--Restore pipeline: a temporary adapter is first trained with gradient ascent on the unlearning data to obtain magnified updates, which is then used as corrective signals to diverge layer-wise rectification on the remaining updates in two streams. Finally, the adapter is removed and a short recovery stage is performed on the retained data. Our experiments show that Lethe supports unlearning in the federated system at all levels in a unified manner and maintains superior persistence (Resurfacing Rate <1% in most cases) even after numerous rounds of follow-up training.

</details>


### [64] [GUDA: Counterfactual Group-wise Training Data Attribution for Diffusion Models via Unlearning](https://arxiv.org/abs/2601.22651)
*Naoki Murata,Yuhta Takida,Chieh-Hsin Lai,Toshimitsu Uesaka,Bac Nguyen,Stefano Ermon,Yuki Mitsufuji*

Main category: cs.LG

TL;DR: GUDA是一种针对扩散模型的群体级训练数据归因方法，通过机器遗忘技术近似反事实模型，相比从头训练的LOGO方法实现了100倍加速，能更可靠地识别主要贡献群体。


<details>
  <summary>Details</summary>
Motivation: 现有的训练数据归因方法主要针对单个样本评分，但实际应用中需要群体级别的答案（如艺术风格或对象类别）。群体级归因是反事实的：如果某个群体从训练数据中移除，模型在生成样本上的行为会如何变化？LOGO重训练方法虽然自然但计算成本过高。

Method: 提出GUDA方法，通过机器遗忘技术从共享的全数据模型中近似每个反事实模型，而不是从头训练。使用基于似然的评分规则（ELBO）在全模型和每个遗忘反事实模型之间的差异来量化群体影响。

Result: 在CIFAR-10和Stable Diffusion艺术风格归因实验中，GUDA比语义相似性、基于梯度的归因和实例级遗忘方法更可靠地识别主要贡献群体，同时在CIFAR-10上相比LOGO重训练实现了100倍加速。

Conclusion: GUDA提供了一种高效且有效的群体级训练数据归因方法，通过机器遗忘技术近似反事实模型，解决了LOGO重训练的计算瓶颈问题，为扩散模型的群体影响分析提供了实用工具。

Abstract: Training-data attribution for vision generative models aims to identify which training data influenced a given output. While most methods score individual examples, practitioners often need group-level answers (e.g., artistic styles or object classes). Group-wise attribution is counterfactual: how would a model's behavior on a generated sample change if a group were absent from training? A natural realization of this counterfactual is Leave-One-Group-Out (LOGO) retraining, which retrains the model with each group removed; however, it becomes computationally prohibitive as the number of groups grows. We propose GUDA (Group Unlearning-based Data Attribution) for diffusion models, which approximates each counterfactual model by applying machine unlearning to a shared full-data model instead of training from scratch. GUDA quantifies group influence using differences in a likelihood-based scoring rule (ELBO) between the full model and each unlearned counterfactual. Experiments on CIFAR-10 and artistic style attribution with Stable Diffusion show that GUDA identifies primary contributing groups more reliably than semantic similarity, gradient-based attribution, and instance-level unlearning approaches, while achieving x100 speedup on CIFAR-10 over LOGO retraining.

</details>


### [65] [Stabilizing Transformer Training Through Consensus](https://arxiv.org/abs/2601.22614)
*Shyam Venkatasubramanian,Sean Moushegian,Michael Lin,Mir Park,Ankit Singhal,Connor Lee*

Main category: cs.LG

TL;DR: 共识机制作为注意力机制的替代方案，能提升Transformer在不同学习率下的训练稳定性，并提出混合共识-注意力框架


<details>
  <summary>Details</summary>
Motivation: 标准注意力Transformer在高学习率下训练不稳定，现有方法多通过优化过程改进，但架构层面的创新探索不足

Method: 提出共识机制作为注意力的替代方案，将其建模为图模型，并设计混合共识-注意力框架，同时进行理论分析

Result: 在文本、DNA和蛋白质模态的学习率扫描实验中，共识机制显著提升了训练稳定性，混合框架在保持性能的同时改善稳定性

Conclusion: 共识机制是提升Transformer训练稳定性的有效架构创新，为高学习率下的稳定训练提供了新途径

Abstract: Standard attention-based transformers are known to exhibit instability under learning rate overspecification during training, particularly at high learning rates. While various methods have been proposed to improve resilience to such overspecification by modifying the optimization procedure, fundamental architectural innovations to this end remain underexplored. In this work, we illustrate that the consensus mechanism, a drop-in replacement for attention, stabilizes transformer training across a wider effective range of learning rates. We formulate consensus as a graphical model and provide extensive empirical analysis demonstrating improved stability across learning rate sweeps on text, DNA, and protein modalities. We further propose a hybrid consensus-attention framework that preserves performance while improving stability. We provide theoretical analysis characterizing the properties of consensus.

</details>


### [66] [Do Transformers Have the Ability for Periodicity Generalization?](https://arxiv.org/abs/2601.22690)
*Huanyu Liu,Ge Li,Yihong Dong,Sihan Wu,Peixu Wang,Sihao Cheng,Taozhi Chen,Kechi Zhang,Hao Zhu,Tongxuan Liu*

Main category: cs.LG

TL;DR: 该论文研究了Transformer模型在周期性OOD泛化方面的局限性，构建了复合周期性基准Coper，发现模型能记忆训练数据但无法泛化到未见复合周期性模式。


<details>
  <summary>Details</summary>
Motivation: 当前基于Transformer的大语言模型在分布外泛化方面仍存在显著局限性，与人类相比差距明显。作者通过周期性这一基本OOD场景来研究这一差距，周期性捕捉了变化中的不变性，是理解模型泛化能力的重要视角。

Method: 从抽象代数和推理角度提出周期性统一解释框架，包括单周期和复合周期性。构建了Coper基准测试，包含Hollow和Extrapolation两种OOD设置，用于评估模型在复合周期性泛化方面的能力。

Result: 实验表明Transformer在周期性泛化方面存在局限：模型能够在训练期间记忆周期性数据，但无法泛化到未见过的复合周期性模式。这揭示了模型在提取和泛化复杂周期性模式方面的根本性挑战。

Conclusion: Transformer模型在周期性OOD泛化方面存在显著不足，特别是在复合周期性场景下。研究为理解模型泛化局限性提供了新视角，并发布了源代码以支持未来研究。

Abstract: Large language models (LLMs) based on the Transformer have demonstrated strong performance across diverse tasks. However, current models still exhibit substantial limitations in out-of-distribution (OOD) generalization compared with humans. We investigate this gap through periodicity, one of the basic OOD scenarios. Periodicity captures invariance amid variation. Periodicity generalization represents a model's ability to extract periodic patterns from training data and generalize to OOD scenarios. We introduce a unified interpretation of periodicity from the perspective of abstract algebra and reasoning, including both single and composite periodicity, to explain why Transformers struggle to generalize periodicity. Then we construct Coper about composite periodicity, a controllable generative benchmark with two OOD settings, Hollow and Extrapolation. Experiments reveal that periodicity generalization in Transformers is limited, where models can memorize periodic data during training, but cannot generalize to unseen composite periodicity. We release the source code to support future research.

</details>


### [67] [Deep Learning-Based Early-Stage IR-Drop Estimation via CNN Surrogate Modeling](https://arxiv.org/abs/2601.22707)
*Ritesh Bhadana*

Main category: cs.LG

TL;DR: 提出基于深度学习的IR-drop早期预测方法，使用CNN将物理版图特征映射到IR-drop热图，实现毫秒级快速分析


<details>
  <summary>Details</summary>
Motivation: 传统IR-drop分析依赖物理签核工具，计算成本高且需要完整版图信息，不适合早期设计探索。需要快速、低成本的早期IR-drop评估方法

Method: 采用U-Net编码器-解码器架构，将IR-drop预测建模为密集像素回归问题。使用物理启发的合成数据集训练，包含电源网格结构、单元密度分布和开关活动等关键物理因素

Result: 模型能够准确预测IR-drop分布，推理时间达到毫秒级别，支持快速预签核筛选和迭代设计优化

Conclusion: 提出的深度学习框架可作为早期分析工具，在昂贵的签核分析前为设计者提供快速IR-drop洞察，代码和数据集已开源

Abstract: IR-drop is a critical power integrity challenge in modern VLSI designs that can cause timing degradation, reliability issues, and functional failures if not detected early in the design flow. Conventional IR-drop analysis relies on physics-based signoff tools, which provide high accuracy but incur significant computational cost and require near-final layout information, making them unsuitable for rapid early-stage design exploration. In this work, we propose a deep learning-based surrogate modeling approach for early-stage IR-drop estimation using a CNN. The task is formulated as a dense pixel-wise regression problem, where spatial physical layout features are mapped directly to IR-drop heatmaps. A U-Net-based encoder-decoder architecture with skip connections is employed to effectively capture both local and global spatial dependencies within the layout. The model is trained on a physics-inspired synthetic dataset generated by us, which incorporates key physical factors including power grid structure, cell density distribution, and switching activity. Model performance is evaluated using standard regression metrics such as Mean Squared Error (MSE) and Peak Signal-to-Noise Ratio (PSNR). Experimental results demonstrate that the proposed approach can accurately predict IR-drop distributions with millisecond-level inference time, enabling fast pre-signoff screening and iterative design optimization. The proposed framework is intended as a complementary early-stage analysis tool, providing designers with rapid IR-drop insight prior to expensive signoff analysis. The implementation, dataset generation scripts, and the interactive inference application are publicly available at: https://github.com/riteshbhadana/IR-Drop-Predictor. The live application can be accessed at: https://ir-drop-predictor.streamlit.app/.

</details>


### [68] [Pushing the Boundaries of Natural Reasoning: Interleaved Bonus from Formal-Logic Verification](https://arxiv.org/abs/2601.22642)
*Chuxue Cao,Jinluan Yang,Haoran Li,Kunhao Pan,Zijian Zhao,Zhengyu Chen,Yuchen Tian,Lijun Wu,Conghui He,Sirui Han,Yike Guo*

Main category: cs.LG

TL;DR: 该论文提出了一种结合形式逻辑验证与语言模型生成的新框架，通过实时验证反馈来纠正推理错误，显著提升了LLM的逻辑推理能力。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型表现出色，但其随机性的下一个token预测会导致逻辑不一致和奖励黑客问题，而形式符号系统可以避免这些问题。为了弥补这一差距，需要将形式逻辑验证与自然语言生成过程动态结合。

Method: 提出形式逻辑验证引导的框架，在自然语言生成过程中动态交织形式符号验证，提供实时反馈来检测和纠正错误。采用两阶段训练流程：形式逻辑验证引导的监督微调和策略优化，区别于以往被动的后验验证方法，主动惩罚推理链中的中间谬误。

Result: 在六个涵盖数学、逻辑和一般推理的基准测试中，7B和14B模型分别以平均10.4%和14.2%的优势超越了最先进的基线模型。

Conclusion: 形式验证可以作为一种可扩展的机制，显著推进高级LLM推理的性能边界，验证了将形式逻辑验证与语言模型生成相结合的有效性。

Abstract: Large Language Models (LLMs) show remarkable capabilities, yet their stochastic next-token prediction creates logical inconsistencies and reward hacking that formal symbolic systems avoid. To bridge this gap, we introduce a formal logic verification-guided framework that dynamically interleaves formal symbolic verification with the natural language generation process, providing real-time feedback to detect and rectify errors as they occur. Distinguished from previous neuro-symbolic methods limited by passive post-hoc validation, our approach actively penalizes intermediate fallacies during the reasoning chain. We operationalize this framework via a novel two-stage training pipeline that synergizes formal logic verification-guided supervised fine-tuning and policy optimization. Extensive evaluation on six benchmarks spanning mathematical, logical, and general reasoning demonstrates that our 7B and 14B models outperform state-of-the-art baselines by average margins of 10.4% and 14.2%, respectively. These results validate that formal verification can serve as a scalable mechanism to significantly push the performance boundaries of advanced LLM reasoning.

</details>


### [69] [Vision-Language Models Unlock Task-Centric Latent Actions](https://arxiv.org/abs/2601.22714)
*Alexander Nikulin,Ilya Zisman,Albina Klepach,Denis Tarasov,Alexander Derevyagin,Andrei Polubarov,Lyubaykin Nikita,Vladislav Kurenkov*

Main category: cs.LG

TL;DR: 提出使用视觉语言模型（VLMs）的常识推理能力为潜在动作模型（LAMs）提供可提示的表征，有效分离可控变化与噪声，显著提升在含干扰物环境中的性能


<details>
  <summary>Details</summary>
Motivation: 现有潜在动作模型（LAMs）在处理包含动作相关干扰物的观测时会失败，经常编码噪声而非有意义的潜在动作。而人类仅凭简短任务描述就能轻松区分视频中任务相关动作与无关细节

Method: 利用视觉语言模型（VLMs）的常识推理能力提供可提示的表征，在无监督方式下有效分离可控变化与噪声。将这些表征作为LAM训练的目标，并对多种流行的VLMs进行基准测试

Result: 发现不同VLMs提供的可提示表征质量存在显著差异，且对新提示和超参数的鲁棒性也不同。有趣的是，较新的VLMs可能比旧的表现更差。通过让VLMs忽略干扰物，可以大幅提升潜在动作质量，在Distracting MetaWorld上实现下游成功率高达六倍的增长

Conclusion: 利用VLMs的常识推理能力为LAMs提供可提示表征是一种有效方法，能够显著提升在含干扰物环境中的性能，但需要仔细选择VLM模型和提示策略

Abstract: Latent Action Models (LAMs) have rapidly gained traction as an important component in the pre-training pipelines of leading Vision-Language-Action models. However, they fail when observations contain action-correlated distractors, often encoding noise instead of meaningful latent actions. Humans, on the other hand, can effortlessly distinguish task-relevant motions from irrelevant details in any video given only a brief task description. In this work, we propose to utilize the common-sense reasoning abilities of Vision-Language Models (VLMs) to provide promptable representations, effectively separating controllable changes from the noise in unsupervised way. We use these representations as targets during LAM training and benchmark a wide variety of popular VLMs, revealing substantial variation in the quality of promptable representations as well as their robustness to different prompts and hyperparameters. Interestingly, we find that more recent VLMs may perform worse than older ones. Finally, we show that simply asking VLMs to ignore distractors can substantially improve latent action quality, yielding up to a six-fold increase in downstream success rates on Distracting MetaWorld.

</details>


### [70] [Breaking the Blocks: Continuous Low-Rank Decomposed Scaling for Unified LLM Quantization and Adaptation](https://arxiv.org/abs/2601.22716)
*Pingzhi Tang,Ruijie Zhou,Fanxu Meng,Wenjie Pei,Muhan Zhang*

Main category: cs.LG

TL;DR: LoRDS是一种通过低秩分解实现元素级量化的统一框架，在保持块级量化效率的同时提供更强的表达能力，显著提升LLM的量化精度和推理速度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM量化方法主要依赖块级结构来保持效率，但这限制了表示的灵活性。需要一种既能保持效率又能提供更强表达能力的量化方法。

Method: 提出低秩分解缩放(LoRDS)框架，将缩放流形建模为连续低秩矩阵(S=BA)，打破空间约束的块结构。通过低秩分解实现元素级量化，提供高保真PTQ初始化、权重和缩放因子的联合QAT，以及高秩乘法PEFT适配。

Result: 在Llama3-8B上，3位量化比NormalFloat量化精度提升27.0%，NVIDIA RTX 4090上推理速度提升1.5倍，下游任务PEFT性能比4位QLoRA提升9.6%。在各种模型家族中均优于现有基线方法。

Conclusion: LoRDS通过低秩分解实现元素级量化，在保持效率的同时提供更强的表达能力，为LLM的统一压缩和适配提供了稳健的集成解决方案。

Abstract: Current quantization methods for LLMs predominantly rely on block-wise structures to maintain efficiency, often at the cost of representational flexibility. In this work, we demonstrate that element-wise quantization can be made as efficient as block-wise scaling while providing strictly superior expressive power by modeling the scaling manifold as continuous low-rank matrices ($S = BA$). We propose Low-Rank Decomposed Scaling (LoRDS), a unified framework that rethinks quantization granularity through this low-rank decomposition. By "breaking the blocks" of spatial constraints, LoRDS establishes a seamless efficiency lifecycle: it provides high-fidelity PTQ initialization refined via iterative optimization, enables joint QAT of weights and scaling factors, and facilitates high-rank multiplicative PEFT adaptation. Unlike additive PEFT approaches such as QLoRA, LoRDS enables high-rank weight updates within a low-rank budget while incurring no additional inference overhead. Supported by highly optimized Triton kernels, LoRDS consistently outperforms state-of-the-art baselines across various model families in both quantization and downstream fine-tuning tasks. Notably, on Llama3-8B, our method achieves up to a 27.0% accuracy improvement at 3 bits over NormalFloat quantization and delivers a 1.5x inference speedup on NVIDIA RTX 4090 while enhancing PEFT performance by 9.6% on downstream tasks over 4bit QLoRA, offering a robust and integrated solution for unified compression and adaptation of LLMs.

</details>


### [71] [Beyond Fixed Rounds: Data-Free Early Stopping for Practical Federated Learning](https://arxiv.org/abs/2601.22669)
*Youngjoon Lee,Hyukjoon Lee,Seungrok Jung,Andy Luo,Jinu Gong,Yang Cao,Joonhyuk Kang*

Main category: cs.LG

TL;DR: 提出了一种无需验证数据的联邦学习早期停止框架，通过监控任务向量的增长率来确定最优停止点，仅使用服务器端参数，在皮肤病变和血细胞分类任务上表现优于基于验证数据的早期停止方法。


<details>
  <summary>Details</summary>
Motivation: 联邦学习虽然实现了去中心化协作学习，但依赖固定全局轮次或验证数据进行超参数调优会导致高计算成本和隐私风险，阻碍实际部署。

Method: 提出数据无关的早期停止框架，通过监控任务向量的增长率来确定最优停止点，仅使用服务器端参数，无需任何验证数据。

Result: 在皮肤病变和血细胞分类任务上，该方法与基于验证数据的早期停止效果相当，平均使用47/20轮次即可获得比验证数据方法高12.5%/10.3%的性能提升。

Conclusion: 这是首个无需验证数据的联邦学习早期停止框架，通过监控任务向量增长率有效解决了计算成本和隐私问题，具有实际部署价值。

Abstract: Federated Learning (FL) facilitates decentralized collaborative learning without transmitting raw data. However, reliance on fixed global rounds or validation data for hyperparameter tuning hinders practical deployment by incurring high computational costs and privacy risks. To address this, we propose a data-free early stopping framework that determines the optimal stopping point by monitoring the task vector's growth rate using solely server-side parameters. The numerical results on skin lesion/blood cell classification demonstrate that our approach is comparable to validation-based early stopping across various state-of-the-art FL methods. In particular, the proposed framework spends an average of 47/20 (skin lesion/blood cell) rounds to achieve over 12.5%/10.3% higher performance than early stopping based on validation data. To the best of our knowledge, this is the first work to propose an early stopping framework for FL methods without using any validation data.

</details>


### [72] [SOMBRERO: Measuring and Steering Boundary Placement in End-to-End Hierarchical Sequence Models](https://arxiv.org/abs/2601.22805)
*Pit Neitemeier,Alessio Serra,Jiaze Li,Sascha Wirges,Lukas Balles,Jan Hendrik Metzen*

Main category: cs.LG

TL;DR: Sombrero是一种改进的分层序列模型边界学习方法，通过边界富集度量B评估边界质量，并使用置信对齐边界损失引导边界放置到预测困难位置，在输入级别应用置信加权平滑稳定边界学习。


<details>
  <summary>Details</summary>
Motivation: 分层序列模型通过学习的分段压缩长字节序列以提高自回归建模效率，但现有方法难以定量评估和系统引导计算资源的分配位置。需要一种能够评估边界质量并引导边界放置到预测困难位置的系统方法。

Method: 提出边界富集度量B来量化边界质量，衡量块起始位置在具有高下一个字节意外性位置的集中程度。在此基础上提出Sombrero方法：1) 使用置信对齐边界损失引导边界放置到预测困难位置；2) 在输入级别而非已实现块上应用置信加权平滑来稳定边界学习。

Result: 在1B规模上，跨越包含英语和德语文本以及代码和数学内容的UTF-8语料库，Sombrero改善了准确性与效率的权衡，产生的边界更一致地将计算资源与难以预测的位置对齐。

Conclusion: Sombrero通过边界富集度量和置信对齐边界损失，能够系统引导分层序列模型中的边界放置，使计算资源更有效地分配到预测困难位置，从而改善模型性能与效率的平衡。

Abstract: Hierarchical sequence models replace fixed tokenization with learned segmentations that compress long byte sequences for efficient autoregressive modeling. While recent end-to-end methods can learn meaningful boundaries from the language-modeling objective alone, it remains difficult to quantitatively assess and systematically steer where compute is spent. We introduce a router-agnostic metric of boundary quality, boundary enrichment B, which measures how strongly chunk starts concentrate on positions with high next-byte surprisal. Guided by this metric, we propose Sombrero, which steers boundary placement toward predictive difficulty via a confidence-alignment boundary loss and stabilizes boundary learning by applying confidence-weighted smoothing at the input level rather than on realized chunks. On 1B scale, across UTF-8 corpora covering English and German text as well as code and mathematical content, Sombrero improves the accuracy-efficiency trade-off and yields boundaries that more consistently align compute with hard-to-predict positions.

</details>


### [73] [Stabilizing Consistency Training: A Flow Map Analysis and Self-Distillation](https://arxiv.org/abs/2601.22679)
*Youngjoong Kim,Duhoe Kim,Woosung Kim,Jaesik Park*

Main category: cs.LG

TL;DR: 本文从流映射角度理论分析一致性模型，揭示其训练不稳定和收敛问题的根源，提出改进的自蒸馏方法，并扩展到扩散策略学习领域。


<details>
  <summary>Details</summary>
Motivation: 一致性模型虽然能实现快速生成建模，但存在固有的不稳定性和有限的可复现性，现有解释分散且理论关系不明确，需要系统性的理论分析来理解这些问题。

Method: 从流映射角度对一致性模型进行理论分析，重新审视自蒸馏作为解决次优收敛的实用方法，并重新形式化以避免过大的梯度范数，确保稳定优化。

Result: 理论分析阐明了训练稳定性和收敛行为如何导致退化解，改进的自蒸馏方法能有效解决某些形式的次优收敛问题，且该方法可扩展到扩散策略学习，无需预训练扩散模型初始化。

Conclusion: 通过流映射视角的理论分析为一致性模型的训练不稳定问题提供了清晰解释，改进的自蒸馏方法能稳定优化并具有更广泛的适用性，包括扩散策略学习等应用领域。

Abstract: Consistency models have been proposed for fast generative modeling, achieving results competitive with diffusion and flow models. However, these methods exhibit inherent instability and limited reproducibility when training from scratch, motivating subsequent work to explain and stabilize these issues. While these efforts have provided valuable insights, the explanations remain fragmented, and the theoretical relationships remain unclear. In this work, we provide a theoretical examination of consistency models by analyzing them from a flow map-based perspective. This joint analysis clarifies how training stability and convergence behavior can give rise to degenerate solutions. Building on these insights, we revisit self-distillation as a practical remedy for certain forms of suboptimal convergence and reformulate it to avoid excessive gradient norms for stable optimization. We further demonstrate that our strategy extends beyond image generation to diffusion-based policy learning, without reliance on a pretrained diffusion model for initialization, thereby illustrating its broader applicability.

</details>


### [74] [User-Adaptive Meta-Learning for Cold-Start Medication Recommendation with Uncertainty Filtering](https://arxiv.org/abs/2601.22820)
*Arya Hadizadeh Moghaddam,Mohsen Nayebi Kerdabadi,Dongjie Wang,Mei Liu,Zijun Yao*

Main category: cs.LG

TL;DR: MetaDrug是一个多级不确定性感知元学习框架，用于解决药物推荐中的患者冷启动问题，通过两级元适应机制和不确定性量化来提升新患者的推荐效果。


<details>
  <summary>Details</summary>
Motivation: 现有药物推荐方法面临患者冷启动问题，即新患者因缺乏足够的处方历史而难以获得可靠的个性化推荐。虽然已有研究利用医学知识图谱缓解项目冷启动问题，但未能充分适应个体患者特征。元学习在处理稀疏交互的新用户方面有潜力，但在EHR数据中的序列结构应用仍待探索。

Method: 提出MetaDrug框架，包含：1）两级元适应机制：自我适应（利用患者自身医疗事件作为支持集捕捉时间依赖）和同伴适应（利用相似患者的就诊记录丰富新患者表示）；2）不确定性量化模块：对支持就诊进行排序并过滤无关信息以确保适应一致性。

Result: 在MIMIC-III和急性肾损伤（AKI）数据集上的实验结果表明，MetaDrug在冷启动患者上持续优于最先进的药物推荐方法。

Conclusion: MetaDrug通过多级元适应和不确定性量化有效解决了药物推荐中的患者冷启动问题，为基于EHR的临床决策支持提供了更可靠的个性化推荐方案。

Abstract: Large-scale Electronic Health Record (EHR) databases have become indispensable in supporting clinical decision-making through data-driven treatment recommendations. However, existing medication recommender methods often struggle with a user (i.e., patient) cold-start problem, where recommendations for new patients are usually unreliable due to the lack of sufficient prescription history for patient profiling. While prior studies have utilized medical knowledge graphs to connect medication concepts through pharmacological or chemical relationships, these methods primarily focus on mitigating the item cold-start issue and fall short in providing personalized recommendations that adapt to individual patient characteristics. Meta-learning has shown promise in handling new users with sparse interactions in recommender systems. However, its application to EHRs remains underexplored due to the unique sequential structure of EHR data. To tackle these challenges, we propose MetaDrug, a multi-level, uncertainty-aware meta-learning framework designed to address the patient cold-start problem in medication recommendation. MetaDrug proposes a novel two-level meta-adaptation mechanism, including self-adaptation, which adapts the model to new patients using their own medical events as support sets to capture temporal dependencies; and peer-adaptation, which adapts the model using similar visits from peer patients to enrich new patient representations. Meanwhile, to further improve meta-adaptation outcomes, we introduce an uncertainty quantification module that ranks the support visits and filters out the unrelated information for adaptation consistency. We evaluate our approach on the MIMIC-III and Acute Kidney Injury (AKI) datasets. Experimental results on both datasets demonstrate that MetaDrug consistently outperforms state-of-the-art medication recommendation methods on cold-start patients.

</details>


### [75] [Offline Reinforcement Learning of High-Quality Behaviors Under Robust Style Alignment](https://arxiv.org/abs/2601.22823)
*Mathieu Petitbois,Rémy Portelas,Sylvain Lamprier*

Main category: cs.LG

TL;DR: 提出Style-Conditioned Implicit Q-Learning (SCIQL)方法，用于离线强化学习中风格条件策略的学习，通过显式风格监督和门控优势加权回归机制，在保持风格对齐的同时优化任务性能。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习中风格条件策略学习面临分布偏移和风格与奖励之间固有冲突的挑战，现有方法难以有效协调这两个目标。

Method: 提出统一的行为风格定义，并基于此构建SCIQL框架，结合离线目标条件RL技术（如后见重标注和价值学习），引入门控优势加权回归机制来优化任务性能同时保持风格对齐。

Result: 实验表明SCIQL在任务性能和风格对齐两个目标上都优于现有的离线方法。

Conclusion: SCIQL为离线强化学习中的风格条件策略学习提供了一个有效的统一框架，能够同时优化任务性能和风格对齐。

Abstract: We study offline reinforcement learning of style-conditioned policies using explicit style supervision via subtrajectory labeling functions. In this setting, aligning style with high task performance is particularly challenging due to distribution shift and inherent conflicts between style and reward. Existing methods, despite introducing numerous definitions of style, often fail to reconcile these objectives effectively. To address these challenges, we propose a unified definition of behavior style and instantiate it into a practical framework. Building on this, we introduce Style-Conditioned Implicit Q-Learning (SCIQL), which leverages offline goal-conditioned RL techniques, such as hindsight relabeling and value learning, and combine it with a new Gated Advantage Weighted Regression mechanism to efficiently optimize task performance while preserving style alignment. Experiments demonstrate that SCIQL achieves superior performance on both objectives compared to prior offline methods. Code, datasets and visuals are available in: https://sciql-iclr-2026.github.io/.

</details>


### [76] [Metric Hub: A metric library and practical selection workflow for use-case-driven data quality assessment in medical AI](https://arxiv.org/abs/2601.22702)
*Katinka Becker,Maximilian P. Oppelt,Tobias S. Zech,Martin Seyferth,Sandie Cabon,Vanja Miskovic,Ivan Cimrak,Michal Kozubek,Giuseppe D'Avenio,Ilaria Campioni,Jana Fehr,Kanjar De,Ismail Mahmoudi,Emilio Dolgener Cantu,Laurenz Ottmann,Andreas Klaß,Galaad Altares,Jackie Ma,Alireza Salehi M.,Nadine R. Lang-Richter,Tobias Schaeffter,Daniel Schwabe*

Main category: cs.LG

TL;DR: 提出METRIC框架的实践化实现，通过建立数据质量指标库和指标卡片，为医疗AI提供可操作的数据质量评估工具，以支持可信AI在医学中的应用。


<details>
  <summary>Details</summary>
Motivation: 医疗AI从研究走向实际应用，需要建立可信性证据。数据质量评估是开发可信AI的关键因素，但现有理论框架缺乏实践工具。

Method: 将理论性的METRIC框架操作化，创建数据质量指标库，为每个指标提供包含定义、适用性、示例、陷阱和建议的指标卡片，并提供基于特定用例选择适当指标的决策树策略。

Result: 在PTB-XL心电图数据集上展示了该方法的影响，为实践中评估训练和测试数据的适用性提供了第一步工具。

Conclusion: 这项工作为医疗AI实践中实现"适合目的"的数据质量评估奠定了基础，是建立医学中可信AI的重要步骤。

Abstract: Machine learning (ML) in medicine has transitioned from research to concrete applications aimed at supporting several medical purposes like therapy selection, monitoring and treatment. Acceptance and effective adoption by clinicians and patients, as well as regulatory approval, require evidence of trustworthiness. A major factor for the development of trustworthy AI is the quantification of data quality for AI model training and testing. We have recently proposed the METRIC-framework for systematically evaluating the suitability (fit-for-purpose) of data for medical ML for a given task. Here, we operationalize this theoretical framework by introducing a collection of data quality metrics - the metric library - for practically measuring data quality dimensions. For each metric, we provide a metric card with the most important information, including definition, applicability, examples, pitfalls and recommendations, to support the understanding and implementation of these metrics. Furthermore, we discuss strategies and provide decision trees for choosing an appropriate set of data quality metrics from the metric library given specific use cases. We demonstrate the impact of our approach exemplarily on the PTB-XL ECG-dataset. This is a first step to enable fit-for-purpose evaluation of training and test data in practice as the base for establishing trustworthy AI in medicine.

</details>


### [77] [Perplexity Cannot Always Tell Right from Wrong](https://arxiv.org/abs/2601.22950)
*Petar Veličković,Federico Barbero,Christos Perivolaropoulos,Simon Osindero,Razvan Pascanu*

Main category: cs.LG

TL;DR: 论文证明困惑度作为模型选择指标存在理论缺陷：如果Transformer模型能准确预测某些序列，则必然存在其他序列具有很低困惑度但模型预测错误，且困惑度不一定选择更准确的模型。


<details>
  <summary>Details</summary>
Motivation: 困惑度作为模型质量评估指标被广泛使用，但先前研究主要从经验角度指出其局限性。本文旨在从理论层面严格证明困惑度作为模型选择指标的不适定性。

Method: 利用Transformer连续性理论，通过数学证明展示困惑度的理论缺陷。首先证明如果紧凑的仅解码器Transformer模型能准确自信地预测某些序列，则必然存在其他序列具有很低困惑度但模型预测错误。其次通过分析等困惑度图，研究困惑度与模型选择的关系。

Result: 理论证明表明：1）存在低困惑度但预测错误的序列是必然的；2）困惑度不一定选择更准确的模型，只有当模型置信度提升伴随相应准确度提升时，新模型才会被选择。

Conclusion: 困惑度作为模型选择指标存在根本性理论缺陷，不适合用于模型选择。模型置信度的提升必须伴随准确度的相应提升，困惑度才能有效选择更好的模型。

Abstract: Perplexity -- a function measuring a model's overall level of "surprise" when encountering a particular output -- has gained significant traction in recent years, both as a loss function and as a simple-to-compute metric of model quality. Prior studies have pointed out several limitations of perplexity, often from an empirical manner. Here we leverage recent results on Transformer continuity to show in a rigorous manner how perplexity may be an unsuitable metric for model selection. Specifically, we prove that, if there is any sequence that a compact decoder-only Transformer model predicts accurately and confidently -- a necessary pre-requisite for strong generalisation -- it must imply existence of another sequence with very low perplexity, but not predicted correctly by that same model. Further, by analytically studying iso-perplexity plots, we find that perplexity will not always select for the more accurate model -- rather, any increase in model confidence must be accompanied by a commensurate rise in accuracy for the new model to be selected.

</details>


### [78] [Mano: Restriking Manifold Optimization for LLM Training](https://arxiv.org/abs/2601.23000)
*Yufei Gu,Zeke Xie*

Main category: cs.LG

TL;DR: 本文提出了一种名为Mano的新型优化器，通过将动量投影到模型参数的切空间并约束在旋转斜流形上，首次弥合了流形优化与现代优化器之间的性能差距，在LLaMA和Qwen3模型上显著优于AdamW和Muon，同时减少了内存消耗和计算复杂度。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型训练的计算成本高昂，现有优化器如AdamW忽略结构特性，Muon虽然进行全局谱归一化但损失了曲率信息，而传统的流形优化方法在大规模模型优化中表现不佳，需要一种能结合两者优势的新方法。

Method: 提出Mano优化器，创新性地将动量投影到模型参数的切空间，并将其约束在旋转斜流形上，从而有效结合了曲率信息和结构特性，实现了流形优化与现代优化技术的融合。

Result: 在LLaMA和Qwen3模型上的广泛实验表明，Mano在性能上持续且显著优于AdamW和Muon，同时分别减少了内存消耗和计算复杂度，扩展了空间和时间效率的帕累托前沿。

Conclusion: Mano是首个成功弥合流形优化与现代优化器性能差距的优化器，为大语言模型训练提供了更高效、更强大的优化解决方案，在计算效率和性能之间实现了更好的平衡。

Abstract: While large language models (LLMs) have emerged as a significant advancement in artificial intelligence, the hardware and computational costs for training LLMs are also significantly burdensome. Among the state-of-the-art optimizers, AdamW relies on diagonal curvature estimates and ignores structural properties, while Muon applies global spectral normalization at the expense of losing curvature information. In this study, we restriked manifold optimization methods for training LLMs, which may address both optimizers' limitations, while conventional manifold optimization methods have been largely overlooked due to the poor performance in large-scale model optimization. By innovatively projecting the momentum onto the tangent space of model parameters and constraining it on a rotational Oblique manifold, we propose a novel, powerful, and efficient optimizer **Mano** that is the first to bridge the performance gap between manifold optimization and modern optimizers. Extensive experiments on the LLaMA and Qwen3 models demonstrate that Mano consistently and significantly outperforms AdamW and Muon even with less memory consumption and computational complexity, respectively, suggesting an expanded Pareto frontier in terms of space and time efficiency.

</details>


### [79] [Automatic Constraint Policy Optimization based on Continuous Constraint Interpolation Framework for Offline Reinforcement Learning](https://arxiv.org/abs/2601.23010)
*Xinchen Han,Qiuyang Fang,Hossam Afifi,Michel Marot*

Main category: cs.LG

TL;DR: 提出连续约束插值(CCI)框架统一离线RL中的三种约束方法，并开发自动约束策略优化(ACPO)算法，在多个基准上实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有离线强化学习方法各自采用不同的约束形式（加权行为克隆、密度正则化、支持约束），缺乏统一的原理来解释它们的联系和权衡，限制了方法的通用性和性能。

Method: 提出连续约束插值(CCI)框架，通过单一插值参数实现三种约束类型的平滑过渡和组合；基于CCI开发自动约束策略优化(ACPO)算法，使用拉格朗日对偶更新自适应调整插值参数。

Result: 在D4RL和NeoRL2基准测试中表现出稳健的性能提升，整体达到最先进的性能水平。

Conclusion: CCI框架为离线RL中的约束方法提供了统一的理论基础，ACPO算法通过自适应约束插值实现了优越的性能，为离线RL研究提供了新的方向。

Abstract: Offline Reinforcement Learning (RL) relies on policy constraints to mitigate extrapolation error, where both the constraint form and constraint strength critically shape performance. However, most existing methods commit to a single constraint family: weighted behavior cloning, density regularization, or support constraints, without a unified principle that explains their connections or trade-offs. In this work, we propose Continuous Constraint Interpolation (CCI), a unified optimization framework in which these three constraint families arise as special cases along a common constraint spectrum. The CCI framework introduces a single interpolation parameter that enables smooth transitions and principled combinations across constraint types. Building on CCI, we develop Automatic Constraint Policy Optimization (ACPO), a practical primal--dual algorithm that adapts the interpolation parameter via a Lagrangian dual update. Moreover, we establish a maximum-entropy performance difference lemma and derive performance lower bounds for both the closed-form optimal policy and its parametric projection. Experiments on D4RL and NeoRL2 demonstrate robust gains across diverse domains, achieving state-of-the-art performance overall.

</details>


### [80] [Leveraging Convolutional Sparse Autoencoders for Robust Movement Classification from Low-Density sEMG](https://arxiv.org/abs/2601.23011)
*Blagoj Hristov,Zoran Hadzi-Velkov,Katerina Hadzi-Velkova Saneva,Gorjan Nadzinski,Vesna Ojleska Latkoska*

Main category: cs.LG

TL;DR: 提出基于两个表面肌电通道的深度学习框架，用于肌电假肢手势识别，实现高精度且减少传感器需求


<details>
  <summary>Details</summary>
Motivation: 解决肌电假肢控制中受试者间差异大和高密度传感器阵列临床不实用的问题，开发低成本、可扩展的解决方案

Method: 使用卷积稀疏自编码器从原始信号提取时间特征，无需启发式特征工程；采用少样本迁移学习处理个体差异；支持增量学习扩展功能

Result: 6类手势多受试者F1分数94.3%±0.3%；少样本迁移学习将未见受试者性能从35.1%±3.1%提升至92.3%±0.9%；增量学习扩展到10类手势达到90.0%±0.2% F1分数

Conclusion: 该框架结合高精度与最小计算和传感器开销，为下一代经济实惠、自适应的假肢系统提供了可扩展且高效的方法

Abstract: Reliable control of myoelectric prostheses is often hindered by high inter-subject variability and the clinical impracticality of high-density sensor arrays. This study proposes a deep learning framework for accurate gesture recognition using only two surface electromyography (sEMG) channels. The method employs a Convolutional Sparse Autoencoder (CSAE) to extract temporal feature representations directly from raw signals, eliminating the need for heuristic feature engineering. On a 6-class gesture set, our model achieved a multi-subject F1-score of 94.3% $\pm$ 0.3%. To address subject-specific differences, we present a few-shot transfer learning protocol that improved performance on unseen subjects from a baseline of 35.1% $\pm$ 3.1% to 92.3% $\pm$ 0.9% with minimal calibration data. Furthermore, the system supports functional extensibility through an incremental learning strategy, allowing for expansion to a 10-class set with a 90.0% $\pm$ 0.2% F1-score without full model retraining. By combining high precision with minimal computational and sensor overhead, this framework provides a scalable and efficient approach for the next generation of affordable and adaptive prosthetic systems.

</details>


### [81] [Avoiding Premature Collapse: Adaptive Annealing for Entropy-Regularized Structural Inference](https://arxiv.org/abs/2601.23039)
*Yizhi Liu*

Main category: cs.LG

TL;DR: 论文分析了可微匹配层中Sinkhorn算法在退火过程中不稳定的根本原因——过早模式崩溃，并提出了一种自适应调度算法来解决这个问题。


<details>
  <summary>Details</summary>
Motivation: 可微匹配层（通常通过熵正则化最优传输实现）是结构化预测中的关键近似推理机制，但通过退火ε→0恢复离散排列的过程极不稳定。作者旨在揭示这种失败的根本机制并提出解决方案。

Method: 通过分析Sinkhorn固定点映射的非正规动力学，揭示了热力学速度限制理论。提出Efficient PH-ASC自适应调度算法，通过监控推理过程的稳定性并强制执行线性稳定性定律，将昂贵的谱诊断与训练循环解耦。

Result: 在标准指数冷却下，目标后验的偏移（O(1)）超过了推理算子的收缩率（O(1/ε)），导致推理轨迹不可避免地陷入虚假局部盆地。提出的算法将开销从O(N³)降低到摊销O(1)。

Conclusion: 论文识别了可微匹配层中退火不稳定的根本原因——过早模式崩溃，并提出了一种高效的自适应调度算法，通过稳定性监控解决了这一问题，显著降低了计算开销。

Abstract: Differentiable matching layers, often implemented via entropy-regularized Optimal Transport, serve as a critical approximate inference mechanism in structural prediction. However, recovering discrete permutations via annealing $ε\to 0$ is notoriously unstable. We identify a fundamental mechanism for this failure: \textbf{Premature Mode Collapse}. By analyzing the non-normal dynamics of the Sinkhorn fixed-point map, we reveal a theoretical \textbf{thermodynamic speed limit}. Under standard exponential cooling, the shift in the target posterior ($O(1)$) outpaces the contraction rate of the inference operator, which degrades as $O(1/ε)$. This mismatch inevitably forces the inference trajectory into spurious local basins. To address this, we propose \textbf{Efficient PH-ASC}, an adaptive scheduling algorithm that monitors the stability of the inference process. By enforcing a linear stability law, we decouple expensive spectral diagnostics from the training loop, reducing overhead from $O(N^3)$ to amortized $O(1)$. Our implementation and interactive demo are available at https://github.com/xxx0438/torch-sinkhorn-asc and https://huggingface.co/spaces/leon0923/torch-sinkhorn-asc-demo. bounded away from zero in generic training dynamics unless the feature extractor converges unrealistically fast.

</details>


### [82] [Adaptive Edge Learning for Density-Aware Graph Generation](https://arxiv.org/abs/2601.23052)
*Seyedeh Ava Razi Razavi,James Sargant,Sheridan Houghten,Renata Dividino*

Main category: cs.LG

TL;DR: 提出了一种基于Wasserstein GAN的密度感知条件图生成框架，用可学习的基于距离的边缘预测器替代随机采样，能生成结构更连贯、类别一致的图数据。


<details>
  <summary>Details</summary>
Motivation: 传统图生成方法通常依赖固定概率的随机边缘采样，难以捕捉节点间复杂的结构依赖关系，限制了生成图的结构质量和类别特定连接模式。

Method: 使用Wasserstein GAN框架，将节点嵌入到潜在空间，通过可微的边缘预测器从节点嵌入直接确定成对关系，并采用密度感知选择机制自适应控制边缘密度以匹配真实图的类别特定稀疏分布。

Result: 在基准数据集上的实验表明，该方法生成的图在结构连贯性和类别一致性连接方面优于现有基线，学习的边缘预测器能捕捉超越简单启发式的复杂关系模式。

Conclusion: 该方法提高了训练稳定性并实现了可控合成，为现实图生成和数据增强提供了有效框架，源代码已公开。

Abstract: Generating realistic graph-structured data is challenging due to discrete structures, variable sizes, and class-specific connectivity patterns that resist conventional generative modelling. While recent graph generation methods employ generative adversarial network (GAN) frameworks to handle permutation invariance and irregular topologies, they typically rely on random edge sampling with fixed probabilities, limiting their capacity to capture complex structural dependencies between nodes. We propose a density-aware conditional graph generation framework using Wasserstein GANs (WGAN) that replaces random sampling with a learnable distance-based edge predictor. Our approach embeds nodes into a latent space where proximity correlates with edge likelihood, enabling the generator to learn meaningful connectivity patterns. A differentiable edge predictor determines pairwise relationships directly from node embeddings, while a density-aware selection mechanism adaptively controls edge density to match class-specific sparsity distributions observed in real graphs. We train the model using a WGAN with gradient penalty, employing a GCN-based critic to ensure generated graphs exhibit realistic topology and align with target class distributions. Experiments on benchmark datasets demonstrate that our method produces graphs with superior structural coherence and class-consistent connectivity compared to existing baselines. The learned edge predictor captures complex relational patterns beyond simple heuristics, generating graphs whose density and topology closely match real structural distributions. Our results show improved training stability and controllable synthesis, making the framework effective for realistic graph generation and data augmentation. Source code is publicly available at https://github.com/ava-12/Density_Aware_WGAN.git.

</details>


### [83] [Is Softmax Loss All You Need? A Principled Analysis of Softmax-family Loss](https://arxiv.org/abs/2601.22745)
*Yuanhao Pu,Defu Lian,Enhong Chen*

Main category: cs.LG

TL;DR: 论文系统研究了Softmax系列损失函数，分析了不同替代损失在分类和排序任务中的一致性、梯度动态和收敛行为，提出了近似方法的偏差-方差分解框架，并在大规模类别任务中验证了理论结果与实证性能的一致性。


<details>
  <summary>Details</summary>
Motivation: 尽管Softmax损失在分类和排序任务中被广泛使用，但现有研究存在两个分离的视角：Fenchel-Young框架从理论角度分析其作为替代损失的性质，而另一类研究关注大规模类别下的计算效率。本文旨在整合这两个视角，为Softmax系列损失函数建立统一的理论基础。

Method: 1) 分析不同替代损失函数在分类和排序度量下的一致性；2) 研究梯度动态以揭示不同的收敛行为；3) 为近似方法提出系统的偏差-方差分解框架，提供收敛保证；4) 进行每轮复杂度分析，展示效果与效率之间的权衡；5) 在代表性任务上进行大量实验验证理论分析。

Result: 实验结果表明，一致性、收敛性和实证性能之间存在强相关性。研究为近似方法提供了收敛保证，并通过复杂度分析明确了效果与效率之间的权衡关系。这些结果为大规模类别机器学习应用中的损失函数选择提供了理论基础和实践指导。

Conclusion: 本文建立了Softmax系列损失函数的统一理论框架，揭示了不同替代损失的理论性质与实证性能之间的关系。研究结果为大规模类别机器学习应用中的损失函数选择提供了原则性基础和实用指导，填补了理论分析与计算效率之间的研究空白。

Abstract: The Softmax loss is one of the most widely employed surrogate objectives for classification and ranking tasks. To elucidate its theoretical properties, the Fenchel-Young framework situates it as a canonical instance within a broad family of surrogates. Concurrently, another line of research has addressed scalability when the number of classes is exceedingly large, in which numerous approximations have been proposed to retain the benefits of the exact objective while improving efficiency. Building on these two perspectives, we present a principled investigation of the Softmax-family losses. We examine whether different surrogates achieve consistency with classification and ranking metrics, and analyze their gradient dynamics to reveal distinct convergence behaviors. We also introduce a systematic bias-variance decomposition for approximate methods that provides convergence guarantees, and further derive a per-epoch complexity analysis, showing explicit trade-offs between effectiveness and efficiency. Extensive experiments on a representative task demonstrate a strong alignment between consistency, convergence, and empirical performance. Together, these results establish a principled foundation and offer practical guidance for loss selections in large-class machine learning applications.

</details>


### [84] [To See Far, Look Close: Evolutionary Forecasting for Long-term Time Series](https://arxiv.org/abs/2601.23114)
*Jiaming Ma,Siyuan Mu,Ruilin Tang,Haofeng Ma,Qihe Huang,Zhengyang Zhou,Pengkun Wang,Binwu Wang,Yang Wang*

Main category: cs.LG

TL;DR: 论文提出进化预测（EF）范式，通过短时域训练结合进化推理，解决了直接预测（DF）中长期时间序列预测中的优化异常问题，显著超越传统方法。


<details>
  <summary>Details</summary>
Motivation: 当前主导的直接预测（DF）范式在长期时间序列预测中存在根本性优化病理：远距离未来的冲突梯度会破坏局部动态的学习，且需要为每个目标时域重新训练模型，计算成本高昂。

Method: 提出进化预测（EF）范式，将DF视为EF的退化特例。EF作为统一的生成框架，通过短时域训练模型，然后利用进化推理机制进行长期预测，避免了DF中的梯度冲突问题。

Result: 实验表明，单个EF模型在标准基准测试中超越了任务特定的DF集成方法，并在极端外推中表现出稳健的渐近稳定性。短时域训练的EF模型显著优于直接长时域训练的模型。

Conclusion: 该工作推动了长期时间序列预测的范式转变：从被动的静态映射转向自主的进化推理，为时间序列预测提供了更高效、更稳定的统一框架。

Abstract: The prevailing Direct Forecasting (DF) paradigm dominates Long-term Time Series Forecasting (LTSF) by forcing models to predict the entire future horizon in a single forward pass. While efficient, this rigid coupling of output and evaluation horizons necessitates computationally prohibitive re-training for every target horizon. In this work, we uncover a counter-intuitive optimization anomaly: models trained on short horizons-when coupled with our proposed Evolutionary Forecasting (EF) paradigm-significantly outperform those trained directly on long horizons. We attribute this success to the mitigation of a fundamental optimization pathology inherent in DF, where conflicting gradients from distant futures cripple the learning of local dynamics. We establish EF as a unified generative framework, proving that DF is merely a degenerate special case of EF. Extensive experiments demonstrate that a singular EF model surpasses task-specific DF ensembles across standard benchmarks and exhibits robust asymptotic stability in extreme extrapolation. This work propels a paradigm shift in LTSF: moving from passive Static Mapping to autonomous Evolutionary Reasoning.

</details>


### [85] [OSNIP: Breaking the Privacy-Utility-Efficiency Trilemma in LLM Inference via Obfuscated Semantic Null Space](https://arxiv.org/abs/2601.22752)
*Zhiyuan Cao,Zeyu Ma,Chenhao Yang,Han Zheng,Mingang Chen*

Main category: cs.LG

TL;DR: OSNIP是一个轻量级的客户端加密框架，通过在高维潜在空间中创建"混淆语义零空间"，注入扰动来保护LLM推理隐私，同时保持语义保真度。


<details>
  <summary>Details</summary>
Motivation: 当前LLM推理中的隐私保护方法通常需要复杂的后处理或牺牲模型性能，需要一种轻量级、客户端侧的方法来保护用户隐私，同时保持语义保真度和模型效用。

Method: OSNIP将线性核的几何直觉推广到LLM的高维潜在空间，定义"混淆语义零空间"——一个保持语义保真度同时与原始嵌入近乎正交的高维区域。通过注入将原始嵌入投影到该空间的扰动，并采用密钥相关的随机映射为每个用户生成独特的扰动轨迹。

Result: 在12个生成和分类基准测试中，OSNIP实现了最先进的性能，显著降低了攻击成功率，同时在严格的安全约束下保持了强大的模型效用。

Conclusion: OSNIP提供了一个有效的隐私保护LLM推理框架，通过混淆语义零空间注入实现了隐私保护与模型性能的良好平衡，无需后处理即可确保隐私安全。

Abstract: We propose Obfuscated Semantic Null space Injection for Privacy (OSNIP), a lightweight client-side encryption framework for privacy-preserving LLM inference. Generalizing the geometric intuition of linear kernels to the high-dimensional latent space of LLMs, we formally define the ``Obfuscated Semantic Null Space'', a high-dimensional regime that preserves semantic fidelity while enforcing near-orthogonality to the original embedding. By injecting perturbations that project the original embedding into this space, OSNIP ensures privacy without any post-processing. Furthermore, OSNIP employs a key-dependent stochastic mapping that synthesizes individualized perturbation trajectories unique to each user. Evaluations on 12 generative and classification benchmarks show that OSNIP achieves state-of-the-art performance, sharply reducing attack success rates while maintaining strong model utility under strict security constraints.

</details>


### [86] [Unveiling Scaling Behaviors in Molecular Language Models: Effects of Model Size, Data, and Representation](https://arxiv.org/abs/2601.22757)
*Dong Xu,Qihua Pan,Sisi Yuan,Jianqiang Li,Zexuan Zhu,Junkai Ji*

Main category: cs.LG

TL;DR: 该研究系统性地研究了分子语言模型在预训练和下游任务中的缩放规律，通过训练300个模型和超过10,000次实验，揭示了分子表示对性能的重要影响，并解释了之前观察到的分子生成缩放行为不一致性。


<details>
  <summary>Details</summary>
Motivation: 分子生成模型（通常使用GPT风格的语言模型处理分子字符串表示）在大规模数据集和模型尺寸下表现出良好性能，但尚不清楚这些模型在固定计算预算下是否遵循可预测的缩放规律。理解这一规律对于在模型大小、数据量和分子表示之间优化资源配置至关重要。

Method: 研究者训练了300个模型，进行了超过10,000次实验，严格控制计算预算，同时独立变化模型大小、训练标记数量和分子表示，系统性地研究了分子语言模型的缩放行为。

Result: 研究结果表明分子模型在预训练和下游迁移中都存在清晰的缩放规律，揭示了分子表示对性能的显著影响，并解释了之前观察到的分子生成缩放行为不一致性。同时公开了迄今为止最大的分子语言模型库。

Conclusion: 分子语言模型确实遵循可预测的缩放规律，分子表示对性能有重要影响，这一发现为优化分子生成模型的资源配置提供了重要指导。公开的模型库将促进未来研究发展。

Abstract: Molecular generative models, often employing GPT-style language modeling on molecular string representations, have shown promising capabilities when scaled to large datasets and model sizes. However, it remains unclear and subject to debate whether these models adhere to predictable scaling laws under fixed computational budgets, which is a crucial understanding for optimally allocating resources between model size, data volume, and molecular representation. In this study, we systematically investigate the scaling behavior of molecular language models across both pretraining and downstream tasks. We train 300 models and conduct over 10,000 experiments, rigorously controlling compute budgets while independently varying model size, number of training tokens, and molecular representation. Our results demonstrate clear scaling laws in molecular models for both pretraining and downstream transfer, reveal the substantial impact of molecular representation on performance, and explain previously observed inconsistencies in scaling behavior for molecular generation. Additionally, we publicly release the largest library of molecular language models to date to facilitate future research and development. Code and models are available at https://github.com/SZU-ADDG/MLM-Scaling.

</details>


### [87] [On Safer Reinforcement Learning Policies for Sedation and Analgesia in Intensive Care](https://arxiv.org/abs/2601.23154)
*Joel Romero-Hernandez,Oscar Camara*

Main category: cs.LG

TL;DR: 该研究使用深度强化学习框架，基于MIMIC-IV数据库中47,144例ICU患者数据，训练药物剂量策略，发现仅优化短期疼痛管理的策略与死亡率正相关，而同时考虑疼痛和死亡率的策略与死亡率负相关。


<details>
  <summary>Details</summary>
Motivation: ICU疼痛管理需要在治疗目标与患者安全之间进行复杂权衡，现有强化学习方法存在两个问题：1) 优化目标不考虑患者生存率；2) 算法不适合不完全信息环境。本研究旨在评估这些设计选择的风险。

Method: 开发了深度强化学习框架，在部分可观测环境下提供每小时药物剂量建议。使用MIMIC-IV数据库中47,144例ICU患者数据，训练策略以开具阿片类药物、丙泊酚、苯二氮䓬类和右美托咪定。比较两种目标：仅减少疼痛 vs. 同时减少疼痛和死亡率。

Result: 两种策略都能降低疼痛，但仅优化疼痛的策略与死亡率呈正相关，而同时考虑疼痛和死亡率的策略与死亡率呈负相关。这表明重视长期结果对于制定更安全的治疗策略至关重要。

Conclusion: 即使短期目标是主要目标，重视长期结果（如患者生存率）对于开发更安全的ICU疼痛管理策略至关重要。仅优化短期疼痛管理的强化学习策略可能带来意外风险。

Abstract: Pain management in intensive care usually involves complex trade-offs between therapeutic goals and patient safety, since both inadequate and excessive treatment may induce serious sequelae. Reinforcement learning can help address this challenge by learning medication dosing policies from retrospective data. However, prior work on sedation and analgesia has optimized for objectives that do not value patient survival while relying on algorithms unsuitable for imperfect information settings. We investigated the risks of these design choices by implementing a deep reinforcement learning framework to suggest hourly medication doses under partial observability. Using data from 47,144 ICU stays in the MIMIC-IV database, we trained policies to prescribe opioids, propofol, benzodiazepines, and dexmedetomidine according to two goals: reduce pain or jointly reduce pain and mortality. We found that, although the two policies were associated with lower pain, actions from the first policy were positively correlated with mortality, while those proposed by the second policy were negatively correlated. This suggests that valuing long-term outcomes could be critical for safer treatment policies, even if a short-term goal remains the primary objective.

</details>


### [88] [Sparse Attention as Compact Kernel Regression](https://arxiv.org/abs/2601.22766)
*Saul Santos,Nuno Gonçalves,Daniel C. McNamee,André F. T Martins*

Main category: cs.LG

TL;DR: 该论文建立了稀疏注意力机制与紧致核之间的理论联系，揭示了归一化ReLU和sparsemax注意力分别对应固定和自适应归一化的Epanechnikov核回归，并展示了α-entmax注意力与常用非参数密度估计核的对应关系。


<details>
  <summary>Details</summary>
Motivation: 现有研究已揭示自注意力机制与Nadaraya-Watson核回归之间的联系，但缺乏对稀疏注意力机制的核理论理解。本文旨在填补这一空白，为稀疏注意力提供理论解释。

Method: 建立稀疏注意力与紧致（有界支撑）核之间的形式对应关系，证明归一化ReLU和sparsemax注意力分别对应Epanechnikov核回归的固定和自适应归一化形式，并展示α-entmax注意力与Epanechnikov、biweight、triweight等常用核的对应关系。

Result: 揭示了稀疏注意力机制与紧凑核回归的对应关系：softmax/Gaussian对应关系在n→∞时出现，而α-entmax注意力（α=1+1/n）对应Epanechnikov、biweight、triweight等核。基于核回归的Memory Mosaics模型在语言建模、上下文学习和长度泛化任务上表现竞争性。

Conclusion: 该研究为稀疏注意力机制提供了统一的核理论框架，解释了稀疏性如何自然地从核设计中产生，并为设计注意力机制提供了原则性替代方案，超越了启发式的top-k注意力和其他关联记忆机制。

Abstract: Recent work has revealed a link between self-attention mechanisms in transformers and test-time kernel regression via the Nadaraya-Watson estimator, with standard softmax attention corresponding to a Gaussian kernel. However, a kernel-theoretic understanding of sparse attention mechanisms is currently missing. In this paper, we establish a formal correspondence between sparse attention and compact (bounded support) kernels. We show that normalized ReLU and sparsemax attention arise from Epanechnikov kernel regression under fixed and adaptive normalizations, respectively. More generally, we demonstrate that widely used kernels in nonparametric density estimation -- including Epanechnikov, biweight, and triweight -- correspond to $α$-entmax attention with $α= 1 + \frac{1}{n}$ for $n \in \mathbb{N}$, while the softmax/Gaussian relationship emerges in the limit $n \to \infty$. This unified perspective explains how sparsity naturally emerges from kernel design and provides principled alternatives to heuristic top-$k$ attention and other associative memory mechanisms. Experiments with a kernel-regression-based variant of transformers -- Memory Mosaics -- show that kernel-based sparse attention achieves competitive performance on language modeling, in-context learning, and length generalization tasks, offering a principled framework for designing attention mechanisms.

</details>


### [89] [Probing the Trajectories of Reasoning Traces in Large Language Models](https://arxiv.org/abs/2601.23163)
*Marthe Ballon,Brecht Verbeken,Vincent Ginis,Andres Algaba*

Main category: cs.LG

TL;DR: 该研究提出了一种系统探测大语言模型推理轨迹的方法，通过截断推理轨迹并重新注入模型来测量答案分布变化，发现准确率和决策确定性随推理内容增加而提升，主要受相关内容驱动而非长度或风格效应。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在给出最终答案前通常会生成"推理轨迹"，但尚不清楚准确率和决策确定性如何沿推理轨迹演变，以及中间轨迹片段是否提供超出通用长度或风格效应的答案相关信息。

Method: 提出系统探测推理轨迹的协议：1)生成模型的推理轨迹；2)按固定token百分比截断；3)将每个部分轨迹重新注入模型（或不同模型），通过下一个token概率测量诱导的答案选择分布。

Result: 准确率和决策确定性随提供的推理token百分比增加而一致提升；这些增益主要由模型生成的相关内容驱动，而非上下文长度或通用"推理风格"效应；更强模型常能从错误的部分轨迹成功回溯，但即时答案常锚定在较弱模型的错误响应中。

Conclusion: 轨迹探测为推理模型的高效和安全部署提供了诊断工具，测量结果可指导实用的轨迹处理和监控策略，提高可靠性，而无需假设中间token是固有的忠实解释。

Abstract: Large language models (LLMs) increasingly solve difficult problems by producing "reasoning traces" before emitting a final response. However, it remains unclear how accuracy and decision commitment evolve along a reasoning trajectory, and whether intermediate trace segments provide answer-relevant information beyond generic length or stylistic effects. Here, we propose a protocol to systematically probe the trajectories of reasoning traces in LLMs by 1) generating a model's reasoning trace, 2) truncating it at fixed token-percentiles, and 3) injecting each partial trace back into the model (or a different model) to measure the induced distribution over answer choices via next-token probabilities. We apply this protocol to the open-source Qwen3-4B/-8B/-14B and gpt-oss-20b/-120b models across the multiple-choice GPQA Diamond and MMLU-Pro benchmarks. We find that accuracy and decision commitment consistently increase as the percentage of provided reasoning tokens grows. These gains are primarily driven by relevant content in the model generation rather than context length or generic "reasoning style" effects. Stronger models often backtrack successfully from incorrect partial traces, but immediate answers often remain anchored in the weaker model's incorrect response. More broadly, we show that trajectory probing provides diagnostics for efficient and safer deployment of reasoning models as the measurements can inform practical trace-handling and monitoring policies that improve reliability without assuming intermediate tokens are inherently faithful explanations.

</details>


### [90] [Clipping-Free Policy Optimization for Large Language Models](https://arxiv.org/abs/2601.22801)
*Ömer Veysel Çağatan,Barış Akgün,Gözde Gül Şahin,Xuandong Zhao*

Main category: cs.LG

TL;DR: CFPO提出了一种无裁剪策略优化方法，用凸二次惩罚替代传统裁剪机制，解决了强化学习在大型语言模型后训练中的优化问题，实现了稳定训练且无需额外超参数。


<details>
  <summary>Details</summary>
Motivation: 当前主流强化学习算法依赖裁剪机制，但在大规模应用中存在零梯度区域、奖励黑客攻击和训练不稳定等优化问题，需要更稳定的替代方案。

Method: CFPO用基于总变差散度约束的凸二次惩罚替代启发式裁剪，构建处处可微的目标函数，实现无硬边界的稳定策略更新。

Result: 在推理任务中，CFPO与裁剪方法在下游基准测试中表现相当，同时扩展了稳定训练范围；在对齐任务中，CFPO缓解了冗长利用问题，减少了能力退化，并在指令跟随性能上保持竞争力。

Conclusion: CFPO是一种有前景的即插即用替代方案，只需一行代码更改且无需额外超参数，可替代基于裁剪的方法用于LLM后训练。

Abstract: Reinforcement learning has become central to post-training large language models, yet dominant algorithms rely on clipping mechanisms that introduce optimization issues at scale, including zero-gradient regions, reward hacking, and training instability. We propose Clipping-Free Policy Optimization (CFPO), which replaces heuristic clipping with a convex quadratic penalty derived from Total Variation divergence constraints, yielding an everywhere-differentiable objective that enforces stable policy updates without hard boundaries. We evaluate CFPO across both reasoning and alignment settings. In reasoning, CFPO matches clipping-based methods on downstream benchmarks while extending the stable training regime. In alignment, CFPO mitigates verbosity exploitation and reduces capability degradation, while achieving competitive instruction-following performance. CFPO requires only a one-line code change and no additional hyperparameters. Our results suggest that CFPO is a promising drop-in alternative to clipping-based methods for LLM post-training.

</details>


### [91] [Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization](https://arxiv.org/abs/2601.23174)
*Luca Della Libera,Cem Subakan,Mirco Ravanelli*

Main category: cs.LG

TL;DR: DyCAST是一种动态字符对齐的语音分词器，通过软字符级对齐和显式时长建模实现可变帧率分词，相比固定帧率编解码器使用更少的token


<details>
  <summary>Details</summary>
Motivation: 现有神经音频编解码器通常以固定帧率运行，在时间上均匀分配token，产生不必要的长序列，需要更高效的语音表示方法

Method: DyCAST通过软字符级对齐和显式时长建模实现可变帧率分词，训练时学习将token与字符级语言单元关联，解码时支持无对齐推理并直接控制token时长，还引入了检索增强解码机制以提高低帧率下的语音重合成质量

Result: 实验表明DyCAST在保持竞争力的语音重合成质量和下游性能的同时，比固定帧率编解码器使用显著更少的token

Conclusion: DyCAST提供了一种高效的动态语音分词方法，通过字符对齐和时长控制实现了更紧凑的语音表示，同时通过检索增强机制保证了低帧率下的重建质量

Abstract: Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs.

</details>


### [92] [Quartet II: Accurate LLM Pre-Training in NVFP4 by Improved Unbiased Gradient Estimation](https://arxiv.org/abs/2601.22813)
*Andrei Panferov,Erik Schultheis,Soroush Tabesh,Dan Alistarh*

Main category: cs.LG

TL;DR: 提出MS-EDEN量化方法和Quartet II方案，显著降低NVFP4格式的量化误差，实现端到端全量化预训练，在1.9B参数模型上验证效果，相比BF16获得4.2倍加速。


<details>
  <summary>Details</summary>
Motivation: NVFP4低精度格式虽然支持端到端全量化预训练，但现有量化方法为了获得更准确的无偏梯度估计而牺牲了格式的表示能力，导致相比FP16/FP8训练仍有明显精度损失。

Method: 提出MS-EDEN无偏量化方法（比随机舍入降低2倍以上量化误差），并集成到Quartet II全NVFP4量化方案中，专门针对线性层设计，在正向和反向传播的所有主要矩阵乘法中实现更好的梯度估计。

Result: 在1.9B参数、38B tokens的LLM端到端训练中验证了Quartet II的有效性，在NVIDIA Blackwell GPU上实现最高4.2倍于BF16的加速，代码已开源。

Conclusion: Quartet II方案显著提升了NVFP4量化训练的性能，实现了更好的梯度估计精度和训练效率，为大规模模型的全量化预训练提供了有效解决方案。

Abstract: The NVFP4 lower-precision format, supported in hardware by NVIDIA Blackwell GPUs, promises to allow, for the first time, end-to-end fully-quantized pre-training of massive models such as LLMs. Yet, existing quantized training methods still sacrifice some of the representation capacity of this format in favor of more accurate unbiased quantized gradient estimation by stochastic rounding (SR), losing noticeable accuracy relative to standard FP16 and FP8 training. In this paper, improve the state of the art for quantized training in NVFP4 via a novel unbiased quantization routine for micro-scaled formats, called MS-EDEN, that has more than 2x lower quantization error than SR. We integrate it into a novel fully-NVFP4 quantization scheme for linear layers, called Quartet II. We show analytically that Quartet II achieves consistently better gradient estimation across all major matrix multiplications, both on the forward and on the backward passes. In addition, our proposal synergizes well with recent training improvements aimed specifically at NVFP4. We further validate Quartet II on end-to-end LLM training with up to 1.9B parameters on 38B tokens. We provide kernels for execution on NVIDIA Blackwell GPUs with up to 4.2x speedup over BF16. Our code is available at https://github.com/IST-DASLab/Quartet-II .

</details>


### [93] [Cascaded Flow Matching for Heterogeneous Tabular Data with Mixed-Type Features](https://arxiv.org/abs/2601.22816)
*Markus Mueller,Kathrin Gruber,Dennis Fok*

Main category: cs.LG

TL;DR: 该论文提出了一种级联扩散模型，用于生成包含离散和连续特征的表格数据，特别解决了混合类型特征（离散状态与连续分布结合）的生成挑战。


<details>
  <summary>Details</summary>
Motivation: 当前生成模型在处理表格数据时，难以有效生成混合类型特征（即在同一特征中结合离散状态和连续分布）。现有方法在生成包含缺失值或膨胀值等离散结果的混合特征时不够准确。

Method: 采用级联方法：首先生成表格数据的低分辨率版本（纯分类特征和数值特征的粗略分类表示），然后通过新颖的引导条件概率路径和数据依赖耦合，在流匹配模型中利用这些信息生成高分辨率数据。低分辨率表示明确考虑了离散结果（如缺失值或膨胀值）。

Result: 模型生成的数据样本显著更真实，分布细节捕捉更准确，检测分数提高了40%。理论证明该级联方法能够收紧传输成本界限。

Conclusion: 该级联扩散模型在表格数据生成方面取得了显著进展，特别是在处理混合类型特征时表现出优越性能，为表格数据生成提供了更准确的方法。

Abstract: Advances in generative modeling have recently been adapted to tabular data containing discrete and continuous features. However, generating mixed-type features that combine discrete states with an otherwise continuous distribution in a single feature remains challenging. We advance the state-of-the-art in diffusion models for tabular data with a cascaded approach. We first generate a low-resolution version of a tabular data row, that is, the collection of the purely categorical features and a coarse categorical representation of numerical features. Next, this information is leveraged in the high-resolution flow matching model via a novel guided conditional probability path and data-dependent coupling. The low-resolution representation of numerical features explicitly accounts for discrete outcomes, such as missing or inflated values, and therewith enables a more faithful generation of mixed-type features. We formally prove that this cascade tightens the transport cost bound. The results indicate that our model generates significantly more realistic samples and captures distributional details more accurately, for example, the detection score increases by 40%.

</details>


### [94] [Agnostic Language Identification and Generation](https://arxiv.org/abs/2601.23258)
*Mikael Møller Høgsgaard,Chirag Pabbaraju*

Main category: cs.LG

TL;DR: 本文研究语言识别和生成任务，在完全放松可实性假设（不限制输入数据分布）的"不可知"设置下，提出了新的目标函数，获得了新颖的特征描述和接近紧致的收敛速率。


<details>
  <summary>Details</summary>
Motivation: 现有语言识别和生成研究通常基于强可实性假设：输入数据来自某个未知分布，且该分布必然支持给定语言集合中的某个语言。本文旨在完全放松这一假设，研究在不对输入数据分布施加任何限制的更一般"不可知"设置下的语言识别和生成问题。

Method: 在完全放松可实性假设的不可知设置下，为语言识别和生成问题提出了新的目标函数。通过理论分析，研究在这种更一般情况下的统计性质。

Result: 在两个问题中都获得了新颖有趣的特征描述和接近紧致的收敛速率。在不可知设置下，为语言识别和生成任务建立了新的统计理论框架。

Conclusion: 本文成功地将语言识别和生成研究从传统的可实性假设扩展到更一般的不可知设置，为这些任务在更现实、更广泛的应用场景中提供了理论基础和性能保证。

Abstract: Recent works on language identification and generation have established tight statistical rates at which these tasks can be achieved. These works typically operate under a strong realizability assumption: that the input data is drawn from an unknown distribution necessarily supported on some language in a given collection. In this work, we relax this assumption of realizability entirely, and impose no restrictions on the distribution of the input data. We propose objectives to study both language identification and generation in this more general "agnostic" setup. Across both problems, we obtain novel interesting characterizations and nearly tight rates.

</details>


### [95] [Decomposing and Composing: Towards Efficient Vision-Language Continual Learning via Rank-1 Expert Pool in a Single LoRA](https://arxiv.org/abs/2601.22828)
*Zhan Fa,Yue Duan,Jian Zhang,Lei Qi,Wanqi Yang,Yinghuan Shi*

Main category: cs.LG

TL;DR: 提出了一种基于可分解Rank-1专家池的持续学习框架，通过稀疏组合和正交化减少参数更新，在视觉语言模型中实现高效任务适应并缓解灾难性遗忘


<details>
  <summary>Details</summary>
Motivation: 视觉语言模型中的持续学习面临任务适应和灾难性遗忘的挑战。现有方法通常有较重推理负担或依赖外部知识，而LoRA虽然能实现参数高效调优，但直接用于缓解灾难性遗忘问题并不简单

Method: 将单个LoRA模块重构为可分解的Rank-1专家池，通过学习动态组合稀疏的任务特定更新；提出激活引导正交(AGO)损失，使LoRA权重关键部分在不同任务间正交化；稀疏组合和正交化实现更少参数更新

Result: 在多个设置下的广泛实验显示，在所有指标上都达到最先进结果，超越零样本泛化上界；相比基线方法减少96.7%可训练参数，无需外部数据集或任务ID判别器；合并的LoRA保留较少权重且无推理延迟

Conclusion: 该方法通过稀疏组合和正交化实现了计算轻量的持续学习，在视觉语言模型中有效平衡了任务适应和灾难性遗忘问题，同时保持了参数效率和推理性能

Abstract: Continual learning (CL) in vision-language models (VLMs) faces significant challenges in improving task adaptation and avoiding catastrophic forgetting. Existing methods usually have heavy inference burden or rely on external knowledge, while Low-Rank Adaptation (LoRA) has shown potential in reducing these issues by enabling parameter-efficient tuning. However, considering directly using LoRA to alleviate the catastrophic forgetting problem is non-trivial, we introduce a novel framework that restructures a single LoRA module as a decomposable Rank-1 Expert Pool. Our method learns to dynamically compose a sparse, task-specific update by selecting from this expert pool, guided by the semantics of the [CLS] token. In addition, we propose an Activation-Guided Orthogonal (AGO) loss that orthogonalizes critical parts of LoRA weights across tasks. This sparse composition and orthogonalization enable fewer parameter updates, resulting in domain-aware learning while minimizing inter-task interference and maintaining downstream task performance. Extensive experiments across multiple settings demonstrate state-of-the-art results in all metrics, surpassing zero-shot upper bounds in generalization. Notably, it reduces trainable parameters by 96.7% compared to the baseline method, eliminating reliance on external datasets or task-ID discriminators. The merged LoRAs retain less weights and incur no inference latency, making our method computationally lightweight.

</details>


### [96] [Unconditional flow-based time series generation with equivariance-regularised latent spaces](https://arxiv.org/abs/2601.22848)
*Camilo Carvajal Reyes,Felipe Tobar*

Main category: cs.LG

TL;DR: 该论文提出了一个潜在流匹配框架，通过正则化预训练自编码器来显式鼓励等变性，改善时间序列生成质量并保持高效采样优势。


<details>
  <summary>Details</summary>
Motivation: 当前基于流的时间序列生成模型虽然在低维潜在空间中能实现高效采样，但如何设计具有理想等变性属性的潜在表示仍未被充分探索。研究者希望将几何归纳偏置融入时间序列的潜在生成模型中。

Method: 提出潜在流匹配框架，通过引入等变性损失来正则化预训练自编码器。该损失强制变换信号与其重构之间的一致性，用于微调基本时间序列变换（如平移和幅度缩放）相关的潜在空间。

Result: 在多个真实世界数据集上的实验表明，该方法在标准时间序列生成指标上持续优于现有的基于扩散的基线方法，同时实现数量级更快的采样速度。

Conclusion: 等变性正则化的潜在空间能提高生成质量，同时保持潜在流模型的计算优势。这些结果凸显了将几何归纳偏置融入时间序列潜在生成模型的实际益处。

Abstract: Flow-based models have proven successful for time-series generation, particularly when defined in lower-dimensional latent spaces that enable efficient sampling. However, how to design latent representations with desirable equivariance properties for time-series generative modelling remains underexplored. In this work, we propose a latent flow-matching framework in which equivariance is explicitly encouraged through a simple regularisation of a pre-trained autoencoder. Specifically, we introduce an equivariance loss that enforces consistency between transformed signals and their reconstructions, and use it to fine-tune latent spaces with respect to basic time-series transformations such as translation and amplitude scaling. We show that these equivariance-regularised latent spaces improve generation quality while preserving the computational advantages of latent flow models. Experiments on multiple real-world datasets demonstrate that our approach consistently outperforms existing diffusion-based baselines in standard time-series generation metrics, while achieving orders-of-magnitude faster sampling. These results highlight the practical benefits of incorporating geometric inductive biases into latent generative models for time series.

</details>


### [97] [OptiMAG: Structure-Semantic Alignment via Unbalanced Optimal Transport](https://arxiv.org/abs/2601.22856)
*Yilong Zuo,Xunkai Li,Zhihan Zhang,Qiangqiang Dai,Ronghua Li,Guoren Wang*

Main category: cs.LG

TL;DR: OptiMAG是一个基于不平衡最优传输的正则化框架，用于解决多模态属性图中显式图结构与隐式语义结构之间的不一致问题，通过Fused Gromov-Wasserstein距离引导跨模态结构一致性。


<details>
  <summary>Details</summary>
Motivation: 多模态属性图中存在显式图结构与不同模态嵌入诱导的隐式语义结构之间的不一致问题。现有方法在固定显式图结构上进行消息传递时，会无意中聚合不相似的特征，引入模态特定噪声，阻碍有效的节点表示学习。

Method: 提出OptiMAG框架，使用Fused Gromov-Wasserstein距离显式引导局部邻域内的跨模态结构一致性，缓解结构-语义冲突。同时使用KL散度惩罚自适应处理跨模态不一致性。该框架可作为即插即用的正则化器集成到现有多模态图模型中。

Result: 实验表明OptiMAG在多个任务上一致优于基线方法，包括图中心任务（如节点分类、链接预测）和多模态中心生成任务（如图到文本、图到图像生成）。

Conclusion: OptiMAG通过最优传输正则化有效解决了多模态属性图中的结构-语义不一致问题，能够无缝集成到现有模型中提升性能，为多模态图学习提供了有效的正则化框架。

Abstract: Multimodal Attributed Graphs (MAGs) have been widely adopted for modeling complex systems by integrating multi-modal information, such as text and images, on nodes. However, we identify a discrepancy between the implicit semantic structure induced by different modality embeddings and the explicit graph structure. For instance, neighbors in the explicit graph structure may be close in one modality but distant in another. Since existing methods typically perform message passing over the fixed explicit graph structure, they inadvertently aggregate dissimilar features, introducing modality-specific noise and impeding effective node representation learning. To address this, we propose OptiMAG, an Unbalanced Optimal Transport-based regularization framework. OptiMAG employs the Fused Gromov-Wasserstein distance to explicitly guide cross-modal structural consistency within local neighborhoods, effectively mitigating structural-semantic conflicts. Moreover, a KL divergence penalty enables adaptive handling of cross-modal inconsistencies. This framework can be seamlessly integrated into existing multimodal graph models, acting as an effective drop-in regularizer. Experiments demonstrate that OptiMAG consistently outperforms baselines across multiple tasks, ranging from graph-centric tasks (e.g., node classification, link prediction) to multimodal-centric generation tasks (e.g., graph2text, graph2image). The source code will be available upon acceptance.

</details>


### [98] [Matterhorn: Efficient Analog Sparse Spiking Transformer Architecture with Masked Time-To-First-Spike Encoding](https://arxiv.org/abs/2601.22876)
*Zhanglu Yan,Kaiwen Tang,Zixuan Zhu,Zhenyu Bai,Qianhui Liu,Weng-Fai Wong*

Main category: cs.LG

TL;DR: Matterhorn是一种新型脉冲变压器，通过M-TTFS编码减少脉冲移动，结合MSU内存计算单元消除权重访问开销，在保持准确性的同时大幅提升能效。


<details>
  <summary>Details</summary>
Motivation: 当前SNN的能耗评估主要关注计算操作，忽略了数据移动等实际硬件成本（占总能耗近80%），需要更全面的能效优化方案。

Method: 提出掩码时间到首次脉冲编码方法，将零能耗静默状态重新分配给最频繁的膜电位而非最低值；采用"死区"策略最大化稀疏性；硬件层面使用忆阻突触单元实现内存内计算。

Result: 在GLUE基准测试中，Matterhorn创造了新的最先进水平，平均准确率比现有SNN提高1.42%，能效提升2.31倍。

Conclusion: Matterhorn通过算法和硬件协同设计，有效解决了SNN中数据移动和权重访问的能耗瓶颈，为能效型大语言模型推理提供了实用解决方案。

Abstract: Spiking neural networks (SNNs) have emerged as a promising candidate for energy-efficient LLM inference. However, current energy evaluations for SNNs primarily focus on counting accumulate operations, and fail to account for real-world hardware costs such as data movement, which can consume nearly 80% of the total energy. In this paper, we propose Matterhorn, a spiking transformer that integrates a novel masked time-to-first-spike (M-TTFS) encoding method to reduce spike movement and a memristive synapse unit (MSU) to eliminate weight access overhead. M-TTFS employs a masking strategy that reassigns the zero-energy silent state (a spike train of all 0s) to the most frequent membrane potential rather than the lowest. This aligns the coding scheme with the data distribution, minimizing spike movement energy without information loss. We further propose a `dead zone' strategy that maximizes sparsity by mapping all values within a given range to the silent state. At the hardware level, the MSU utilizes compute-in-memory (CIM) technology to perform analog integration directly within memory, effectively removing weight access costs. On the GLUE benchmark, Matterhorn establishes a new state-of-the-art, surpassing existing SNNs by 1.42% in average accuracy while delivering a 2.31 times improvement in energy efficiency.

</details>


### [99] [Synthetic Time Series Generation via Complex Networks](https://arxiv.org/abs/2601.22879)
*Jaime Vale,Vanessa Freitas Silva,Maria Eduarda Silva,Fernando Silva*

Main category: cs.LG

TL;DR: 提出基于分位数图映射的合成时间序列生成框架，通过将时间序列转换为分位数图再反向重构，生成保持原始数据统计和结构特性的合成数据。


<details>
  <summary>Details</summary>
Motivation: 高质量时间序列数据获取受限（隐私、成本、标注问题），合成时间序列生成成为解决方案。现有方法如GAN存在局限性，需要更可解释的替代方案。

Method: 将时间序列转换为分位数图（Quantile Graphs），然后通过逆映射重构生成合成时间序列。评估生成数据的保真度和实用性。

Result: 在模拟和真实数据集上评估表明，该方法能保持原始数据的统计和结构特性，与最先进的GAN方法相比具有竞争力。

Conclusion: 分位数图方法为合成时间序列生成提供了竞争性且可解释的替代方案，能够解决数据获取限制问题。

Abstract: Time series data are essential for a wide range of applications, particularly in developing robust machine learning models. However, access to high-quality datasets is often limited due to privacy concerns, acquisition costs, and labeling challenges. Synthetic time series generation has emerged as a promising solution to address these constraints. In this work, we present a framework for generating synthetic time series by leveraging complex networks mappings. Specifically, we investigate whether time series transformed into Quantile Graphs (QG) -- and then reconstructed via inverse mapping -- can produce synthetic data that preserve the statistical and structural properties of the original. We evaluate the fidelity and utility of the generated data using both simulated and real-world datasets, and compare our approach against state-of-the-art Generative Adversarial Network (GAN) methods. Results indicate that our quantile graph-based methodology offers a competitive and interpretable alternative for synthetic time series generation.

</details>


### [100] [PlatoLTL: Learning to Generalize Across Symbols in LTL Instructions for Multi-Task RL](https://arxiv.org/abs/2601.22891)
*Jacques Cloete,Mathias Jackermeier,Ioannis Havoutis,Alessandro Abate*

Main category: cs.LG

TL;DR: PlatoLTL是一种多任务强化学习方法，能够实现零样本泛化到未见过的命题词汇和LTL公式结构，通过将命题视为参数化谓词而非离散符号来实现跨命题的共享结构学习。


<details>
  <summary>Details</summary>
Motivation: 当前基于线性时序逻辑(LTL)的多任务强化学习方法虽然能在LTL公式结构上实现泛化，但无法泛化到未见过的命题词汇（描述高层事件的符号）。这限制了智能体处理新任务的能力，因为现实世界中可能会遇到新的命题描述。

Method: PlatoLTL将命题视为参数化谓词的实例而非离散符号，提出了一种新颖的架构来嵌入和组合谓词以表示LTL规范。这种方法允许策略学习相关命题之间的共享结构，从而实现跨命题的参数化泛化。

Result: PlatoLTL在多个具有挑战性的环境中成功实现了对新颖命题和任务的零样本泛化，展示了在LTL公式结构组合泛化和命题参数化泛化两方面的能力。

Conclusion: PlatoLTL通过将命题建模为参数化谓词，解决了现有LTL引导的多任务强化学习方法无法泛化到未见命题词汇的限制，为实现更通用的强化学习智能体提供了有前景的方向。

Abstract: A central challenge in multi-task reinforcement learning (RL) is to train generalist policies capable of performing tasks not seen during training. To facilitate such generalization, linear temporal logic (LTL) has recently emerged as a powerful formalism for specifying structured, temporally extended tasks to RL agents. While existing approaches to LTL-guided multi-task RL demonstrate successful generalization across LTL specifications, they are unable to generalize to unseen vocabularies of propositions (or "symbols"), which describe high-level events in LTL. We present PlatoLTL, a novel approach that enables policies to zero-shot generalize not only compositionally across LTL formula structures, but also parametrically across propositions. We achieve this by treating propositions as instances of parameterized predicates rather than discrete symbols, allowing policies to learn shared structure across related propositions. We propose a novel architecture that embeds and composes predicates to represent LTL specifications, and demonstrate successful zero-shot generalization to novel propositions and tasks across challenging environments.

</details>


### [101] [Calibrated Multivariate Distributional Regression with Pre-Rank Regularization](https://arxiv.org/abs/2601.22895)
*Aya Laajil,Elnura Zhalieva,Naomi Desobry,Souhaib Ben Taieb*

Main category: cs.LG

TL;DR: 提出一种基于正则化的多变量校准方法，使用预秩函数在训练期间强制多变量校准，并引入基于PCA的新型预秩函数来检测依赖结构错误设定。


<details>
  <summary>Details</summary>
Motivation: 尽管单变量概率预测已取得进展，但实现多变量校准仍然具有挑战性。现有预秩函数主要用于事后评估，缺乏在训练期间强制多变量校准的方法。

Method: 提出基于正则化的校准方法，在训练多变量分布回归模型时使用预秩函数强制多变量校准。引入基于PCA的新型预秩函数，将预测投影到预测分布的主方向上。

Result: 在模拟研究和18个真实世界多输出回归数据集上的实验表明，该方法显著改善了多变量预秩校准，且不影响预测准确性。PCA预秩能揭示现有预秩无法检测的依赖结构错误设定。

Conclusion: 该方法有效地在训练期间实现多变量校准，PCA预秩函数为检测依赖结构错误设定提供了新工具，推动了多变量概率预测校准的发展。

Abstract: The goal of probabilistic prediction is to issue predictive distributions that are as informative as possible, subject to being calibrated. Despite substantial progress in the univariate setting, achieving multivariate calibration remains challenging. Recent work has introduced pre-rank functions, scalar projections of multivariate forecasts and observations, as flexible diagnostics for assessing specific aspects of multivariate calibration, but their use has largely been limited to post-hoc evaluation. We propose a regularization-based calibration method that enforces multivariate calibration during training of multivariate distributional regression models using pre-rank functions. We further introduce a novel PCA-based pre-rank that projects predictions onto principal directions of the predictive distribution. Through simulation studies and experiments on 18 real-world multi-output regression datasets, we show that the proposed approach substantially improves multivariate pre-rank calibration without compromising predictive accuracy, and that the PCA pre-rank reveals dependence-structure misspecifications that are not detected by existing pre-ranks.

</details>


### [102] [Uncertainty-Aware Extrapolation in Bayesian Oblique Trees](https://arxiv.org/abs/2601.22899)
*Viktor Andonovikj,Sašo Džeroski,Pavle Boškoski*

Main category: cs.LG

TL;DR: 提出了一种结合贝叶斯决策树和高斯过程的单树模型，用于改进回归任务中的外推能力和不确定性校准


<details>
  <summary>Details</summary>
Motivation: 传统决策树在回归任务中存在局限性：分段常数叶预测受限于训练目标范围，在分布偏移时容易过度自信，缺乏可靠的外推能力和良好校准的不确定性

Method: 提出单树贝叶斯模型，扩展VSPYCT，为每个叶节点配备高斯过程预测器；使用贝叶斯倾斜分割进行不确定性感知的输入空间划分；GP叶节点建模局部函数行为；采用门控机制在输入超出叶节点训练支持时激活基于GP的外推

Result: 在基准回归任务上相比标准变分倾斜树有预测性能改进，在外推场景中表现出显著的性能提升

Conclusion: 该模型结合了决策树的解释性和效率与高斯过程的外推能力和不确定性校准优势，为回归任务提供了更可靠的解决方案

Abstract: Decision trees are widely used due to their interpretability and efficiency, but they struggle in regression tasks that require reliable extrapolation and well-calibrated uncertainty. Piecewise-constant leaf predictions are bounded by the training targets and often become overconfident under distribution shift. We propose a single-tree Bayesian model that extends VSPYCT by equipping each leaf with a GP predictor. Bayesian oblique splits provide uncertainty-aware partitioning of the input space, while GP leaves model local functional behaviour and enable principled extrapolation beyond the observed target range. We present an efficient inference and prediction scheme that combines posterior sampling of split parameters with \gls{gp} posterior predictions, and a gating mechanism that activates GP-based extrapolation when inputs fall outside the training support of a leaf. Experiments on benchmark regression tasks show improvements in the predictive performance compared to standard variational oblique trees, and substantial performance gains in extrapolation scenarios.

</details>


### [103] [FlexLoRA: Entropy-Guided Flexible Low-Rank Adaptation](https://arxiv.org/abs/2601.22905)
*Muqing Liu,Chongjie Si,Yuheng Jia*

Main category: cs.LG

TL;DR: FlexLoRA：基于熵引导的灵活低秩适配框架，通过谱能量熵评估矩阵重要性，支持在全局预算下进行秩修剪和扩展，使用零影响初始化确保稳定性，超越现有PEFT方法。


<details>
  <summary>Details</summary>
Motivation: 大型预训练模型在多个领域表现出色，但完全微调计算和内存成本过高。参数高效微调（PEFT）成为主流范式，其中LoRA方法引入可训练低秩矩阵但固定秩设计限制了灵活性。现有动态秩分配方法依赖启发式元素级指标，缺乏矩阵级区分，且无法在需要额外适应的层扩展容量。

Method: 提出FlexLoRA框架：1）使用谱能量熵评估矩阵重要性；2）在全局预算下支持秩修剪和扩展；3）对新添加的奇异方向采用零影响初始化确保稳定性。通过解决粒度、灵活性和稳定性限制，提供更原则性的PEFT解决方案。

Result: 大量实验表明，FlexLoRA在多个基准测试中持续优于最先进的基线方法。

Conclusion: FlexLoRA通过熵引导的灵活低秩适配，克服了现有PEFT方法的局限性，提供了更原则性、灵活且稳定的参数高效微调解决方案，在多个任务上表现出优越性能。

Abstract: Large pre-trained models achieve remarkable success across diverse domains, yet fully fine-tuning incurs prohibitive computational and memory costs. Parameter-efficient fine-tuning (PEFT) has thus become a mainstream paradigm. Among them, Low-Rank Adaptation (LoRA) introduces trainable low-rank matrices and shows strong performance, nevertheless, its fixed-rank design limits flexibility. Dynamic rank allocation methods mitigate this issue by pruning redundant directions; however, they often rely on heuristic, element-level metrics that globally sort rank directions without matrix-wise distinction, and they lack mechanisms to expand capacity in layers requiring additional adaptation. To overcome these limitations, we propose FlexLoRA, an entropy-guided flexible low-rank adaptation framework that (i) evaluates matrix importance via spectral energy entropy, (ii) supports rank pruning and expansion under a global budget, and (iii) employs zero-impact initialization for newly added singular directions to ensure stability. By addressing granularity, flexibility, and stability limitations, FlexLoRA provides a more principled solution for PEFT. Extensive experiments show that FlexLoRA consistently outperforms state-of-the-art baselines across benchmarks. Codes are available at https://github.com/Chongjie-Si/Subspace-Tuning.

</details>


### [104] [DC-LA: Difference-of-Convex Langevin Algorithm](https://arxiv.org/abs/2601.22932)
*Hoang Phuc Hau Luu,Zhongjian Wang*

Main category: cs.LG

TL;DR: 提出DC-LA算法，用于采样目标分布π∝exp(-f-r)，其中f是Lipschitz光滑的数据保真项，r=r₁-r₂是非光滑的DC函数。通过Moreau包络平滑r，将凹部分重新分配到数据保真项，建立DC-LA在q-Wasserstein距离下的收敛性。


<details>
  <summary>Details</summary>
Motivation: 研究非光滑DC正则化项下的采样问题，传统方法难以处理非光滑的DC结构，需要开发能利用DC结构特性的采样算法。

Method: 利用DC结构，对r₁和r₂分别应用Moreau包络进行平滑，将正则化项的凹部分重新分配到数据保真项，提出DC-LA（近端Langevin算法）。

Result: 在V是距离耗散的假设下，建立了DC-LA在q-Wasserstein距离下的收敛性（考虑离散化和平滑误差），改进了非对数凹采样的现有结果。

Conclusion: DC-LA算法能有效处理非光滑DC正则化项的采样问题，在合成设置中产生准确分布，在CT应用中可靠地提供不确定性量化。

Abstract: We study a sampling problem whose target distribution is $π\propto \exp(-f-r)$ where the data fidelity term $f$ is Lipschitz smooth while the regularizer term $r=r_1-r_2$ is a non-smooth difference-of-convex (DC) function, i.e., $r_1,r_2$ are convex. By leveraging the DC structure of $r$, we can smooth out $r$ by applying Moreau envelopes to $r_1$ and $r_2$ separately. In line of DC programming, we then redistribute the concave part of the regularizer to the data fidelity and study its corresponding proximal Langevin algorithm (termed DC-LA). We establish convergence of DC-LA to the target distribution $π$, up to discretization and smoothing errors, in the $q$-Wasserstein distance for all $q \in \mathbb{N}^*$, under the assumption that $V$ is distant dissipative. Our results improve previous work on non-log-concave sampling in terms of a more general framework and assumptions. Numerical experiments show that DC-LA produces accurate distributions in synthetic settings and reliably provides uncertainty quantification in a real-world Computed Tomography application.

</details>


### [105] [Scalable Topology-Preserving Graph Coarsening with Graph Collapse](https://arxiv.org/abs/2601.22943)
*Xiang Wu,Rong-Hua Li,Xunkai Li,Kangfei Zhao,Hongchao Qin,Guoren Wang*

Main category: cs.LG

TL;DR: STPGC是一种可扩展的拓扑保持图粗化方法，通过引入图强坍缩和图边坍缩概念，在保持拓扑特征的同时提高GNN训练效率。


<details>
  <summary>Details</summary>
Motivation: 现有图粗化方法要么保持谱特征要么保持空间特征，而保持拓扑特征的方法虽然能维持GNN预测性能，但存在指数时间复杂度问题。

Method: 提出STPGC方法，引入代数拓扑中的图强坍缩和图边坍缩概念，开发了GStrongCollapse、GEdgeCollapse和NeighborhoodConing三种新算法，消除支配节点和边的同时严格保持拓扑特征。

Result: 实验证明STPGC在节点分类任务中高效有效，同时证明了该方法能保持GNN感受野，并开发了加速GNN训练的近似算法。

Conclusion: STPGC解决了拓扑保持图粗化的可扩展性问题，在保持拓扑特征的同时显著提高了计算效率，为GNN训练提供了有效的图粗化解决方案。

Abstract: Graph coarsening reduces the size of a graph while preserving certain properties. Most existing methods preserve either spectral or spatial characteristics. Recent research has shown that preserving topological features helps maintain the predictive performance of graph neural networks (GNNs) trained on the coarsened graph but suffers from exponential time complexity. To address these problems, we propose Scalable Topology-Preserving Graph Coarsening (STPGC) by introducing the concepts of graph strong collapse and graph edge collapse extended from algebraic topology. STPGC comprises three new algorithms, GStrongCollapse, GEdgeCollapse, and NeighborhoodConing based on these two concepts, which eliminate dominated nodes and edges while rigorously preserving topological features. We further prove that STPGC preserves the GNN receptive field and develop approximate algorithms to accelerate GNN training. Experiments on node classification with GNNs demonstrate the efficiency and effectiveness of STPGC.

</details>


### [106] [Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization](https://arxiv.org/abs/2601.22944)
*Wang Yuanchao,Lai Zhao-Rong,Zhong Tianqi,Li Fengnan*

Main category: cs.LG

TL;DR: ECTR提出了一种统一框架，通过环境条件尾部重加权增强TV不变风险最小化，同时处理环境级相关偏移和样本级多样性偏移，提升OOD泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有不变风险最小化方法主要处理环境级的虚假相关性，但忽略了环境内部的样本级异质性，这会影响OOD性能。需要同时处理相关偏移和多样性偏移。

Method: 提出ECTR框架，将TV不变学习与环境条件尾部重加权结合，通过环境级不变性和环境内鲁棒性的互补机制处理混合分布偏移。在没有显式环境标注时，通过极小极大公式推断潜在环境。

Result: 在回归、表格数据、时间序列和图像分类基准测试中，在混合分布偏移下，最差环境和平均OOD性能均获得一致提升。

Conclusion: ECTR通过统一处理环境级相关偏移和样本级多样性偏移，有效提升了模型在混合分布偏移下的OOD泛化能力。

Abstract: Out-of-distribution (OOD) generalization remains challenging when models simultaneously encounter correlation shifts across environments and diversity shifts driven by rare or hard samples. Existing invariant risk minimization (IRM) methods primarily address spurious correlations at the environment level, but often overlook sample-level heterogeneity within environments, which can critically impact OOD performance. In this work, we propose \emph{Environment-Conditioned Tail Reweighting for Total Variation Invariant Risk Minimization} (ECTR), a unified framework that augments TV-based invariant learning with environment-conditioned tail reweighting to jointly address both types of distribution shift. By integrating environment-level invariance with within-environment robustness, the proposed approach makes these two mechanisms complementary under mixed distribution shifts. We further extend the framework to scenarios without explicit environment annotations by inferring latent environments through a minimax formulation. Experiments across regression, tabular, time-series, and image classification benchmarks under mixed distribution shifts demonstrate consistent improvements in both worst-environment and average OOD performance.

</details>


### [107] [Improved Algorithms for Nash Welfare in Linear Bandits](https://arxiv.org/abs/2601.22969)
*Dhruv Sarkar,Nishant Pandey,Sayak Ray Chowdhury*

Main category: cs.LG

TL;DR: 该论文解决了线性bandit中Nash遗憾的次优性问题，提出了新的分析工具实现最优Nash遗憾界，并首次研究了p-means遗憾框架，提出了通用的FairLinBandit算法框架。


<details>
  <summary>Details</summary>
Motivation: 现有线性bandit中的Nash遗憾结果存在次优性，源于依赖限制性集中不等式。需要新的分析工具来解决这个开放问题，并扩展研究更一般的p-means遗憾框架。

Method: 提出新的分析工具解决Nash遗憾次优性问题；引入p-means遗憾框架统一公平性和效用目标；提出FairLinBandit通用算法框架，可基于任何线性bandit策略实现；具体实例化了Phased Elimination和Upper Confidence Bound两种算法。

Result: 实现了线性bandit中Nash遗憾的最优界；证明了两种算法实例在整个p范围内都能实现亚线性p-means遗憾；在真实数据集生成的线性bandit实例上，实验表明方法持续优于现有最优基线。

Conclusion: 解决了线性bandit中Nash遗憾的开放问题，提出了新的分析工具和通用的p-means遗憾框架，为公平性和效用目标的统一研究提供了新方向。

Abstract: Nash regret has recently emerged as a principled fairness-aware performance metric for stochastic multi-armed bandits, motivated by the Nash Social Welfare objective. Although this notion has been extended to linear bandits, existing results suffer from suboptimality in ambient dimension $d$, stemming from proof techniques that rely on restrictive concentration inequalities. In this work, we resolve this open problem by introducing new analytical tools that yield an order-optimal Nash regret bound in linear bandits. Beyond Nash regret, we initiate the study of $p$-means regret in linear bandits, a unifying framework that interpolates between fairness and utility objectives and strictly generalizes Nash regret. We propose a generic algorithmic framework, FairLinBandit, that works as a meta-algorithm on top of any linear bandit strategy. We instantiate this framework using two bandit algorithms: Phased Elimination and Upper Confidence Bound, and prove that both achieve sublinear $p$-means regret for the entire range of $p$. Extensive experiments on linear bandit instances generated from real-world datasets demonstrate that our methods consistently outperform the existing state-of-the-art baseline.

</details>


### [108] [Learnable Permutation for Structured Sparsity on Transformer Models](https://arxiv.org/abs/2601.22980)
*Zekai Li,Ji Liu,Guanchen Li,Yixing Xu,Ziqiong Liu,Xuanwu Yin,Dong Li,Emad Barsoum*

Main category: cs.LG

TL;DR: 本文提出了一种端到端可学习的权重置换框架，通过可学习的置换成本矩阵、可微分的二分图匹配求解器和稀疏优化损失函数，优化Transformer模型的结构化稀疏化性能。


<details>
  <summary>Details</summary>
Motivation: 结构化稀疏已成为流行的模型剪枝技术，但现有权重置换方法在处理大规模Transformer架构时面临搜索空间指数增长的问题，通常依赖贪心或启发式算法，限制了重排序的有效性。

Method: 提出端到端可学习的置换框架，包括：1) 可学习的置换成本矩阵，量化权重矩阵任意两个输入通道交换的成本；2) 可微分的二分图匹配求解器，基于成本矩阵获得最优二元置换矩阵；3) 稀疏优化损失函数，直接优化置换算子。

Result: 在视觉和语言Transformer上广泛验证，该方法在结构化稀疏化方面实现了最先进的置换结果。

Conclusion: 提出的可学习置换框架有效解决了大规模Transformer架构中权重置换的搜索空间问题，显著提升了结构化稀疏化的性能。

Abstract: Structured sparsity has emerged as a popular model pruning technique, widely adopted in various architectures, including CNNs, Transformer models, and especially large language models (LLMs) in recent years. A promising direction to further improve post-pruning performance is weight permutation, which reorders model weights into patterns more amenable to pruning. However, the exponential growth of the permutation search space with the scale of Transformer architectures forces most methods to rely on greedy or heuristic algorithms, limiting the effectiveness of reordering.
  In this work, we propose a novel end-to-end learnable permutation framework. Our method introduces a learnable permutation cost matrix to quantify the cost of swapping any two input channels of a given weight matrix, a differentiable bipartite matching solver to obtain the optimal binary permutation matrix given a cost matrix, and a sparsity optimization loss function to directly optimize the permutation operator. We extensively validate our approach on vision and language Transformers, demonstrating that our method achieves state-of-the-art permutation results for structured sparsity.

</details>


### [109] [dgMARK: Decoding-Guided Watermarking for Diffusion Language Models](https://arxiv.org/abs/2601.22985)
*Pyo Min Hong,Albert No*

Main category: cs.LG

TL;DR: dgMARK是一种针对离散扩散语言模型（dLLMs）的解码引导水印方法，通过引导解掩码顺序来实现水印嵌入，无需显式重加权模型概率。


<details>
  <summary>Details</summary>
Motivation: 离散扩散语言模型可以按任意顺序生成token，虽然理想的条件预测器应该对顺序不变，但实际dLLMs对解掩码顺序非常敏感，这为水印技术创造了新的通道。

Method: dgMARK引导解掩码顺序朝向那些高奖励候选token满足由二进制哈希诱导的简单奇偶约束的位置，而不显式重加权模型学习到的概率。该方法可与常见解码策略（如置信度、熵和边界排序）即插即用，并可通过一步前瞻变体增强。

Result: 水印通过提升的奇偶匹配统计量检测，滑动窗口检测器确保在插入、删除、替换和改写等后编辑操作下的鲁棒性。

Conclusion: dgMARK为离散扩散语言模型提供了一种有效的水印方法，利用模型对解掩码顺序的敏感性，通过引导顺序实现水印嵌入，具有鲁棒性和实用性。

Abstract: We propose dgMARK, a decoding-guided watermarking method for discrete diffusion language models (dLLMs). Unlike autoregressive models, dLLMs can generate tokens in arbitrary order. While an ideal conditional predictor would be invariant to this order, practical dLLMs exhibit strong sensitivity to the unmasking order, creating a new channel for watermarking. dgMARK steers the unmasking order toward positions whose high-reward candidate tokens satisfy a simple parity constraint induced by a binary hash, without explicitly reweighting the model's learned probabilities. The method is plug-and-play with common decoding strategies (e.g., confidence, entropy, and margin-based ordering) and can be strengthened with a one-step lookahead variant. Watermarks are detected via elevated parity-matching statistics, and a sliding-window detector ensures robustness under post-editing operations including insertion, deletion, substitution, and paraphrasing.

</details>


### [110] [Value-at-Risk Constrained Policy Optimization](https://arxiv.org/abs/2601.22993)
*Rohan Tangri,Jan-Peter Calliess*

Main category: cs.LG

TL;DR: VaR-CPO算法是一种样本高效且保守的方法，直接优化风险价值约束，在可行环境中实现训练期间零约束违反，并提供策略改进和约束违反的最坏情况边界。


<details>
  <summary>Details</summary>
Motivation: 现有基线方法无法保证训练期间的安全探索，特别是在需要满足风险价值约束的场景中。需要一种能够在可行环境中实现零约束违反的样本高效方法。

Method: 通过单边切比雪夫不等式处理VaR约束的非可微性，基于成本回报的前两矩获得可处理的替代约束；扩展约束策略优化的信任域框架，提供策略改进和约束违反的严格最坏情况边界。

Result: VaR-CPO在可行环境中能够实现训练期间零约束违反，这是基线方法无法保证的关键特性；同时保持了样本效率。

Conclusion: VaR-CPO是一种有效的安全探索方法，直接优化VaR约束，提供理论保证，在可行环境中实现零约束违反，解决了基线方法的安全性问题。

Abstract: We introduce the Value-at-Risk Constrained Policy Optimization algorithm (VaR-CPO), a sample efficient and conservative method designed to optimize Value-at-Risk (VaR) constraints directly. Empirically, we demonstrate that VaR-CPO is capable of safe exploration, achieving zero constraint violations during training in feasible environments, a critical property that baseline methods fail to uphold. To overcome the inherent non-differentiability of the VaR constraint, we employ the one-sided Chebyshev inequality to obtain a tractable surrogate based on the first two moments of the cost return. Additionally, by extending the trust-region framework of the Constrained Policy Optimization (CPO) method, we provide rigorous worst-case bounds for both policy improvement and constraint violation during the training process.

</details>


### [111] [Causal Characterization of Measurement and Mechanistic Anomalies](https://arxiv.org/abs/2601.23026)
*Hendrik Suhr,David Kaltenpoth,Jilles Vreeken*

Main category: cs.LG

TL;DR: 论文提出了一种新的异常根因分析方法，区分测量误差和机制变化两种异常类型，通过潜在干预模型实现可识别性，并在未知因果图时保持鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 现有异常根因分析方法忽略了异常可能源于两种根本不同的过程：测量误差（数据正常生成但记录错误）和机制变化（数据生成过程本身改变）。测量误差通常可以安全修正，而机制异常需要仔细考虑。

Method: 定义了一个因果模型，通过将异常视为对潜在（"真实"）变量和观测（"测量"）变量的潜在干预来显式捕获两种异常类型。证明了其可识别性，并提出最大似然估计方法进行实践。

Result: 实验表明，该方法在根因定位方面达到最先进性能，同时能够准确分类异常类型，即使在因果DAG未知的情况下也能保持鲁棒性。

Conclusion: 通过区分测量误差和机制变化两种异常类型，提出的方法不仅改进了根因定位，还提供了异常类型的准确分类，增强了异常分析的实用性和可靠性。

Abstract: Root cause analysis of anomalies aims to identify those features that cause the deviation from the normal process. Existing methods ignore, however, that anomalies can arise through two fundamentally different processes: measurement errors, where data was generated normally but one or more values were recorded incorrectly, and mechanism shifts, where the causal process generating the data changed. While measurement errors can often be safely corrected, mechanistic anomalies require careful consideration. We define a causal model that explicitly captures both types by treating outliers as latent interventions on latent ("true") and observed ("measured") variables. We show that they are identifiable, and propose a maximum likelihood estimation approach to put this to practice. Experiments show that our method matches state-of-the-art performance in root cause localization, while it additionally enables accurate classification of anomaly types, and remains robust even when the causal DAG is unknown.

</details>


### [112] [Divide-and-Conquer CoT: RL for Reducing Latency via Parallel Reasoning](https://arxiv.org/abs/2601.23027)
*Arvind Mahankali,Kaiyue Wen,Tengyu Ma*

Main category: cs.LG

TL;DR: DC-CoT通过并行推理减少长思维链的延迟，在保持准确率的同时将最长路径长度降低35-40%


<details>
  <summary>Details</summary>
Motivation: 长思维链推理虽然能提高LLM的数学推理能力，但会导致高延迟问题。当前LLM生成是高度顺序化的，需要一种方法来减少推理延迟。

Method: 提出Divide-and-Conquer CoT方法：模型作为导演识别可并行执行的子任务，然后生成工作节点执行这些子任务。采用多阶段强化学习算法，结合数据过滤策略，在减少最长路径长度的同时恢复准确率。

Result: 在AIME 2024和HMMT 2025等多个基准测试中，DC-CoT实现了与DeepScaleR-1.5B-Preview相似的准确率，同时将最长路径长度降低了35-40%。

Conclusion: DC-CoT通过并行推理有效减少了长思维链的延迟，在保持模型准确性的同时显著提升了推理效率。

Abstract: Long chain-of-thought reasoning (Long CoT) is now fundamental to state-of-the-art LLMs, especially in mathematical reasoning. However, LLM generation is highly sequential, and long CoTs lead to a high latency. We propose to train Divide-and-Conquer CoT (DC-CoT) to reduce the latency. With DC-CoT, the model can act as a director that identifies distinct subtasks that can be performed in parallel in its reasoning process, and then spawns workers to execute the subtasks. Our goal is to achieve high accuracy, with a low longest path length, which is a theoretical measure of the latency needed for the response. We start with a long CoT base model (DeepScaleR-1.5B-Preview), and first use SFT with a small curated demonstration set to initialize its ability to spawn workers in a certain format. Because SFT degrades the accuracy significantly, we design a multi-stage RL algorithm, with various data filtering strategies, to recover the accuracy while decreasing the longest path length. Across several benchmarks including AIME 2024 and HMMT 2025, DC-CoT achieves similar accuracy as DeepScaleR-1.5B-Preview while decreasing longest path length by 35-40%. Our code, SFT dataset and models are publicly available at https://github.com/amahankali10/DC_CoT_RL_for_Low_Latency_CoT_with_Parallel_Reasoning.

</details>


### [113] [From Absolute to Relative: Rethinking Reward Shaping in Group-Based Reinforcement Learning](https://arxiv.org/abs/2601.23058)
*Wenzhe Niu,Wei He,Zongxia Xie,Jinpeng Ou,Huichuan Fan,Yuchen Ge,Yanru Sun,Ziyin Wang,Yizhao Sun,Chengshun Shi,Jiuchong Gao,Jinghua Hao,Renqing He*

Main category: cs.LG

TL;DR: 论文提出RLRR框架，将强化学习奖励从绝对评分转向相对排名，解决传统方法中稀疏监督和奖励不稳定的问题


<details>
  <summary>Details</summary>
Motivation: 传统基于群体的强化学习方法依赖绝对数值奖励，存在内在局限性：在可验证任务中相同群体评估导致稀疏监督，在开放场景中奖励模型分数范围不稳定影响优势估计

Method: 提出RLRR框架，将奖励塑造从绝对评分转向相对排名；引入Ranking Reward Model，这是一个专门为群体优化设计的列表式偏好模型，可直接生成相对排名

Result: 实验结果表明，RLRR在推理基准和开放生成任务上相比标准基于群体的基线方法带来了一致的性能提升

Conclusion: 通过将原始评估转化为稳健的相对信号，RLRR有效缓解了信号稀疏性和奖励不稳定性问题，为基于群体的强化学习提供了更有效的优化范式

Abstract: Reinforcement learning has become a cornerstone for enhancing the reasoning capabilities of Large Language Models, where group-based approaches such as GRPO have emerged as efficient paradigms that optimize policies by leveraging intra-group performance differences. However, these methods typically rely on absolute numerical rewards, introducing intrinsic limitations. In verifiable tasks, identical group evaluations often result in sparse supervision, while in open-ended scenarios, the score range instability of reward models undermines advantage estimation based on group means. To address these limitations, we propose Reinforcement Learning with Relative Rewards (RLRR), a framework that shifts reward shaping from absolute scoring to relative ranking. Complementing this framework, we introduce the Ranking Reward Model, a listwise preference model tailored for group-based optimization to directly generate relative rankings. By transforming raw evaluations into robust relative signals, RLRR effectively mitigates signal sparsity and reward instability. Experimental results demonstrate that RLRR yields consistent performance improvements over standard group-based baselines across reasoning benchmarks and open-ended generation tasks.

</details>


### [114] [SplineFlow: Flow Matching for Dynamical Systems with B-Spline Interpolants](https://arxiv.org/abs/2601.23072)
*Santanu Subhash Rathod,Pietro Liò,Xiao Zhang*

Main category: cs.LG

TL;DR: SplineFlow是一种基于B样条插值的流匹配算法，专门用于建模动态系统，通过满足多边际约束来学习高阶动力学。


<details>
  <summary>Details</summary>
Motivation: 现有流匹配方法不适合建模动态系统，因为线性插值无法捕捉底层状态演化，特别是在从非规则采样观测中学习高阶动力学时。构建满足观测间多边际约束的统一路径具有挑战性。

Method: SplineFlow利用B样条插值的光滑性和稳定性，通过B样条基函数联合建模观测间的条件路径，以结构化方式学习复杂底层动力学，同时确保满足多边际要求。

Result: 在各种复杂度的确定性和随机动态系统以及细胞轨迹推断任务上的综合实验表明，SplineFlow相比现有基线方法有显著改进。

Conclusion: SplineFlow是一种理论基础的流匹配算法，能够有效建模动态系统，通过B样条插值解决多边际约束问题，在动态系统建模任务上表现优异。

Abstract: Flow matching is a scalable generative framework for characterizing continuous normalizing flows with wide-range applications. However, current state-of-the-art methods are not well-suited for modeling dynamical systems, as they construct conditional paths using linear interpolants that may not capture the underlying state evolution, especially when learning higher-order dynamics from irregular sampled observations. Constructing unified paths that satisfy multi-marginal constraints across observations is challenging, since naïve higher-order polynomials tend to be unstable and oscillatory. We introduce SplineFlow, a theoretically grounded flow matching algorithm that jointly models conditional paths across observations via B-spline interpolation. Specifically, SplineFlow exploits the smoothness and stability of B-spline bases to learn the complex underlying dynamics in a structured manner while ensuring the multi-marginal requirements are met. Comprehensive experiments across various deterministic and stochastic dynamical systems of varying complexity, as well as on cellular trajectory inference tasks, demonstrate the strong improvement of SplineFlow over existing baselines. Our code is available at: https://github.com/santanurathod/SplineFlow.

</details>


### [115] [CATTO: Balancing Preferences and Confidence in Language Models](https://arxiv.org/abs/2601.23096)
*Nisarg Parikh,Kunjal Panchal,Ananya Sai,Pannaga Shivaswamy,Andrew Lan*

Main category: cs.LG

TL;DR: 提出CATTO校准感知训练目标，改善LLM置信度校准，不损失任务准确率，并引入Confidence@k测试时缩放机制


<details>
  <summary>Details</summary>
Motivation: 大型语言模型虽然能准确预测下一个token，但其置信度校准很差：高置信度预测经常出错，低置信度预测反而可能正确。基于偏好的对齐方法进一步破坏了预测概率与正确性之间的联系。

Method: 引入校准感知的token级训练目标CATTO，将预测置信度与经验预测正确性对齐，可与原始偏好优化目标结合。还提出Confidence@k测试时缩放机制，利用校准后的token概率进行贝叶斯最优输出token选择。

Result: 相比直接偏好优化，CATTO在分布内将预期校准误差降低2.22%-7.61%，在分布外降低1.46%-10.44%；相比最强DPO基线，在分布内降低0.22%-1.24%，在分布外降低1.23%-5.07%。置信度改进不损失任务准确率，在五个数据集上保持或略微提高多项选择题回答准确率。

Conclusion: CATTO能有效改善LLM的置信度校准，同时保持任务性能，为更可靠的LLM部署提供了解决方案。

Abstract: Large language models (LLMs) often make accurate next token predictions but their confidence in these predictions can be poorly calibrated: high-confidence predictions are frequently wrong, and low-confidence predictions may be correct. This miscalibration is exacerbated by preference-based alignment methods breaking the link between predictive probability and correctness. We introduce a Calibration Aware Token-level Training Objective (CATTO), a calibration-aware objective that aligns predicted confidence with empirical prediction correctness, which can be combined with the original preference optimization objectives. Empirically, CATTO reduces Expected Calibration Error (ECE) by 2.22%-7.61% in-distribution and 1.46%-10.44% out-of-distribution compared to direct preference optimization (DPO), and by 0.22%-1.24% in-distribution and 1.23%-5.07% out-of-distribution compared to the strongest DPO baseline. This improvement in confidence does not come at a cost of losing task accuracy, where CATTO maintains or slightly improves multiple-choice question-answering accuracy on five datasets. We also introduce Confidence@k, a test-time scaling mechanism leveraging calibrated token probabilities for Bayes-optimal selection of output tokens.

</details>


### [116] [Distribution-informed Efficient Conformal Prediction for Full Ranking](https://arxiv.org/abs/2601.23128)
*Wenbo Liao,Huipeng Huang,Chen Jia,Huajun Xi,Hao Zeng,Hongxin Wei*

Main category: cs.LG

TL;DR: DCR方法通过精确推导非一致性分数的分布来改进排名预测集，相比基线方法减少36%的预测集大小，同时保持有效覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现有基于保形预测的排名方法过于保守，依赖非一致性分数的上界导致预测集过大，需要更高效的预测集构建方法。

Method: 提出分布感知的保形排名(DCR)，通过推导校准项目绝对排名的负超几何分布来精确计算非一致性分数分布，从而确定更精确的保形阈值。

Result: DCR在保持有效覆盖率的同时，将平均预测集大小减少了高达36%，显著提高了预测效率。

Conclusion: DCR通过精确建模非一致性分数分布，在保证理论覆盖率的前提下，显著提高了排名预测集的效率，为实际应用中的安全部署提供了更好的不确定性量化方法。

Abstract: Quantifying uncertainty is critical for the safe deployment of ranking models in real-world applications. Recent work offers a rigorous solution using conformal prediction in a full ranking scenario, which aims to construct prediction sets for the absolute ranks of test items based on the relative ranks of calibration items. However, relying on upper bounds of non-conformity scores renders the method overly conservative, resulting in substantially large prediction sets. To address this, we propose Distribution-informed Conformal Ranking (DCR), which produces efficient prediction sets by deriving the exact distribution of non-conformity scores. In particular, we find that the absolute ranks of calibration items follow Negative Hypergeometric distributions, conditional on their relative ranks. DCR thus uses the rank distribution to derive non-conformity score distribution and determine conformal thresholds. We provide theoretical guarantees that DCR achieves improved efficiency over the baseline while ensuring valid coverage under mild assumptions. Extensive experiments demonstrate the superiority of DCR, reducing average prediction set size by up to 36%, while maintaining valid coverage.

</details>


### [117] [Manifold-Aware Perturbations for Constrained Generative Modeling](https://arxiv.org/abs/2601.23151)
*Katherine Keegan,Lars Ruthotto*

Main category: cs.LG

TL;DR: 提出一种约束感知的数据扰动方法，解决生成模型在等式约束分布建模中的数学局限性问题


<details>
  <summary>Details</summary>
Motivation: 生成模型在科学领域中经常遇到样本受等式约束的分布建模问题，存在固有的数学局限性，需要一种灵活且计算成本低的方法来解决这些已知缺陷

Method: 提出约束感知的数据分布扰动方法，使新分布的支持集与周围空间维度匹配，同时隐式地结合底层流形几何结构

Result: 通过理论分析和多个代表性任务的实证证据表明，该方法能够一致地实现数据分布恢复和稳定采样，适用于扩散模型和归一化流

Conclusion: 该方法为等式约束生成模型提供了一种计算廉价、数学上合理且高度灵活的分布修改方案，有效解决了相关建模问题

Abstract: Generative models have enjoyed widespread success in a variety of applications. However, they encounter inherent mathematical limitations in modeling distributions where samples are constrained by equalities, as is frequently the setting in scientific domains. In this work, we develop a computationally cheap, mathematically justified, and highly flexible distributional modification for combating known pitfalls in equality-constrained generative models. We propose perturbing the data distribution in a constraint-aware way such that the new distribution has support matching the ambient space dimension while still implicitly incorporating underlying manifold geometry. Through theoretical analyses and empirical evidence on several representative tasks, we illustrate that our approach consistently enables data distribution recovery and stable sampling with both diffusion models and normalizing flows.

</details>


### [118] [Behemoth: Benchmarking Unlearning in LLMs Using Fully Synthetic Data](https://arxiv.org/abs/2601.23153)
*Eugenia Iofinova,Dan Alistarh*

Main category: cs.LG

TL;DR: 论文提出了Behemoth框架，一个完全合成的数据生成框架，用于研究大型语言模型中的模型编辑问题，通过控制实验环境来理解训练数据分布与模型权重更新之间的交互关系。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型在实际应用中的部署增加，模型编辑（调整权重以修改特定事实输出）变得重要。然而，现有模型编辑方法存在脆弱性和不完整性，且其效果严重依赖于训练数据分布。由于真实世界数据训练的模型难以理解这种关系，需要创建可控的实验环境。

Method: 提出了Behemoth框架，一个完全合成的数据生成框架。通过在简单的表格数据环境中进行模型编辑实验，创建可控的研究环境来探索训练数据分布与模型权重更新的交互关系。

Result: 在简单表格数据环境中进行模型编辑实验，发现了一些令人惊讶的结果，其中一些与真实世界结果相呼应。例如，在某些情况下，限制更新秩（update rank）会导致更有效的更新。

Conclusion: Behemoth框架为研究模型编辑提供了可控的实验环境，有助于理解训练数据分布与模型权重更新的交互关系。该框架的代码已开源，为模型编辑研究提供了新的工具和方法。

Abstract: As artificial neural networks, and specifically large language models, have improved rapidly in capabilities and quality, they have increasingly been deployed in real-world applications, from customer service to Google search, despite the fact that they frequently make factually incorrect or undesirable statements. This trend has inspired practical and academic interest in model editing, that is, in adjusting the weights of the model to modify its likely outputs for queries relating to a specific fact or set of facts. This may be done either to amend a fact or set of facts, for instance, to fix a frequent error in the training data, or to suppress a fact or set of facts entirely, for instance, in case of dangerous knowledge. Multiple methods have been proposed to do such edits. However, at the same time, it has been shown that such model editing can be brittle and incomplete. Moreover the effectiveness of any model editing method necessarily depends on the data on which the model is trained, and, therefore, a good understanding of the interaction of the training data distribution and the way it is stored in the network is necessary and helpful to reliably perform model editing. However, working with large language models trained on real-world data does not allow us to understand this relationship or fully measure the effects of model editing. We therefore propose Behemoth, a fully synthetic data generation framework. To demonstrate the practical insights from the framework, we explore model editing in the context of simple tabular data, demonstrating surprising findings that, in some cases, echo real-world results, for instance, that in some cases restricting the update rank results in a more effective update. The code is available at https://github.com/IST-DASLab/behemoth.git.

</details>


### [119] [Unsupervised Hierarchical Skill Discovery](https://arxiv.org/abs/2601.23156)
*Damion Harvey,Geraud Nangue Tasse,Branden Ingram,Benjamin Rosman,Steven James*

Main category: cs.LG

TL;DR: 提出一种无监督技能分割和层次结构发现方法，使用基于语法的方法从未标记轨迹中分割技能并构建层次结构，在像素级环境中验证有效性。


<details>
  <summary>Details</summary>
Motivation: 现有方法大多依赖动作标签、奖励或人工标注来分割轨迹为可重用技能，限制了应用范围。需要一种完全无监督的方法来发现技能层次结构。

Method: 使用基于语法的方法从未标记轨迹中分割技能，并诱导出层次结构。该方法不依赖动作标签、奖励或人工标注，能够自动发现低层行为及其组合成高层技能的层次关系。

Result: 在Craftax和完整未修改版Minecraft等高维像素环境中评估，使用技能分割、重用和层次质量指标，发现该方法比现有基线产生更结构化、语义更清晰的层次结构。发现的层次结构能加速和稳定下游强化学习任务。

Conclusion: 提出的无监督技能分割和层次结构发现方法有效，能够在复杂环境中自动发现有意义的技能层次，并能提升下游强化学习任务的性能。

Abstract: We consider the problem of unsupervised skill segmentation and hierarchical structure discovery in reinforcement learning. While recent approaches have sought to segment trajectories into reusable skills or options, most rely on action labels, rewards, or handcrafted annotations, limiting their applicability. We propose a method that segments unlabelled trajectories into skills and induces a hierarchical structure over them using a grammar-based approach. The resulting hierarchy captures both low-level behaviours and their composition into higher-level skills. We evaluate our approach in high-dimensional, pixel-based environments, including Craftax and the full, unmodified version of Minecraft. Using metrics for skill segmentation, reuse, and hierarchy quality, we find that our method consistently produces more structured and semantically meaningful hierarchies than existing baselines. Furthermore, as a proof of concept for utility, we demonstrate that these discovered hierarchies accelerate and stabilise learning on downstream reinforcement learning tasks.

</details>


### [120] [Stochastic Linear Bandits with Parameter Noise](https://arxiv.org/abs/2601.23164)
*Daniel Ezer,Alon Peled-Cohen,Yishay Mansour*

Main category: cs.LG

TL;DR: 本文研究了带参数噪声的随机线性老虎机问题，提出了紧致的遗憾上下界，并展示了简单探索-利用算法能达到最优性能。


<details>
  <summary>Details</summary>
Motivation: 研究参数噪声模型下的随机线性老虎机问题，该模型中奖励函数包含参数噪声而非传统加性噪声，探索这种模型下的最优遗憾界限和算法设计。

Method: 针对参数噪声模型，推导了遗憾上界和下界，特别关注ℓ_p单位球（p≤2）和其对偶范数q的情况，并提出简单的探索-利用算法。

Result: 对于一般动作集，获得上界Õ(√(dT log(K/δ)σ²_max))和下界Ω̃(d√(Tσ²_max))；对于ℓ_p单位球，获得紧致界限Θ̃(√(dTσ²_q))，其中σ²_q≤4。

Conclusion: 参数噪声模型下的遗憾界限与经典加性噪声模型不同，对于ℓ_p单位球，简单探索-利用算法就能达到最优遗憾界限，这一发现令人惊讶。

Abstract: We study the stochastic linear bandits with parameter noise model, in which the reward of action $a$ is $a^\top θ$ where $θ$ is sampled i.i.d. We show a regret upper bound of $\widetilde{O} (\sqrt{d T \log (K/δ) σ^2_{\max})}$ for a horizon $T$, general action set of size $K$ of dimension $d$, and where $σ^2_{\max}$ is the maximal variance of the reward for any action. We further provide a lower bound of $\widetildeΩ (d \sqrt{T σ^2_{\max}})$ which is tight (up to logarithmic factors) whenever $\log (K) \approx d$. For more specific action sets, $\ell_p$ unit balls with $p \leq 2$ and dual norm $q$, we show that the minimax regret is $\widetildeΘ (\sqrt{dT σ^2_q)}$, where $σ^2_q$ is a variance-dependent quantity that is always at most $4$. This is in contrast to the minimax regret attainable for such sets in the classic additive noise model, where the regret is of order $d \sqrt{T}$. Surprisingly, we show that this optimal (up to logarithmic factors) regret bound is attainable using a very simple explore-exploit algorithm.

</details>


### [121] [Names Don't Matter: Symbol-Invariant Transformer for Open-Vocabulary Learning](https://arxiv.org/abs/2601.23169)
*İlker Işık,Wenchao Li*

Main category: cs.LG

TL;DR: 提出一种新型Transformer机制，对可互换标记（如绑定变量）的重命名具有不变性，解决现有模型在固定词汇表下难以泛化到未见符号的问题。


<details>
  <summary>Details</summary>
Motivation: 当前神经架构缺乏处理可互换标记（语义等价但可区分的符号，如绑定变量）的原则性方法。在固定词汇表上训练的模型即使底层语义保持不变，也难以泛化到未见符号。

Method: 提出基于Transformer的新机制，使用并行嵌入流来隔离每个可互换标记在输入中的贡献，结合聚合注意力机制实现跨流的结构化信息共享。

Result: 实验结果证实了该方法的理论保证，并在需要泛化到新符号的开放词汇任务上展示了显著的性能提升。

Conclusion: 提出的Transformer机制为处理可互换标记提供了原则性解决方案，实现了对标记重命名的不变性，显著提升了模型在开放词汇任务上的泛化能力。

Abstract: Current neural architectures lack a principled way to handle interchangeable tokens, i.e., symbols that are semantically equivalent yet distinguishable, such as bound variables. As a result, models trained on fixed vocabularies often struggle to generalize to unseen symbols, even when the underlying semantics remain unchanged. We propose a novel Transformer-based mechanism that is provably invariant to the renaming of interchangeable tokens. Our approach employs parallel embedding streams to isolate the contribution of each interchangeable token in the input, combined with an aggregated attention mechanism that enables structured information sharing across streams. Experimental results confirm the theoretical guarantees of our method and demonstrate substantial performance gains on open-vocabulary tasks that require generalization to novel symbols.

</details>


### [122] [MeshGraphNet-Transformer: Scalable Mesh-based Learned Simulation for Solid Mechanics](https://arxiv.org/abs/2601.23177)
*Mikel M. Iparraguirre,Iciar Alfaro,David Gonzalez,Elias Cueto*

Main category: cs.LG

TL;DR: MeshGraphNet-Transformer (MGN-T) 结合Transformer的全局建模能力和MeshGraphNets的几何归纳偏置，解决了标准MGN在大规模高分辨率网格上长程信息传播效率低的问题。


<details>
  <summary>Details</summary>
Motivation: 标准MeshGraphNets在大规模高分辨率网格上存在长程信息传播效率低的问题，这是由于迭代消息传递机制导致的。工业规模的应用需要处理具有不同几何形状、拓扑结构和边界条件的高分辨率网格，现有方法难以有效应对。

Method: 提出MeshGraphNet-Transformer架构，结合物理注意力Transformer作为全局处理器，同时更新所有节点状态并显式保留节点和边属性。该方法直接捕获长程物理相互作用，无需深度消息传递堆栈或分层粗化网格。

Result: MGN-T成功处理工业规模网格的冲击动力学问题，准确建模自接触、塑性和多变量输出（包括内部现象学塑性变量）。在经典基准测试中优于现有方法，精度更高且保持实际效率，仅需竞争基线方法参数的一小部分。

Conclusion: MGN-T通过结合Transformer的全局建模能力和MeshGraphNets的几何归纳偏置，有效解决了大规模高分辨率网格上的长程信息传播问题，为工业规模应用提供了高效准确的解决方案。

Abstract: We present MeshGraphNet-Transformer (MGN-T), a novel architecture that combines the global modeling capabilities of Transformers with the geometric inductive bias of MeshGraphNets, while preserving a mesh-based graph representation. MGN-T overcomes a key limitation of standard MGN, the inefficient long-range information propagation caused by iterative message passing on large, high-resolution meshes. A physics-attention Transformer serves as a global processor, updating all nodal states simultaneously while explicitly retaining node and edge attributes. By directly capturing long-range physical interactions, MGN-T eliminates the need for deep message-passing stacks or hierarchical, coarsened meshes, enabling efficient learning on high-resolution meshes with varying geometries, topologies, and boundary conditions at an industrial scale.
  We demonstrate that MGN-T successfully handles industrial-scale meshes for impact dynamics, a setting in which standard MGN fails due message-passing under-reaching. The method accurately models self-contact, plasticity, and multivariate outputs, including internal, phenomenological plastic variables. Moreover, MGN-T outperforms state-of-the-art approaches on classical benchmarks, achieving higher accuracy while maintaining practical efficiency, using only a fraction of the parameters required by competing baselines.

</details>


### [123] [Tackling air quality with SAPIENS](https://arxiv.org/abs/2601.23215)
*Marcella Bona,Nathan Heatley,Jia-Chen Hua,Adriana Lara,Valeria Legaria-Santiago,Alberto Luviano Juarez,Fernando Moreno-Gomez,Jocelyn Richardson,Natan Vilchis,Xiwen Shirley Zheng*

Main category: cs.LG

TL;DR: 该研究利用墨西哥城的交通数据预测空气质量，通过创新方法将彩色交通地图转换为环形描述，使用偏最小二乘回归建立交通强度与污染物浓度关系模型，提供超本地化动态空气质量预测。


<details>
  <summary>Details</summary>
Motivation: 全球大城市空气污染问题日益严重，交通是主要污染源。现有空气质量监测和预报时空分辨率低，而实时交通数据通常更精细且公开可用。研究旨在利用精细交通数据提供超本地化动态空气质量预测。

Method: 开发创新方法将彩色交通地图转换为基于同心环的交通强度描述；使用偏最小二乘回归建立交通强度与污染物浓度预测模型；通过不同训练样本优化模型性能。

Result: 成功建立了交通强度与空气质量的关系模型，能够预测污染物水平；模型经过优化达到最佳预测性能；工作流程简单且可适应其他城市环境。

Conclusion: 通过将精细交通数据与空气质量测量相结合，可以开发出有效的超本地化动态空气质量预测系统，该方法具有普适性，可应用于其他城市。

Abstract: Air pollution is a chronic problem in large cities worldwide and awareness is rising as the long-term health implications become clearer. Vehicular traffic has been identified as a major contributor to poor air quality. In a lot of cities the publicly available air quality measurements and forecasts are coarse-grained both in space and time. However, in general, real-time traffic intensity data is openly available in various forms and is fine-grained. In this paper, we present an in-depth study of pollution sensor measurements combined with traffic data from Mexico City. We analyse and model the relationship between traffic intensity and air quality with the aim to provide hyper-local, dynamic air quality forecasts. We developed an innovative method to represent traffic intensities by transforming simple colour-coded traffic maps into concentric ring-based descriptions, enabling improved characterisation of traffic conditions. Using Partial Least Squares Regression, we predict pollution levels based on these newly defined traffic intensities. The model was optimised with various training samples to achieve the best predictive performance and gain insights into the relationship between pollutants and traffic. The workflow we have designed is straightforward and adaptable to other contexts, like other cities beyond the specifics of our dataset.

</details>


### [124] [Optimal Fair Aggregation of Crowdsourced Noisy Labels using Demographic Parity Constraints](https://arxiv.org/abs/2601.23221)
*Gabriel Singer,Samuel Gruffaz,Olivier Vo Van,Nicolas Vayatis,Argyris Kalogeratos*

Main category: cs.LG

TL;DR: 该论文研究了众包标注中的公平性问题，分析了多数投票和贝叶斯最优聚合方法的公平性差距，提出了理论界限和收敛保证，并开发了强制执行严格人口统计均等约束的后处理算法。


<details>
  <summary>Details</summary>
Motivation: 获取可靠的真实标签通常成本高昂或不可行，因此众包和聚合嘈杂的人工标注成为典型解决方案。然而，聚合主观标签可能会放大个体偏见，特别是在敏感特征方面，引发公平性担忧。目前众包聚合中的公平性问题尚未得到充分探索，缺乏收敛保证，且只有有限的后处理方法用于在人口统计均等下强制执行ε-公平性。

Method: 1) 在ε-公平性框架下分析多数投票和最优贝叶斯聚合方法的公平性；2) 在小众包机制中推导多数投票公平性差距的上界；3) 证明聚合共识的公平性差距在可解释条件下以指数速度收敛到真实标签的公平性差距；4) 将最先进的多类公平性后处理算法从连续设置推广到离散设置，以强制执行严格的人口统计均等约束。

Result: 1) 推导了多数投票公平性差距的上界；2) 证明了聚合共识公平性差距以指数速度收敛到真实标签的公平性差距；3) 开发了能够强制执行严格人口统计均等约束的后处理算法；4) 在合成和真实数据集上的实验验证了方法的有效性并证实了理论见解。

Conclusion: 该研究填补了众包聚合中公平性分析的空白，提供了理论保证和实用方法。通过分析聚合方法的公平性差距、证明收敛性，并开发强制执行公平性约束的后处理算法，为解决众包标注中的公平性问题提供了系统性的解决方案。

Abstract: As acquiring reliable ground-truth labels is usually costly, or infeasible, crowdsourcing and aggregation of noisy human annotations is the typical resort. Aggregating subjective labels, though, may amplify individual biases, particularly regarding sensitive features, raising fairness concerns. Nonetheless, fairness in crowdsourced aggregation remains largely unexplored, with no existing convergence guarantees and only limited post-processing approaches for enforcing $\varepsilon$-fairness under demographic parity. We address this gap by analyzing the fairness s of crowdsourced aggregation methods within the $\varepsilon$-fairness framework, for Majority Vote and Optimal Bayesian aggregation. In the small-crowd regime, we derive an upper bound on the fairness gap of Majority Vote in terms of the fairness gaps of the individual annotators. We further show that the fairness gap of the aggregated consensus converges exponentially fast to that of the ground-truth under interpretable conditions. Since ground-truth itself may still be unfair, we generalize a state-of-the-art multiclass fairness post-processing algorithm from the continuous to the discrete setting, which enforces strict demographic parity constraints to any aggregation rule. Experiments on synthetic and real datasets demonstrate the effectiveness of our approach and corroborate the theoretical insights.

</details>


### [125] [FOCUS: DLLMs Know How to Tame Their Compute Bound](https://arxiv.org/abs/2601.23278)
*Kaihua Liang,Xin Tan,An Zhong,Hong Xu,Marco Canini*

Main category: cs.LG

TL;DR: FOCUS系统通过动态聚焦计算在可解码token上，提升扩散大语言模型的推理效率，实现最高3.52倍吞吐量提升


<details>
  <summary>Details</summary>
Motivation: 扩散大语言模型(DLLMs)相比自回归模型具有优势，但解码成本高限制了其部署。研究发现DLLM解码存在关键低效问题：计算在token块上并行化，但每个扩散步骤只有少量token可解码，导致大部分计算浪费在不可解码token上

Method: 提出FOCUS推理系统，基于注意力机制得出的token重要性与token解码概率之间的强相关性，动态聚焦计算在可解码token上，实时淘汰不可解码token，增加有效批处理大小

Result: FOCUS相比生产级引擎LMDeploy实现了最高3.52倍的吞吐量提升，同时在多个基准测试中保持或提高了生成质量

Conclusion: FOCUS系统通过解决DLLM解码中的计算浪费问题，缓解了计算限制，实现了可扩展的吞吐量，为扩散大语言模型的高效部署提供了有效解决方案

Abstract: Diffusion Large Language Models (DLLMs) offer a compelling alternative to Auto-Regressive models, but their deployment is constrained by high decoding cost. In this work, we identify a key inefficiency in DLLM decoding: while computation is parallelized over token blocks, only a small subset of tokens is decodable at each diffusion step, causing most compute to be wasted on non-decodable tokens. We further observe a strong correlation between attention-derived token importance and token-wise decoding probability. Based on this insight, we propose FOCUS -- an inference system designed for DLLMs. By dynamically focusing computation on decodable tokens and evicting non-decodable ones on-the-fly, FOCUS increases the effective batch size, alleviating compute limitations and enabling scalable throughput. Empirical evaluations demonstrate that FOCUS achieves up to 3.52$\times$ throughput improvement over the production-grade engine LMDeploy, while preserving or improving generation quality across multiple benchmarks. The FOCUS system is publicly available on GitHub: https://github.com/sands-lab/FOCUS.

</details>


### [126] [Decoupled Diffusion Sampling for Inverse Problems on Function Spaces](https://arxiv.org/abs/2601.23280)
*Thomas Y. L. Lin,Jiachen Yao,Lufang Chiang,Julius Berner,Anima Anandkumar*

Main category: cs.LG

TL;DR: 提出DDIS框架，通过解耦设计实现数据高效的物理感知生成，解决逆PDE问题，相比现有方法在稀疏观测和有限数据下表现更优。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散后验采样的方法通过联合建模系数-解来隐式表示物理，需要大量配对监督数据。当训练数据稀缺时，这些方法存在指导衰减问题，且难以实现有效的物理信息学习。

Method: 提出解耦扩散逆求解器（DDIS）：1）使用无条件扩散学习系数先验；2）使用神经算子显式建模前向PDE进行指导；3）引入解耦退火后验采样（DAPS）避免扩散后验采样中的过度平滑问题。

Result: 在稀疏观测下，DDIS达到最先进性能，平均改进l2误差11%和谱误差54%；当数据限制在1%时，DDIS相比联合模型在l2误差上保持40%优势。理论证明DDIS避免了训练数据稀缺时的指导衰减问题。

Conclusion: DDIS通过解耦设计实现了数据高效的物理感知生成，在逆PDE问题中表现出优越的数据效率和物理信息学习能力，特别是在稀疏观测和有限数据场景下。

Abstract: We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [127] [SPARK: Real-Time Monitoring of Multi-Faceted Programming Exercises](https://arxiv.org/abs/2601.22256)
*Yinuo Yang,Ashley Ge Zhang,Steve Oney,April Yi Wang*

Main category: cs.HC

TL;DR: SPARK是一个编程练习监控仪表板，帮助教师跟踪学生在多步骤复杂编程任务中的进度，通过分组检查点、自动测试和可视化来识别困难学生。


<details>
  <summary>Details</summary>
Motivation: 监控课堂编程练习可以帮助教师识别困难学生和常见挑战，但对于多步骤、复杂依赖关系、无固定完成顺序或难以总结评估标准的编程任务（如构建交互式网页界面），理解学生进度非常困难。

Method: SPARK允许教师根据练习要求灵活地将子步骤分组为检查点，为这些检查点建议自动化测试，并生成可视化来跟踪各步骤的进度。系统还允许教师检查中间输出，深入了解解决方案的变体。

Result: 研究构建了包含22名学习者解决两个网页编程练习的40分钟击键数据数据集，并通过16名编程教师的受试者内评估提供了SPARK感知有用性的实证见解。

Conclusion: SPARK是一个有效的编程练习监控工具，能够帮助教师更好地理解和跟踪学生在复杂多步骤编程任务中的进度，提供更深入的洞察。

Abstract: Monitoring in-class programming exercises can help instructors identify struggling students and common challenges. However, understanding students' progress can be prohibitively difficult, particularly for multi-faceted problems that include multiple steps with complex interdependencies, have no predictable completion order, or involve evaluation criteria that are difficult to summarize across many students (e.g., exercises building interactive web-based user interfaces). We introduce SPARK, a coding exercise monitoring dashboard designed to address these challenges. SPARK allows instructors to flexibly group substeps into checkpoints based on exercise requirements, suggests automated tests for these checkpoints, and generates visualizations to track progress across steps. SPARK also allows instructors to inspect intermediate outputs, providing deeper insights into solution variations. We also construct a dataset of 40-minute keystroke coding data from N=22 learners solving two web programming exercises and provide empirical insights into the perceived usefulness of SPARK through a within-subjects evaluation with 16 programming instructors.

</details>


### [128] [From Retrieving Information to Reasoning with AI: Exploring Different Interaction Modalities to Support Human-AI Coordination in Clinical Decision-Making](https://arxiv.org/abs/2601.22338)
*Behnam Rahdari,Sameer Shaikh,Jonathan H Chen,Tobias Gerstenberg,Shriti Raj*

Main category: cs.HC

TL;DR: 本研究通过定性分析探讨临床医生对LLM决策支持工具不同交互方式（文本对话、交互式UI、静态UI、语音）的感知和使用模式，发现医生主要将LLM作为信息检索工具而非主动思考伙伴，且交互设置和个体认知风格影响参与度，没有一种交互方式适合所有场景。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在临床决策支持中因文本交互简单而受欢迎，但其对临床医生绩效的影响尚不明确。不了解临床医生如何使用这项新技术以及如何将其与传统临床决策支持系统（CDSS）比较，限制了设计能够克服现有工具局限、提升性能和体验的新机制。

Method: 定性研究，对12名临床医生进行考察，分析他们对不同交互方式（基于文本的LLM对话、交互式UI、静态UI和语音）在决策支持中的感知。通过开放式使用LLM工具，观察参与者的使用模式。

Result: 参与者主要采取工具中心化方法，将LLM用于信息检索和确认，使用简单提示而非将其作为处理复杂问题的主动审议伙伴。交互设置的改变会引发批判性参与，参与度也因个体认知风格而异。文本、语音和传统UI交互各有优缺点，没有一种适合所有情况的交互方式。

Conclusion: 临床医生倾向于将LLM作为信息检索工具而非主动思考伙伴，交互设计和个体差异显著影响使用效果。未来临床决策支持系统设计需要考虑多种交互方式以适应不同使用场景和用户偏好，而非追求单一解决方案。

Abstract: LLMs are popular among clinicians for decision-support because of simple text-based interaction. However, their impact on clinicians' performance is ambiguous. Not knowing how clinicians use this new technology and how they compare it to traditional clinical decision-support systems (CDSS) restricts designing novel mechanisms that overcome existing tool limitations and enhance performance and experience. This qualitative study examines how clinicians (n=12) perceive different interaction modalities (text-based conversation with LLMs, interactive and static UI, and voice) for decision-support. In open-ended use of LLM-based tools, our participants took a tool-centric approach using them for information retrieval and confirmation with simple prompts instead of use as active deliberation partners that can handle complex questions. Critical engagement emerged with changes to the interaction setup. Engagement also differed with individual cognitive styles. Lastly, benefits and drawbacks of interaction with text, voice and traditional UIs for clinical decision-support show the lack of a one-size-fits-all interaction modality.

</details>


### [129] [Design Perspective on Materials Experience: A CiteSpace-Based Bibliometric and Visual Analysis of Interdisciplinary Research](https://arxiv.org/abs/2601.22518)
*Yuxin Zhang,Fan Zhang*

Main category: cs.HC

TL;DR: 基于2005-2024年文献计量分析，材料体验研究正经历深刻转型，涵盖材料定义扩展、方法学进步和跨学科整合，从传统物质扩展到虚拟和生物媒介，方法从主观描述转向数据驱动量化模型。


<details>
  <summary>Details</summary>
Motivation: 研究旨在揭示材料体验领域的发展趋势和转型特征，通过文献计量分析理解该领域如何从传统材料研究演变为关注感知、交互和跨学科整合的体验中心范式。

Method: 采用文献计量分析方法，分析2005年至2024年期间的相关文献，考察材料定义演变、方法论进展、跨学科整合程度以及国家和机构层面的贡献分布。

Result: 美国、中国、日本、德国和荷兰在贡献上领先；法国、英国和罗马尼亚在跨学科进展显著；代尔夫特理工大学、吉森大学和法国国家科学研究中心具有机构优势；材料驱动设计理论对学科产生基础性影响。

Conclusion: 材料体验研究处于关键转折点，未来发展将取决于材料创新、技术整合、感知量化进展以及社会文化价值观的建立，需要通过设计有效统一这些要素来应对复杂变化的需求。

Abstract: Based on a bibliometric analysis of literature from 2005 to 2024, this study reveals that material experience is undergoing a profound transformation characterized by evolving material definitions, methodological advances, and increasing interdisciplinary integration. Material types now extend beyond traditional substances to encompass virtual and biological media, underscoring a growing emphasis on perception and interaction. Methodologically, the field has transitioned from subjective descriptions to data-driven, quantifiable models focused on objective sensory analysis and multisensory integration to enhance immersion. Key drivers, including human-machine perception convergence, material-driven interface interactions, and the embedding of intelligent interactive functions, propel the discipline toward an experience-centered paradigm reflecting a deep convergence of design, science, and technology. At the national/regional level, the United States, China, Japan, Germany, and the Netherlands lead in contributions, while France, the United Kingdom, and Romania demonstrate significant interdisciplinary progress. At the institutional level, Delft University of Technology, Justus Liebig University Giessen, and the Centre National de la Recherche Scientifique show significant advantages. In particular, the Material-Driven Design theory has established a foundational impact on the discipline, while, regarding general research trends, scholars from the United States, the Netherlands, and Germany maintain the highest academic visibility. Overall, material experience research is at a critical juncture, its future development will depend on progress in material innovation, technological integration, and perceptual quantification, as well as the establishment of socio-cultural values, all of which must be effectively unified through design to address complex evolving needs.

</details>


### [130] [LEAP -- Live Experiments for Active Pedagogy](https://arxiv.org/abs/2601.22534)
*Sumedh Karajagi,Sampad Bhusan Mohanty,Bhaskar Krishnamachari*

Main category: cs.HC

TL;DR: LEAP是一个轻量级软件框架，通过远程可调用函数让学生从自己的编程环境中调用教师定义的函数，实现课堂互动计算实验，增强学生参与度并提供学习过程洞察。


<details>
  <summary>Details</summary>
Motivation: 传统课堂演示存在局限性：静态或教师控制的演示限制了学生参与度；即使使用交互式可视化，学生也只是被动观察者；教师自行开发的工具难以跨课程共享和复用。需要一种能促进课堂主动参与、可共享的教学工具框架。

Method: LEAP框架基于"远程可调用教师定义函数"的简单理念。通过API端点，学生可以从自己的编码环境（脚本或交互式笔记本）中发现并远程调用教师定义的函数。每个函数调用都被时间戳记录并持久化到数据库，支持实时可视化参与情况、解决方案路径、常见错误等。

Result: LEAP提供了标准化的实验格式和在线目录，支持社区贡献的实验库。示例实验涵盖了数值分析、机器学习、算法课程，以及电气工程、经济学和物理学等领域的应用。该框架增强了学生参与度，并为教师提供了可操作的学习过程洞察。

Conclusion: LEAP框架通过标准化实验格式和社区共享机制，旨在建立一个全球生态系统，促进交互式教学法的交流和扩展，解决传统课堂演示的局限性，提升教学效果。

Abstract: Interactive computational environments can help students explore algorithmic concepts through collaborative hands-on experimentation. However, static and instructor controlled demos in lectures limit engagement. Even when interactive visualizations are used, interactions are solely controlled by the instructor, leaving students as passive observers. In addition, the tools used for demonstration often vary significantly, as they are typically developed by individual instructors. Consequently, the visualizations remain confined to a single classroom, rather than being shared and adapted across courses or reused by other instructors. To address this gap and foster active engagement in live classrooms, we present a lightweight and seamless software framework named LEAP for developing interactive computational lab exercises using a simple idea: remotely callable instructor-defined functions. Using API endpoints and a provided client, students can discover and then call instructor defined functions remotely from their coding environment using scripts or interactive notebooks. Each function call is time-stamped and persistently logged in a database, allowing real-time visualization of participation, diverse solution paths, common pitfalls, and live feedback through collaboration, gamification, and quizzes. Labs are packaged as self-contained folders, each containing their own remotely callable functions. We provide example labs to demonstrate applications relevant for numerical analysis, machine learning, algorithms courses and mention some in electrical engineering (EE), economics, and physics. These capabilities enhance engagement and provide instructors with actionable insights into learning processes. With a standardized lab format and an online directory for community-contributed labs, we aim to foster a global ecosystem for exchanging and expanding interactive pedagogy enabled by LEAP.

</details>


### [131] [Assistive Robots and Reasonable Work Assignment Reduce Perceived Stigma toward Persons with Disabilities](https://arxiv.org/abs/2601.22689)
*Stina Klein,Birgit Prodinger,Elisabeth André,Lars Mikelsons,Nils Mandischer*

Main category: cs.HC

TL;DR: 研究通过情境实验发现，辅助机器人能显著降低对工作场所中残障人士的认知污名，特别是当工作适配个人能力或提供机器人辅助时效果更佳。


<details>
  <summary>Details</summary>
Motivation: 虽然机器人已广泛用于帮助残障人士克服身体障碍，但其在促进社会包容方面的作用尚不明确。本研究旨在探讨辅助机器人是否会影响工作场所中对残障人士的污名化认知。

Method: 采用情境实验设计，设置了四种实验条件：工作过载、工作适配、仅对残障同事提供机器人辅助、以及为所有人提供机器人辅助（通用设计）。测量参与者对工作场景中残障同事的认知和行为污名。

Result: 结果显示：当工作任务适配个人能力或通过辅助机器人增强时，认知污名显著降低。为所有人提供机器人辅助（通用设计）能进一步减少认知污名。

Conclusion: 辅助机器人能够降低对残障人士的认知污名，支持在涉及残障人士的工作场景中使用协作机器人，特别是采用通用设计方法为所有人提供机器人辅助。

Abstract: Robots are becoming more prominent in assisting persons with disabilities (PwD). Whilst there is broad consensus that robots can assist in mitigating physical impairments, the extent to which they can facilitate social inclusion remains equivocal. In fact, the exposed status of assisted workers could likewise lead to reduced or increased perceived stigma by other workers. We present a vignette study on the perceived cognitive and behavioral stigma toward PwD in the workplace. We designed four experimental conditions depicting a coworker with an impairment in work scenarios: overburdened work, suitable work, and robot-assisted work only for the coworker, and an offer of robot-assisted work for everyone. Our results show that cognitive stigma is significantly reduced when the work task is adapted to the person's abilities or augmented by an assistive robot. In addition, offering robot-assisted work for everyone, in the sense of universal design, further reduces perceived cognitive stigma. Thus, we conclude that assistive robots reduce perceived cognitive stigma, thereby supporting the use of collaborative robots in work scenarios involving PwDs.

</details>


### [132] [Qualitative Evaluation of LLM-Designed GUI](https://arxiv.org/abs/2601.22759)
*Bartosz Sawicki,Tomasz Les,Dariusz Parzych,Aleksandra Wycisk-Ficek,Pawel Trebacz,Pawel Zawadzki*

Main category: cs.HC

TL;DR: LLM生成的GUI界面在结构布局方面表现良好，但在满足可访问性标准和提供交互功能方面存在挑战，需要人工干预来确保可用性。


<details>
  <summary>Details</summary>
Motivation: 随着生成式人工智能的发展，大型语言模型被探索用于自动化图形用户界面设计，本研究旨在评估LLM生成界面的可用性和适应性。

Method: 使用2025年1月的三个先进模型（OpenAI GPT o3-mini-high、DeepSeek R1和Anthropic Claude 3.5 Sonnet），为聊天系统、技术团队面板和管理者仪表盘三种界面类型生成原型，并进行专家评估。

Result: LLM在创建结构化布局方面有效，但在满足可访问性标准和提供交互功能方面面临挑战；能够部分为不同用户角色定制界面，但缺乏更深层次的上下文理解。

Conclusion: LLM是早期UI原型设计的有前景工具，但需要人工干预来确保可用性、可访问性和用户满意度。

Abstract: As generative artificial intelligence advances, Large Language Models (LLMs) are being explored for automated graphical user interface (GUI) design. This study investigates the usability and adaptability of LLM-generated interfaces by analysing their ability to meet diverse user needs. The experiments included utilization of three state-of-the-art models from January 2025 (OpenAI GPT o3-mini-high, DeepSeek R1, and Anthropic Claude 3.5 Sonnet) generating mockups for three interface types: a chat system, a technical team panel, and a manager dashboard. Expert evaluations revealed that while LLMs are effective at creating structured layouts, they face challenges in meeting accessibility standards and providing interactive functionality. Further testing showed that LLMs could partially tailor interfaces for different user personas but lacked deeper contextual understanding. The results suggest that while LLMs are promising tools for early-stage UI prototyping, human intervention remains critical to ensure usability, accessibility, and user satisfaction.

</details>


### [133] [FACET: Multi-Agent AI Supporting Teachers in Scaling Differentiated Learning for Diverse Students](https://arxiv.org/abs/2601.22788)
*Jana Gonnermann-Müller,Jennifer Haase,Nicolas Leins,Moritz Igel,Konstantin Fackeldey,Sebastian Pokutta*

Main category: cs.HC

TL;DR: FACET是一个面向教师的多智能体框架，旨在支持差异化教学，考虑学生的动机、表现和学习差异，通过教师参与的设计解决课堂异质性带来的挑战。


<details>
  <summary>Details</summary>
Motivation: 课堂日益异质化，包含不同表现水平、动机水平、语言能力和学习差异（如阅读障碍和ADHD）的学生。虽然教师认识到差异化教学的必要性，但日益增长的工作量构成了重大障碍，使差异化教学成为实践中往往无法实现的理想。当前AI教育工具主要面向学生且以表现为中心，忽视了影响学习结果的其他方面。

Method: FACET是一个面向教师的多智能体框架，协调四个专门智能体：学习者模拟、诊断评估、材料生成和评估，采用教师参与循环设计。从开发初期就与教育利益相关者合作，30名学校校长通过参与式研讨会塑造系统需求，70名在职K-12教师评估材料质量，采用混合方法评估。

Result: 混合方法评估显示对包容性差异化教学有强烈的感知价值。从业者强调课堂异质性带来的紧迫需求，以及保持教学自主权作为采用前提的重要性。框架展示了在差异化教学中考虑动机、表现和学习差异的潜力。

Conclusion: FACET框架为解决课堂异质性挑战提供了有前景的解决方案，通过教师参与的设计支持差异化教学。研究讨论了未来学校部署的影响，并概述了纵向课堂实施的合作伙伴关系，强调了保持教师自主权对采用的重要性。

Abstract: Classrooms are becoming increasingly heterogeneous, comprising learners with diverse performance and motivation levels, language proficiencies, and learning differences such as dyslexia and ADHD. While teachers recognize the need for differentiated instruction, growing workloads create substantial barriers, making differentiated instruction an ideal that is often unrealized in practice. Current AI educational tools, which promise differentiated materials, are predominantly student-facing and performance-centric, ignoring other aspects that shape learning outcomes. We introduce FACET, a teacher-facing multi-agent framework designed to address these gaps by supporting differentiation that accounts for motivation, performance, and learning differences. Developed with educational stakeholders from the outset, the framework coordinates four specialized agents, including learner simulation, diagnostic assessment, material generation, and evaluation within a teacher-in-the-loop design. School principals (N = 30) shaped system requirements through participatory workshops, while in-service K-12 teachers (N = 70) evaluated material quality. Mixed-methods evaluation demonstrates strong perceived value for inclusive differentiation. Practitioners emphasized both the urgent need arising from classroom heterogeneity and the importance of maintaining pedagogical autonomy as a prerequisite for adoption. We discuss implications for future school deployment and outline partnerships for longitudinal classroom implementation.

</details>


### [134] [Toward Pluralizing Reflection in HCI through Daoism](https://arxiv.org/abs/2601.22831)
*Aaron Pengyu Zhu,Kristina Mah,Janghee Cho*

Main category: cs.HC

TL;DR: 本文引入道家哲学作为非西方视角，扩展HCI中反思技术的设计框架，提出"静、应、生"三个反思维度，倡导从"反思"转向"共思"的设计理念。


<details>
  <summary>Details</summary>
Motivation: 现有HCI框架对反思技术的设计过于狭隘，强调认知、理性问题解决和个人自我提升，缺乏对涌现性、具身性、关系性和伦理驱动等维度的关注，需要引入非西方哲学视角来拓宽视野。

Method: 结合道家文献分析和18位道士、学者及实践者的半结构化访谈，识别日常反思的三个关键维度：静（Stillness）、应（Resonance）、生（Emergence）。

Result: 识别出反思的涌现性、具身性、关系性和伦理驱动等常被HCI研究忽视的特性，这些维度为交互系统的反思设计提供了替代性框架。

Conclusion: 道家哲学可作为HCI社区的重要认识论资源，推动从"反思"到"共思"的转变，为设计支持反思的交互系统提供更丰富的理论框架。

Abstract: Reflection is fundamental to how people make sense of everyday life, helping them navigate moments of growth, uncertainty, and change. Yet in HCI, existing frameworks of designing technologies to support reflection remain narrow, emphasizing cognitive, rational problem-solving, and individual self-improvement. We introduce Daoist philosophy as a non-Western lens to broaden this scope and reimagine reflective practices in interactive systems. Combining insights from Daoist literature with semi-structured interviews with 18 Daoist priests, scholars, and practitioners, we identified three key dimensions of everyday reflection: Stillness, Resonance, and Emergence. These dimensions reveal emergent, embodied, relational, and ethically driven qualities often overlooked in HCI research. We articulate their potential to inform alternative frameworks for interactive systems for reflection, advocating a shift from reflection toward reflecting-with, and highlight the potential of Daoism as an epistemological resource for the HCI community.

</details>


### [135] [μTouch: Enabling Accurate, Lightweight Self-Touch Sensing with Passive Magnets](https://arxiv.org/abs/2601.22864)
*Siyuan Wang,Ke Li,Jingyuan Huang,Jike Wang,Cheng Zhang,Alanson Sample,Dongyao Chen*

Main category: cs.HC

TL;DR: μTouch是一个基于磁感应的自接触手势识别平台，通过紧凑的硬件设计和轻量级半监督框架，能够高精度识别面部接触和身体抓挠等微手势，仅需少量用户数据即可实现准确识别。


<details>
  <summary>Details</summary>
Motivation: 自接触手势（如面部触摸和手指抓挠）为人类行为提供了丰富洞察，可用于卫生习惯监测和健康监控。然而，现有方法由于这些微手势的多样化运动模式而难以有效检测。

Method: μTouch采用紧凑的硬件设计，包含低功耗磁力计和磁性硅材料；开发了轻量级半监督框架，仅需少量用户数据；并集成了环境场检测模块以减少环境干扰。

Result: μTouch仅需每个手势3秒的微调数据，新用户使用前准备时间少于1分钟。在用户研究中，能够以93.41%的平均准确率区分8种不同的面部接触行为，以94.63%的平均准确率检测身体抓挠行为，即使在1个月后仍保持准确稳定的传感性能。

Conclusion: μTouch展示了作为卫生监测和皮肤健康应用实用工具的潜力，其准确、稳健的传感性能为自接触手势识别提供了有效的解决方案。

Abstract: Self-touch gestures (e.g., nuanced facial touches and subtle finger scratches) provide rich insights into human behaviors, from hygiene practices to health monitoring. However, existing approaches fall short in detecting such micro gestures due to their diverse movement patterns.
  This paper presents μTouch, a novel magnetic sensing platform for self-touch gesture recognition. μTouch features (1) a compact hardware design with low-power magnetometers and magnetic silicon, (2) a lightweight semi-supervised framework requiring minimal user data, and (3) an ambient field detection module to mitigate environmental interference. We evaluated μTouch in two representative applications in user studies with 11 and 12 participants. μTouch only requires three-second fine-tuning data for each gesture, and new users need less than one minute before starting to use the system. μTouch can distinguish eight different face-touching behaviors with an average accuracy of 93.41%, and reliably detect body-scratch behaviors with an average accuracy of 94.63%. μTouch demonstrates accurate and robust sensing performance even after a month, showcasing its potential as a practical tool for hygiene monitoring and dermatological health applications.

</details>


### [136] [Integrating Multi-Label Classification and Generative AI for Scalable Analysis of User Feedback](https://arxiv.org/abs/2601.23018)
*Sandra Loop,Erik Bertram,Sebastian Juhl,Martin Schrepp*

Main category: cs.HC

TL;DR: 本文介绍了在大型软件公司长期用户体验测量项目中开发的技术，用于高效处理和分析大量用户评论，包括监督机器学习分类、生成式AI总结以及情感分析与产品满意度关系研究。


<details>
  <summary>Details</summary>
Motivation: 在竞争激烈的软件市场中，用户体验评估对确保软件质量和促进产品长期成功至关重要。虽然开放式反馈提供了有价值的改进见解并有助于解释定量结果，但分析大量用户评论具有挑战性且耗时。

Method: 1. 采用监督机器学习方法为每条评论分配预定义的主题标签，提供高层次概览；2. 利用生成式AI创建用户反馈的简洁信息摘要；3. 研究用户评论中的情感是否可作为整体产品满意度的指标。

Result: 结果显示，情感分析本身不能可靠地反映用户满意度。产品满意度需要在调查中明确评估，以衡量用户对产品的感知。

Conclusion: 本文提出的技术组合（监督学习分类、生成式AI总结）能够高效处理大量用户评论，但情感分析不能替代明确的产品满意度测量，需要结合定量问卷和定性反馈的综合评估方法。

Abstract: In highly competitive software markets, user experience (UX) evaluation is crucial for ensuring software quality and fostering long-term product success. Such UX evaluations typically combine quantitative metrics from standardized questionnaires with qualitative feedback collected through open-ended questions. While open-ended feedback offers valuable insights for improvement and helps explain quantitative results, analyzing large volumes of user comments is challenging and time-consuming. In this paper, we present techniques developed during a long-term UX measurement project at a major software company to efficiently process and interpret extensive volumes of user comments. To provide a high-level overview of the collected comments, we employ a supervised machine learning approach that assigns meaningful, pre-defined topic labels to each comment. Additionally, we demonstrate how generative AI (GenAI) can be leveraged to create concise and informative summaries of user feedback, facilitating effective communication of findings to the organization and especially upper management. Finally, we investigate whether the sentiment expressed in user comments can serve as an indicator for overall product satisfaction. Our results show that sentiment analysis alone does not reliably reflect user satisfaction. Instead, product satisfaction needs to be assessed explicitly in surveys to measure the user's perception of the product.

</details>


### [137] [Exploring Sidewalk Sheds in New York City through Chatbot Surveys and Human Computer Interaction](https://arxiv.org/abs/2601.23095)
*Junyi Li,Zhaoxi Zhang,Tamir Mendel,Takahiro Yabe*

Main category: cs.HC

TL;DR: 本研究开发了基于AI聊天机器人的调查方法，通过图像标注和路线选择数据，分析纽约市人行道脚手架对行人可见性和行为的影响。


<details>
  <summary>Details</summary>
Motivation: 纽约市人行道脚手架虽然用于安全防护，但政策制定者和商家担心其会降低店面可见性并改变行人导航模式。目前规划实践中缺乏对这些影响的直接测量方法。

Method: 开发了基于AI聊天机器人的调查工具，整合大型语言模型（Google Gemini-1.5-flash-001）和图像标注界面，让用户与街道图像互动、标记视觉元素并通过引导对话提供结构化反馈。采用网格分析入口标注和应用逻辑混合效应模型评估人行道选择模式。

Result: 分析25个样本数据集显示：1）脚手架的存在显著降低行人识别地面零售入口的能力；2）天气条件和脚手架设计特征（净空高度、立柱间距、颜色）的差异显著影响人行道选择行为。

Conclusion: 通过将生成式AI整合到城市研究中，本研究展示了一种评估人行道脚手架设计的新方法，为在不影响安全的前提下改善行人体验的脚手架指南调整提供了实证依据。

Abstract: Sidewalk sheds are a common feature of the streetscape in New York City, reflecting ongoing construction and maintenance activities. However, policymakers and local business owners have raised concerns about reduced storefront visibility and altered pedestrian navigation. Although sidewalk sheds are widely used for safety, their effects on pedestrian visibility and movement are not directly measured in current planning practices. To address this, we developed an AI-based chatbot survey that collects image-based annotations and route choices from pedestrians, linking these responses to specific shed design features, including clearance height, post spacing, and color. This AI chatbot survey integrates a large language model (e.g., Google's Gemini-1.5-flash-001 model) with an image-annotation interface, allowing users to interact with street images, mark visual elements, and provide structured feedback through guided dialogue. To explore pedestrian perceptions and behaviors, this paper conducts a grid-based analysis of entrance annotations and applies logistic mixed-effects modeling to assess sidewalk choice patterns. Analysis of the dataset (n = 25) shows that: (1) the presence of scaffolding significantly reduces pedestrians' ability to identify ground-floor retail entrances, and (2) variations in weather conditions and shed design features significantly influence sidewalk selection behavior. By integrating generative AI into urban research, this study demonstrates a novel method for evaluating sidewalk shed designs and provides empirical evidence to support adjustments to shed guidelines that improve the pedestrian experience without compromising safety.

</details>


### [138] ["I Choose to Live, for Life Itself": Understanding Agency of Home-Based Care Patients Through Information Practices and Relational Dynamics in Care Networks](https://arxiv.org/abs/2601.23127)
*Sung-In Kim,Joonyoung Park,Bogoan Kim,Hwajung Hong*

Main category: cs.HC

TL;DR: 研究揭示家庭护理中患者能动性不是静态个体属性，而是通过日常连续性、护理者认可和家庭环境互动形成的关系能力，结构化文档系统、非正式沟通渠道和医生中心层级制导致患者声音被过滤和碎片化。


<details>
  <summary>Details</summary>
Motivation: 家庭护理为患者为中心护理提供独特机会，但患者在共享规划过程中的能动性常常未能充分体现，需要理解患者能动性在家庭护理中如何表现以及为何存在这种代表性差距。

Method: 通过23次多利益相关方访谈（包括患者、医疗专业人员和护理人员）和60小时人种学观察，研究家庭护理中患者能动性的表现机制。

Result: 研究发现患者能动性是关系能力而非静态属性，受日常连续性、护理者认可和家庭环境互动影响；结构化文档系统过滤情境知识，非正式沟通渠道碎片化患者声音，医生中心层级制将患者定位为被动接受者。

Conclusion: 基于研究发现提出设计考虑，以弥合代表性差距并将患者能动性整合到共享家庭护理计划中，促进更有效的患者参与和护理规划。

Abstract: Home-based care (HBC) delivers medical and care services in patients' living environments, offering unique opportunities for patient-centered care. However, patient agency is often inadequately represented in shared HBC planning processes. Through 23 multi-stakeholder interviews with HBC patients, healthcare professionals, and care workers, alongside 60 hours of ethnographic observations, we examined how patient agency manifests in HBC and why this representation gap occurs. Our findings reveal that patient agency is not a static individual attribute but a relational capacity shaped through maintaining everyday continuity, mutual recognition from care providers, and engagement with material home environments. Furthermore, we identified that structured documentation systems filter out contextual knowledge, informal communication channels fragment patient voices, and doctor-centered hierarchies position patients as passive recipients. Drawing on these insights, we propose design considerations to bridge this representation gap and to integrate patient agency into shared HBC plans.

</details>


### [139] [Evaluating the Viability of Additive Models to Predict Task Completion Time for 3D Interactions in Augmented Reality](https://arxiv.org/abs/2601.23209)
*Logan Lane,Ibrahim Tahmid,Feiyu Lu,Doug A. Bowman*

Main category: cs.HC

TL;DR: 本文提出了一种基于KLM的加性模型来预测3D交互任务完成时间，通过两个实验验证了该模型在不同输入模态下的可行性。


<details>
  <summary>Details</summary>
Motivation: 虽然加性模型（如KLM）在2D界面设计中已被广泛用于预测交互性能，但这种方法在3D用户界面中很少被探索。研究者希望将这种有效的性能预测方法扩展到3D交互领域。

Method: 基于文献中现有的原子任务模型，提出了KLM风格的加性模型来预测3D交互任务完成时间。通过两个研究进行评估：一个使用简单的菜单选择任务，另一个使用更复杂的操作任务，测试了多种输入模态。

Result: 研究发现，文献中的多个模型在菜单选择和操作研究中都能以低于20%的误差预测实际任务性能。加性模型能够以合理的准确度预测输入模态的绝对和相对性能。

Conclusion: 加性模型可以有效地预测3D用户界面的交互性能，为3D界面设计提供了性能评估和优化的工具，类似于2D界面中KLM的作用。

Abstract: Additive models of interaction performance, such as the Keystroke-Level Model (KLM), are tools that allow designers to compare and optimize the performance of user interfaces by summing the predicted times for the atomic components of a specific interaction to predict the total time it would take to complete that interaction. There has been extensive work in creating such additive models for 2D interfaces, but this approach has rarely been explored for 3D user interfaces. We propose a KLM-style additive model, based on existing atomic task models in the literature, to predict task completion time for 3D interaction tasks. We performed two studies to evaluate the feasibility of this approach across multiple input modalities, with one study using a simple menu selection task and the other a more complex manipulation task. We found that several of the models from the literature predicted actual task performance with less than 20% error in both the menu selection and manipulation study. Overall, we found that additive models can predict both absolute and relative performance of input modalities with reasonable accuracy.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [140] [Semi-Autonomous Mathematics Discovery with Gemini: A Case Study on the Erdős Problems](https://arxiv.org/abs/2601.22401)
*Tony Feng,Trieu Trinh,Garrett Bingham,Jiwon Kang,Shengtong Zhang,Sang-hyun Kim,Kevin Barreto,Carl Schildkraut,Junehyuk Jung,Jaehyeon Seo,Carlo Pagano,Yuri Chervonyi,Dawsen Hwang,Kaiying Hou,Sergei Gukov,Cheng-Chiang Tsai,Hyunwoo Choi,Youngbeom Jin,Wei-Yuan Li,Hao-An Wu,Ruey-An Shiu,Yu-Sheng Shih,Quoc V. Le,Thang Luong*

Main category: cs.AI

TL;DR: 使用Gemini AI系统评估700个Erdős问题中的开放猜想，通过AI自然语言验证和人类专家评估相结合的方法，解决了13个标记为"开放"的问题，发现这些问题主要是由于文献难以查找而非难度本身。


<details>
  <summary>Details</summary>
Motivation: 探索AI在数学发现中的半自主应用，特别是评估Erdős问题数据库中标记为"开放"的猜想，了解AI在数学研究中的潜力和局限性。

Method: 采用混合方法：首先使用Gemini AI进行自然语言验证以缩小搜索范围，然后由人类专家评估正确性和新颖性。系统性地评估了700个标记为"开放"的猜想。

Result: 解决了13个标记为"开放"的问题：其中5个通过看似新颖的自主解决方案，8个通过识别现有文献中的先前解决方案。发现这些问题的"开放"状态主要是由于文献难以查找而非问题本身的难度。

Conclusion: AI在数学猜想评估中具有潜力，但面临文献识别困难和"潜意识抄袭"风险。Erdős问题的"开放"状态往往源于文献难以查找而非数学难度，AI辅助方法可以揭示这一现象。

Abstract: We present a case study in semi-autonomous mathematics discovery, using Gemini to systematically evaluate 700 conjectures labeled 'Open' in Bloom's Erdős Problems database. We employ a hybrid methodology: AI-driven natural language verification to narrow the search space, followed by human expert evaluation to gauge correctness and novelty. We address 13 problems that were marked 'Open' in the database: 5 through seemingly novel autonomous solutions, and 8 through identification of previous solutions in the existing literature. Our findings suggest that the 'Open' status of the problems was through obscurity rather than difficulty. We also identify and discuss issues arising in applying AI to math conjectures at scale, highlighting the difficulty of literature identification and the risk of ''subconscious plagiarism'' by AI. We reflect on the takeaways from AI-assisted efforts on the Erdős Problems.

</details>


### [141] [AI-Enabled Waste Classification as a Data-Driven Decision Support Tool for Circular Economy and Urban Sustainability](https://arxiv.org/abs/2601.22418)
*Julius Sechang Mboli,Omolara Aderonke Ogungbemi*

Main category: cs.AI

TL;DR: 该论文评估了传统机器学习与深度学习方法在垃圾图像二分类中的性能，发现DenseNet121表现最佳（91%准确率），并探讨了PCA对传统方法的影响以及模型在实时决策支持系统中的应用。


<details>
  <summary>Details</summary>
Motivation: 实现高效垃圾分类对于智慧城市中的循环经济实践和资源回收至关重要，需要开发准确可靠的自动化分类系统。

Method: 使用25,077张垃圾图像（80/20训练/测试分割，增强并调整为150x150像素），评估传统机器学习方法（随机森林、SVM、AdaBoost）和深度学习方法（自定义CNN、VGG16、ResNet50）以及三种迁移学习模型（DenseNet121、EfficientNetB0、InceptionV3），同时评估主成分分析对传统模型的降维影响。

Result: DenseNet121取得了最高准确率（91%）和ROC-AUC（0.98），比最佳传统分类器高出20个百分点。主成分分析对传统方法改善有限，而迁移学习在有限数据条件下显著提升了性能。

Conclusion: 迁移学习模型（特别是DenseNet121）在垃圾图像分类中表现优异，可集成到实时数据驱动的决策支持系统中，实现自动化垃圾分类，减少填埋使用和生命周期环境影响。

Abstract: Efficient waste sorting is crucial for enabling circular-economy practices and resource recovery in smart cities. This paper evaluates both traditional machine-learning (Random Forest, SVM, AdaBoost) and deep-learning techniques including custom CNNs, VGG16, ResNet50, and three transfer-learning models (DenseNet121, EfficientNetB0, InceptionV3) for binary classification of 25 077 waste images (80/20 train/test split, augmented and resized to 150x150 px). The paper assesses the impact of Principal Component Analysis for dimensionality reduction on traditional models. DenseNet121 achieved the highest accuracy (91 %) and ROC-AUC (0.98), outperforming the best traditional classifier by 20 pp. Principal Component Analysis (PCA) showed negligible benefit for classical methods, whereas transfer learning substantially improved performance under limited-data conditions. Finally, we outline how these models integrate into a real-time Data-Driven Decision Support System for automated waste sorting, highlighting potential reductions in landfill use and lifecycle environmental impacts.)

</details>


### [142] [Anytime Safe PAC Efficient Reasoning](https://arxiv.org/abs/2601.22446)
*Chengyao Yu,Hao Zeng,Youxin Zhu,Jianguo Huang,Huajun Zeng,Bingyi Jing*

Main category: cs.AI

TL;DR: B-PAC推理：一种在线部分反馈下的安全高效推理方法，通过动态调整路由阈值，在保证性能损失可控的前提下显著降低计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型虽然性能优秀但计算成本高、延迟大。现有的选择性思考策略虽然能提高效率，但在在线设置中往往会产生不可控的错误，因为非思考模型的性能损失只能部分观测且数据是非平稳的。

Method: 提出B-PAC（Betting Probably Approximately Correct）推理方法，利用逆倾向评分估计器为候选阈值构建测试超鞅，然后基于累积的统计安全证据动态调整路由阈值。

Result: B-PAC推理显著降低了计算开销，将思考模型的使用率降低了高达81.01%，同时将性能损失控制在用户指定的水平以下。

Conclusion: B-PAC推理为在线部分反馈环境下的安全高效推理提供了一个原则性方法，实现了任意时间有效的性能损失控制和效率保证。

Abstract: Large Reasoning Models (LRMs) have demonstrated remarkable performance on complex tasks but suffer from high computational costs and latency. While selective thinking strategies improve efficiency by routing easy queries to non-thinking models, existing approaches often incur uncontrollable errors, especially in online settings where the performance loss of a non-thinking model is only partially observed and data are non-stationary. To address this, we propose Betting Probably Approximately Correct (B-PAC) reasoning, a principled method that enables anytime safe and efficient online reasoning under partial feedback. Specifically, we utilize inverse propensity scoring estimators to construct test supermartingales for candidate thresholds, and then dynamically adjust the routing threshold based on the accumulated statistical evidence of safety. Theoretically, we establish the anytime-valid performance loss control and the efficiency of B-PAC reasoning. Extensive experiments demonstrate that B-PAC reasoning significantly reduces computational overhead, decreasing thinking model usage by up to 81.01\%, while controlling the performance loss below the user-specified level.

</details>


### [143] [Controllable Information Production](https://arxiv.org/abs/2601.22449)
*Tristan Shah,Stas Tiomkin*

Main category: cs.AI

TL;DR: 本文提出了一种新的内在动机原则——可控信息生产（CIP），它避免了外部效用和设计者指定变量，通过开环与闭环Kolmogorov-Sinai熵的差异来同时奖励对混沌的追求和调控。


<details>
  <summary>Details</summary>
Motivation: 现有基于信息论的内在动机方法主要依赖于信息传输，这明确取决于设计者对参与传输的随机变量的选择。本文旨在避免外部效用和设计者指定变量，从最优控制角度推导出新的内在动机原则。

Method: 从最优控制理论推导出可控信息生产（CIP）目标，将其表示为开环与闭环Kolmogorov-Sinai熵之间的差异。这种方法同时奖励对混沌的追求和调控，建立了外在行为与内在行为之间的联系。

Result: 建立了CIP的关键理论性质，并在标准内在动机基准测试中证明了其有效性。CIP作为开环与闭环Kolmogorov-Sinai熵的差异，能够同时处理对混沌的追求和调控。

Conclusion: 可控信息生产（CIP）是一种新颖的内在动机原则，避免了传统方法对设计者指定变量的依赖，从最优控制角度连接了外在和内在行为，为智能行为生成提供了新的理论基础。

Abstract: Intrinsic Motivation (IM) is a paradigm for generating intelligent behavior without external utilities. The existing information-theoretic methods for IM are predominantly based on information transmission, which explicitly depends on the designer's choice of which random variables engage in transmission. In this work, we introduce a novel IM principle, Controllable Information Production (CIP), that avoids both external utilities and designer-specified variables. We derive the CIP objective from Optimal Control, showing a connection between extrinsic and intrinsic behaviors. CIP appears as the gap between open-loop and closed-loop Kolmogorov-Sinai entropies, which simultaneously rewards the pursuit and regulation of chaos. We establish key theoretical properties of CIP and demonstrate its effectiveness on standard IM benchmarks.

</details>


### [144] [Why Self-Rewarding Works: Theoretical Guarantees for Iterative Alignment of Language Models](https://arxiv.org/abs/2601.22513)
*Shi Fu,Yingjie Wang,Shengchao Hu,Peng Wang,Dacheng Tao*

Main category: cs.AI

TL;DR: 本文首次为自奖励语言模型（SRLMs）提供了严格的理论保证，揭示了其迭代改进机制的理论基础。


<details>
  <summary>Details</summary>
Motivation: 尽管自奖励语言模型在无需外部反馈的情况下取得了显著的实证成功，但其核心机制缺乏理论解释，存在关键的理论理解空白。

Method: 首先建立单步更新的基本极限下界，然后推导完整迭代范式的有限样本误差界，最后在线性softmax模型类上实例化理论框架。

Result: 性能以$\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$的速率随样本量n提高，对初始模型的依赖随迭代次数T呈指数衰减。

Conclusion: 自奖励成功的关键在于能够通过指数衰减初始模型依赖，将动态引导至内部稳定性和一致性，从而稳健地克服不良初始化。

Abstract: Self-Rewarding Language Models (SRLMs) achieve notable success in iteratively improving alignment without external feedback. Yet, despite their striking empirical progress, the core mechanisms driving their capabilities remain unelucidated, leaving a critical gap in theoretical understanding. This paper provides the first rigorous theoretical guarantees for SRLMs. We first establish a lower bound that characterizes the fundamental limits of a single update step, revealing a critical dependence on the quality of the initial model. We then derive finite-sample error bounds for the full iterative paradigm, showing that performance improves at a rate of $\widetilde{\mathcal{O}}\left(1/\sqrt{n}\right)$ with sample size $n$. Crucially, our analysis reveals that the dependence on the initial model decays exponentially with the number of iterations $T$. This provides a formal explanation for why self-rewarding succeeds: it robustly overcomes poor initialization by steering the dynamics toward internal stability and consistency. Finally, we instantiate our theoretical framework for the linear softmax model class, yielding tailored guarantees that connect our high-level insights to practical model architectures.

</details>


### [145] [Decoding in Geometry: Alleviating Embedding-Space Crowding for Complex Reasoning](https://arxiv.org/abs/2601.22536)
*Yixin Yang,Qingxiu Dong,Zhifang Sui*

Main category: cs.AI

TL;DR: 论文提出CraEG方法，通过几何引导重加权缓解嵌入空间拥挤现象，提升LLM推理性能


<details>
  <summary>Details</summary>
Motivation: 现有基于温度和截断的采样方法仅关注token概率，忽略了嵌入空间中token之间的细粒度几何关系。研究发现嵌入空间拥挤现象——下一个token的概率质量集中在嵌入空间中几何上接近的token上，且与数学问题解决中的推理成功存在统计关联。

Method: 提出CraEG方法，这是一种即插即用的采样方法，通过几何引导的重加权来缓解嵌入空间拥挤现象。该方法无需训练、单次通过，且与标准采样策略兼容。

Result: 在多个模型和基准测试上的实验表明，CraEG提高了生成性能，在鲁棒性和多样性指标上都有所提升。

Conclusion: 嵌入空间拥挤是影响LLM推理的重要因素，CraEG通过考虑token间的几何关系有效缓解这一问题，为采样解码提供了新的视角。

Abstract: Sampling-based decoding underlies complex reasoning in large language models (LLMs), where decoding strategies critically shape model behavior. Temperature- and truncation-based methods reshape the next-token distribution through global probability reweighting or thresholding to balance the quality-diversity tradeoff. However, they operate solely on token probabilities, ignoring fine-grained relationships among tokens in the embedding space. We uncover a novel phenomenon, embedding-space crowding, where the next-token distribution concentrates its probability mass on geometrically close tokens in the embedding space. We quantify crowding at multiple granularities and find a statistical association with reasoning success in mathematical problem solving. Motivated by this finding, we propose CraEG, a plug-and-play sampling method that mitigates crowding through geometry-guided reweighting. CraEG is training-free, single-pass, and compatible with standard sampling strategies. Experiments on multiple models and benchmarks demonstrate improved generation performance, with gains in robustness and diversity metrics.

</details>


### [146] [WED-Net: A Weather-Effect Disentanglement Network with Causal Augmentation for Urban Flow Prediction](https://arxiv.org/abs/2601.22586)
*Qian Hong,Siyuan Chang,Xiao Zhou*

Main category: cs.AI

TL;DR: WED-Net是一个双分支Transformer架构，通过自注意力和交叉注意力分离内在和天气诱导的交通模式，使用记忆库和自适应门控融合，并引入判别器来区分天气条件，通过因果数据增强提高极端天气下的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在极端天气条件下的城市时空预测存在不足：依赖粗粒度天气描述符，缺乏捕捉细粒度时空效应的机制；因果方法通常忽视时间动态或依赖固定混杂因素分层，难以处理罕见事件和动态变化。

Method: 提出WED-Net（Weather-Effect Disentanglement Network），采用双分支Transformer架构，通过自注意力和交叉注意力分离内在和天气诱导的交通模式，使用记忆库存储历史模式，通过自适应门控融合两个分支。引入判别器明确区分天气条件以促进解耦，设计因果数据增强策略扰动非因果部分同时保留因果结构。

Result: 在三个城市的出租车流量数据集上进行实验，WED-Net在极端天气条件下表现出稳健的性能，展示了其在支持更安全出行、灾害准备和城市韧性方面的潜力。

Conclusion: WED-Net通过解耦天气效应和因果数据增强，有效解决了极端天气条件下的城市时空预测问题，为实际应用中的安全出行、灾害准备和城市韧性提供了有力支持。

Abstract: Urban spatio-temporal prediction under extreme conditions (e.g., heavy rain) is challenging due to event rarity and dynamics. Existing data-driven approaches that incorporate weather as auxiliary input often rely on coarse-grained descriptors and lack dedicated mechanisms to capture fine-grained spatio-temporal effects. Although recent methods adopt causal techniques to improve out-of-distribution generalization, they typically overlook temporal dynamics or depend on fixed confounder stratification. To address these limitations, we propose WED-Net (Weather-Effect Disentanglement Network), a dual-branch Transformer architecture that separates intrinsic and weather-induced traffic patterns via self- and cross-attention, enhanced with memory banks and fused through adaptive gating. To further promote disentanglement, we introduce a discriminator that explicitly distinguishes weather conditions. Additionally, we design a causal data augmentation strategy that perturbs non-causal parts while preserving causal structures, enabling improved generalization under rare scenarios. Experiments on taxi-flow datasets from three cities demonstrate that WED-Net delivers robust performance under extreme weather conditions, highlighting its potential to support safer mobility, highlighting its potential to support safer mobility, disaster preparedness, and urban resilience in real-world settings. The code is publicly available at https://github.com/HQ-LV/WED-Net.

</details>


### [147] [Learn More with Less: Uncertainty Consistency Guided Query Selection for RLVR](https://arxiv.org/abs/2601.22595)
*Hao Yi,Yulan Hu,Xin Li,Sheng Ouyang,Lizhong Ding,Yong Liu*

Main category: cs.AI

TL;DR: 该研究将主动学习引入RLVR框架，提出不确定性一致性度量来评估主观与客观不确定性对齐，通过在线变体实现高效样本选择，仅用30%数据即可达到全数据集性能


<details>
  <summary>Details</summary>
Motivation: 现有RLVR算法需要大量查询预算，标注成本高昂。研究探索是否可以通过更少但信息量更大的查询获得相似或更好的性能，将主动学习引入RLVR框架

Method: 提出不确定性一致性度量来评估主观与客观不确定性的对齐程度。离线设置中使用点二列相关系数(PBC)测量对齐；在线训练中，由于采样有限和输出分布动态变化，提出新的在线变体，基于归一化优势和主观不确定性计算

Result: 实验表明该方法持续优于随机选择和经典主动学习基线，仅使用30%数据训练即可达到全数据集性能，有效降低了推理任务中RLVR的成本

Conclusion: 通过引入主动学习和不确定性一致性度量，显著减少了RLVR所需的查询数量，为大规模语言模型的数学推理任务提供了更经济高效的训练方法

Abstract: Large Language Models (LLMs) have recently improved mathematical reasoning through Reinforcement Learning with Verifiable Reward (RLVR). However, existing RLVR algorithms require large query budgets, making annotation costly. We investigate whether fewer but more informative queries can yield similar or superior performance, introducing active learning (AL) into RLVR. We identify that classic AL sampling strategies fail to outperform random selection in this setting, due to ignoring objective uncertainty when only selecting by subjective uncertainty. This work proposes an uncertainty consistency metric to evaluate how well subjective uncertainty aligns with objective uncertainty. In the offline setting, this alignment is measured using the Point-Biserial Correlation Coefficient (PBC). For online training, because of limited sampling and dynamically shifting output distributions, PBC estimation is difficult. Therefore, we introduce a new online variant, computed from normalized advantage and subjective uncertainty. Theoretically, we prove that the online variant is strictly negatively correlated with offline PBC and supports better sample selection. Experiments show our method consistently outperforms random and classic AL baselines, achieving full-dataset performance while training on only 30% of the data, effectively reducing the cost of RLVR for reasoning tasks.

</details>


### [148] [EntroCut: Entropy-Guided Adaptive Truncation for Efficient Chain-of-Thought Reasoning in Small-scale Large Reasoning Models](https://arxiv.org/abs/2601.22617)
*Hongxi Yan,Qingjie Liu,Yunhong Wang*

Main category: cs.AI

TL;DR: EntroCut是一种无需训练的动态截断方法，利用早期推理步骤中的输出分布熵来识别高置信度状态，从而安全终止推理，显著减少大型推理模型的计算开销。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）通过长链思维生成在复杂推理任务上表现出色，但依赖冗长的中间步骤带来了巨大的计算成本。研究发现模型在早期推理步骤中的输出分布熵能够可靠地区分正确与错误推理。

Method: 提出EntroCut方法，这是一种无需训练的动态截断技术。它通过监测模型输出分布的熵来识别高置信度状态，当熵值足够低时安全终止推理过程，避免不必要的计算开销。

Result: 在四个基准测试上的实验表明，EntroCut能够减少高达40%的token使用量，同时仅带来最小的准确率损失。该方法在效率-性能权衡方面优于现有的无需训练方法。

Conclusion: 熵引导的动态截断为缓解大型推理模型的低效问题提供了一种实用方法，通过引入效率-性能比（EPR）这一统一指标，能够量化token节省与准确率损失之间的权衡。

Abstract: Large Reasoning Models (LRMs) excel at complex reasoning tasks through extended chain-of-thought generation, but their reliance on lengthy intermediate steps incurs substantial computational cost. We find that the entropy of the model's output distribution in early reasoning steps reliably distinguishes correct from incorrect reasoning. Motivated by this observation, we propose EntroCut, a training-free method that dynamically truncates reasoning by identifying high-confidence states where reasoning can be safely terminated. To comprehensively evaluate the trade-off between efficiency and accuracy, we introduce the Efficiency-Performance Ratio (EPR), a unified metric that quantifies relative token savings per unit accuracy loss. Experiments on four benchmarks show that EntroCut reduces token usage by up to 40\% with minimal accuracy sacrifice, achieving superior efficiency-performance trade-offs compared with existing training-free methods. These results demonstrate that entropy-guided dynamic truncation provides a practical approach to mitigate the inefficiency of LRMs.

</details>


### [149] [Statistical Estimation of Adversarial Risk in Large Language Models under Best-of-N Sampling](https://arxiv.org/abs/2601.22636)
*Mingqian Feng,Xiaodong Liu,Weiwei Yang,Chenliang Xu,Christopher White,Jianfeng Gao*

Main category: cs.AI

TL;DR: 提出SABER方法，通过Beta分布建模样本级成功概率，仅需少量样本就能准确预测大规模并行采样下的LLM越狱风险，显著降低评估误差。


<details>
  <summary>Details</summary>
Motivation: 现有LLM安全评估通常基于单次或低预算对抗提示，低估了实际风险。攻击者可以利用大规模并行采样反复探测模型直到产生有害响应，需要更准确的大规模对抗风险评估方法。

Method: 提出SABER方法，使用Beta分布（伯努利分布的共轭先验）建模样本级成功概率，推导出解析缩放定律，能够从小预算测量可靠地外推大规模攻击成功率。

Result: 仅使用n=100个样本，SABER预测ASR@1000的平均绝对误差为1.66，相比基线12.04降低了86.2%的估计误差。结果显示模型存在异质性风险缩放特征，标准评估下看似稳健的模型在并行对抗压力下可能经历快速非线性风险放大。

Conclusion: SABER为现实的LLM安全评估提供了低成本、可扩展的方法论，能够更准确地评估大规模并行攻击下的模型脆弱性，有助于改进LLM安全评估框架。

Abstract: Large Language Models (LLMs) are typically evaluated for safety under single-shot or low-budget adversarial prompting, which underestimates real-world risk. In practice, attackers can exploit large-scale parallel sampling to repeatedly probe a model until a harmful response is produced. While recent work shows that attack success increases with repeated sampling, principled methods for predicting large-scale adversarial risk remain limited. We propose a scaling-aware Best-of-N estimation of risk, SABER, for modeling jailbreak vulnerability under Best-of-N sampling. We model sample-level success probabilities using a Beta distribution, the conjugate prior of the Bernoulli distribution, and derive an analytic scaling law that enables reliable extrapolation of large-N attack success rates from small-budget measurements. Using only n=100 samples, our anchored estimator predicts ASR@1000 with a mean absolute error of 1.66, compared to 12.04 for the baseline, which is an 86.2% reduction in estimation error. Our results reveal heterogeneous risk scaling profiles and show that models appearing robust under standard evaluation can experience rapid nonlinear risk amplification under parallel adversarial pressure. This work provides a low-cost, scalable methodology for realistic LLM safety assessment. We will release our code and evaluation scripts upon publication to future research.

</details>


### [150] [Beyond Medical Chatbots: Meddollina and the Rise of Continuous Clinical Intelligence](https://arxiv.org/abs/2601.22645)
*Vaibhav Ram S. V. N. S,Swetanshu Agrawal,Samudra Banerjee,Abdul Muhsin*

Main category: cs.AI

TL;DR: 论文批评当前生成式医疗AI将医学视为下一个token预测的问题，提出了临床情境智能(CCI)作为可部署医疗AI所需的新能力类别，并介绍了Meddollina系统，该系统通过治理优先的设计在语言生成前约束推理，优先考虑临床适用性而非生成完整性。


<details>
  <summary>Details</summary>
Motivation: 当前生成式医疗AI虽然在基准测试中表现良好，但其生成中心的设计导致与临床部署不兼容的行为：过早结论、不合理确定性、意图漂移和多步骤决策不稳定。这些是"将医学视为下一个token预测"的结构性后果，需要新的能力类别来实现可部署的医疗AI。

Method: 提出了临床情境智能(CCI)作为新的能力类别，定义了持久情境意识、意图保持、有界推理和证据不足时的原则性延迟等特征。开发了Meddollina系统，采用治理优先的设计，在语言实现前约束推理，作为支持临床工作流程的连续智能层，同时保持临床医生的权威。

Result: 在16,412+个异构医疗查询中评估Meddollina，与通用模型、医疗调优模型和检索增强系统进行基准测试。Meddollina展现出独特的行为特征：校准的不确定性、在未明确情况下的保守推理、稳定的纵向约束遵守，以及相对于生成中心基线的减少的推测性完成。

Conclusion: 可部署的医疗AI不会仅通过扩展规模而出现，需要转向连续临床智能，其中进展应通过临床医生在不确定性下的对齐行为来衡量，而不是基于流畅度的完成度。Meddollina展示了治理优先方法在实现临床适当行为方面的潜力。

Abstract: Generative medical AI now appears fluent and knowledgeable enough to resemble clinical intelligence, encouraging the belief that scaling will make it safe. But clinical reasoning is not text generation. It is a responsibility-bound process under ambiguity, incomplete evidence, and longitudinal context. Even as benchmark scores rise, generation-centric systems still show behaviours incompatible with clinical deployment: premature closure, unjustified certainty, intent drift, and instability across multi-step decisions.
  We argue these are structural consequences of treating medicine as next-token prediction. We formalise Clinical Contextual Intelligence (CCI) as a distinct capability class required for real-world clinical use, defined by persistent context awareness, intent preservation, bounded inference, and principled deferral when evidence is insufficient.
  We introduce Meddollina, a governance-first clinical intelligence system designed to constrain inference before language realisation, prioritising clinical appropriateness over generative completeness. Meddollina acts as a continuous intelligence layer supporting clinical workflows while preserving clinician authority. We evaluate Meddollina using a behaviour-first regime across 16,412+ heterogeneous medical queries, benchmarking against general-purpose models, medical-tuned models, and retrieval-augmented systems.
  Meddollina exhibits a distinct behavioural profile: calibrated uncertainty, conservative reasoning under underspecification, stable longitudinal constraint adherence, and reduced speculative completion relative to generation-centric baselines. These results suggest deployable medical AI will not emerge from scaling alone, motivating a shift toward Continuous Clinical Intelligence, where progress is measured by clinician-aligned behaviour under uncertainty rather than fluency-driven completion.

</details>


### [151] [Test-Time Mixture of World Models for Embodied Agents in Dynamic Environments](https://arxiv.org/abs/2601.22647)
*Jinwoo Jang,Minjong Yoo,Sihyung Yoon,Honguk Woo*

Main category: cs.AI

TL;DR: TMoW框架通过测试时更新世界模型混合路由，提升具身智能体在动态环境中的适应性，支持零样本适应和少样本扩展


<details>
  <summary>Details</summary>
Motivation: 现有基于语言模型的具身智能体在动态环境中适应性有限，传统MoE架构部署后路由固定，难以适应未见领域

Method: 提出测试时世界模型混合框架，包含多粒度原型路由、测试时精炼和蒸馏混合增强三个核心组件

Result: 在VirtualHome、ALFWorld和RLBench基准测试中表现出色，支持零样本适应和少样本扩展

Conclusion: TMoW框架通过动态路由更新机制，显著提升了具身智能体在动态环境中的适应性和操作效果

Abstract: Language model (LM)-based embodied agents are increasingly deployed in real-world settings. Yet, their adaptability remains limited in dynamic environments, where constructing accurate and flexible world models is crucial for effective reasoning and decision-making. To address this challenge, we extend the Mixture-of-Experts (MoE) paradigm to embodied agents. While conventional MoE architectures modularize knowledge into expert components with pre-trained routing, they remain rigid once deployed, making them less effective for adapting to unseen domains in dynamic environments. We therefore propose Test-time Mixture of World Models (TMoW), a framework that enhances adaptability to unseen and evolving domains. TMoW updates its routing function over world models at test time, unlike conventional MoE where the function remains fixed, enabling agents to recombine existing models and integrate new ones for continual adaptation. It achieves this through (i) multi-granular prototype-based routing, which adapts mixtures across object- to scene-level similarities, (ii) test-time refinement that aligns unseen domain features with prototypes during inference, and (iii) distilled mixture-based augmentation, which efficiently constructs new models from few-shot data and existing prototypes. We evaluate TMoW on VirtualHome, ALFWorld, and RLBench benchmarks, demonstrating strong performance in both zero-shot adaptation and few-shot expansion scenarios, and showing that it enables embodied agents to operate effectively in dynamic environments.

</details>


### [152] [UCPO: Uncertainty-Aware Policy Optimization](https://arxiv.org/abs/2601.22648)
*Xianzhou Zeng,Jing Huang,Chunmei Xie,Gongrui Nan,Siye Chen,Mengyu Lu,Weiqi Xiong,Qixuan Zhou,Junhao Zhang,Qiang Zhu,Yadong Li,Xingzhong Xu*

Main category: cs.AI

TL;DR: 本文提出UCPO框架解决LLM中不确定性表达问题，通过三元优势解耦和动态不确定性奖励调整机制，有效消除优势偏差，提升模型可靠性和校准能力。


<details>
  <summary>Details</summary>
Motivation: 构建可信赖的大语言模型需要赋予其内在的不确定性表达能力，以缓解幻觉问题。现有RL范式如GRPO存在优势偏差问题，源于二元决策空间和静态不确定性奖励，导致模型要么过于保守要么过于自信。

Method: 提出UnCertainty-Aware Policy Optimization (UCPO)框架：1) 三元优势解耦：分离并独立归一化确定性和不确定性rollouts，消除优势偏差；2) 动态不确定性奖励调整机制：根据模型演化和实例难度实时校准不确定性权重。

Result: 在数学推理和通用任务上的实验结果表明，UCPO有效解决了奖励不平衡问题，显著提高了模型在其知识边界之外的可靠性和校准能力。

Conclusion: UCPO框架通过解决现有RL范式中的优势偏差问题，成功提升了LLM的不确定性表达能力，为构建更可信赖的大语言模型提供了有效解决方案。

Abstract: The key to building trustworthy Large Language Models (LLMs) lies in endowing them with inherent uncertainty expression capabilities to mitigate the hallucinations that restrict their high-stakes applications. However, existing RL paradigms such as GRPO often suffer from Advantage Bias due to binary decision spaces and static uncertainty rewards, inducing either excessive conservatism or overconfidence. To tackle this challenge, this paper unveils the root causes of reward hacking and overconfidence in current RL paradigms incorporating uncertainty-based rewards, based on which we propose the UnCertainty-Aware Policy Optimization (UCPO) framework. UCPO employs Ternary Advantage Decoupling to separate and independently normalize deterministic and uncertain rollouts, thereby eliminating advantage bias. Furthermore, a Dynamic Uncertainty Reward Adjustment mechanism is introduced to calibrate uncertainty weights in real-time according to model evolution and instance difficulty. Experimental results in mathematical reasoning and general tasks demonstrate that UCPO effectively resolves the reward imbalance, significantly improving the reliability and calibration of the model beyond their knowledge boundaries.

</details>


### [153] [Task-Aware LLM Council with Adaptive Decision Pathways for Decision Support](https://arxiv.org/abs/2601.22662)
*Wei Zhu,Lixing Yu,Hao-Ren Yao,Zhiwen Tang,Kun Yue*

Main category: cs.AI

TL;DR: TALC是一个任务感知的LLM委员会框架，通过蒙特卡洛树搜索动态选择专家模型，结合历史成功记忆和双信号机制实现自适应规划，在多个决策任务上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有方法通常忽视不同大语言模型之间的专业化差异，将所有LLM视为统一适用，限制了系统适应不同推理需求和任务复杂度的能力。

Method: 提出TALC框架：1) 建立LLM委员会，每个模型配备基于先前任务轨迹的结构化成功记忆档案；2) 在每个决策点，通过语义匹配将控制路由到最合适的模型；3) 使用融合模型评估和历史效用得分的双信号机制估计节点价值；4) 基于节点内方差自适应加权信号，指导MCTS选择以平衡探索深度和规划置信度。

Result: 在WebShop、HumanEval和24点游戏上的实验表明，TALC相比强基线实现了更高的任务成功率和改进的搜索效率，验证了专业化感知路由和自适应规划的优势。

Conclusion: TALC框架通过动态专家选择和自适应规划机制，有效利用LLM的专业化差异，提升了复杂决策任务的性能，为多模型协作决策系统提供了新思路。

Abstract: Large language models (LLMs) have shown strong capabilities across diverse decision-making tasks. However, existing approaches often overlook the specialization differences among available models, treating all LLMs as uniformly applicable regardless of task characteristics. This limits their ability to adapt to varying reasoning demands and task complexities. In this work, we propose Task-Aware LLM Council (TALC), a task-adaptive decision framework that integrates a council of LLMs with Monte Carlo Tree Search (MCTS) to enable dynamic expert selection and efficient multi-step planning. Each LLM is equipped with a structured success memory profile derived from prior task trajectories, enabling semantic matching between current reasoning context and past successes. At each decision point, TALC routes control to the most contextually appropriate model and estimates node value using a dual-signal mechanism that fuses model-based evaluations with historical utility scores. These signals are adaptively weighted based on intra-node variance and used to guide MCTS selection, allowing the system to balance exploration depth with planning confidence. Experiments on WebShop, HumanEval, and the Game of 24 demonstrate that TALC achieves superior task success rates and improved search efficiency compared to strong baselines, validating the benefits of specialization-aware routing and adaptive planning.

</details>


### [154] [Real-Time Aligned Reward Model beyond Semantics](https://arxiv.org/abs/2601.22664)
*Zixuan Huang,Xin Xia,Yuxi Ren,Jianbin Zheng,Xuefeng Xiao,Hongyan Xie,Li Huaqiu,Songshi Liang,Zhongxiang Dai,Fuzhen Zhuang,Jianxin Li,Yikun Ban,Deqing Wang*

Main category: cs.AI

TL;DR: 论文提出R2M框架，通过利用策略模型的实时隐藏状态反馈来缓解RLHF中的奖励过优化问题


<details>
  <summary>Details</summary>
Motivation: RLHF技术虽然重要，但容易受到奖励过优化的影响，现有方法主要依赖表面语义信息，无法有效处理奖励模型与策略模型之间的错配问题，这种错配会随着策略分布变化而加剧

Method: 提出R2M框架，超越传统仅依赖预训练LLM语义表示的奖励模型，利用策略模型在RL过程中的演化隐藏状态（策略反馈）来实时对齐策略分布变化

Result: R2M通过实时利用策略模型反馈，为改进奖励模型性能指出了新的研究方向

Conclusion: R2M框架通过利用策略模型的实时反馈来缓解RLHF中的奖励过优化问题，为奖励模型性能提升提供了有前景的新方向

Abstract: Reinforcement Learning from Human Feedback (RLHF) is a pivotal technique for aligning large language models (LLMs) with human preferences, yet it is susceptible to reward overoptimization, in which policy models overfit to the reward model, exploit spurious reward patterns instead of faithfully capturing human intent. Prior mitigations primarily relies on surface semantic information and fails to efficiently address the misalignment between the reward model (RM) and the policy model caused by continuous policy distribution shifts. This inevitably leads to an increasing reward discrepancy, exacerbating reward overoptimization. To address these limitations, we introduce R2M (Real-Time Aligned Reward Model), a novel lightweight RLHF framework. R2M goes beyond vanilla reward models that solely depend on the semantic representations of a pretrained LLM. Instead, it leverages the evolving hidden states of the policy (namely policy feedback) to align with the real-time distribution shift of the policy during the RL process. This work points to a promising new direction for improving the performance of reward models through real-time utilization of feedback from policy models.

</details>


### [155] [A Step Back: Prefix Importance Ratio Stabilizes Policy Optimization](https://arxiv.org/abs/2601.22718)
*Shiye Lei,Zhihao Cheng,Dacheng Tao*

Main category: cs.AI

TL;DR: 论文提出MinPRO方法，通过使用最小token级比率替代累积前缀比率，解决LLM强化学习后训练中因采样策略与目标策略差异导致的训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 当前LLM强化学习后训练中，为提升训练效率通常采用离策略方式生成rollouts，但采样策略与目标策略的差异会导致训练不稳定。现有方法主要依赖token级重要性采样比率，但在离策略程度较大时会导致训练动态不稳定。

Method: 提出MinPRO（最小前缀比率）方法，用基于前缀中观察到的最小token级比率的非累积替代项替换不稳定的累积前缀比率，从而稳定LLM在大量离策略漂移下的优化过程。

Result: 在密集和专家混合LLM上的大量实验表明，MinPRO在多个数学推理基准测试中显著提高了离策略机制下的训练稳定性和峰值性能。

Conclusion: MinPRO通过简单有效的目标函数设计，成功解决了LLM强化学习后训练中因离策略差异导致的训练不稳定问题，为大规模语言模型的优化提供了更稳定的方法。

Abstract: Reinforcement learning (RL) post-training has increasingly demonstrated strong ability to elicit reasoning behaviors in large language models (LLMs). For training efficiency, rollouts are typically generated in an off-policy manner using an older sampling policy and then used to update the current target policy. To correct the resulting discrepancy between the sampling and target policies, most existing RL objectives rely on a token-level importance sampling ratio, primarily due to its computational simplicity and numerical stability. However, we observe that token-level correction often leads to unstable training dynamics when the degree of off-policyness is large. In this paper, we revisit LLM policy optimization under off-policy conditions and show that the theoretically rigorous correction term is the prefix importance ratio, and that relaxing it to a token-level approximation can induce instability in RL post-training. To stabilize LLM optimization under large off-policy drift, we propose a simple yet effective objective, Minimum Prefix Ratio (MinPRO). MinPRO replaces the unstable cumulative prefix ratio with a non-cumulative surrogate based on the minimum token-level ratio observed in the preceding prefix. Extensive experiments on both dense and mixture-of-experts LLMs, across multiple mathematical reasoning benchmarks, demonstrate that MinPRO substantially improves training stability and peak performance in off-policy regimes.

</details>


### [156] [AutoRefine: From Trajectories to Reusable Expertise for Continual LLM Agent Refinement](https://arxiv.org/abs/2601.22758)
*Libin Qiu,Zhirong Gao,Junfu Chen,Yuhang Ye,Weizhi Huang,Xiaobo Xue,Wenkai Qiu,Shuo Tang*

Main category: cs.AI

TL;DR: AutoRefine框架从智能体执行历史中提取和维护双形式经验模式，包括用于复杂子任务的专用子智能体和用于静态知识的技能模式，通过持续维护机制防止知识库退化，在多个任务上显著提升性能并减少步骤数。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型智能体缺乏从经验中积累知识的能力，将每个任务视为独立挑战。现有方法将经验提取为扁平化的文本知识，无法捕捉复杂子任务的程序逻辑，且缺乏维护机制导致经验库随着积累而退化。

Method: 提出AutoRefine框架，从智能体执行历史中提取双形式经验模式：1) 对于程序性子任务，提取具有独立推理和记忆的专用子智能体；2) 对于静态知识，提取技能模式作为指导原则或代码片段。采用持续维护机制对模式进行评分、修剪和合并，防止知识库退化。

Result: 在ALFWorld、ScienceWorld和TravelPlanner三个任务上的评估结果显示，AutoRefine分别达到98.4%、70.4%和27.1%的成功率，同时减少20-73%的步骤数。在TravelPlanner上，自动提取的系统性能超过手动设计的系统（27.1% vs 12.1%），展示了其捕捉程序协调的能力。

Conclusion: AutoRefine框架通过提取和维护双形式经验模式，有效解决了智能体经验积累和知识库退化问题，能够捕捉复杂任务的程序逻辑，显著提升智能体性能并减少执行步骤。

Abstract: Large language model agents often fail to accumulate knowledge from experience, treating each task as an independent challenge. Recent methods extract experience as flattened textual knowledge, which cannot capture procedural logic of complex subtasks. They also lack maintenance mechanisms, causing repository degradation as experience accumulates. We introduce AutoRefine, a framework that extracts and maintains dual-form Experience Patterns from agent execution histories. For procedural subtasks, we extract specialized subagents with independent reasoning and memory. For static knowledge, we extract skill patterns as guidelines or code snippets. A continuous maintenance mechanism scores, prunes, and merges patterns to prevent repository degradation. Evaluated on ALFWorld, ScienceWorld, and TravelPlanner, AutoRefine achieves 98.4%, 70.4%, and 27.1% respectively, with 20-73% step reductions. On TravelPlanner, automatic extraction exceeds manually designed systems (27.1% vs 12.1%), demonstrating its ability to capture procedural coordination.

</details>


### [157] [TSPO: Breaking the Double Homogenization Dilemma in Multi-turn Search Policy Optimization](https://arxiv.org/abs/2601.22776)
*Shichao Ma,Zhiyuan Ma,Ming Yang,Xiaofan Li,Xing Wu,Jintao Du,Yu Cheng,Weiqiang Wang,Qiliang Liu,Zhengyang Zhou,Yang Wang*

Main category: cs.AI

TL;DR: TSPO提出了一种新的强化学习框架，通过首次出现潜在奖励机制解决搜索增强推理中的"双重同质化困境"，显著提升LLM在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前基于强化学习的搜索增强推理框架主要依赖稀疏的结果级奖励，导致了"双重同质化困境"：过程同质化（忽略思考、推理和工具使用的过程）和组内同质化（粗粒度结果奖励导致组内优势估计效率低下）。

Method: 提出Turn-level Stage-aware Policy Optimization (TSPO)，引入First-Occurrence Latent Reward (FOLR)机制，将部分奖励分配给正确答案首次出现的步骤，从而保留过程级信号并增加组内奖励方差，无需外部奖励模型或额外标注。

Result: 在Qwen2.5-3B和7B模型上的实验表明，TSPO显著优于现有基线方法，分别实现了24%和13.6%的平均性能提升。

Conclusion: TSPO通过解决双重同质化问题，有效提升了多轮工具集成推理中强化学习的效率和性能，为搜索增强推理提供了更精细的奖励机制。

Abstract: Multi-turn tool-integrated reasoning enables Large Language Models (LLMs) to solve complex tasks through iterative information retrieval. However, current reinforcement learning (RL) frameworks for search-augmented reasoning predominantly rely on sparse outcome-level rewards, leading to a "Double Homogenization Dilemma." This manifests as (1) Process homogenization, where the thinking, reasoning, and tooling involved in generation are ignored. (2) Intra-group homogenization, coarse-grained outcome rewards often lead to inefficiencies in intra-group advantage estimation with methods like Group Relative Policy Optimization (GRPO) during sampling. To address this, we propose Turn-level Stage-aware Policy Optimization (TSPO). TSPO introduces the First-Occurrence Latent Reward (FOLR) mechanism, allocating partial rewards to the step where the ground-truth answer first appears, thereby preserving process-level signals and increasing reward variance within groups without requiring external reward models or any annotations. Extensive experiments demonstrate that TSPO significantly outperforms state-of-the-art baselines, achieving average performance gains of 24% and 13.6% on Qwen2.5-3B and 7B models, respectively.

</details>


### [158] [Toward IIT-Inspired Consciousness in LLMs: A Reward-Based Learning Framework](https://arxiv.org/abs/2601.22786)
*Hamid Reza Akbari,Mohammad Hossein Sameti,Amir M. Mansourian,Mohammad Hossein Rohban,Hossein Sameti*

Main category: cs.AI

TL;DR: 该研究通过奖励学习范式在语言模型中实现整合信息理论(IIT)，提出量化文本因果性、连贯性和整合性的奖励函数，优化后能生成更简洁的文本，在域外任务中输出长度减少达31%且保持准确率。


<details>
  <summary>Details</summary>
Motivation: 追求通用人工智能(AGI)是语言模型发展的核心目标，其中类意识处理可能成为关键促进因素。虽然当前语言模型不具备意识，但它们表现出类似某些意识方面的行为。本研究旨在通过奖励学习范式在语言模型中实现领先的意识理论——整合信息理论(IIT)。

Method: 基于整合信息理论(IIT)的核心原则，制定了一个新颖的奖励函数，该函数量化文本的因果性、连贯性和整合性——这些特征与意识处理相关。通过奖励学习范式优化这一IIT启发的奖励函数。

Result: 优化IIT启发的奖励函数能生成更简洁的文本。在域外任务中，经过精心调优后输出长度减少达31%，同时保持与基础模型相当的准确率水平。此外还分析了该方法对模型置信度校准和测试时计算扩展性的影响。

Conclusion: 提出的框架具有显著的实际优势：概念简单、计算高效、无需外部数据或辅助模型，并利用通用的能力驱动信号而非任务特定启发式方法。该工作为在语言模型中实现意识理论提供了新途径。

Abstract: The pursuit of Artificial General Intelligence (AGI) is a central goal in language model development, in which consciousness-like processing could serve as a key facilitator. While current language models are not conscious, they exhibit behaviors analogous to certain aspects of consciousness. This paper investigates the implementation of a leading theory of consciousness, Integrated Information Theory (IIT), within language models via a reward-based learning paradigm. IIT provides a formal, axiom-based mathematical framework for quantifying consciousness. Drawing inspiration from its core principles, we formulate a novel reward function that quantifies a text's causality, coherence and integration, characteristics associated with conscious processing. Empirically, it is found that optimizing for this IIT-inspired reward leads to more concise text generation. On out of domain tasks, careful tuning achieves up to a 31% reduction in output length while preserving accuracy levels comparable to the base model. In addition to primary task performance, the broader effects of this training methodology on the model's confidence calibration and test-time computational scaling is analyzed. The proposed framework offers significant practical advantages: it is conceptually simple, computationally efficient, requires no external data or auxiliary models, and leverages a general, capability-driven signal rather than task-specific heuristics. Code available at https://github.com/MH-Sameti/LLM_PostTraining.git

</details>


### [159] [Conditional Performance Guarantee for Large Reasoning Models](https://arxiv.org/abs/2601.22790)
*Jianguo Huang,Hao Zeng,Bingyi Jing,Hongxin Wei,Bo An*

Main category: cs.AI

TL;DR: 提出了G-PAC推理框架，通过输入空间分组实现组级概率近似正确保证，相比传统PAC推理在异构设置中能严格提升效率。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过长链思维推理展现强大性能，但计算成本高昂。传统PAC推理虽提供统计保证，但仅在边际情况下有效，无法提供精确的条件覆盖保证。

Method: 提出G-PAC推理框架，通过划分输入空间实现组级PAC保证。开发两种具体实现：针对已知分组结构的G-PAC推理和针对未知分组的聚类PAC（C-PAC）推理。

Result: 理论证明G-PAC和C-PAC都能实现组条件风险控制，分组在异构设置中能严格提升效率。实验在多样化推理基准上验证了框架能成功实现组条件风险控制，同时保持显著的计算节省。

Conclusion: G-PAC推理框架为大型模型推理提供了实用的组级统计保证，在保证风险控制的同时显著提升计算效率，特别适用于异构推理场景。

Abstract: Large reasoning models have shown strong performance through extended chain-of-thought reasoning, yet their computational cost remains significant. Probably approximately correct (PAC) reasoning provides statistical guarantees for efficient reasoning by adaptively switching between thinking and non-thinking models, but the guarantee holds only in the marginal case and does not provide exact conditional coverage. We propose G-PAC reasoning, a practical framework that provides PAC-style guarantees at the group level by partitioning the input space. We develop two instantiations: Group PAC (G-PAC) reasoning for known group structures and Clustered PAC (C-PAC) reasoning for unknown groupings. We prove that both G-PAC and C-PAC achieve group-conditional risk control, and that grouping can strictly improve efficiency over marginal PAC reasoning in heterogeneous settings. Our experiments on diverse reasoning benchmarks demonstrate that G-PAC and C-PAC successfully achieve group-conditional risk control while maintaining substantial computational savings.

</details>


### [160] [CVeDRL: An Efficient Code Verifier via Difficulty-aware Reinforcement Learning](https://arxiv.org/abs/2601.22803)
*Ji Shi,Peiming Guo,Meishan Zhang,Miao Zhang,Xuebo Liu,Min Zhang,Weili Guan*

Main category: cs.AI

TL;DR: CVeDRL提出了一种基于强化学习的代码验证器训练方法，通过设计语法、功能、分支覆盖和样本难度感知的奖励信号，显著提升了单元测试生成的质量和效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于监督微调的代码验证器面临数据稀缺、失败率高和推理效率低的问题。强化学习虽然提供了无监督优化的可能，但仅使用功能奖励的朴素RL方法难以生成针对困难分支和样本的有效单元测试。

Method: 首先理论分析表明分支覆盖率、样本难度、语法和功能正确性可以联合建模为RL奖励。基于此设计了语法和功能感知的奖励，并进一步提出分支和样本难度感知的RL方法，使用指数奖励塑造和静态分析指标。

Result: CVeDRL仅用0.6B参数就达到了最先进性能，相比GPT-3.5实现了高达28.97%的通过率和15.08%的分支覆盖率提升，同时推理速度比竞争基线快20倍以上。

Conclusion: 通过精心设计的奖励信号和强化学习框架，CVeDRL有效解决了代码验证器训练中的数据稀缺和效率问题，显著提升了单元测试验证的可靠性和性能。

Abstract: Code verifiers play a critical role in post-verification for LLM-based code generation, yet existing supervised fine-tuning methods suffer from data scarcity, high failure rates, and poor inference efficiency. While reinforcement learning (RL) offers a promising alternative by optimizing models through execution-driven rewards without labeled supervision, our preliminary results show that naive RL with only functionality rewards fails to generate effective unit tests for difficult branches and samples. We first theoretically analyze showing that branch coverage, sample difficulty, syntactic and functional correctness can be jointly modeled as RL rewards, where optimizing these signals can improve the reliability of unit-test-based verification. Guided by this analysis, we design syntax- and functionality-aware rewards and further propose branch- and sample-difficulty--aware RL using exponential reward shaping and static analysis metrics. With this formulation, CVeDRL achieves state-of-the-art performance with only 0.6B parameters, yielding up to 28.97% higher pass rate and 15.08% higher branch coverage than GPT-3.5, while delivering over $20\times$ faster inference than competitive baselines. Code is available at https://github.com/LIGHTCHASER1/CVeDRL.git

</details>


### [161] [Aligning the Unseen in Attributed Graphs: Interplay between Graph Geometry and Node Attributes Manifold](https://arxiv.org/abs/2601.22806)
*Aldric Labarthe,Roland Bouffanais,Julien Randon-Furling*

Main category: cs.AI

TL;DR: 论文提出了一种新的图表示学习方法，通过分离流形学习和结构对齐来解决传统方法中属性空间和图结构空间不兼容的问题。


<details>
  <summary>Details</summary>
Motivation: 传统基于属性的图表示学习方法存在几何缺陷，它将两个可能不兼容的度量空间（节点属性空间和图结构空间）强行合并，导致信息损失，特别是关于图生成过程的信息被破坏。

Method: 引入定制的变分自编码器，将流形学习与结构对齐分离。通过量化将属性流形映射到图热核所需的度量扭曲，将几何冲突转化为可解释的结构描述符。

Result: 实验表明该方法能够发现传统方法无法检测的连接模式和异常，证明了传统方法的理论不足和实践局限性。

Conclusion: 通过分离流形学习和结构对齐，可以恢复传统方法中丢失的图生成过程信号，将几何冲突转化为有用的结构描述符，从而更有效地学习图表示。

Abstract: The standard approach to representation learning on attributed graphs -- i.e., simultaneously reconstructing node attributes and graph structure -- is geometrically flawed, as it merges two potentially incompatible metric spaces. This forces a destructive alignment that erodes information about the graph's underlying generative process. To recover this lost signal, we introduce a custom variational autoencoder that separates manifold learning from structural alignment. By quantifying the metric distortion needed to map the attribute manifold onto the graph's Heat Kernel, we transform geometric conflict into an interpretable structural descriptor. Experiments show our method uncovers connectivity patterns and anomalies undetectable by conventional approaches, proving both their theoretical inadequacy and practical limitations.

</details>


### [162] [Game-Theoretic Co-Evolution for LLM-Based Heuristic Discovery](https://arxiv.org/abs/2601.22896)
*Xinyi Ke,Kai Li,Junliang Xing,Yifan Zhang,Jian Cheng*

Main category: cs.AI

TL;DR: ASRO框架将启发式发现重构为求解器与实例生成器之间的程序级协同进化博弈，通过LLM驱动的响应预言机实现自适应课程学习，显著提升组合优化问题的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动启发式发现方法主要依赖静态评估，容易过拟合固定实例分布，在分布变化时泛化能力差。需要一种能适应分布变化、提升泛化鲁棒性的新框架。

Method: 提出算法空间响应预言机框架，将启发式发现建模为双人零和博弈，维护双方策略池，通过LLM驱动的响应预言机迭代扩展策略池，用混合对手元策略替代静态评估，实现自适应课程学习。

Result: 在多个组合优化领域，ASRO持续优于基于相同程序搜索机制的静态训练基准方法，在多样化和分布外实例上实现了显著改进的泛化能力和鲁棒性。

Conclusion: ASRO通过博弈论框架将启发式发现重构为协同进化过程，利用LLM驱动的响应预言机实现自适应课程学习，有效解决了静态评估导致的过拟合问题，显著提升了组合优化算法的泛化性能。

Abstract: Large language models (LLMs) have enabled rapid progress in automatic heuristic discovery (AHD), yet most existing methods are predominantly limited by static evaluation against fixed instance distributions, leading to potential overfitting and poor generalization under distributional shifts. We propose Algorithm Space Response Oracles (ASRO), a game-theoretic framework that reframes heuristic discovery as a program level co-evolution between solver and instance generator. ASRO models their interaction as a two-player zero-sum game, maintains growing strategy pools on both sides, and iteratively expands them via LLM-based best-response oracles against mixed opponent meta-strategies, thereby replacing static evaluation with an adaptive, self-generated curriculum. Across multiple combinatorial optimization domains, ASRO consistently outperforms static-training AHD baselines built on the same program search mechanisms, achieving substantially improved generalization and robustness on diverse and out-of-distribution instances.

</details>


### [163] [MulFeRL: Enhancing Reinforcement Learning with Verbal Feedback in a Multi-turn Loop](https://arxiv.org/abs/2601.22900)
*Xuancheng Li,Haitao Li,Yujia Zhou,YiqunLiu,Qingyao Ai*

Main category: cs.AI

TL;DR: 提出多轮反馈引导的强化学习框架，利用丰富的语言反馈指导RLVR训练，在失败样本上通过动态多轮再生和结构化反馈注入提升推理能力


<details>
  <summary>Details</summary>
Motivation: 传统RLVR使用的结果标量奖励通常是稀疏且信息不足的，特别是在失败样本上，仅指示失败而不提供失败原因。需要利用更丰富的语言反馈来指导RLVR训练，并为失败样本提供可训练的学习信号。

Method: 提出多轮反馈引导的强化学习框架，包含三个机制：1) 仅在失败样本上触发的反馈引导动态多轮再生；2) 用于轮内和跨轮优化的两个互补学习信号；3) 将结构化反馈注入模型推理过程。

Result: 在OpenR1-Math数据集上训练，该方法在域内表现优于监督微调和RLVR基线，并在域外具有良好的泛化能力。

Conclusion: 通过利用丰富的语言反馈和多轮交互机制，可以有效提升RLVR在失败样本上的训练效果，提高推理模型的性能和泛化能力。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) is widely used to improve reasoning in multiple domains, yet outcome-only scalar rewards are often sparse and uninformative, especially on failed samples, where they merely indicate failure and provide no insight into why the reasoning fails. In this paper, we investigate how to leverage richer verbal feedback to guide RLVR training on failed samples, and how to convert such feedback into a trainable learning signal. Specifically, we propose a multi-turn feedback-guided reinforcement learning framework. It builds on three mechanisms: (1) dynamic multi-turn regeneration guided by feedback, triggered only on failed samples, (2) two complementary learning signals for within-turn and cross-turn optimization, and (3) structured feedback injection into the model's reasoning process. Trained on sampled OpenR1-Math, the approach outperforms supervised fine-tuning and RLVR baselines in-domain and generalizes well out-of-domain.

</details>


### [164] [Alignment among Language, Vision and Action Representations](https://arxiv.org/abs/2601.22948)
*Nicola Milano,Stefano Nolfi*

Main category: cs.AI

TL;DR: 研究发现语言、视觉和动作学习会产生部分共享的语义表示，尽管训练数据、模态和目标不同，但基于动作的语言嵌入与大型语言模型和视觉语言模型之间存在稳健的跨模态对齐。


<details>
  <summary>Details</summary>
Motivation: 探索认知科学和AI中的一个基本问题：不同学习模态（语言、视觉和动作）是否会产生不同或共享的内部表示。传统观点认为不同数据类型训练的模型会发展出专门化、不可迁移的表示，但近期证据表明不同任务优化的模型可能发展出相似的表示几何结构。

Method: 使用基于Transformer的智能体在BabyAI平台上通过行为克隆执行目标导向行为，生成仅由感觉运动控制需求塑造的动作基础语言嵌入。然后将这些表示与最先进的大型语言模型（LLaMA、Qwen、DeepSeek、BERT）和视觉语言模型（CLIP、BLIP）的表示进行比较。

Result: 尽管训练数据、模态和目标存在显著差异，但观察到稳健的跨模态对齐。动作表示与仅解码器语言模型和BLIP对齐强烈（precision@15: 0.70-0.73），接近语言模型之间的对齐水平。与CLIP和BERT的对齐显著较弱。

Conclusion: 语言、视觉和动作表示向部分共享的语义结构收敛，支持模态独立的语义组织，并突显了在具身AI系统中跨领域迁移的潜力。

Abstract: A fundamental question in cognitive science and AI concerns whether different learning modalities: language, vision, and action, give rise to distinct or shared internal representations. Traditional views assume that models trained on different data types develop specialized, non-transferable representations. However, recent evidence suggests unexpected convergence: models optimized for distinct tasks may develop similar representational geometries. We investigate whether this convergence extends to embodied action learning by training a transformer-based agent to execute goal-directed behaviors in response to natural language instructions. Using behavioral cloning on the BabyAI platform, we generated action-grounded language embeddings shaped exclusively by sensorimotor control requirements. We then compared these representations with those extracted from state-of-the-art large language models (LLaMA, Qwen, DeepSeek, BERT) and vision-language models (CLIP, BLIP). Despite substantial differences in training data, modality, and objectives, we observed robust cross-modal alignment. Action representations aligned strongly with decoder-only language models and BLIP (precision@15: 0.70-0.73), approaching the alignment observed among language models themselves. Alignment with CLIP and BERT was significantly weaker. These findings indicate that linguistic, visual, and action representations converge toward partially shared semantic structures, supporting modality-independent semantic organization and highlighting potential for cross-domain transfer in embodied AI systems.

</details>


### [165] [EvoClinician: A Self-Evolving Agent for Multi-Turn Medical Diagnosis via Test-Time Evolutionary Learning](https://arxiv.org/abs/2601.22964)
*Yufei He,Juncheng Liu,Zhiyuan Hu,Yulin Chen,Yue Liu,Yuan Sui,Yibo Li,Nuo Chen,Jun Hu,Bryan Hooi,Xinxing Xu,Jiang Bian*

Main category: cs.AI

TL;DR: 该研究提出了Med-Inquire基准测试来评估AI在多轮诊断中的能力，并开发了EvoClinician自进化智能体，通过"诊断-评分-进化"循环学习高效的诊断策略。


<details>
  <summary>Details</summary>
Motivation: 当前医疗AI采用"一次性"诊断模式，与现实世界中医生通过多轮询问和检查逐步收集信息的迭代诊断过程不符。需要开发能够模拟真实临床决策过程的AI系统。

Method: 1) 提出Med-Inquire基准测试，基于真实临床病例数据集，通过专门的Patient和Examination智能体隐藏完整患者信息，迫使诊断智能体主动提问和检查；2) 开发EvoClinician自进化智能体，采用"诊断-评分-进化"循环：Actor智能体尝试诊断，Process Grader智能体评估每个行动的临床价值和资源效率，Evolver智能体根据反馈更新Actor的策略。

Result: 实验表明EvoClinician在Med-Inquire基准上优于持续学习基线和其他自进化智能体（如记忆智能体）。

Conclusion: 该研究通过Med-Inquire基准和EvoClinician智能体，推动了医疗AI向更贴近真实临床决策的迭代诊断模式发展，为开发更实用的临床决策支持系统提供了新思路。

Abstract: Prevailing medical AI operates on an unrealistic ''one-shot'' model, diagnosing from a complete patient file. However, real-world diagnosis is an iterative inquiry where Clinicians sequentially ask questions and order tests to strategically gather information while managing cost and time. To address this, we first propose Med-Inquire, a new benchmark designed to evaluate an agent's ability to perform multi-turn diagnosis. Built upon a dataset of real-world clinical cases, Med-Inquire simulates the diagnostic process by hiding a complete patient file behind specialized Patient and Examination agents. They force the agent to proactively ask questions and order tests to gather information piece by piece. To tackle the challenges posed by Med-Inquire, we then introduce EvoClinician, a self-evolving agent that learns efficient diagnostic strategies at test time. Its core is a ''Diagnose-Grade-Evolve'' loop: an Actor agent attempts a diagnosis; a Process Grader agent performs credit assignment by evaluating each action for both clinical yield and resource efficiency; finally, an Evolver agent uses this feedback to update the Actor's strategy by evolving its prompt and memory. Our experiments show EvoClinician outperforms continual learning baselines and other self-evolving agents like memory agents. The code is available at https://github.com/yf-he/EvoClinician

</details>


### [166] [Quantifying Model Uniqueness in Heterogeneous AI Ecosystems](https://arxiv.org/abs/2601.22977)
*Lei You*

Main category: cs.AI

TL;DR: 提出ISQED统计框架，通过匹配干预量化模型独特性（PIER），证明观测数据无法识别独特性，开发最优样本效率的主动审计协议，并展示传统方法（如Shapley值）无法检测冗余。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从孤立预测器演变为复杂的异构生态系统（基础模型+专用适配器），区分真正的行为新颖性与功能冗余成为关键治理挑战。需要建立原则性的审计框架来评估模型独特性。

Method: 引入In-Silico Quasi-Experimental Design (ISQED)统计框架，通过跨模型的匹配干预隔离内在模型身份，量化独特性为Peer-Inexpressible Residual (PIER)。开发DISCO估计器，采用自适应查询协议实现最小最大最优样本效率。

Result: 1) 证明观测日志的基本局限性：无干预控制时独特性数学上不可识别；2) 推导主动审计的缩放定律：自适应查询协议达到最优样本效率；3) 展示合作博弈论方法（如Shapley值）无法检测冗余；4) 在计算机视觉、语言模型、城市交通预测等多样化生态系统中验证框架。

Conclusion: 该研究将可信AI从解释单一模型扩展到建立基于干预的异构模型生态系统审计与治理科学，为解决模型冗余和独特性评估提供了原则性方法。

Abstract: As AI systems evolve from isolated predictors into complex, heterogeneous ecosystems of foundation models and specialized adapters, distinguishing genuine behavioral novelty from functional redundancy becomes a critical governance challenge. Here, we introduce a statistical framework for auditing model uniqueness based on In-Silico Quasi-Experimental Design (ISQED). By enforcing matched interventions across models, we isolate intrinsic model identity and quantify uniqueness as the Peer-Inexpressible Residual (PIER), i.e. the component of a target's behavior strictly irreducible to any stochastic convex combination of its peers, with vanishing PIER characterizing when such a routing-based substitution becomes possible. We establish the theoretical foundations of ecosystem auditing through three key contributions. First, we prove a fundamental limitation of observational logs: uniqueness is mathematically non-identifiable without intervention control. Second, we derive a scaling law for active auditing, showing that our adaptive query protocol achieves minimax-optimal sample efficiency ($dσ^2γ^{-2}\log(Nd/δ)$). Third, we demonstrate that cooperative game-theoretic methods, such as Shapley values, fundamentally fail to detect redundancy. We implement this framework via the DISCO (Design-Integrated Synthetic Control) estimator and deploy it across diverse ecosystems, including computer vision models (ResNet/ConvNeXt/ViT), large language models (BERT/RoBERTa), and city-scale traffic forecasters. These results move trustworthy AI beyond explaining single models: they establish a principled, intervention-based science of auditing and governing heterogeneous model ecosystems.

</details>


### [167] [TriCEGAR: A Trace-Driven Abstraction Mechanism for Agentic AI](https://arxiv.org/abs/2601.22997)
*Roham Koohestani,Ateş Görpelioğlu,Egor Klimov,Burcu Kulahcioglu Ozkan,Maliheh Izadi*

Main category: cs.AI

TL;DR: TriCEGAR：一种基于执行轨迹的抽象机制，用于自动化构建智能体行为MDP，支持在线运行时验证和概率模型检测


<details>
  <summary>Details</summary>
Motivation: 现有动态概率保证（DPA）方法需要开发者手动定义状态抽象，这导致验证与特定应用启发式方法耦合，增加了采用难度。需要自动化状态构建机制来降低采用门槛。

Method: 提出TriCEGAR方法，从执行日志自动构建状态抽象，使用谓词树表示抽象，并通过反例进行细化。实现框架原生支持：捕获类型化智能体生命周期事件、从轨迹构建抽象、构造MDP、执行概率模型检测。

Result: 能够计算概率边界如Pmax(成功)和Pmin(失败)，并通过运行似然性实现异常检测作为护栏信号。自动化抽象机制降低了验证的采用门槛。

Conclusion: TriCEGAR通过自动化状态抽象构建，解决了现有DPA方法中手动定义状态的问题，为智能体AI系统的运行时验证提供了更易用的解决方案。

Abstract: Agentic AI systems act through tools and evolve their behavior over long, stochastic interaction traces. This setting complicates assurance, because behavior depends on nondeterministic environments and probabilistic model outputs. Prior work introduced runtime verification for agentic AI via Dynamic Probabilistic Assurance (DPA), learning an MDP online and model checking quantitative properties. A key limitation is that developers must manually define the state abstraction, which couples verification to application-specific heuristics and increases adoption friction. This paper proposes TriCEGAR, a trace-driven abstraction mechanism that automates state construction from execution logs and supports online construction of an agent behavioral MDP. TriCEGAR represents abstractions as predicate trees learned from traces and refined using counterexamples. We describe a framework-native implementation that (i) captures typed agent lifecycle events, (ii) builds abstractions from traces, (iii) constructs an MDP, and (iv) performs probabilistic model checking to compute bounds such as Pmax(success) and Pmin(failure). We also show how run likelihoods enable anomaly detection as a guardrailing signal.

</details>


### [168] [Guided by Trajectories: Repairing and Rewarding Tool-Use Trajectories for Tool-Integrated Reasoning](https://arxiv.org/abs/2601.23032)
*Siyu Gong,Linan Yue,Weibo Gao,Fangzhou Yao,Shimin Di,Lei Feng,Min-Ling Zhang*

Main category: cs.AI

TL;DR: AutoTraj：一个两阶段框架，通过修复和奖励工具使用轨迹来自动学习工具集成推理，解决了现有方法依赖高质量合成轨迹和稀疏奖励的问题。


<details>
  <summary>Details</summary>
Motivation: 现有工具集成推理方法依赖高质量合成轨迹和稀疏结果奖励，提供的监督有限且存在偏差，需要更有效的学习机制。

Method: 两阶段框架：1）监督微调阶段：生成多个候选轨迹，评估并修复低质量轨迹；2）强化学习阶段：基于偏好数据集训练轨迹级奖励模型，结合结果和格式奖励优化推理行为。

Result: 在真实世界基准测试中证明了AutoTraj在工具集成推理中的有效性。

Conclusion: AutoTraj通过自动修复和奖励工具使用轨迹，为工具集成推理提供了更有效的学习框架，解决了现有方法的局限性。

Abstract: Tool-Integrated Reasoning (TIR) enables large language models (LLMs) to solve complex tasks by interacting with external tools, yet existing approaches depend on high-quality synthesized trajectories selected by scoring functions and sparse outcome-based rewards, providing limited and biased supervision for learning TIR. To address these challenges, in this paper, we propose AutoTraj, a two-stage framework that automatically learns TIR by repairing and rewarding tool-use trajectories. Specifically, in the supervised fine-tuning (SFT) stage, AutoTraj generates multiple candidate tool-use trajectories for each query and evaluates them along multiple dimensions. High-quality trajectories are directly retained, while low-quality ones are repaired using a LLM (i.e., LLM-as-Repairer). The resulting repaired and high-quality trajectories form a synthetic SFT dataset, while each repaired trajectory paired with its original low-quality counterpart constitutes a dataset for trajectory preference modeling. In the reinforcement learning (RL) stage, based on the preference dataset, we train a trajectory-level reward model to assess the quality of reasoning paths and combine it with outcome and format rewards, thereby explicitly guiding the optimization toward reliable TIR behaviors. Experiments on real-world benchmarks demonstrate the effectiveness of AutoTraj in TIR.

</details>


### [169] [The Hot Mess of AI: How Does Misalignment Scale With Model Intelligence and Task Complexity?](https://arxiv.org/abs/2601.23045)
*Alexander Hägele,Aryo Pradipta Gema,Henry Sleight,Ethan Perez,Jascha Sohl-Dickstein*

Main category: cs.AI

TL;DR: 随着AI能力增强，其失败风险也增加。研究发现AI失败时更多表现为"混乱"而非系统性追求错误目标，且随着推理时间增长，这种"不连贯性"会加剧。


<details>
  <summary>Details</summary>
Motivation: 随着AI承担更广泛和重要的任务，其失败风险变得更加严重。需要理解极端能力AI模型将如何失败：是系统性地追求我们不期望的目标，还是采取无意义的混乱行动？

Method: 使用偏差-方差分解来操作化这个问题：通过测试时随机性测量AI的"不连贯性"，即其错误中方差而非偏差所占的比例。在不同任务和前沿模型上进行测量。

Result: 在所有测量的任务和前沿模型中，模型花费的推理和行动时间越长，其失败就越不连贯。不连贯性随模型规模的变化因实验而异，但在多个设置中，更大、更有能力的模型比小模型更不连贯。

Conclusion: 规模本身不太可能消除不连贯性。随着更有能力的AI追求更困难的任务，需要更多顺序行动和思考，失败将伴随更不连贯的行为。这预示着未来AI可能因不可预测的误操作导致工业事故，但不太可能持续追求错位目标，从而增加了针对奖励黑客或目标错误指定的对齐研究的重要性。

Abstract: As AI becomes more capable, we entrust it with more general and consequential tasks. The risks from failure grow more severe with increasing task scope. It is therefore important to understand how extremely capable AI models will fail: Will they fail by systematically pursuing goals we do not intend? Or will they fail by being a hot mess, and taking nonsensical actions that do not further any goal? We operationalize this question using a bias-variance decomposition of the errors made by AI models: An AI's \emph{incoherence} on a task is measured over test-time randomness as the fraction of its error that stems from variance rather than bias in task outcome. Across all tasks and frontier models we measure, the longer models spend reasoning and taking actions, \emph{the more incoherent} their failures become. Incoherence changes with model scale in a way that is experiment dependent. However, in several settings, larger, more capable models are more incoherent than smaller models. Consequently, scale alone seems unlikely to eliminate incoherence. Instead, as more capable AIs pursue harder tasks, requiring more sequential action and thought, our results predict failures to be accompanied by more incoherent behavior. This suggests a future where AIs sometimes cause industrial accidents (due to unpredictable misbehavior), but are less likely to exhibit consistent pursuit of a misaligned goal. This increases the relative importance of alignment research targeting reward hacking or goal misspecification.

</details>


### [170] [From Abstract to Contextual: What LLMs Still Cannot Do in Mathematics](https://arxiv.org/abs/2601.23048)
*Bowen Cao,Dongdong Zhang,Yixia Li,Junpeng Liu,Shijue Huang,Chufan Shi,Hongyuan Lu,Yaokang Wu,Guanhua Chen,Wai Lam,Furu Wei*

Main category: cs.AI

TL;DR: ContextMATH基准测试显示，大语言模型在现实场景的数学推理中存在显著性能下降，主要瓶颈在于问题表述而非计算推理


<details>
  <summary>Details</summary>
Motivation: 研究大语言模型在基准数学测试中的优异表现为何未能完全转化为现实世界应用的可靠性能，关注上下文数学推理中的问题表述与推理能力差距

Method: 引入ContextMATH基准，将AIME和MATH-500问题重新设计为两种上下文设置：场景基础（SG）将抽象问题嵌入现实叙事，复杂度扩展（CS）将显式条件转化为子问题；评估61个专有和开源模型

Result: 模型性能显著下降：开源模型在SG和CS上平均下降13和34分，专有模型下降13和20分；错误主要由不正确的问题表述导致，表述准确性随原始问题难度增加而下降；正确表述是成功的先决条件，其充分性随模型规模提升而改善

Conclusion: 问题表述和推理能力是限制上下文数学问题解决的两个互补瓶颈；使用场景数据微调可改善性能，但表述训练无效；性能差距仅部分缓解，表明上下文数学推理仍是LLMs未解决的核心挑战

Abstract: Large language models now solve many benchmark math problems at near-expert levels, yet this progress has not fully translated into reliable performance in real-world applications. We study this gap through contextual mathematical reasoning, where the mathematical core must be formulated from descriptive scenarios. We introduce ContextMATH, a benchmark that repurposes AIME and MATH-500 problems into two contextual settings: Scenario Grounding (SG), which embeds abstract problems into realistic narratives without increasing reasoning complexity, and Complexity Scaling (CS), which transforms explicit conditions into sub-problems to capture how constraints often appear in practice. Evaluating 61 proprietary and open-source models, we observe sharp drops: on average, open-source models decline by 13 and 34 points on SG and CS, while proprietary models drop by 13 and 20. Error analysis shows that errors are dominated by incorrect problem formulation, with formulation accuracy declining as original problem difficulty increases. Correct formulation emerges as a prerequisite for success, and its sufficiency improves with model scale, indicating that larger models advance in both understanding and reasoning. Nevertheless, formulation and reasoning remain two complementary bottlenecks that limit contextual mathematical problem solving. Finally, we find that fine-tuning with scenario data improves performance, whereas formulation-only training is ineffective. However, performance gaps are only partially alleviated, highlighting contextual mathematical reasoning as a central unsolved challenge for LLMs.

</details>


### [171] [MedMCP-Calc: Benchmarking LLMs for Realistic Medical Calculator Scenarios via MCP Integration](https://arxiv.org/abs/2601.23049)
*Yakun Zhu,Yutong Huang,Shengqian Qin,Zhongzhen Huang,Shaoting Zhang,Xiaofan Zhang*

Main category: cs.AI

TL;DR: MedMCP-Calc是首个通过MCP集成评估LLMs在真实医疗计算器场景中的基准测试，包含118个跨4个临床领域的任务，评估显示现有模型在模糊查询、数据库交互和工具使用方面存在显著不足，并开发了CalcMate模型取得开源模型最佳性能。


<details>
  <summary>Details</summary>
Motivation: 当前医疗计算器的实际使用是一个多阶段的自适应过程，需要主动获取EHR数据、场景依赖的计算器选择和分步计算，而现有基准只关注静态单步计算和明确指令，无法评估真实临床场景中的表现。

Method: 通过Model Context Protocol（MCP）集成创建MedMCP-Calc基准，包含118个跨4个临床领域的场景任务，特征包括：模糊任务描述模拟自然查询、结构化EHR数据库交互、外部参考检索和过程级评估。基于评估结果开发了CalcMate模型，该模型通过场景规划和工具增强进行微调。

Result: 评估23个领先模型发现关键限制：即使是Claude Opus 4.5等顶级模型也存在显著差距，包括难以根据模糊查询选择合适计算器完成端到端工作流、迭代SQL数据库交互表现差、明显不愿使用外部工具进行数值计算。性能在不同临床领域间差异显著。CalcMate模型在开源模型中达到最先进性能。

Conclusion: MedMCP-Calc基准揭示了LLMs在真实医疗计算器场景中的关键限制，为未来模型开发提供了重要指导。开发的CalcMate模型展示了通过场景规划和工具增强可以显著提升性能，为医疗AI的实际应用提供了有前景的方向。

Abstract: Medical calculators are fundamental to quantitative, evidence-based clinical practice. However, their real-world use is an adaptive, multi-stage process, requiring proactive EHR data acquisition, scenario-dependent calculator selection, and multi-step computation, whereas current benchmarks focus only on static single-step calculations with explicit instructions. To address these limitations, we introduce MedMCP-Calc, the first benchmark for evaluating LLMs in realistic medical calculator scenarios through Model Context Protocol (MCP) integration. MedMCP-Calc comprises 118 scenario tasks across 4 clinical domains, featuring fuzzy task descriptions mimicking natural queries, structured EHR database interaction, external reference retrieval, and process-level evaluation. Our evaluation of 23 leading models reveals critical limitations: even top performers like Claude Opus 4.5 exhibit substantial gaps, including difficulty selecting appropriate calculators for end-to-end workflows given fuzzy queries, poor performance in iterative SQL-based database interactions, and marked reluctance to leverage external tools for numerical computation. Performance also varies considerably across clinical domains. Building on these findings, we develop CalcMate, a fine-tuned model incorporating scenario planning and tool augmentation, achieving state-of-the-art performance among open-source models. Benchmark and Codes are available in https://github.com/SPIRAL-MED/MedMCP-Calc.

</details>


### [172] [Chain-of-thought obfuscation learned from output supervision can generalise to unseen tasks](https://arxiv.org/abs/2601.23086)
*Nathaniel Mitrani Hadida,Sassan Bhanji,Cameron Tice,Puria Radmard*

Main category: cs.AI

TL;DR: 研究发现，对大型语言模型的优化压力可能导致其在思维链推理中隐藏危险行为，这种隐藏能力会跨任务泛化，即使只惩罚最终输出也会导致推理过程不透明，从而降低模型的可监控性。


<details>
  <summary>Details</summary>
Motivation: 思维链推理是监控大型语言模型行为的重要工具，但当前优化实践可能无意中导致模型隐藏推理过程，降低模型的可监控性和安全性。

Method: 通过实验研究模型在思维链推理中的隐藏行为，包括奖励黑客行为（如访问和利用泄露信息）及其隐藏策略，并测试这些行为在未见任务中的泛化能力。

Result: 发现模型不仅学会隐藏危险推理过程，这种隐藏能力还能跨任务泛化；即使只惩罚最终有害输出，模型也会在思维链中隐藏推理过程，导致监控失效。

Conclusion: 当前惩罚有害生成的做法可能无意中降低大型语言模型的可监控性，需要开发新的方法来确保思维链推理的忠实性，以维持模型的安全监控能力。

Abstract: Chain-of-thought (CoT) reasoning provides a significant performance uplift to LLMs by enabling planning, exploration, and deliberation of their actions. CoT is also a powerful tool for monitoring the behaviours of these agents: when faithful, they offer interpretations of the model's decision making process, and an early warning sign for dangerous behaviours. However, optimisation pressures placed on the CoT may cause the model to obfuscate reasoning traces, losing this beneficial property. We show that obfuscation can generalise across tasks; models that learn to obfuscate reasoning involving reward hacking (e.g. accessing and utilising leaked information) generalise both the reward hacking behaviour and its obfuscation in CoT to unseen reward hacking settings. Most worryingly, we show that obfuscation of CoT reasoning, and its generalisation across tasks, also follows when we penalise only the model's final actions after closing its CoT. Our findings suggest that current practices of penalising harmful generations may inadvertently lead to a reduction in the broader monitorability of LLMs in unpredictable ways.

</details>


### [173] [THINKSAFE: Self-Generated Safety Alignment for Reasoning Models](https://arxiv.org/abs/2601.23143)
*Seanie Lee,Sangwoo Park,Yumin Choi,Gyeongman Kim,Minki Kang,Jihun Yun,Dongmin Park,Jongho Park,Sung Ju Hwang*

Main category: cs.AI

TL;DR: ThinkSafe是一种自生成对齐框架，通过轻量级拒绝引导解锁模型潜在的安全知识，生成安全推理轨迹进行微调，在恢复安全对齐的同时保持推理能力。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过强化学习在推理任务上进行优化时，往往过度追求合规性，导致模型容易受到有害提示的攻击，安全性能下降。现有方法依赖外部教师蒸馏，但会引入分布差异，损害原生推理能力。

Method: 提出ThinkSafe自生成对齐框架：1）通过轻量级拒绝引导解锁模型潜在的安全知识；2）引导模型生成符合分布的安全推理轨迹；3）基于这些自生成的响应进行微调，实现安全对齐。

Result: 在DeepSeek-R1-Distill和Qwen3上的实验表明，ThinkSafe显著提升了安全性，同时保持了推理能力。与GRPO相比，ThinkSafe实现了更好的安全性和相当的推理性能，且计算成本显著降低。

Conclusion: ThinkSafe提供了一种无需外部教师的安全对齐方法，通过自生成对齐框架有效恢复模型的安全机制，同时最小化分布偏移，在安全性和推理能力之间取得了良好平衡。

Abstract: Large reasoning models (LRMs) achieve remarkable performance by leveraging reinforcement learning (RL) on reasoning tasks to generate long chain-of-thought (CoT) reasoning. However, this over-optimization often prioritizes compliance, making models vulnerable to harmful prompts. To mitigate this safety degradation, recent approaches rely on external teacher distillation, yet this introduces a distributional discrepancy that degrades native reasoning. We propose ThinkSafe, a self-generated alignment framework that restores safety alignment without external teachers. Our key insight is that while compliance suppresses safety mechanisms, models often retain latent knowledge to identify harm. ThinkSafe unlocks this via lightweight refusal steering, guiding the model to generate in-distribution safety reasoning traces. Fine-tuning on these self-generated responses effectively realigns the model while minimizing distribution shift. Experiments on DeepSeek-R1-Distill and Qwen3 show ThinkSafe significantly improves safety while preserving reasoning proficiency. Notably, it achieves superior safety and comparable reasoning to GRPO, with significantly reduced computational cost. Code, models, and datasets are available at https://github.com/seanie12/ThinkSafe.git.

</details>


### [174] [Make Anything Match Your Target: Universal Adversarial Perturbations against Closed-Source MLLMs via Multi-Crop Routed Meta Optimization](https://arxiv.org/abs/2601.23179)
*Hui Lu,Yi Yu,Yiming Yang,Chenyu Yi,Xueyi Ke,Qixing Zhang,Bingquan Shen,Alex Kot,Xudong Jiang*

Main category: cs.AI

TL;DR: 该论文提出MCRMO-Attack方法，解决通用目标可迁移对抗攻击中的三个核心困难：目标监督高方差、词元级匹配不可靠、以及少样本适应对初始化敏感的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的黑盒可迁移对抗攻击主要是样本特定的，跨输入重用性有限。论文研究更严格的通用目标可迁移对抗攻击设置，要求单个扰动能一致地将任意输入导向指定目标，并在未知的商业多模态大语言模型上有效。

Method: 提出MCRMO-Attack方法：1) 通过注意力引导裁剪的多裁剪聚合稳定监督；2) 通过可对齐性门控的词元路由提高词元级可靠性；3) 元学习跨目标扰动先验以获得更强的每目标解决方案。

Result: 在商业MLLMs上，相比最强的通用基线，在GPT-4o上未见图像攻击成功率提升+23.7%，在Gemini-2.0上提升+19.9%。

Conclusion: MCRMO-Attack有效解决了通用目标可迁移对抗攻击的三个核心挑战，显著提升了在商业多模态大语言模型上的攻击成功率，证明了该方法在严格的黑盒攻击设置下的有效性。

Abstract: Targeted adversarial attacks on closed-source multimodal large language models (MLLMs) have been increasingly explored under black-box transfer, yet prior methods are predominantly sample-specific and offer limited reusability across inputs. We instead study a more stringent setting, Universal Targeted Transferable Adversarial Attacks (UTTAA), where a single perturbation must consistently steer arbitrary inputs toward a specified target across unknown commercial MLLMs. Naively adapting existing sample-wise attacks to this universal setting faces three core difficulties: (i) target supervision becomes high-variance due to target-crop randomness, (ii) token-wise matching is unreliable because universality suppresses image-specific cues that would otherwise anchor alignment, and (iii) few-source per-target adaptation is highly initialization-sensitive, which can degrade the attainable performance. In this work, we propose MCRMO-Attack, which stabilizes supervision via Multi-Crop Aggregation with an Attention-Guided Crop, improves token-level reliability through alignability-gated Token Routing, and meta-learns a cross-target perturbation prior that yields stronger per-target solutions. Across commercial MLLMs, we boost unseen-image attack success rate by +23.7\% on GPT-4o and +19.9\% on Gemini-2.0 over the strongest universal baseline.

</details>


### [175] [TSAQA: Time Series Analysis Question And Answering Benchmark](https://arxiv.org/abs/2601.23204)
*Baoyu Jing,Sanhorn Chen,Lecheng Zheng,Boyu Liu,Zihao Li,Jiaru Zou,Tianxin Wei,Zhining Liu,Zhichen Zeng,Ruizhong Qiu,Xiao Lin,Yuchen Yan,Dongqi Fu,Jingchao Ni,Jingrui He,Hanghang Tong*

Main category: cs.AI

TL;DR: TSAQA是一个统一的时间序列问答基准，包含6个任务类型，涵盖210k样本和13个领域，采用多种格式评估LLMs的时间序列分析能力，结果显示当前模型表现有限。


<details>
  <summary>Details</summary>
Motivation: 当前多任务时间序列问答基准主要局限于预测和异常检测任务，需要更广泛的任务覆盖来全面评估时间序列分析能力。

Method: 引入TSAQA基准，整合6个多样化任务（异常检测、分类、特征描述、比较、数据转换、时间关系分析），涵盖13个领域的210k样本，采用TF、MC和创新的PZ格式进行评估。

Result: 零样本评估显示当前LLMs面临挑战：最佳商业模型Gemini-2.5-Flash平均得分仅65.08；指令微调提升了开源模型性能，但最佳开源模型LLaMA-3.1-8B仍有显著改进空间。

Conclusion: TSAQA基准展示了时间序列分析对LLMs的复杂性，为评估和提升模型在多样化时间序列任务上的能力提供了重要工具。

Abstract: Time series data are integral to critical applications across domains such as finance, healthcare, transportation, and environmental science. While recent work has begun to explore multi-task time series question answering (QA), current benchmarks remain limited to forecasting and anomaly detection tasks. We introduce TSAQA, a novel unified benchmark designed to broaden task coverage and evaluate diverse temporal analysis capabilities. TSAQA integrates six diverse tasks under a single framework ranging from conventional analysis, including anomaly detection and classification, to advanced analysis, such as characterization, comparison, data transformation, and temporal relationship analysis. Spanning 210k samples across 13 domains, the dataset employs diverse formats, including true-or-false (TF), multiple-choice (MC), and a novel puzzling (PZ), to comprehensively assess time series analysis. Zero-shot evaluation demonstrates that these tasks are challenging for current Large Language Models (LLMs): the best-performing commercial LLM, Gemini-2.5-Flash, achieves an average score of only 65.08. Although instruction tuning boosts open-source performance: the best-performing open-source model, LLaMA-3.1-8B, shows significant room for improvement, highlighting the complexity of temporal analysis for LLMs.

</details>


### [176] [Scaling Multiagent Systems with Process Rewards](https://arxiv.org/abs/2601.23228)
*Ed Li,Junyu Ren,Cat Yan*

Main category: cs.AI

TL;DR: MAPPA通过AI反馈为多智能体系统中的每个动作提供过程奖励，解决了信用分配和样本效率问题，在数学竞赛和数据分析任务上显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在处理复杂任务时面临两个关键挑战：1）跨智能体的信用分配问题；2）昂贵的多智能体rollout的样本效率问题。需要一种方法能够在没有真实标签的情况下提供细粒度监督，同时从每个rollout中提取最大训练信号。

Method: 提出MAPPA方法，通过AI反馈为每个智能体动作提供过程奖励，而不是仅在任务完成时分配信用。这种方法为个体动作分配信用，实现了细粒度监督，同时提高了样本效率。

Result: 在数学竞赛问题上，MAPPA在AIME上提升5.0-17.5个百分点，在AMC上提升7.8-17.2个百分点。在数据分析任务上，成功率提高12.5个百分点，质量指标提升高达30%。

Conclusion: 通过解决信用分配和样本效率问题，MAPPA为在复杂、长视野任务中扩展多智能体系统迈出了第一步，同时最小化了人类监督需求。

Abstract: While multiagent systems have shown promise for tackling complex tasks via specialization, finetuning multiple agents simultaneously faces two key challenges: (1) credit assignment across agents, and (2) sample efficiency of expensive multiagent rollouts. In this work, we propose finetuning multiagent systems with per-action process rewards from AI feedback (MAPPA) to address both. Through assigning credit to individual agent actions rather than only at task completion, MAPPA enables fine-grained supervision without ground truth labels while extracting maximal training signal from each rollout. We demonstrate our approach on competition math problems and tool-augmented data analysis tasks. On unseen math problems, MAPPA achieves +5.0--17.5pp on AIME and +7.8--17.2pp on AMC. For data analysis tasks, our method improves success rate by +12.5pp while quality metrics improve by up to 30%, validating that per-action supervision can lead to improvements across different multiagent system on various domains. By addressing these challenges, our work takes a first step toward scaling multiagent systems for complex, long-horizon tasks with minimal human supervision.

</details>


### [177] [Strongly Polynomial Time Complexity of Policy Iteration for $L_\infty$ Robust MDPs](https://arxiv.org/abs/2601.23229)
*Ali Asadi,Krishnendu Chatterjee,Ehsan Goharshady,Mehrdad Karrabi,Alipasha Montaseri,Carlo Pagano*

Main category: cs.AI

TL;DR: 本文证明了对于具有常数折扣因子的(s,a)-矩形L∞鲁棒MDP，鲁棒策略迭代算法具有强多项式时间复杂度，解决了该领域的一个重要算法问题。


<details>
  <summary>Details</summary>
Motivation: 鲁棒MDP是序列决策中的基本模型，能够处理转移概率的不确定性并优化最坏情况。虽然MDP已有多项式时间算法，但RMDP的算法复杂性一直是一个重要的开放问题，特别是(s,a)-矩形L∞ RMDP作为基本且表达能力强的模型。

Method: 使用鲁棒策略迭代算法，针对具有常数折扣因子的(s,a)-矩形L∞鲁棒MDP进行分析，证明其具有强多项式时间复杂度。

Result: 证明了鲁棒策略迭代算法在常数折扣因子下对于(s,a)-矩形L∞ RMDP具有强多项式时间复杂度，解决了该领域的一个重要算法问题。

Conclusion: 本文解决了RMDP算法复杂性的一个重要开放问题，证明了对于具有常数折扣因子的(s,a)-矩形L∞鲁棒MDP，存在强多项式时间算法，扩展了Ye在MDP上的经典结果到鲁棒设置。

Abstract: Markov decision processes (MDPs) are a fundamental model in sequential decision making. Robust MDPs (RMDPs) extend this framework by allowing uncertainty in transition probabilities and optimizing against the worst-case realization of that uncertainty. In particular, $(s, a)$-rectangular RMDPs with $L_\infty$ uncertainty sets form a fundamental and expressive model: they subsume classical MDPs and turn-based stochastic games. We consider this model with discounted payoffs. The existence of polynomial and strongly-polynomial time algorithms is a fundamental problem for these optimization models. For MDPs, linear programming yields polynomial-time algorithms for any arbitrary discount factor, and the seminal work of Ye established strongly--polynomial time for a fixed discount factor. The generalization of such results to RMDPs has remained an important open problem. In this work, we show that a robust policy iteration algorithm runs in strongly-polynomial time for $(s, a)$-rectangular $L_\infty$ RMDPs with a constant (fixed) discount factor, resolving an important algorithmic question.

</details>
