<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 5]
- [cs.AI](#cs.AI) [Total: 2]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [On the Origin of Algorithmic Progress in AI](https://arxiv.org/abs/2511.21622)
*Hans Gundlach,Alex Fogelson,Jayson Lynch,Ana Trisovic,Jonathan Rosenfeld,Anmol Sandhu,Neil Thompson*

Main category: cs.LG

TL;DR: 研究发现算法效率提升主要依赖于计算规模，而非传统认为的小规模算法创新。LSTM到Transformer的转换贡献了大部分效率增益，而小规模模型算法进展远低于预期。


<details>
  <summary>Details</summary>
Motivation: 旨在解释2012-2023年间AI训练效率22000倍提升的来源，发现现有小规模实验只能解释不到100倍增益，存在巨大差距。

Method: 通过小规模消融实验分析关键创新，进行扩展实验比较LSTM和Transformer的规模依赖效率差异，使用实验外推和文献估计。

Result: 发现算法效率增益与计算规模密切相关，LSTM到Transformer转换贡献了主要效率提升，最终解释了6930倍效率增益。

Conclusion: 算法效率衡量具有强参考依赖性，小规模模型算法进展比预期慢得多，规模依赖的效率改进是理解算法进步的关键。

Abstract: Algorithms have been estimated to increase AI training FLOP efficiency by a factor of 22,000 between 2012 and 2023 [Ho et al., 2024]. Running small-scale ablation experiments on key innovations from this time period, we are able to account for less than 10x of these gains. Surveying the broader literature, we estimate that additional innovations not included in our ablations account for less than 10x, yielding a total under 100x. This leads us to conduct scaling experiments, which reveal that much of this efficiency gap can be explained by algorithms with scale-dependent efficiency improvements. In particular, we conduct scaling experiments between LSTMs and Transformers, finding exponent differences in their compute-optimal scaling law while finding little scaling difference for many other innovations. These experiments demonstrate that - contrary to standard assumptions - an algorithm's efficiency gains are tied to compute scale. Using experimental extrapolation and literature estimates, we account for 6,930x efficiency gains over the same time period, with the scale-dependent LSTM-to-Transformer transition accounting for the majority of gains. Our results indicate that algorithmic progress for small models has been far slower than previously assumed, and that measures of algorithmic efficiency are strongly reference-dependent.

</details>


### [2] [Mechanisms of Non-Monotonic Scaling in Vision Transformers](https://arxiv.org/abs/2511.21635)
*Anantha Padmanaban Krishna Kumar*

Main category: cs.LG

TL;DR: 研究发现深层Vision Transformers性能不如浅层，揭示了Cliff-Plateau-Climb三阶段模式，表明更好的性能与[CLS]令牌边缘化相关，信息扩散比增加参数更重要。


<details>
  <summary>Details</summary>
Motivation: 解决深层Vision Transformers性能不如浅层的问题，挑战传统的缩放假设，探索表示随深度演化的模式。

Method: 对ViT-S、ViT-B和ViT-L在ImageNet上进行系统实证分析，使用信息混洗指数量化信息混合模式。

Result: 发现三阶段演化模式，ViT-L中信息-任务权衡比ViT-B晚10层出现，额外层与信息扩散相关而非性能提升。

Conclusion: Transformer架构应更注重精心校准的深度以实现清晰的阶段转换，而非简单增加参数，信息混洗指数为未来架构设计提供诊断工具。

Abstract: Deeper Vision Transformers often perform worse than shallower ones, which challenges common scaling assumptions. Through a systematic empirical analysis of ViT-S, ViT-B, and ViT-L on ImageNet, we identify a consistent three-phase Cliff-Plateau-Climb pattern that governs how representations evolve with depth. We observe that better performance is associated with progressive marginalization of the [CLS] token, originally designed as a global aggregation hub, in favor of distributed consensus among patch tokens. We quantify patterns of information mixing with an Information Scrambling Index, and show that in ViT-L the information-task tradeoff emerges roughly 10 layers later than in ViT-B, and that these additional layers correlate with increased information diffusion rather than improved task performance. Taken together, these results suggest that transformer architectures in this regime may benefit more from carefully calibrated depth that executes clean phase transitions than from simply increasing parameter count. The Information Scrambling Index provides a useful diagnostic for existing models and suggests a potential design target for future architectures. All code is available at: https://github.com/AnanthaPadmanaban-KrishnaKumar/Cliff-Plateau-Climb.

</details>


### [3] [Through the telecom lens: Are all training samples important?](https://arxiv.org/abs/2511.21668)
*Shruti Bothe,Illyyne Saffar,Aurelie Boisbunon,Hasan Farooq,Julien Forgeat,Md Moin Uddin Chowdhury*

Main category: cs.LG

TL;DR: 论文质疑了电信AI训练中所有样本同等重要的假设，通过样本级梯度分析识别影响力和冗余模式，提出选择性优先处理重要数据的框架，在三个真实电信数据集上验证了在保持性能的同时减少数据需求和计算开销。


<details>
  <summary>Details</summary>
Motivation: 电信AI应用中数据量大、噪声多、存储处理成本高，但标准工作流仍假设所有训练样本贡献均等。下一代系统需要准确、高效且可持续的AI模型，因此需要重新评估样本重要性假设。

Method: 进行跨epoch的样本级梯度分析，识别模型学习中的影响和冗余模式，基于此提出样本重要性框架，选择性优先处理有影响力的数据并减少计算。

Result: 在三个真实世界电信数据集上的实验表明，该方法在保持性能的同时减少了数据需求和计算开销，推进了电信领域可持续AI的目标。

Conclusion: 通过样本重要性分析和选择性训练，可以在不牺牲准确性的前提下优化电信AI系统的计算和能源使用，实现更可持续的AI部署。

Abstract: The rise of AI in telecommunications, from optimizing Radio Access Networks to managing user experience, has sharply increased data volumes and training demands. Telecom data is often noisy, high-dimensional, costly to store, process, and label. Despite Ai's critical role, standard workflows still assume all training samples contribute equally. On the other hand, next generation systems require AI models that are accurate, efficient, and sustainable.The paper questions the assumptions of equal importance by focusing on applying and analyzing the roles of individual samples in telecom training and assessing whether the proposed model optimizes computation and energy use. we perform sample-level gradient analysis across epochs to identify patterns of influence and redundancy in model learning. Based on this, we propose a sample importance framework thats electively prioritizes impactful data and reduces computation without compromising accuracy. Experiments on three real-world telecom datasets show that our method [reserves performance while reducing data needs and computational overhead while advancing the goals of sustainable AI in telecommunications.

</details>


### [4] [Visualizing LLM Latent Space Geometry Through Dimensionality Reduction](https://arxiv.org/abs/2511.21594)
*Alex Ning,Vainateya Rangaraju*

Main category: cs.LG

TL;DR: 该论文通过降维方法提取、处理和可视化基于Transformer的语言模型的潜在状态几何结构，揭示了注意力机制与MLP组件在中间层的明显分离等新发现。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在自然语言任务中表现出色，但其内部机制仍难以解释。研究旨在通过可视化潜在状态几何结构来支持系统的Transformer内部机制分析，促进可复现的可解释性研究。

Method: 使用主成分分析（PCA）和均匀流形逼近（UMAP）等降维技术，在Transformer块内的多个点捕获逐层激活，对GPT-2和LLaMa模型进行系统分析。

Result: 发现了中间层注意力与MLP组件输出的明显分离、初始序列位置潜在状态的高范数特性、GPT-2位置嵌入的高维螺旋结构、LLaMa的序列级几何模式等新几何模式。

Conclusion: 该方法支持对Transformer内部机制的系统分析，为可解释性研究提供了新的可视化工具和发现，代码已开源供进一步研究使用。

Abstract: Large language models (LLMs) achieve state-of-the-art results across many natural language tasks, but their internal mechanisms remain difficult to interpret. In this work, we extract, process, and visualize latent state geometries in Transformer-based language models through dimensionality reduction. We capture layerwise activations at multiple points within Transformer blocks and enable systematic analysis through Principal Component Analysis (PCA) and Uniform Manifold Approximation (UMAP). We demonstrate experiments on GPT-2 and LLaMa models, where we uncover interesting geometric patterns in latent space. Notably, we identify a clear separation between attention and MLP component outputs across intermediate layers, a pattern not documented in prior work to our knowledge. We also characterize the high norm of latent states at the initial sequence position and visualize the layerwise evolution of latent states. Additionally, we demonstrate the high-dimensional helical structure of GPT-2's positional embeddings, the sequence-wise geometric patterns in LLaMa, and experiment with repeating token sequences. We aim to support systematic analysis of Transformer internals with the goal of enabling further reproducible interpretability research. We make our code available at https://github.com/Vainateya/Feature_Geometry_Visualization.

</details>


### [5] [Aligning LLMs Toward Multi-Turn Conversational Outcomes Using Iterative PPO](https://arxiv.org/abs/2511.21638)
*Daniel R. Jiang,Jalaj Bhandari,Yukai Yang,Rémi Munos,Tyler Lu*

Main category: cs.LG

TL;DR: 本文提出了一种将多轮对话强化学习问题转化为单轮RLHF问题的方法，通过迭代PPO算法在保持稳定性的同时实现策略优化。


<details>
  <summary>Details</summary>
Motivation: 优化大型语言模型在多轮对话中的表现面临挑战，特别是在目标导向场景中，存在稀疏奖励和响应级别规划与令牌级别生成之间的差异。

Method: 将多轮RL问题形式化地转化为一系列单轮RLHF问题，使用学习到的多轮Q函数作为单轮问题的奖励模型，提出迭代PPO算法交替拟合Q函数和改进策略。

Result: 证明了使用标准令牌级别PPO解决单轮RL问题等价于在多轮问题中进行策略改进步骤，该方法能够直接利用稳定的单轮RLHF工具。

Conclusion: 该方法在完全在线和完全离线方法之间找到了平衡点，既保持了在线更新的适应性，又获得了离线训练的稳定性优势。

Abstract: Optimizing large language models (LLMs) for multi-turn conversational outcomes remains a significant challenge, especially in goal-oriented settings like AI marketing or sales agents who facilitate transactions via messaging platforms. The difficulty stems from sparse, long-horizon rewards and the discrepancy between response-level planning and token-level generation. In this technical note, we propose a formal reduction of the multi-turn RL problem into a sequence of single-turn RLHF-style problems. This is achieved by setting a learned multi-turn Q-function as the reward model for the single-turn problem. We demonstrate and prove a key insight: solving this single-turn RL problem with standard token-level PPO is equivalent to a policy improvement step within the multi-turn problem. This insight naturally leads to Iterative PPO, a batch online policy iteration algorithm that alternates between fitting Q-functions from logged conversation trajectories and improving the policy. A major practical advantage is that Iterative PPO directly leverages stable, off-the-shelf single-turn RLHF tools, making it straightforward to implement. Our method occupies a middle ground between fully online and fully offline approaches, retaining the adaptability of online updates while gaining the stability benefits of offline training.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [6] [On the Limits of Innate Planning in Large Language Models](https://arxiv.org/abs/2511.21591)
*Charles Schepanowski,Charles Ling*

Main category: cs.AI

TL;DR: 本文研究了大型语言模型在8拼图任务中的规划和状态推理能力，发现即使有反馈机制和外部验证器，模型仍存在内部状态表示脆弱和启发式规划能力弱的问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在许多基准测试中表现优异，但其规划和状态推理能力尚不明确，需要直接评估这些核心能力。

Method: 使用8拼图任务测试4个模型，采用零样本、思维链和算法思维等提示方法，并引入分层纠正反馈和外部移动验证器。

Result: 反馈机制对某些模型-提示组合有所改善，但成功运行通常冗长且计算昂贵。即使有外部验证器，所有模型都无法解决任何拼图。

Conclusion: 当前LLMs在规划方面存在显著局限性，需要开发维护显式状态和执行结构化搜索的机制来取得进一步进展。

Abstract: Large language models (LLMs) achieve impressive results on many benchmarks, yet their capacity for planning and stateful reasoning remains unclear. We study these abilities directly, without code execution or other tools, using the 8-puzzle: a classic task that requires state tracking and goal-directed planning while allowing precise, step-by-step evaluation. Four models are tested under common prompting conditions (Zero-Shot, Chain-of-Thought, Algorithm-of-Thought) and with tiered corrective feedback. Feedback improves success rates for some model-prompt combinations, but many successful runs are long, computationally expensive, and indirect. We then examine the models with an external move validator that provides only valid moves. Despite this level of assistance, none of the models solve any puzzles in this setting. Qualitative analysis reveals two dominant deficits across all models: (1) brittle internal state representations, leading to frequent invalid moves, and (2) weak heuristic planning, with models entering loops or selecting actions that do not reduce the distance to the goal state. These findings indicate that, in the absence of external tools such as code interpreters, current LLMs have substantial limitations in planning and that further progress may require mechanisms for maintaining explicit state and performing structured search.

</details>


### [7] [Bridging the Unavoidable A Priori: A Framework for Comparative Causal Modeling](https://arxiv.org/abs/2511.21636)
*Peter S. Hovmand,Kari O'Donnell,Callie Ogland-Hand,Brian Biroscak,Douglas D. Gunzler*

Main category: cs.AI

TL;DR: 本文提出了一个将系统动力学和结构方程建模结合到统一数学框架中的方法，用于支持负责任AI/ML的发展，解决不同方法间基础假设差异的障碍。


<details>
  <summary>Details</summary>
Motivation: AI/ML模型在解决未解决问题时可能放大人类偏见，需要利用更丰富的系统动力学因果模型来指导负责任AI/ML的发展，但不同方法的基础假设差异构成了主要障碍。

Method: 将系统动力学和结构方程建模整合到一个共同的数学框架中，用于从分布生成系统、开发方法，并比较结果以理解系统动力学在数据科学和AI/ML应用中的认识论基础。

Result: 建立了一个统一的数学框架，能够协调系统动力学和结构方程建模这两种不同基础假设的方法。

Conclusion: 通过将系统动力学和结构方程建模整合到共同框架中，为推进负责任AI/ML的发展提供了方法论基础，有助于更好地理解和应用系统动力学在AI/ML领域的认识论价值。

Abstract: AI/ML models have rapidly gained prominence as innovations for solving previously unsolved problems and their unintended consequences from amplifying human biases. Advocates for responsible AI/ML have sought ways to draw on the richer causal models of system dynamics to better inform the development of responsible AI/ML. However, a major barrier to advancing this work is the difficulty of bringing together methods rooted in different underlying assumptions (i.e., Dana Meadow's "the unavoidable a priori"). This paper brings system dynamics and structural equation modeling together into a common mathematical framework that can be used to generate systems from distributions, develop methods, and compare results to inform the underlying epistemology of system dynamics for data science and AI/ML applications.

</details>
