<div id=toc></div>

# Table of Contents

- [cs.LG](#cs.LG) [Total: 78]
- [cs.AI](#cs.AI) [Total: 19]
- [cs.HC](#cs.HC) [Total: 3]


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [1] [Quantifying Modality Contributions via Disentangling Multimodal Representations](https://arxiv.org/abs/2511.19470)
*Padegal Amit,Omkar Mahesh Kashyap,Namitha Rayasam,Nidhi Shekhar,Surabhi Narayan*

Main category: cs.LG

TL;DR: 提出了基于部分信息分解(PID)的框架，通过分解内部嵌入中的预测信息为独特、冗余和协同成分来量化模态贡献，使用迭代比例拟合程序(IPFP)实现无需重新训练的可扩展分析。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖基于准确性的方法，将移除模态后的性能下降解释为其影响力，但这种结果驱动的指标无法区分模态是固有信息丰富还是仅通过与其他模态交互产生价值，特别是在交叉注意力架构中。

Method: 基于部分信息分解(PID)框架，将内部嵌入中的预测信息分解为独特、冗余和协同成分，开发基于迭代比例拟合程序(IPFP)的算法，无需重新训练即可计算层和数据集级别的贡献。

Result: 提供了原则性的、表示级别的多模态行为视图，比基于结果的指标提供更清晰和可解释的见解。

Conclusion: 该框架能够量化模态贡献，区分模态的固有信息价值和交互价值，在多模态模型分析中提供更精确的贡献评估。

Abstract: Quantifying modality contributions in multimodal models remains a challenge, as existing approaches conflate the notion of contribution itself. Prior work relies on accuracy-based approaches, interpreting performance drops after removing a modality as indicative of its influence. However, such outcome-driven metrics fail to distinguish whether a modality is inherently informative or whether its value arises only through interaction with other modalities. This distinction is particularly important in cross-attention architectures, where modalities influence each other's representations. In this work, we propose a framework based on Partial Information Decomposition (PID) that quantifies modality contributions by decomposing predictive information in internal embeddings into unique, redundant, and synergistic components. To enable scalable, inference-only analysis, we develop an algorithm based on the Iterative Proportional Fitting Procedure (IPFP) that computes layer and dataset-level contributions without retraining. This provides a principled, representation-level view of multimodal behavior, offering clearer and more interpretable insights than outcome-based metrics.

</details>


### [2] [PrefixGPT: Prefix Adder Optimization by a Generative Pre-trained Transformer](https://arxiv.org/abs/2511.19472)
*Ruogu Ding,Xin Ning,Ulf Schlichtmann,Weikang Qian*

Main category: cs.LG

TL;DR: PrefixGPT是一种基于GPT的生成式预训练Transformer模型，能够从零开始直接生成优化的前缀加法器，通过二维坐标序列表示拓扑结构并应用合法性掩码确保设计有效性。


<details>
  <summary>Details</summary>
Motivation: 前缀加法器在计算密集型应用中广泛使用，但由于严格的设计规则和指数级大的设计空间，设计优化的前缀加法器具有挑战性。

Method: 使用自定义的解码器Transformer架构，首先在随机合成的有效前缀加法器语料库上进行预训练以学习设计规则，然后微调以在设计空间中导航优化设计质量。

Result: 相比现有工作，PrefixGPT不仅找到了新的最优设计，面积延迟积(ADP)提高了7.7%，而且展现出优越的探索质量，平均ADP降低了高达79.1%。

Conclusion: 这证明了GPT风格模型在掌握复杂硬件设计原理并应用于更高效设计优化方面的潜力。

Abstract: Prefix adders are widely used in compute-intensive applications for their high speed. However, designing optimized prefix adders is challenging due to strict design rules and an exponentially large design space. We introduce PrefixGPT, a generative pre-trained Transformer (GPT) that directly generates optimized prefix adders from scratch. Our approach represents an adder's topology as a two-dimensional coordinate sequence and applies a legality mask during generation, ensuring every design is valid by construction. PrefixGPT features a customized decoder-only Transformer architecture. The model is first pre-trained on a corpus of randomly synthesized valid prefix adders to learn design rules and then fine-tuned to navigate the design space for optimized design quality. Compared with existing works, PrefixGPT not only finds a new optimal design with a 7.7% improved area-delay product (ADP) but exhibits superior exploration quality, lowering the average ADP by up to 79.1%. This demonstrates the potential of GPT-style models to first master complex hardware design principles and then apply them for more efficient design optimization.

</details>


### [3] [WavefrontDiffusion: Dynamic Decoding Schedule or Improved Reasoning](https://arxiv.org/abs/2511.19473)
*Haojin Yang,Rui Hu,Zequn Sun,Rui Zhou,Yujun Cai,Yiwei Wang*

Main category: cs.LG

TL;DR: WavefrontDiffusion是一种动态解码方法，通过从已确定位置向外扩展活跃令牌波前，在保持与基于块的方法相同计算成本的同时，实现更连贯和高效的文本生成。


<details>
  <summary>Details</summary>
Motivation: 解决主流去噪策略（标准扩散和块扩散）在文本生成中的局限性：标准扩散容易过早结束序列，块扩散的刚性结构可能破坏语义连贯性。

Method: 提出WavefrontDiffusion方法，动态扩展活跃令牌波前，从已确定位置向外传播，遵循语义结构的自然流动。

Result: 在推理和代码生成的四个基准测试中，WavefrontDiffusion实现了最先进的性能，并产生具有更高语义保真度的输出。

Conclusion: 自适应调度对于实现更连贯和高效的生成具有重要价值，WavefrontDiffusion展示了在扩散语言模型中动态解码方法的优势。

Abstract: Diffusion Language Models (DLMs) have shown strong potential for text generation and are becoming a competitive alternative to autoregressive models. The denoising strategy plays an important role in determining the quality of their outputs. Mainstream denoising strategies include Standard Diffusion and BlockDiffusion. Standard Diffusion performs global denoising without restricting the update range, often finalizing incomplete context and causing premature end-of-sequence predictions. BlockDiffusion updates fixed-size blocks in a preset order, but its rigid structure can break apart coherent semantic units and disrupt reasoning. We present WavefrontDiffusion, a dynamic decoding approach that expands a wavefront of active tokens outward from finalized positions. This adaptive process follows the natural flow of semantic structure while keeping computational cost equal to block-based methods. Across four benchmarks in reasoning and code generation, WavefrontDiffusion achieves state-of-the-art performance while producing outputs with higher semantic fidelity, showing the value of adaptive scheduling for more coherent and efficient generation.

</details>


### [4] [Exploiting the Experts: Unauthorized Compression in MoE-LLMs](https://arxiv.org/abs/2511.19480)
*Pinaki Prasad Guha Neogi,Ahmad Mohammadshirazi,Dheeraj Kulshrestha,Rajiv Ramnath*

Main category: cs.LG

TL;DR: 本文系统研究了MoE-LLMs在任务特定使用下的可剪枝性，揭示了知识丢失与恢复的权衡关系，并提出了防御策略来防止未经授权的模型压缩和微调。


<details>
  <summary>Details</summary>
Motivation: MoE架构的模块化特性引入了独特的安全漏洞：攻击者可以通过剪枝专家并廉价微调剩余部分来压缩或重新利用模型，从而绕过许可和安全约束。

Method: 开发了专家归因框架识别特定任务的关键专家子集，评估剪枝和重新对齐这些专家的性能权衡，使用主动学习驱动的微调方法。

Result: 研究发现存在关键的知识丢失-恢复权衡：虽然可以隔离某些专家来保持任务准确性，但如果没有针对性的重新对齐，会出现显著性能下降。

Conclusion: 通过将专家剪枝定位为威胁向量和防御目标，这项工作突出了MoE模块化的双重用途性质，并为MoE-LLMs的安全专业化提供了首个系统评估框架。

Abstract: Mixture-of-Experts (MoE) architectures are increasingly adopted in large language models (LLMs) for their scalability and efficiency. However, their modular structure introduces a unique vulnerability: adversaries can attempt to compress or repurpose models by pruning experts and cheaply fine-tuning the remainder, effectively bypassing licensing and security constraints. In this paper, we systematically study the prunability of MoE-LLMs under task-specific usage. We first develop an expert attribution framework that identifies the subset of experts most responsible for a given task, then evaluate the performance trade-offs of pruning and re-aligning these experts using active learning-driven fine-tuning. Our findings reveal a critical knowledge loss--recovery trade-off: while certain experts can be isolated to retain task accuracy, significant degradation occurs without targeted re-alignment. Based on this analysis, we propose defense strategies that aim to make MoE models harder to compress and fine-tune without authorization, including entangled expert training and selective fine-tuning protocols that resist unauthorized adaptation. By positioning expert pruning as both a threat vector and a defense target, this work highlights the dual-use nature of MoE modularity and provides the first systematic evaluation framework for secure specialization of MoE-LLMs.

</details>


### [5] [Quality analysis and evaluation prediction of RAG retrieval based on machine learning algorithms](https://arxiv.org/abs/2511.19481)
*Ruoxin Zhang,Zhizhao Wen,Chao Wang,Chenchen Tang,Puyang Xu,Yifan Jiang*

Main category: cs.LG

TL;DR: 本文提出了一种基于特征工程和粒子群优化的XGBoost机器学习回归模型，用于优化RAG系统中的检索质量，实验结果显示该模型在各项评估指标上均优于对比模型。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型的快速发展，检索增强生成技术因其能够整合外部知识提高输出准确性而被广泛应用。然而，系统性能高度依赖检索模块的质量，如果检索结果与用户需求相关性低或包含噪声信息，将直接导致生成内容失真。

Method: 针对现有模型在处理表格特征方面的性能瓶颈，本文提出了基于特征工程和粒子群优化的XGBoost机器学习回归模型。通过相关性分析发现文档相关性与答案质量呈正相关，而语义相似性、冗余性与多样性之间存在权衡关系。

Result: 实验结果表明，VMD PSO BiLSTM模型在所有评估指标上均优于决策树、AdaBoost等对比模型，其MSE、RMSE、MAE和MAPE显著降低，R2值更高，表明其预测精度、稳定性和数据解释能力更出色。

Conclusion: 该成果为优化RAG系统的检索质量和提升生成效果提供了有效路径，对推动相关技术的落地应用具有重要价值。

Abstract: With the rapid evolution of large language models, retrieval enhanced generation technology has been widely used due to its ability to integrate external knowledge to improve output accuracy. However, the performance of the system is highly dependent on the quality of the retrieval module. If the retrieval results have low relevance to user needs or contain noisy information, it will directly lead to distortion of the generated content. In response to the performance bottleneck of existing models in processing tabular features, this paper proposes an XGBoost machine learning regression model based on feature engineering and particle swarm optimization. Correlation analysis shows that answer_quality is positively correlated with doc_delevance by 0.66, indicating that document relevance has a significant positive effect on answer quality, and improving document relevance may enhance answer quality; The strong negative correlations between semantic similarity, redundancy, and diversity were -0.89 and -0.88, respectively, indicating a trade- off between semantic similarity, redundancy, and diversity. In other words, as the former two increased, diversity significantly decreased. The experimental results comparing decision trees, AdaBoost, etc. show that the VMD PSO BiLSTM model is superior in all evaluation indicators, with significantly lower MSE, RMSE, MAE, and MAPE compared to the comparison model. The R2 value is higher, indicating that its prediction accuracy, stability, and data interpretation ability are more outstanding. This achievement provides an effective path for optimizing the retrieval quality and improving the generation effect of RAG system, and has important value in promoting the implementation and application of related technologies.

</details>


### [6] [OmniTFT: Omni Target Forecasting for Vital Signs and Laboratory Result Trajectories in Multi Center ICU Data](https://arxiv.org/abs/2511.19485)
*Wanzhe Xu,Yutong Dai,Yitao Yang,Martin Loza,Weihang Zhang,Yang Cui,Xin Zeng,Sung Joon Park,Kenta Nakai*

Main category: cs.LG

TL;DR: OmniTFT是一个基于Temporal Fusion Transformer的深度学习框架，用于联合预测ICU中的高频生命体征和稀疏采样的实验室结果，通过四种创新策略提升性能，在多个数据集上取得了显著改进。


<details>
  <summary>Details</summary>
Motivation: ICU中生命体征存在噪声和快速波动，实验室测试存在缺失值、测量延迟和设备特定偏差等问题，使得综合预测极具挑战性，需要开发能够统一处理这些异质临床目标的模型。

Method: 提出OmniTFT框架，采用四种策略：滑动窗口均衡采样平衡生理状态、频率感知嵌入收缩稳定罕见类别表示、分层变量选择引导模型关注信息特征簇、影响对齐注意力校准增强生理突变时的鲁棒性。

Result: 在MIMIC-III、MIMIC-IV和eICU数据集上的预测任务中，OmniTFT对生命体征和实验室结果都实现了显著的性能提升，其注意力模式可解释且与已知病理生理学一致。

Conclusion: OmniTFT减少了对目标特定架构和广泛特征工程的依赖，能够统一建模多个异质临床目标，同时保持跨机构泛化能力，在临床护理中具有定量决策支持的潜在应用价值。

Abstract: Accurate multivariate time-series prediction of vital signs and laboratory results is crucial for early intervention and precision medicine in intensive care units (ICUs). However, vital signs are often noisy and exhibit rapid fluctuations, while laboratory tests suffer from missing values, measurement lags, and device-specific bias, making integrative forecasting highly challenging. To address these issues, we propose OmniTFT, a deep learning framework that jointly learns and forecasts high-frequency vital signs and sparsely sampled laboratory results based on the Temporal Fusion Transformer (TFT). Specifically, OmniTFT implements four novel strategies to enhance performance: sliding window equalized sampling to balance physiological states, frequency-aware embedding shrinkage to stabilize rare-class representations, hierarchical variable selection to guide model attention toward informative feature clusters, and influence-aligned attention calibration to enhance robustness during abrupt physiological changes. By reducing the reliance on target-specific architectures and extensive feature engineering, OmniTFT enables unified modeling of multiple heterogeneous clinical targets while preserving cross-institutional generalizability. Across forecasting tasks, OmniTFT achieves substantial performance improvement for both vital signs and laboratory results on the MIMIC-III, MIMIC-IV, and eICU datasets. Its attention patterns are interpretable and consistent with known pathophysiology, underscoring its potential utility for quantitative decision support in clinical care.

</details>


### [7] [Efficient Inference Using Large Language Models with Limited Human Data: Fine-Tuning then Rectification](https://arxiv.org/abs/2511.19486)
*Lei Wang,Zikun Ye,Jinglong Zhao*

Main category: cs.LG

TL;DR: 本文提出了一个结合微调和校正的框架，通过优化分配有限标记样本来提升大语言模型在市场研究和社会科学应用中的表现。


<details>
  <summary>Details</summary>
Motivation: 随着人工智能的进步，大语言模型在市场研究和社会科学应用中显示出生成类人响应的潜力，但需要改进模型性能。现有方法包括微调和校正，但如何最优结合这两种方法并分配有限样本仍是一个挑战。

Method: 开发了一个结合微调和校正的框架，提出以预测误差方差最小化为微调目标（这对下游校正阶段最优），并利用经验缩放定律开发数据驱动方法来最优分配样本到两个阶段。

Result: 实证分析验证了该框架，相比单独使用微调或校正，该框架在估计和推断性能方面都有改进。

Conclusion: 提出的结合微调和校正的框架能够有效提升大语言模型的性能，通过优化样本分配策略实现了比单一方法更好的效果。

Abstract: Driven by recent advances in artificial intelligence (AI), a growing body of work demonstrates the potential of using large language models (LLMs) to generate human-like responses in market research and social science applications. Two primary approaches can be applied to improve the performance of LLMs: fine-tuning, which aligns LLM predictions more closely with human responses, and rectification, which corrects biases in LLM outputs. In this paper, we develop a framework that combines fine-tuning and rectification, and optimally allocates limited labeled samples across the two stages. Unlike the conventional objective that minimizes the mean squared prediction errors, we propose to minimize the variance of the prediction errors as the fine-tuning objective, which is optimal for the downstream rectification stage. Building on this insight, we leverage empirical scaling laws to develop a data-driven method for optimally splitting samples between the fine-tuning and rectification stages. Empirical analysis validates our framework, demonstrating improved estimation and inference performance compared to using either fine-tuning or rectification alone.

</details>


### [8] [The Generalized Proximity Forest](https://arxiv.org/abs/2511.19487)
*Ben Shaw,Adam Rustad,Sofia Pelagalli Maia,Jake S. Rhodes,Kevin R. Moon*

Main category: cs.LG

TL;DR: 本文提出了广义邻近森林模型，将随机森林邻近性扩展到所有基于距离的监督机器学习场景，并引入了回归任务的变体，同时将其作为元学习框架用于监督插补。


<details>
  <summary>Details</summary>
Motivation: 随机森林邻近性在多种监督学习任务中很有用，但其效用依赖于随机森林模型本身的表现，而随机森林并非在所有场景下都是理想模型。需要将邻近性扩展到更广泛的机器学习环境。

Method: 提出了广义邻近森林模型，将随机森林邻近性扩展到所有基于距离的监督机器学习场景；引入了回归任务的变体；将广义邻近森林模型作为元学习框架，扩展监督插补能力到任何预训练分类器。

Result: 实验证明广义邻近森林模型相比随机森林模型和k近邻模型具有独特优势。

Conclusion: 广义邻近森林模型成功扩展了随机森林邻近性的应用范围，为基于距离的监督机器学习提供了更通用的框架，并在多个任务中表现出优越性能。

Abstract: Recent work has demonstrated the utility of Random Forest (RF) proximities for various supervised machine learning tasks, including outlier detection, missing data imputation, and visualization. However, the utility of the RF proximities depends upon the success of the RF model, which itself is not the ideal model in all contexts. RF proximities have recently been extended to time series by means of the distance-based Proximity Forest (PF) model, among others, affording time series analysis with the benefits of RF proximities. In this work, we introduce the generalized PF model, thereby extending RF proximities to all contexts in which supervised distance-based machine learning can occur. Additionally, we introduce a variant of the PF model for regression tasks. We also introduce the notion of using the generalized PF model as a meta-learning framework, extending supervised imputation capability to any pre-trained classifier. We experimentally demonstrate the unique advantages of the generalized PF model compared with both the RF model and the $k$-nearest neighbors model.

</details>


### [9] [OpenCML: End-to-End Framework of Open-world Machine Learning to Learn Unknown Classes Incrementally](https://arxiv.org/abs/2511.19491)
*Jitendra Parmar,Praveen Singh Thakur*

Main category: cs.LG

TL;DR: 该论文提出了一种开放世界机器学习模型，通过发现未知类别并创建新类别，以及增量学习新类别，实现持续学习能力。该模型在开放世界学习和持续学习方面优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型遵循封闭世界假设，难以保留先前学到的知识用于未来任务。自动化智能系统需要学习新类别和已知任务，因此需要开放和持续的学习环境。

Method: 模型包含两个相互连接的任务：首先发现数据中的未知类别并创建新类别，然后对每个新类别进行增量学习。这两个任务共同实现持续学习。

Result: 该模型在开放世界学习中优于现有方法，在持续学习中表现出色，在四次迭代中最高平均准确率达到82.54%，最低准确率为65.87%。

Conclusion: 提出的开放世界机器学习模型能够有效扩展对数据的理解并随时间改进，为自动化智能系统提供了持续学习能力。

Abstract: Open-world machine learning is an emerging technique in artificial intelligence, where conventional machine learning models often follow closed-world assumptions, which can hinder their ability to retain previously learned knowledge for future tasks. However, automated intelligence systems must learn about novel classes and previously known tasks. The proposed model offers novel learning classes in an open and continuous learning environment. It consists of two different but connected tasks. First, it discovers unknown classes in the data and creates novel classes; next, it learns how to perform class incrementally for each new class. Together, they enable continual learning, allowing the system to expand its understanding of the data and improve over time. The proposed model also outperformed existing approaches in open-world learning. Furthermore, it demonstrated strong performance in continuous learning, achieving a highest average accuracy of 82.54% over four iterations and a minimum accuracy of 65.87%.

</details>


### [10] [A Systematic Study of Compression Ordering for Large Language Models](https://arxiv.org/abs/2511.19495)
*Shivansh Chhawri,Rahul Mahadik,Suparna Rooj*

Main category: cs.LG

TL;DR: 本文系统研究了知识蒸馏、结构化剪枝和低比特量化三种LLM压缩技术的独立效果和组合顺序，发现在Qwen2.5 3B模型上，P-KD-Q（剪枝-知识蒸馏-量化）序列能实现3.68倍压缩比并保持最佳性能，而早期应用量化会导致严重性能下降。


<details>
  <summary>Details</summary>
Motivation: LLM需要大量计算资源，模型压缩对受限环境部署至关重要。虽然各压缩技术单独效果已有研究，但它们的交互作用和最优顺序仍不清楚。

Method: 在Qwen2.5 3B模型上系统评估多种压缩流程，包括单技术和三技术序列，使用困惑度、G-Eval、清晰度、提示对齐和压缩比等指标。

Result: 量化提供最大的独立压缩，剪枝引入中等质量下降。技术顺序显著影响最终模型质量：P-KD-Q序列表现最佳，达到3.68倍压缩比并保持强指令跟随和语言理解能力。早期应用量化的流程因不可逆信息损失而性能严重下降。

Conclusion: 这项研究为在资源受限环境中部署LLM提供了实用的、顺序感知的压缩流程设计指导。

Abstract: Large Language Models (LLMs) require substantial computational resources, making model compression essential for efficient deployment in constrained environments. Among the dominant compression techniques: knowledge distillation, structured pruning, and low-bit quantization, their individual effects are well studied, but their interactions and optimal sequencing remain unclear. This work systematically examines how these techniques perform both independently and in combination when applied to the Qwen2.5 3B model. We evaluate multiple compression pipelines, including single, and proposed three-technique sequences, using perplexity, G-Eval, clarity, prompt alignment, and compression ratio as metrics. Our experiments show that quantization provides the greatest standalone compression, while pruning introduces moderate quality degradation. Critically, the ordering of techniques significantly affects the final model quality: the sequence Pruning, Knowledge Distillation, Quantization (P-KD-Q) yields the best balance, achieving a 3.68x compression ratio while preserving strong instruction-following and language understanding capabilities. Conversely, pipelines applying quantization early suffer severe performance degradation due to irreversible information loss that impairs subsequent training. Overall, this study offers practical insight into designing effective, ordering-aware compression pipelines for deploying LLMs in resource-limited settings.

</details>


### [11] [Hierarchical Dual-Strategy Unlearning for Biomedical and Healthcare Intelligence Using Imperfect and Privacy-Sensitive Medical Data](https://arxiv.org/abs/2511.19498)
*Yi Zhang,Tianxiang Xu,Zijian Li,Chao Zhang,Kunyu Zhang,Zhan Gao,Meinuo Li,Xiaohan Zhang,Qichao Qi,Bing Chen*

Main category: cs.LG

TL;DR: 提出了一种分层双策略框架，用于选择性知识遗忘，在医疗领域精确移除专业知识同时保留基础医学能力，仅需修改0.1%参数即可实现82.7%遗忘率和88.5%知识保留。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在医疗场景中因训练数据记忆带来的隐私风险，特别是涉及不完整或隐私敏感的患者信息时，需要满足监管合规、可审计性和伦理标准。

Method: 采用分层双策略框架：几何约束梯度更新选择性调节目标参数，概念感知令牌级干预通过统一四级医学概念层次区分保留关键令牌和遗忘目标令牌。

Result: 在MedMCQA（外科）和MHQA（焦虑、抑郁、创伤）数据集上评估，达到82.7%遗忘率和88.5%知识保留，仅修改0.1%参数即可保持强大的隐私保证。

Conclusion: 该框架有效解决了医疗领域LLM的隐私风险问题，同时满足监管合规和伦理标准需求，为临床研究提供了实用的隐私保护解决方案。

Abstract: Large language models (LLMs) exhibit exceptional performance but pose substantial privacy risks due to training data memorization, particularly within healthcare contexts involving imperfect or privacy-sensitive patient information. We present a hierarchical dual-strategy framework for selective knowledge unlearning that precisely removes specialized knowledge while preserving fundamental medical competencies. Our approach synergistically integrates geometric-constrained gradient updates to selectively modulate target parameters with concept-aware token-level interventions that distinguish between preservation-critical and unlearning-targeted tokens via a unified four-level medical concept hierarchy. Comprehensive evaluations on the MedMCQA (surgical) and MHQA (anxiety, depression, trauma) datasets demonstrate superior performance, achieving an 82.7% forgetting rate and 88.5% knowledge preservation. Notably, our framework maintains robust privacy guarantees while requiring modification of only 0.1% of parameters, addressing critical needs for regulatory compliance, auditability, and ethical standards in clinical research.

</details>


### [12] [Beyond Binary Classification: A Semi-supervised Approach to Generalized AI-generated Image Detection](https://arxiv.org/abs/2511.19499)
*Hong-Hanh Nguyen-Le,Van-Tuan Tran,Dinh-Thuc Nguyen,Nhien-An Le-Khac*

Main category: cs.LG

TL;DR: 本文提出TriDetect检测器，通过半监督方法发现假图像中的潜在架构模式，解决GAN和扩散模型检测器跨架构泛化能力不足的问题。


<details>
  <summary>Details</summary>
Motivation: 当前取证技术面临检测器无法实现跨生成器泛化的关键漏洞，特别是跨越架构边界时（如从GANs到扩散模型），这源于不同架构产生的伪影存在根本差异。

Method: 提出TriDetect检测器，采用半监督方法，通过Sinkhorn-Knopp算法进行平衡聚类分配和跨视图一致性机制，学习基本的架构差异。

Result: 在两个标准基准和三个真实数据集上评估，与13个基线方法对比，证明了其对未见生成器的泛化能力。

Conclusion: TriDetect通过发现假图像类中的潜在架构模式，有效提升了跨生成器检测的泛化性能。

Abstract: The rapid advancement of generators (e.g., StyleGAN, Midjourney, DALL-E) has produced highly realistic synthetic images, posing significant challenges to digital media authenticity. These generators are typically based on a few core architectural families, primarily Generative Adversarial Networks (GANs) and Diffusion Models (DMs). A critical vulnerability in current forensics is the failure of detectors to achieve cross-generator generalization, especially when crossing architectural boundaries (e.g., from GANs to DMs). We hypothesize that this gap stems from fundamental differences in the artifacts produced by these \textbf{distinct architectures}. In this work, we provide a theoretical analysis explaining how the distinct optimization objectives of the GAN and DM architectures lead to different manifold coverage behaviors. We demonstrate that GANs permit partial coverage, often leading to boundary artifacts, while DMs enforce complete coverage, resulting in over-smoothing patterns. Motivated by this analysis, we propose the \textbf{Tri}archy \textbf{Detect}or (TriDetect), a semi-supervised approach that enhances binary classification by discovering latent architectural patterns within the "fake" class. TriDetect employs balanced cluster assignment via the Sinkhorn-Knopp algorithm and a cross-view consistency mechanism, encouraging the model to learn fundamental architectural distincts. We evaluate our approach on two standard benchmarks and three in-the-wild datasets against 13 baselines to demonstrate its generalization capability to unseen generators.

</details>


### [13] [Position: The Complexity of Perfect AI Alignment -- Formalizing the RLHF Trilemma](https://arxiv.org/abs/2511.19504)
*Subramanyam Sahoo,Aman Chadha,Vinija Jain,Divya Chaudhary*

Main category: cs.LG

TL;DR: 本文提出了对齐三难困境：RLHF系统无法同时实现多样性代表性、多项式可扩展性和鲁棒性，证明了在全局规模上同时达到高代表性和高鲁棒性需要超多项式复杂度。


<details>
  <summary>Details</summary>
Motivation: 当前RLHF实践面临安全性提升导致公平性下降、扩展到多样化人群计算不可行、鲁棒性增强放大多数偏见等问题，需要理论框架解释这些系统性困境。

Method: 通过结合统计学习理论和鲁棒优化的复杂性理论分析，证明在全局规模上同时实现高代表性(ε≤0.01)和高鲁棒性(δ≤0.001)需要Ω(2^{d_context})操作。

Result: 分析显示当前RLHF实现通过牺牲代表性来解决三难困境，仅从同质标注者池收集10^3-10^4样本，而真实全局代表性需要10^7-10^8样本。

Conclusion: 该框架为RLHF病理现象提供了统一解释，并为通过战略性地放宽对齐要求来应对这些基本权衡提供了具体方向。

Abstract: Reinforcement Learning from Human Feedback (RLHF) is widely used for aligning large language models, yet practitioners face a persistent puzzle: improving safety often reduces fairness, scaling to diverse populations becomes computationally intractable, and making systems robust often amplifies majority biases. We formalize this tension as the Alignment Trilemma: no RLHF system can simultaneously achieve (i) epsilon-representativeness across diverse human values, (ii) polynomial tractability in sample and compute complexity, and (iii) delta-robustness against adversarial perturbations and distribution shift. Through a complexity-theoretic analysis integrating statistical learning theory and robust optimization, we prove that achieving both representativeness (epsilon <= 0.01) and robustness (delta <= 0.001) for global-scale populations requires Omega(2^{d_context}) operations, which is super-polynomial in the context dimensionality. We show that current RLHF implementations resolve this trilemma by sacrificing representativeness: they collect only 10^3--10^4 samples from homogeneous annotator pools while 10^7--10^8 samples are needed for true global representation. Our framework provides a unified explanation for documented RLHF pathologies including preference collapse, sycophancy, and systematic bias amplification. We conclude with concrete directions for navigating these fundamental trade-offs through strategic relaxations of alignment requirements.

</details>


### [14] [Row-stochastic matrices can provably outperform doubly stochastic matrices in decentralized learning](https://arxiv.org/abs/2511.19513)
*Bing Liu,Boao Kong,Limin Lu,Kun Yuan,Chengcheng Zhao*

Main category: cs.LG

TL;DR: 本文重新审视了去中心化学习中处理异构节点权重的两种策略：嵌入权重到本地损失函数（保持双随机矩阵）和使用行随机矩阵。通过加权希尔伯特空间框架，发现行随机矩阵在该几何中成为自伴算子，而双随机矩阵则不是，这产生了额外的惩罚项，放大了共识误差并减缓收敛。


<details>
  <summary>Details</summary>
Motivation: 澄清在去中心化学习中，处理异构节点权重的两种策略（嵌入权重和使用行随机矩阵）在欧几里得空间中具有相同的期望下降方向，但其收敛行为的根本差异尚不清楚。

Method: 开发加权希尔伯特空间框架L²(λ;ℝᵈ)，在该几何中分析两种策略的收敛性。使用Rayleigh商和Loewner序特征值比较来获得拓扑条件。

Result: 在加权希尔伯特空间几何中，行随机矩阵成为自伴算子而双随机矩阵不是，产生额外的惩罚项放大共识误差。获得了比欧几里得分析更严格的收敛率，并推导出即使谱隙较小，行随机设计也能更快收敛的充分条件。

Conclusion: 收敛差异不仅来自谱隙，还来自加权希尔伯特空间几何中产生的惩罚项。通过拓扑条件分析，为实际拓扑设计提供了指导原则，确保行随机策略的优势。

Abstract: Decentralized learning often involves a weighted global loss with heterogeneous node weights $λ$. We revisit two natural strategies for incorporating these weights: (i) embedding them into the local losses to retain a uniform weight (and thus a doubly stochastic matrix), and (ii) keeping the original losses while employing a $λ$-induced row-stochastic matrix. Although prior work shows that both strategies yield the same expected descent direction for the global loss, it remains unclear whether the Euclidean-space guarantees are tight and what fundamentally differentiates their behaviors. To clarify this, we develop a weighted Hilbert-space framework $L^2(λ;\mathbb{R}^d)$ and obtain convergence rates that are strictly tighter than those from Euclidean analysis. In this geometry, the row-stochastic matrix becomes self-adjoint whereas the doubly stochastic one does not, creating additional penalty terms that amplify consensus error, thereby slowing convergence. Consequently, the difference in convergence arises not only from spectral gaps but also from these penalty terms. We then derive sufficient conditions under which the row-stochastic design converges faster even with a smaller spectral gap. Finally, by using a Rayleigh-quotient and Loewner-order eigenvalue comparison, we further obtain topology conditions that guarantee this advantage and yield practical topology-design guidelines.

</details>


### [15] [Automating Deception: Scalable Multi-Turn LLM Jailbreaks](https://arxiv.org/abs/2511.19517)
*Adarsh Kumarappan,Ananya Mujoo*

Main category: cs.LG

TL;DR: 本文提出了一种自动化生成多轮对话越狱数据集的方法，基于心理学原理（如登门槛技术）创建了1500个场景的基准测试，评估了7个主流大语言模型在单轮和多轮对话条件下的安全性能。


<details>
  <summary>Details</summary>
Motivation: 多轮对话攻击利用心理学原理绕过大语言模型的安全对齐机制，但目前防御进展受到手动创建数据集难以扩展的限制。

Method: 开发了自动化流水线，系统地将登门槛技术操作化为可复现模板，生成大规模心理学基础的多轮越狱数据集。

Result: GPT系列模型对对话历史表现出显著脆弱性，攻击成功率最多增加32个百分点；Gemini 2.5 Flash展现出卓越韧性，几乎免疫这些攻击；Claude 3 Haiku表现出强但不完美的抵抗力。

Conclusion: 当前安全架构在处理对话上下文方面存在关键差异，需要能够抵抗基于叙事操纵的防御机制。

Abstract: Multi-turn conversational attacks, which leverage psychological principles like Foot-in-the-Door (FITD), where a small initial request paves the way for a more significant one, to bypass safety alignments, pose a persistent threat to Large Language Models (LLMs). Progress in defending against these attacks is hindered by a reliance on manual, hard-to-scale dataset creation. This paper introduces a novel, automated pipeline for generating large-scale, psychologically-grounded multi-turn jailbreak datasets. We systematically operationalize FITD techniques into reproducible templates, creating a benchmark of 1,500 scenarios across illegal activities and offensive content. We evaluate seven models from three major LLM families under both multi-turn (with history) and single-turn (without history) conditions. Our results reveal stark differences in contextual robustness: models in the GPT family demonstrate a significant vulnerability to conversational history, with Attack Success Rates (ASR) increasing by as much as 32 percentage points. In contrast, Google's Gemini 2.5 Flash exhibits exceptional resilience, proving nearly immune to these attacks, while Anthropic's Claude 3 Haiku shows strong but imperfect resistance. These findings highlight a critical divergence in how current safety architectures handle conversational context and underscore the need for defenses that can resist narrative-based manipulation.

</details>


### [16] [Online Sparse Feature Selection in Data Streams via Differential Evolution](https://arxiv.org/abs/2511.19555)
*Ruiyang Xu*

Main category: cs.LG

TL;DR: 本文提出了一种新颖的在线差分进化稀疏特征选择方法（ODESFS），用于处理高维流数据中的特征选择问题，特别解决了数据不完整性和特征评估的挑战。


<details>
  <summary>Details</summary>
Motivation: 现有的在线稀疏流特征选择方法在特征评估方面存在显著局限性，导致性能下降。为了解决数据不完整性和特征评估的不足，需要开发更有效的方法。

Method: ODESFS方法包含两个关键创新：(1) 使用潜在因子分析模型进行缺失值填补；(2) 通过差分进化进行特征重要性评估。

Result: 在六个真实世界数据集上的综合实验表明，ODESFS始终优于最先进的OSFS和OS2FS方法，能够选择最优特征子集并获得更高的准确率。

Conclusion: ODESFS方法通过结合潜在因子分析和差分进化，有效解决了高维流数据中的特征选择问题，在性能和准确性方面均优于现有方法。

Abstract: The processing of high-dimensional streaming data commonly utilizes online streaming feature selection (OSFS) techniques. However, practical implementations often face challenges with data incompleteness due to equipment failures and technical constraints. Online Sparse Streaming Feature Selection (OS2FS) tackles this issue through latent factor analysis-based missing data imputation. Despite this advancement, existing OS2FS approaches exhibit substantial limitations in feature evaluation, resulting in performance deterioration. To address these shortcomings, this paper introduces a novel Online Differential Evolution for Sparse Feature Selection (ODESFS) in data streams, incorporating two key innovations: (1) missing value imputation using a latent factor analysis model, and (2) feature importance evaluation through differential evolution. Comprehensive experiments conducted on six real-world datasets demonstrate that ODESFS consistently outperforms state-of-the-art OSFS and OS2FS methods by selecting optimal feature subsets and achieving superior accuracy.

</details>


### [17] [Merging without Forgetting: Continual Fusion of Task-Specific Models via Optimal Transport](https://arxiv.org/abs/2511.19561)
*Zecheng Pan,Zhikang Chen,Ding Li,Min Zhang,Sen Cui,Hongshuo Jin,Luqi Tao,Yi Yang,Deheng Ye,Yu Zhang,Tingting Zhu,Tianling Ren*

Main category: cs.LG

TL;DR: OTMF提出了一种基于最优传输理论的模型融合框架，通过发现任务向量上的共享掩码来对齐任务特定模型的语义几何，解决参数插值带来的分布偏移问题。


<details>
  <summary>Details</summary>
Motivation: 现有的模型融合方法主要依赖权重空间的参数插值，这会引入特征空间的显著分布偏移并削弱任务特定知识。

Method: OTMF使用最优传输计划发现任务向量上的共享掩码，选择性提取可迁移和任务无关的组件，同时保持每个任务的独特结构身份。支持持续融合范式，增量整合新任务向量。

Result: 在多个视觉和语言基准测试上的综合实验表明，OTMF在准确性和效率方面都达到了最先进的性能。

Conclusion: 该方法在模型融合方面具有实践和理论价值，能够有效解决分布偏移问题并实现高效的多任务融合。

Abstract: Merging models fine-tuned for different tasks into a single unified model has become an increasingly important direction for building versatile, efficient multi-task systems. Existing approaches predominantly rely on parameter interpolation in weight space, which we show introduces significant distribution shift in the feature space and undermines task-specific knowledge. In this paper, we propose OTMF (Optimal Transport-based Masked Fusion), a novel model merging framework rooted in optimal transport theory to address the distribution shift that arises from naive parameter interpolation. Instead of directly aggregating features or weights, OTMF aligns the semantic geometry of task-specific models by discovering common masks applied to task vectors through optimal transport plans. These masks selectively extract transferable and task-agnostic components while preserving the unique structural identities of each task. To ensure scalability in real-world settings, OTMF further supports a continual fusion paradigm that incrementally integrates each new task vector without revisiting previous ones, maintaining a bounded memory footprint and enabling efficient fusion across a growing number of tasks. We conduct comprehensive experiments on multiple vision and language benchmarks, and results show that OTMF achieves state-of-the-art performance in terms of both accuracy and efficiency. These findings highlight the practical and theoretical value of our approach to model merging.

</details>


### [18] [An Invariant Latent Space Perspective on Language Model Inversion](https://arxiv.org/abs/2511.19569)
*Wentao Ye,Jiaqi Hu,Haobo Wang,Xinpeng Ti,Zhiqing Xiao,Hao Chen,Liyao Li,Lei Feng,Sai Wu,Junbo Zhao*

Main category: cs.LG

TL;DR: 本文提出语言模型逆向攻击(LMI)的威胁，并基于不变潜在空间假设提出Inv^2A方法，通过轻量级逆编码器从输出恢复隐藏提示，在9个数据集上优于基线方法4.77% BLEU分数。


<details>
  <summary>Details</summary>
Motivation: 语言模型逆向攻击(LMI)对用户隐私和系统安全构成威胁，需要开发有效的攻击方法来评估和防御此类风险。

Method: 提出不变潜在空间假设，构建Inv^2A方法：将LLM视为不变解码器，学习轻量级逆编码器将输出映射到去噪伪表示；使用稀疏拼接增加信息密度；两阶段训练（对比对齐和监督强化）。

Result: 在9个数据集上，Inv^2A平均比基线方法提高4.77% BLEU分数，同时减少对大逆语料库的依赖。分析显示现有防御措施保护有限。

Conclusion: Inv^2A有效提升了语言模型逆向攻击性能，揭示了当前防御措施的不足，强调了开发更强防御策略的必要性。

Abstract: Language model inversion (LMI), i.e., recovering hidden prompts from outputs, emerges as a concrete threat to user privacy and system security. We recast LMI as reusing the LLM's own latent space and propose the Invariant Latent Space Hypothesis (ILSH): (1) diverse outputs from the same source prompt should preserve consistent semantics (source invariance), and (2) input<->output cyclic mappings should be self-consistent within a shared latent space (cyclic invariance). Accordingly, we present Inv^2A, which treats the LLM as an invariant decoder and learns only a lightweight inverse encoder that maps outputs to a denoised pseudo-representation. When multiple outputs are available, they are sparsely concatenated at the representation layer to increase information density. Training proceeds in two stages: contrastive alignment (source invariance) and supervised reinforcement (cyclic invariance). An optional training-free neighborhood search can refine local performance. Across 9 datasets covering user and system prompt scenarios, Inv^2A outperforms baselines by an average of 4.77% BLEU score while reducing dependence on large inverse corpora. Our analysis further shows that prevalent defenses provide limited protection, underscoring the need for stronger strategies. The source code and data involved in this paper can be found in https://github.com/yyy01/Invariant_Attacker.

</details>


### [19] [Learning Massively Multitask World Models for Continuous Control](https://arxiv.org/abs/2511.19584)
*Nicklas Hansen,Hao Su,Xiaolong Wang*

Main category: cs.LG

TL;DR: 本文提出了一个包含200个多样化任务的基准测试，并介绍了Newt——一个语言条件化的多任务世界模型，通过演示预训练获取任务感知表示和动作先验，然后在所有任务上进行在线交互联合优化。


<details>
  <summary>Details</summary>
Motivation: 通用控制需要能够在多种任务和实体上操作的智能体，但目前连续控制的强化学习研究仍以单任务或离线模式为主，这强化了在线强化学习无法扩展的观点。受基础模型方法（大规模预训练+轻量强化学习）启发，探索单个智能体是否可以通过在线交互在数百个任务上进行训练。

Method: 1）引入包含200个多样化任务的基准测试，每个任务都有语言指令、演示和可选的图像观察；2）提出Newt语言条件化多任务世界模型，首先在演示上进行预训练以获取任务感知表示和动作先验，然后通过在线交互在所有任务上进行联合优化。

Result: 实验表明，Newt在多任务性能和数据效率方面优于一组强基线方法，表现出强大的开环控制能力，并能快速适应未见过的任务。

Conclusion: 研究表明单个智能体可以通过在线交互在数百个任务上进行训练，Newt模型在多任务控制方面表现出色，为通用控制研究提供了新的方向和工具。

Abstract: General-purpose control demands agents that act across many tasks and embodiments, yet research on reinforcement learning (RL) for continuous control remains dominated by single-task or offline regimes, reinforcing a view that online RL does not scale. Inspired by the foundation model recipe (large-scale pretraining followed by light RL) we ask whether a single agent can be trained on hundreds of tasks with online interaction. To accelerate research in this direction, we introduce a new benchmark with 200 diverse tasks spanning many domains and embodiments, each with language instructions, demonstrations, and optionally image observations. We then present \emph{Newt}, a language-conditioned multitask world model that is first pretrained on demonstrations to acquire task-aware representations and action priors, and then jointly optimized with online interaction across all tasks. Experiments show that Newt yields better multitask performance and data-efficiency than a set of strong baselines, exhibits strong open-loop control, and enables rapid adaptation to unseen tasks. We release our environments, demonstrations, code for training and evaluation, as well as 200+ checkpoints.

</details>


### [20] [Lower Complexity Bounds for Nonconvex-Strongly-Convex Bilevel Optimization with First-Order Oracles](https://arxiv.org/abs/2511.19656)
*Kaiyi Ji*

Main category: cs.LG

TL;DR: 本文针对双层优化问题，在光滑非凸-强凸设置下建立了新的下界。在确定性情况下，证明任何一阶零尊重算法至少需要Ω(κ^{3/2}ε^{-2})次oracle调用才能找到ε-精确的稳定点；在随机情况下，需要至少Ω(κ^{5/2}ε^{-4})次随机oracle调用。这些下界比单层非凸优化和非凸-强凸min-max问题的最优下界更强。


<details>
  <summary>Details</summary>
Motivation: 尽管双层优化的上界保证已被广泛研究，但由于双层结构的复杂性，下界的进展有限。本文旨在填补这一空白，为双层优化建立非平凡的下界。

Method: 开发新的困难实例，在确定性和随机一阶oracle模型下进行分析。针对光滑非凸-强凸设置，构造能够产生非平凡下界的硬实例。

Result: 在确定性情况下，获得了Ω(κ^{3/2}ε^{-2})的下界；在随机情况下，获得了Ω(κ^{5/2}ε^{-4})的下界。这些结果比相关设置中已知的最佳下界更强。

Conclusion: 研究结果揭示了当前双层优化上下界之间的显著差距，表明即使是简化机制（如具有二次下层目标的机制）也需要进一步研究，以理解标准一阶oracle下双层优化的最优复杂度。

Abstract: Although upper bound guarantees for bilevel optimization have been widely studied, progress on lower bounds has been limited due to the complexity of the bilevel structure. In this work, we focus on the smooth nonconvex-strongly-convex setting and develop new hard instances that yield nontrivial lower bounds under deterministic and stochastic first-order oracle models. In the deterministic case, we prove that any first-order zero-respecting algorithm requires at least $Ω(κ^{3/2}ε^{-2})$ oracle calls to find an $ε$-accurate stationary point, improving the optimal lower bounds known for single-level nonconvex optimization and for nonconvex-strongly-convex min-max problems. In the stochastic case, we show that at least $Ω(κ^{5/2}ε^{-4})$ stochastic oracle calls are necessary, again strengthening the best known bounds in related settings. Our results expose substantial gaps between current upper and lower bounds for bilevel optimization and suggest that even simplified regimes, such as those with quadratic lower-level objectives, warrant further investigation toward understanding the optimal complexity of bilevel optimization under standard first-order oracles.

</details>


### [21] [Structured Noise Modeling for Enhanced Time-Series Forecasting](https://arxiv.org/abs/2511.19657)
*Sepideh Koohfar*

Main category: cs.LG

TL;DR: 提出了一个预测-模糊-去噪框架，通过结构化噪声建模提高时间序列预测的保真度，结合可学习的高斯过程模块生成平滑相关扰动，同时使用专门的细化模型恢复高分辨率时间细节。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的时间序列预测面临挑战，因为时间模式在多个尺度上运行，从广泛的情境趋势到驱动关键决策的快速、细粒度波动。现有神经模型难以表示这些相互作用的动态，导致预测不稳定和下游应用可靠性降低。

Method: 采用预测-模糊-去噪框架，包含可学习的高斯过程模块生成平滑相关扰动，鼓励预测主干捕获长程结构，同时专门的细化模型恢复高分辨率时间细节。组件联合训练实现自然能力划分。

Result: 在电力、交通和太阳能数据集上的实验显示，在多时间范围准确性和稳定性方面获得了一致的提升。模块化设计还允许模糊-去噪层作为预训练模型的轻量级增强。

Conclusion: 通过增强细粒度时间预测的可靠性和可解释性，该框架有助于在能源、基础设施和其他时间关键领域中构建更值得信赖的AI系统。

Abstract: Time-series forecasting remains difficult in real-world settings because temporal patterns operate at multiple scales, from broad contextual trends to fast, fine-grained fluctuations that drive critical decisions. Existing neural models often struggle to represent these interacting dynamics, leading to unstable predictions and reduced reliability in downstream applications. This work introduces a forecast-blur-denoise framework that improves temporal fidelity through structured noise modeling. The approach incorporates a learnable Gaussian Process module that generates smooth, correlated perturbations, encouraging the forecasting backbone to capture long-range structure while a dedicated refinement model restores high-resolution temporal detail. Training the components jointly enables natural competence division and avoids the artifacts commonly produced by isotropic corruption methods. Experiments across electricity, traffic, and solar datasets show consistent gains in multi-horizon accuracy and stability. The modular design also allows the blur-denoise layer to operate as a lightweight enhancement for pretrained models, supporting efficient adaptation in limited-data scenarios. By strengthening the reliability and interpretability of fine-scale temporal predictions, this framework contributes to more trustworthy AI systems used in forecasting-driven decision support across energy, infrastructure, and other time-critical domains.

</details>


### [22] [Demystifying Diffusion Objectives: Reweighted Losses are Better Variational Bounds](https://arxiv.org/abs/2511.19664)
*Jiaxin Shi,Michalis K. Titsias*

Main category: cs.LG

TL;DR: 本文提出了一种新的理论解释，用于分析扩散模型中广泛使用的重加权损失函数。通过构建时间相关的变分下界级联，该方法在理论上改进了标准的证据下界，并减少了数据-模型KL散度。


<details>
  <summary>Details</summary>
Motivation: 为扩散模型中广泛使用的重加权损失函数提供理论依据，改进标准证据下界，提升生成模型性能。

Method: 构建时间相关的变分下界级联，形成重加权目标函数，适用于连续高斯扩散和掩码扩散模型。

Result: 在掩码扩散模型中展示了显著改进，像素空间图像建模性能接近连续扩散模型，并为掩码图像模型的简单加权方案提供了理论证明。

Conclusion: 提出的理论框架为扩散模型的重加权损失提供了新的理论解释，并在实践中取得了显著性能提升。

Abstract: We derive a new theoretical interpretation of the reweighted losses that are widely used for training diffusion models. Our method is based on constructing a cascade of time-dependent variational lower bounds on the data log-likelihood, that provably improves upon the standard evidence lower bound and results in reduced data-model KL-divergences. Combining such bounds gives rise to reweighted objectives that can be applied to any generative diffusion model including both continuous Gaussian diffusion and masked (discrete) diffusion models. Then, we showcase this framework in masked diffusion and report significant improvements over previous training losses in pixel-space image modeling, approaching sample quality comparable to continuous diffusion models. Our results also provide a theoretical justification for the simple weighting scheme widely used in masked image models.

</details>


### [23] [TiCT: A Synthetically Pre-Trained Foundation Model for Time Series Classification](https://arxiv.org/abs/2511.19694)
*Chin-Chia Michael Yeh,Uday Singh Saini,Junpeng Wang,Xin Dai,Xiran Fan,Jiarui Sun,Yujie Fan,Yan Zheng*

Main category: cs.LG

TL;DR: TiCT是一个基于Transformer的时间序列基础模型，专门用于上下文学习分类任务，仅使用合成数据进行预训练，无需微调即可在推理时通过少量示例实现竞争性性能。


<details>
  <summary>Details</summary>
Motivation: 时间序列数据普遍存在，但标注成本高昂。现有的时间序列基础模型主要关注预测任务，缺乏无需微调、支持上下文学习的通用分类模型。

Method: 提出TiCT模型：1）新颖架构，包括可扩展的基于比特的标签编码和特殊输出注意力机制以处理任意类别数；2）合成预训练框架，结合Mixup启发的过程和数据增强来促进泛化和噪声不变性。

Result: 在UCR Archive上的广泛评估显示，TiCT仅使用上下文示例在推理时就能达到与最先进监督方法竞争的性能，且无需更新任何模型权重。

Conclusion: TiCT证明了仅使用合成数据预训练的时间序列基础模型能够有效支持上下文学习分类，为时间序列分析提供了强大的通用解决方案。

Abstract: The ubiquity of time series data creates a strong demand for general-purpose foundation models, yet developing them for classification remains a significant challenge, largely due to the high cost of labeled data. Foundation models capable of in-context learning (ICL) offer a powerful solution, adapting to new tasks with minimal examples and reducing the need for extensive retraining. However, prior work on large-scale time series models has predominantly focused on forecasting, leaving a critical gap for versatile, fine-tuning-free classification. To address this, we introduce TiCT (Time-series in-Context Transformer), a transformer-based model pre-trained exclusively on synthetic data to perform in-context classification. We make two primary technical contributions: 1) a novel architecture featuring a scalable bit-based label encoding and a special output attention mechanism to handle an arbitrary number of classes; and 2) a synthetic pre-training framework that combines a Mixup-inspired process with data augmentation to foster generalization and noise invariance. Extensive evaluations on the UCR Archive show that TiCT achieves competitive performance against state-of-the-art supervised methods. Crucially, this is accomplished using only in-context examples at inference time, without updating a single model weight.

</details>


### [24] [Training-Free Active Learning Framework in Materials Science with Large Language Models](https://arxiv.org/abs/2511.19730)
*Hongchen Wang,Rafael Espinosa Castañeda,Jay R. Werber,Yao Fehlis,Edward Kim,Jason Hattrick-Simpers*

Main category: cs.LG

TL;DR: LLM-AL框架利用大语言模型的预训练知识和通用表示能力，在材料科学四个数据集上相比传统机器学习模型，将找到最优候选实验所需的实验次数减少70%以上，表现出更强的探索性和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 传统主动学习中的机器学习模型存在冷启动问题和领域特定特征工程的限制，影响其泛化能力。大语言模型提供了新的可能，可以利用预训练知识和通用表示直接从文本描述中提出实验建议。

Method: 提出基于LLM的主动学习框架(LLM-AL)，在迭代少样本设置下运行。探索了两种提示策略：简洁数值输入（适用于成分和结构化特征数据集）和扩展描述性文本（适用于实验和过程特征数据集）。

Result: 在所有数据集上，LLM-AL将找到最优候选实验所需的实验次数减少70%以上，始终优于传统ML模型。LLM-AL执行更广泛和探索性搜索，同时以更少迭代达到最优。性能在不同运行间保持稳定，与传统ML方法的变异性范围一致。

Conclusion: LLM-AL可以作为传统主动学习管道的通用替代方案，实现更高效和可解释的实验选择，并具有LLM驱动自主发现的潜力。

Abstract: Active learning (AL) accelerates scientific discovery by prioritizing the most informative experiments, but traditional machine learning (ML) models used in AL suffer from cold-start limitations and domain-specific feature engineering, restricting their generalizability. Large language models (LLMs) offer a new paradigm by leveraging their pretrained knowledge and universal token-based representations to propose experiments directly from text-based descriptions. Here, we introduce an LLM-based active learning framework (LLM-AL) that operates in an iterative few-shot setting and benchmark it against conventional ML models across four diverse materials science datasets. We explored two prompting strategies: one using concise numerical inputs suited for datasets with more compositional and structured features, and another using expanded descriptive text suited for datasets with more experimental and procedural features to provide additional context. Across all datasets, LLM-AL could reduce the number of experiments needed to reach top-performing candidates by over 70% and consistently outperformed traditional ML models. We found that LLM-AL performs broader and more exploratory searches while still reaching the optima with fewer iterations. We further examined the stability boundaries of LLM-AL given the inherent non-determinism of LLMs and found its performance to be broadly consistent across runs, within the variability range typically observed for traditional ML approaches. These results demonstrate that LLM-AL can serve as a generalizable alternative to conventional AL pipelines for more efficient and interpretable experiment selection and potential LLM-driven autonomous discovery.

</details>


### [25] [DISCO: A Browser-Based Privacy-Preserving Framework for Distributed Collaborative Learning](https://arxiv.org/abs/2511.19750)
*Julien T. T. Vignoud,Valérian Rousset,Hugo El Guedj,Ignacio Aleman,Walid Bennaceur,Batuhan Faik Derinbay,Eduard Ďurech,Damien Gengler,Lucas Giordano,Felix Grimberg,Franziska Lippoldt,Christina Kopidaki,Jiafan Liu,Lauris Lopata,Nathan Maire,Paul Mansat,Martin Milenkoski,Emmanuel Omont,Güneş Özgün,Mina Petrović,Francesco Posa,Morgan Ridel,Giorgio Savini,Marcel Torne,Lucas Trognon,Alyssa Unell,Olena Zavertiaieva,Sai Praneeth Karimireddy,Tahseen Rabbani,Mary-Anne Hartley,Martin Jaggi*

Main category: cs.LG

TL;DR: DISCO是一个开源分布式协作学习平台，允许非技术用户在不共享原始数据的情况下协作构建机器学习模型，支持联邦学习和去中心化范式，提供隐私保护和模型个性化功能。


<details>
  <summary>Details</summary>
Motivation: 解决数据因隐私、知识产权和法律约束而无法共享的问题，避免统计能力碎片化和可访问性偏差，使模型准确性能够更公平地分布。

Method: 开发基于浏览器的web应用程序，在本地训练模型，采用模块化设计支持联邦学习和去中心化范式，提供不同级别的隐私保证和多种权重聚合策略。

Result: 创建了开源平台DISCO，代码库和展示界面已公开，支持跨平台使用包括智能手机，无需编程知识即可使用。

Conclusion: DISCO为非技术用户提供了安全、可访问的协作机器学习解决方案，能够在保护数据隐私的同时实现模型协作训练。

Abstract: Data is often impractical to share for a range of well considered reasons, such as concerns over privacy, intellectual property, and legal constraints. This not only fragments the statistical power of predictive models, but creates an accessibility bias, where accuracy becomes inequitably distributed to those who have the resources to overcome these concerns. We present DISCO: an open-source DIStributed COllaborative learning platform accessible to non-technical users, offering a means to collaboratively build machine learning models without sharing any original data or requiring any programming knowledge. DISCO's web application trains models locally directly in the browser, making our tool cross-platform out-of-the-box, including smartphones. The modular design of \disco offers choices between federated and decentralized paradigms, various levels of privacy guarantees and several approaches to weight aggregation strategies that allow for model personalization and bias resilience in the collaborative training. Code repository is available at https://github.com/epfml/disco and a showcase web interface at https://discolab.ai

</details>


### [26] [When +1% Is Not Enough: A Paired Bootstrap Protocol for Evaluating Small Improvements](https://arxiv.org/abs/2511.19794)
*Wenzhang Du*

Main category: cs.LG

TL;DR: 该论文针对机器学习研究中1-2个百分点的微小改进是否具有统计显著性的问题，提出了一种基于配对多种子运行、BCa自助置信区间和符号翻转置换测试的保守评估协议，以在有限计算预算下防止过度声称。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习论文通常仅报告单次运行的基准测试结果，这些微小的改进对随机种子、数据排序和实现细节高度敏感，但很少提供不确定性估计或显著性检验，难以区分真实算法进步与噪声。

Method: 提出基于配对多种子运行、偏差校正加速自助置信区间和基于每种子增量的符号翻转置换测试的评估协议，该协议设计为保守的，旨在防止过度声称。

Result: 在CIFAR-10、CIFAR-10N和AG News数据集上的实验表明，单次运行和非配对t检验往往对0.6-2.0个百分点的改进给出显著结果，而使用仅三个种子的配对协议在这些设置下从未声明显著性。

Conclusion: 在有限计算预算下，这种保守的评估方法对于微小改进是更安全的默认选择，有助于避免过度声称算法进步。

Abstract: Recent machine learning papers often report 1-2 percentage point improvements from a single run on a benchmark. These gains are highly sensitive to random seeds, data ordering, and implementation details, yet are rarely accompanied by uncertainty estimates or significance tests. It is therefore unclear when a reported +1-2% reflects a real algorithmic advance versus noise.
  We revisit this problem under realistic compute budgets, where only a few runs are affordable. We propose a simple, PC-friendly evaluation protocol based on paired multi-seed runs, bias-corrected and accelerated (BCa) bootstrap confidence intervals, and a sign-flip permutation test on per-seed deltas. The protocol is intentionally conservative and is meant as a guardrail against over-claiming.
  We instantiate it on CIFAR-10, CIFAR-10N, and AG News using synthetic no-improvement, small-gain, and medium-gain scenarios. Single runs and unpaired t-tests often suggest significant gains for 0.6-2.0 point improvements, especially on text. With only three seeds, our paired protocol never declares significance in these settings. We argue that such conservative evaluation is a safer default for small gains under tight budgets.

</details>


### [27] [Terminal Velocity Matching](https://arxiv.org/abs/2511.19797)
*Linqi Zhou,Mathias Parger,Ayaan Haque,Jiaming Song*

Main category: cs.LG

TL;DR: TVM是一种流匹配的泛化方法，通过建模任意两个扩散时间步之间的转换，并在终端时间而非初始时间进行正则化，实现高保真的一步和少步生成建模。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型需要多步推理，计算成本高。TVM旨在实现高效的一步和少步生成，同时保持生成质量。

Method: 1. 提出终端速度匹配(TVM)框架；2. 引入最小架构修改实现稳定单阶段训练；3. 开发支持Jacobian-Vector Products反向传播的融合注意力核。

Result: 在ImageNet-256x256上：1步推理FID为3.29，4步推理FID为1.99；在ImageNet-512x512上：1步推理FID为4.32，4步推理FID为2.94。

Conclusion: TVM在一步和少步生成模型中达到了最先进的性能，为高效高质量生成提供了新方案。

Abstract: We propose Terminal Velocity Matching (TVM), a generalization of flow matching that enables high-fidelity one- and few-step generative modeling. TVM models the transition between any two diffusion timesteps and regularizes its behavior at its terminal time rather than at the initial time. We prove that TVM provides an upper bound on the $2$-Wasserstein distance between data and model distributions when the model is Lipschitz continuous. However, since Diffusion Transformers lack this property, we introduce minimal architectural changes that achieve stable, single-stage training. To make TVM efficient in practice, we develop a fused attention kernel that supports backward passes on Jacobian-Vector Products, which scale well with transformer architectures. On ImageNet-256x256, TVM achieves 3.29 FID with a single function evaluation (NFE) and 1.99 FID with 4 NFEs. It similarly achieves 4.32 1-NFE FID and 2.94 4-NFE FID on ImageNet-512x512, representing state-of-the-art performance for one/few-step models from scratch.

</details>


### [28] [BlockCert: Certified Blockwise Extraction of Transformer Mechanisms](https://arxiv.org/abs/2511.17645)
*Sandro Andric*

Main category: cs.LG

TL;DR: BlockCert是一个用于认证式块级提取transformer机制并支持认证式局部编辑的框架，通过提取结构化替代实现并提供机器可检查的证书来约束近似误差。


<details>
  <summary>Details</summary>
Motivation: 机制可解释性和模型编辑领域通常缺乏正式保证，无法确定提取或编辑后的模型在相关输入上与原始模型的偏离程度。

Method: 给定预训练transformer和提示分布，BlockCert提取残差块的结构化替代实现，提供约束近似误差、记录覆盖指标和哈希底层工件的机器可检查证书，并在Lean 4中形式化基于Lipschitz的组合定理。

Result: 在GPT-2 small、TinyLlama-1.1B-Chat和Llama-3.2-3B上的实验显示，获得了高块级覆盖率和小的残差误差，在TinyLlama设置中完全拼接模型在压力提示上的困惑度与基线相差约6e-5。

Conclusion: 块级提取与显式证书对于真实transformer语言模型是可行的，为机制可解释性和模型行为的形式推理提供了实用桥梁。

Abstract: Mechanistic interpretability aspires to reverse-engineer neural networks into explicit algorithms, while model editing seeks to modify specific behaviours without retraining. Both areas are typically evaluated with informal evidence and ad-hoc experiments, with few explicit guarantees about how far an extracted or edited model can drift from the original on relevant inputs. We introduce BlockCert, a framework for certified blockwise extraction of transformer mechanisms, and outline how a lightweight extension can support certified local edits. Given a pre-trained transformer and a prompt distribution, BlockCert extracts structured surrogate implementations for residual blocks together with machine-checkable certificates that bound approximation error, record coverage metrics, and hash the underlying artifacts. We formalize a simple Lipschitz-based composition theorem in Lean 4 that lifts these local guarantees to a global deviation bound. Empirically, we apply the framework to GPT-2 small, TinyLlama-1.1B-Chat, and Llama-3.2-3B. Across these models we obtain high per-block coverage and small residual errors on the evaluated prompts, and in the TinyLlama setting we show that a fully stitched model matches the baseline perplexity within approximately 6e-5 on stress prompts. Our results suggest that blockwise extraction with explicit certificates is feasible for real transformer language models and offers a practical bridge between mechanistic interpretability and formal reasoning about model behaviour.

</details>


### [29] [Cisco Time Series Model Technical Report](https://arxiv.org/abs/2511.19841)
*Liang Gou,Archit Khare,Praneet Pabolu,Prachi Patel,Joseph Ross,Hercy Shen,Yuhan,Song,Jingze Sun,Kristal Curtis,Vedant Dharnidharka,Abhinav Mathur,Hao Yang*

Main category: cs.LG

TL;DR: Cisco时间序列模型是一个单变量零样本预测器，通过多分辨率输入架构创新，在超过3000亿数据点上训练，在可观测性数据集上表现优异，同时保持通用预测基准的相似性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够处理多分辨率输入的时间序列基础模型，特别针对可观测性领域，同时保持通用预测能力。

Method: 对TimesFM模型进行架构创新，使其能够接受多分辨率输入，训练超过3000亿个独特数据点（其中一半以上来自可观测性领域）。

Result: 模型在可观测性数据集上实现卓越性能，在通用预测基准（GIFT-Eval）上保持相似性能，多分辨率结构使模型在长上下文输入上预测更准确。

Conclusion: 多分辨率解码器模型在保持通用预测能力的同时，在可观测性领域表现出色，长上下文预测能力得到提升。

Abstract: We introduce the Cisco Time Series Model, a univariate zero-shot forecaster. This time series foundation model is the result of a general architectural innovation to a time series model enabling it to accept multiresolution input, applied to a popular decoder-only time series model (TimesFM). The resulting multiresolution decoder-only model is trained on over 300B unique data points, with more than half coming from the observability domain. Quantitative and qualitative evaluations demonstrate that the resulting model achieves superior performance on observability datasets while retaining very similar performance on a standard general-purpose forecasting benchmark (GIFT-Eval), and suggest that the multiresolution structure enables the model to make more accurate predictions on long context input.

</details>


### [30] [Accelerating Wireless Distributed Learning via Hybrid Split and Federated Learning Optimization](https://arxiv.org/abs/2511.19851)
*Kun Guo,Xuefei Li,Xijun Wang,Howard H. Yang,Wei Feng,Tony Q. S. Quek*

Main category: cs.LG

TL;DR: 本文提出了一种混合分割与联邦学习（HSFL）加速方法，通过联合优化学习模式选择、批量大小以及通信计算资源来减少整体学习延迟。


<details>
  <summary>Details</summary>
Motivation: 联邦学习（FL）支持低延迟并行训练但可能收敛到较低精度模型，而分割学习（SL）能获得更高精度但延迟较大。为了结合两者的优势，需要研究如何加速混合学习模式。

Method: 首先分析收敛性揭示学习模式与批量大小的相互作用，然后制定延迟最小化问题并提出两阶段解决方案：使用块坐标下降法求解松弛问题获得局部最优解，再通过舍入算法恢复整数批量大小。

Result: 实验结果表明，与现有方法相比，该方法能显著加速达到目标精度的收敛过程。

Conclusion: 通过联合优化学习模式、批量大小和资源分配，可以有效加速混合分割与联邦学习，实现更好的性能平衡。

Abstract: Federated learning (FL) and split learning (SL) are two effective distributed learning paradigms in wireless networks, enabling collaborative model training across mobile devices without sharing raw data. While FL supports low-latency parallel training, it may converge to less accurate model. In contrast, SL achieves higher accuracy through sequential training but suffers from increased delay. To leverage the advantages of both, hybrid split and federated learning (HSFL) allows some devices to operate in FL mode and others in SL mode. This paper aims to accelerate HSFL by addressing three key questions: 1) How does learning mode selection affect overall learning performance? 2) How does it interact with batch size? 3) How can these hyperparameters be jointly optimized alongside communication and computational resources to reduce overall learning delay? We first analyze convergence, revealing the interplay between learning mode and batch size. Next, we formulate a delay minimization problem and propose a two-stage solution: a block coordinate descent method for a relaxed problem to obtain a locally optimal solution, followed by a rounding algorithm to recover integer batch sizes with near-optimal performance. Experimental results demonstrate that our approach significantly accelerates convergence to the target accuracy compared to existing methods.

</details>


### [31] [Frailty-Aware Transformer for Recurrent Survival Modeling of Driver Retention in Ride-Hailing Platforms](https://arxiv.org/abs/2511.19893)
*Shuoyan Xu,Yu Zhang,Eric J. Miller*

Main category: cs.LG

TL;DR: 提出基于Transformer的生存分析框架FACT，用于建模网约车司机的空闲行为，在时间依赖C指数和Brier得分上优于传统和深度学习生存模型。


<details>
  <summary>Details</summary>
Motivation: 网约车平台具有高频、行为驱动的特点，虽然生存分析在其他领域已用于建模重复事件，但在网约车司机行为建模中的应用仍较少探索。

Method: 将空闲行为建模为重复生存过程，使用Transformer框架捕捉长期时间依赖关系，采用因果掩码并结合司机特定嵌入来建模潜在异质性。

Result: 在多伦多网约车数据上的结果显示，FACT模型在时间依赖C指数和Brier得分上表现最佳，优于经典和深度学习生存模型。

Conclusion: 该方法能够提供更准确的风险估计，支持平台留存策略，并提供政策相关的见解。

Abstract: Ride-hailing platforms are characterized by high-frequency, behavior-driven environments. Although survival analysis has been applied to recurrent events in other domains, its use in modeling ride-hailing driver behavior remains largely unexplored. This study formulates idle behavior as a recurrent survival process using large-scale platform data and proposes a Transformer-based framework that captures long-term temporal dependencies with causal masking and incorporates driver-specific embeddings to model latent heterogeneity. Results on Toronto ride-hailing data demonstrate that the proposed Frailty-Aware Cox Transformer (FACT) achieves the highest time-dependent C-indices and lowest Brier Scores, outperforming classical and deep learning survival models. This approach enables more accurate risk estimation, supports platform retention strategies, and provides policy-relevant insights.

</details>


### [32] [EfficientXpert: Efficient Domain Adaptation for Large Language Models via Propagation-Aware Pruning](https://arxiv.org/abs/2511.19935)
*Songlin Zhao,Michael Pitts,Zhuwei Qin*

Main category: cs.LG

TL;DR: EfficientXpert是一个轻量级领域剪枝框架，通过传播感知剪枝准则和高效适配器更新算法，将通用预训练模型一步转换为稀疏的领域专家模型，在40%稀疏度下保持98%性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在专业领域应用需求增长，但模型尺寸大限制了在资源受限环境中的部署，现有压缩方法要么跨领域泛化能力差，要么开销高。

Method: 结合传播感知剪枝准则(Foresight Mask)和高效适配器更新算法(Partial Brain Surgeon)，集成到LoRA微调过程中，实现一步转换。

Result: 在健康和司法任务中，40%稀疏度下保持高达98%的稠密模型性能，优于现有最先进方法。分析显示领域依赖的结构变化会降低通用剪枝掩码的有效性。

Conclusion: 需要针对每个领域量身定制的自适应、领域感知剪枝策略。

Abstract: The rapid advancement of large language models (LLMs) has increased the demand for domain-specialized variants in areas such as law, healthcare, and finance. However, their large size remains a barrier to deployment in resource-constrained environments, and existing compression methods either generalize poorly across domains or incur high overhead. In this work, we propose \textbf{EfficientXpert}, a lightweight domain-pruning framework that combines a propagation-aware pruning criterion (Foresight Mask) with an efficient adapter-update algorithm (Partial Brain Surgeon). Integrated into the LoRA fine-tuning process, EfficientXpert enables a one-step transformation of general pretrained models into sparse, domain-adapted experts. Across health and legal tasks, it retains up to 98% of dense-model performance at 40% sparsity, outperforming state-of-the-art methods. Further analysis reveals substantial domain-dependent structural shifts that degrade the effectiveness of general pruning masks, underscoring the need for adaptive, domain-aware pruning strategies tailored to each domain.

</details>


### [33] [Optimize Flip Angle Schedules In MR Fingerprinting Using Reinforcement Learning](https://arxiv.org/abs/2511.19941)
*Shenjun Zhong,Zhifeng Chen,Zhaolin Chen*

Main category: cs.LG

TL;DR: 本文提出了一个强化学习框架来优化磁共振指纹成像中的翻转角调度，展示了学习到的非周期性模式可以增强指纹可分离性，并可能减少重复时间以加速采集。


<details>
  <summary>Details</summary>
Motivation: 磁共振指纹成像利用可调采集参数产生的瞬态信号动态，使得设计最优、稳健的序列成为一个复杂的高维顺序决策问题。强化学习为自动化参数选择提供了有前景的方法，以优化脉冲序列来最大化参数空间中指纹的可区分性。

Method: 引入强化学习框架来优化磁共振指纹成像中的翻转角调度，通过RL算法自动选择参数，学习出非周期性的翻转角模式。

Result: 展示了学习到的调度表现出非周期性模式，增强了指纹可分离性。有趣的是，RL优化的调度可能减少重复时间数量，潜在地加速磁共振指纹成像采集。

Conclusion: 强化学习框架能够有效优化磁共振指纹成像中的翻转角调度，产生非周期性模式以提高指纹区分能力，并可能通过减少重复时间来加速采集过程。

Abstract: Magnetic Resonance Fingerprinting (MRF) leverages transient-state signal dynamics generated by the tunable acquisition parameters, making the design of an optimal, robust sequence a complex, high-dimensional sequential decision problem, such as optimizing one of the key parameters, flip angle. Reinforcement learning (RL) offers a promising approach to automate parameter selection, to optimize pulse sequences that maximize the distinguishability of fingerprints across the parameter space. In this work, we introduce an RL framework for optimizing the flip-angle schedule in MRF and demonstrate a learned schedule exhibiting non-periodic patterns that enhances fingerprint separability. Additionally, an interesting observation is that the RL-optimized schedule may enable a reduction in the number of repetition time, potentially accelerate MRF acquisitions.

</details>


### [34] [Differential Smoothing Mitigates Sharpening and Improves LLM Reasoning](https://arxiv.org/abs/2511.19942)
*Jingchu Gai,Guanning Zeng,Huaqing Zhang,Aditi Raghunathan*

Main category: cs.LG

TL;DR: 本文针对RL微调大语言模型时出现的多样性崩溃问题，提出了理论分析和解决方案。通过证明RL微调存在选择和强化偏差导致多样性崩溃，作者提出了差分平滑方法，在保证正确性的同时提升多样性。


<details>
  <summary>Details</summary>
Motivation: 现有的对抗多样性崩溃的启发式方法存在随意性，经常在正确性和多样性之间权衡，效果因任务而异，有时甚至相互矛盾。需要建立一个理论基础来系统解决这一问题。

Method: 首先形式化证明了RL微调导致多样性崩溃的原因（选择和强化偏差），然后基于"只需在正确轨迹上应用奖励修改"的关键观察，提出了差分平滑方法。

Result: 在1B到7B参数的模型上，在CountDown和真实世界数学推理等领域的广泛实验表明，差分平滑方法在Pass@1和Pass@k指标上均优于普通RL和基于熵的启发式方法，在AIME24数据集上提升达6.7%。

Conclusion: 差分平滑方法在理论上优于现有启发式方法，实验证明能同时提升正确性和多样性，为RL微调中的多样性问题提供了原则性解决方案。

Abstract: It is widely recognized that reinforcement learning (RL) fine-tuning of large language models often leads to \textit{diversity collapse}, where outputs lack variety. Prior work has proposed a range of heuristics to counteract this effect, but these methods are ad hoc: they frequently trade off correctness for diversity, their effectiveness varies across tasks, and in some cases they even contradict one another. In this work, we place these observations on a rigorous foundation. We first provide a formal proof of why RL fine-tuning exhibits diversity collapse via a selection and reinforcement bias. Next, we make a key observation that any reward modification to address diversity collapse only needs to be applied on the correct trajectories. Building directly on this analysis, we introduce a principled method -- \textit{differential smoothing} -- that provably improves both correctness and diversity, outperforming vanilla RL as well as widely used entropy-based heuristics. Our theory precisely characterizes when existing heuristics help and why they fail, while showing that differential smoothing is universally superior. Extensive experiments with models from 1B to 7B parameters, across domains including CountDown and real-world mathematical reasoning, demonstrate consistent gains. Differential smoothing improves both Pass@1 and Pass@k, with up to 6.7\% improvements on AIME24 dataset.

</details>


### [35] [Prompt Fairness: Sub-group Disparities in LLMs](https://arxiv.org/abs/2511.19956)
*Meiyu Zhong,Noel Teku,Ravi Tandon*

Main category: cs.LG

TL;DR: 本文研究LLM中的提示公平性问题，发现相同问题但不同用户/风格的提示表述会导致LLM产生不同质量的响应。作者提出信息论指标量化偏差，并通过多数投票和提示中性化等干预措施减少响应差异。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在不同用户/风格的提示表述下会产生显著不同的响应质量，这种提示公平性问题需要被量化和解决。

Method: 使用信息论指标（子群敏感性和跨群一致性）量化偏差，并提出多数投票和提示中性化等实际干预措施来减少响应差异。

Result: 实验显示不同人口统计子群存在明显的提示敏感性差异：缓解前跨群分歧值达0.28，通常在0.14-0.22范围内；应用中性化和多代策略后，分歧持续减少，最大差距降至0.22，许多距离降至0.17或以下。

Conclusion: 通过提出的干预措施，可以显著改善LLM在不同用户子群间的响应稳定性和公平性，减少结构性不平等。

Abstract: Large Language Models (LLMs), though shown to be effective in many applications, can vary significantly in their response quality. In this paper, we investigate this problem of prompt fairness: specifically, the phrasing of a prompt by different users/styles, despite the same question being asked in principle, may elicit different responses from an LLM. To quantify this disparity, we propose to use information-theoretic metrics that can capture two dimensions of bias: subgroup sensitivity, the variability of responses within a subgroup and cross group consistency, the variability of responses across subgroups. Our analysis reveals that certain subgroups exhibit both higher internal variability and greater divergence from others. Our empirical analysis reveals that certain demographic sub groups experience both higher internal variability and greater divergence from others, indicating structural inequities in model behavior. To mitigate these disparities, we propose practical interventions, including majority voting across multiple generations and prompt neutralization, which together improve response stability and enhance fairness across user populations. In the experiments, we observe clear prompt sensitivity disparities across demographic subgroups: before mitigation, cross-group divergence values reach 0.28 and typically fall in the from 0.14 to 0.22 range. After applying our neutralization and multi generation strategy, these divergences consistently decrease, with the largest gap reduced to 0.22 and many distances falling to 0.17 or below, indicating more stable and consistent outputs across subgroups.

</details>


### [36] [ParaBlock: Communication-Computation Parallel Block Coordinate Federated Learning for Large Language Models](https://arxiv.org/abs/2511.19959)
*Yujia Wang,Yuanpu Cao,Jinghui Chen*

Main category: cs.LG

TL;DR: ParaBlock是一种用于联邦学习大语言模型的新方法，通过建立通信和计算的并行线程来提高通信效率，同时保持与标准联邦块坐标下降方法相同的收敛速度。


<details>
  <summary>Details</summary>
Motivation: 在联邦学习大语言模型时代，即使单个块也包含大量参数，给资源受限的客户端带来显著的通信延迟挑战。

Method: 提出ParaBlock方法，建立通信和计算的并行线程来增强通信效率。

Result: 经验评估表明，ParaBlock不仅保持强大的性能，还显著提高了通信效率。

Conclusion: ParaBlock在保持性能的同时显著改善了联邦学习大语言模型的通信效率。

Abstract: Federated learning (FL) has been extensively studied as a privacy-preserving training paradigm. Recently, federated block coordinate descent scheme has become a popular option in training large-scale models, as it allows clients to train only a subset of the model locally instead of the entire model. However, in the era of large language models (LLMs), even a single block can contain a significant number of parameters, posing substantial communication latency, particularly for resource-constrained clients. To address this challenge in federated training/fine-tuning LLMs, we propose ParaBlock, a novel approach that establishes two parallel threads for communication and computation to enhance communication efficiency. We theoretically prove that the proposed ParaBlock achieves the same convergence rate as the standard federated block coordinate descent methods. Empirical evaluations on fine-tuning LLMs on general instruction following and mathematical reasoning confirm that ParaBlock not only maintains strong performance but also significantly improves communication efficiency.

</details>


### [37] [Stragglers Can Contribute More: Uncertainty-Aware Distillation for Asynchronous Federated Learning](https://arxiv.org/abs/2511.19966)
*Yujia Wang,Fenglong Ma,Jinghui Chen*

Main category: cs.LG

TL;DR: FedEcho是一个新颖的异步联邦学习框架，通过不确定性感知蒸馏技术解决异步延迟和数据异构性带来的挑战，在保持性能的同时不访问私有客户端数据。


<details>
  <summary>Details</summary>
Motivation: 异步联邦学习虽然提高了效率和可扩展性，但面临过时更新和快速客户端主导学习过程的偏见问题，现有方法通常只能解决其中一个问题。

Method: 采用不确定性感知蒸馏技术，服务器评估滞后客户端预测的可靠性，根据估计的不确定性动态调整这些预测的影响，优先考虑更确定的预测同时利用所有客户端的多样化信息。

Result: 通过广泛实验证明，FedEcho在异步联邦学习基准上持续优于现有方法，实现了鲁棒的性能。

Conclusion: FedEcho有效缓解了过时更新和数据异构性的负面影响，为异步联邦学习提供了有效的解决方案。

Abstract: Asynchronous federated learning (FL) has recently gained attention for its enhanced efficiency and scalability, enabling local clients to send model updates to the server at their own pace without waiting for slower participants. However, such a design encounters significant challenges, such as the risk of outdated updates from straggler clients degrading the overall model performance and the potential bias introduced by faster clients dominating the learning process, especially under heterogeneous data distributions. Existing methods typically address only one of these issues, creating a conflict where mitigating the impact of outdated updates can exacerbate the bias created by faster clients, and vice versa. To address these challenges, we propose FedEcho, a novel framework that incorporates uncertainty-aware distillation to enhance the asynchronous FL performances under large asynchronous delays and data heterogeneity. Specifically, uncertainty-aware distillation enables the server to assess the reliability of predictions made by straggler clients, dynamically adjusting the influence of these predictions based on their estimated uncertainty. By prioritizing more certain predictions while still leveraging the diverse information from all clients, FedEcho effectively mitigates the negative impacts of outdated updates and data heterogeneity. Through extensive experiments, we demonstrate that FedEcho consistently outperforms existing asynchronous federated learning baselines, achieving robust performance without requiring access to private client data.

</details>


### [38] [On-Demand Multi-Task Sparsity for Efficient Large-Model Deployment on Edge Devices](https://arxiv.org/abs/2511.19986)
*Lianming Huang,Haibo Hu,Qiao Li,Nan Guan,Chun Jason Xue*

Main category: cs.LG

TL;DR: 提出了一种按需多任务稀疏框架，通过最大化参数重用来最小化任务切换时的I/O开销。该方法将权重分解为可重用的块粒度单元，并在任务间对齐稀疏结构以实现最大重叠。


<details>
  <summary>Details</summary>
Motivation: 现有稀疏方法在优化单个任务时忽略了频繁任务切换带来的显著I/O开销，无法满足资源受限边缘平台上部署大型模型的需求。

Method: 将权重分解为可重用的块粒度单元，在任务间对齐稀疏结构以最大化重叠，动态加载下一个任务所需的少量差异块集合。

Result: 在真实自动驾驶平台上的实验表明，该框架实现了优异的切换效率，相比现有稀疏方法平均加速任务切换超过6.6倍。

Conclusion: 该按需多任务稀疏框架通过参数重用和动态块加载，有效缓解了传统整体方法固有的冷启动延迟问题。

Abstract: Sparsity is essential for deploying large models on resource constrained edge platforms. However, optimizing sparsity patterns for individual tasks in isolation ignores the significant I/O overhead incurred during frequent task switching. We introduce an on-demand multi-task sparsity framework specifically designed to minimize switching costs by maximizing parameter reuse. Unlike monolithic approaches, we decompose weights into reusable block-granular units and align sparse structures across tasks to maximize overlap. By dynamically loading only the small differential set of blocks required for the next task, our method effectively mitigates the cold-start latency inherent in traditional monolithic approaches.Experiments on a real-world autonomous driving platform demonstrate that our framework achieves superior switching efficiency, accelerating task switching by over 6.6X on average compared to existing sparsity methods.

</details>


### [39] [RankOOD - Class Ranking-based Out-of-Distribution Detection](https://arxiv.org/abs/2511.19996)
*Dishanika Denipitiyage,Naveen Karunanayake,Suranga Seneviratne,Sanjay Chawla*

Main category: cs.LG

TL;DR: RankOOD是一种基于排序的离群检测方法，通过Placket-Luce损失训练模型，利用ID类预测中的排序模式来检测OOD样本。


<details>
  <summary>Details</summary>
Motivation: 现有基于交叉熵损失的深度学习模型在ID类预测中会产生排序模式，但OOD样本可能不遵循这种排序规律，这为OOD检测提供了新思路。

Method: 首先使用初始分类器为每个类提取排序列表，然后使用Plackett-Luce损失进行第二轮训练，将类排序作为预测变量。

Result: 在TinyImageNet近OOD评估基准上达到最先进性能，将FPR95降低了4.3%。

Conclusion: RankOOD通过利用类预测的排序模式，有效提升了OOD检测性能，为OOD检测提供了新的解决方案。

Abstract: We propose RankOOD, a rank-based Out-of-Distribution (OOD) detection approach based on training a model with the Placket-Luce loss, which is now extensively used for preference alignment tasks in foundational models. Our approach is based on the insight that with a deep learning model trained using the Cross Entropy Loss, in-distribution (ID) class prediction induces a ranking pattern for each ID class prediction. The RankOOD framework formalizes the insight by first extracting a rank list for each class using an initial classifier and then uses another round of training with the Plackett-Luce loss, where the class rank, a fixed permutation for each class, is the predicted variable. An OOD example may get assigned with high probability to an ID example, but the probability of it respecting the ranking classification is likely to be small. RankOOD, achieves SOTA performance on the near-ODD TinyImageNet evaluation benchmark, reducing FPR95 by 4.3%.

</details>


### [40] [REWA: Witness-Overlap Theory -- Foundations for Composable Binary Similarity Systems](https://arxiv.org/abs/2511.19998)
*Nikit Phadke*

Main category: cs.LG

TL;DR: REWA提出了一种基于见证重叠结构的通用相似性理论，证明只要概念间的相似性可以表示为单调见证重叠，就能压缩编码并保持排序，统一了多种相似性方法。


<details>
  <summary>Details</summary>
Motivation: 现有相似性方法（如Bloom过滤器、minhash等）缺乏统一的理论基础，REWA旨在提供一个基于见证重叠的通用框架，使各种相似性定义都能获得可证明的编码效率保证。

Method: REWA系统包含三个组件：有限见证集、半随机比特分配和单调性假设。通过证明在见证重叠条件下，仅需对数级比特数就能保持top-k排序。

Result: 理论证明在见证重叠条件下，使用m=O(log(|V|/δ))比特即可保持top-k排序，统一了多种相似性方法，提供了模块化构建相似性系统的框架。

Conclusion: REWA为相似性系统提供了基于见证重叠的通用理论基础，支持模块化组合设计，统一了现有方法并开辟了广阔的设计空间。

Abstract: REWA introduces a general theory of similarity based on witness-overlap structures. We show that whenever similarity between concepts can be expressed as monotone witness overlap -- whether arising from graph neighborhoods, causal relations, temporal structure, topological features, symbolic patterns, or embedding-based neighborhoods -- it admits a reduction to compact encodings with provable ranking preservation guarantees. REWA systems consist of: (1) finite witness sets $W(v)$, (2) semi-random bit assignments generated from each witness, and (3) monotonicity of expected similarity in the overlap $Δ(u, v) = |W(u) \cap W(v)|$. We prove that under an overlap-gap condition on the final witness sets -- independent of how they were constructed -- top-$k$ rankings are preserved using $m = O(\log(|V|/δ))$ bits. The witness-set formulation is compositional: any sequence of structural, temporal, causal, topological, information-theoretic, or learned transformations can be combined into pipelines that terminate in discrete witness sets. The theory applies to the final witness overlap, enabling modular construction of similarity systems from reusable primitives. This yields a vast design space: millions of composable similarity definitions inherit logarithmic encoding complexity. REWA subsumes and unifies Bloom filters, minhash, LSH bitmaps, random projections, sketches, and hierarchical filters as special cases. It provides a principled foundation for similarity systems whose behavior is governed by witness overlap rather than hash-function engineering. This manuscript presents the axioms, the main reducibility theorem, complete proofs with explicit constants, and a detailed discussion of compositional design, limitations, and future extensions including multi-bit encodings, weighted witnesses, and non-set representations.

</details>


### [41] [Zero-Shot Transfer Capabilities of the Sundial Foundation Model for Leaf Area Index Forecasting](https://arxiv.org/abs/2511.20004)
*Peining Zhang,Hongchen Qin,Haochen Zhang,Ziqi Guo,Guiling Wang,Jinbo Bi*

Main category: cs.LG

TL;DR: 研究发现，在零样本设置下，Sundial基础模型在输入上下文窗口足够长（覆盖超过1-2个完整季节周期）时，能够超越完全训练的LSTM模型，首次证明通用基础模型无需任务特定调优即可在遥感时间序列预测中超越专业监督模型。


<details>
  <summary>Details</summary>
Motivation: 研究时间序列基础模型在农业监测中叶片面积指数（LAI）预测的零样本预测能力，探索通用基础模型在遥感时间序列预测中的潜力。

Method: 使用HiQ数据集（美国，2000-2022），系统比较统计基线、全监督LSTM和Sundial基础模型在多种评估协议下的表现。

Result: Sundial在零样本设置下，当输入上下文窗口足够长（覆盖超过1-2个完整季节周期）时，能够超越完全训练的LSTM模型。

Conclusion: 预训练的时间序列基础模型在农业和环境应用中具有作为即插即用预测器的强大潜力，通用基础模型无需任务特定调优即可超越专业监督模型。

Abstract: This work investigates the zero-shot forecasting capability of time-series foundation models for Leaf Area Index (LAI) forecasting in agricultural monitoring. Using the HiQ dataset (U.S., 2000-2022), we systematically compare statistical baselines, a fully supervised LSTM, and the Sundial foundation model under multiple evaluation protocols. We find that Sundial, in the zero-shot setting, can outperform a fully trained LSTM provided that the input context window is sufficiently long-specifically, when covering more than one or two full seasonal cycles. This demonstrates, for the first time, that a general-purpose foundation model can surpass specialized supervised models on remote-sensing time series prediction without any task-specific tuning. These results highlight the strong potential of pretrained time-series foundation models to serve as effective plug-and-play forecasters in agricultural and environmental applications.

</details>


### [42] [iRadioDiff: Physics-Informed Diffusion Model for Indoor Radio Map Construction and Localization](https://arxiv.org/abs/2511.20015)
*Xiucheng Wang,Tingwei Yuan,Yang Cao,Nan Cheng,Ruijin Sun,Weihua Zhuang*

Main category: cs.LG

TL;DR: iRadioDiff是一个基于扩散模型的室内无线电地图构建框架，通过物理信息提示和多路径关键先验来生成高保真度的电磁场分布图，解决了传统方法在异构室内环境中的局限性。


<details>
  <summary>Details</summary>
Motivation: 构建高保真度室内无线电地图面临挑战，传统电磁求解器延迟高，基于学习的方法依赖稀疏测量或假设均匀材料，与室内环境的异构和多路径丰富特性不符。

Method: 提出iRadioDiff框架，基于扩散模型，条件输入包括接入点位置、材料反射和传输系数的物理信息提示，以及衍射点、强传输边界和视距轮廓等多路径关键先验，通过条件通道和边界加权目标指导生成过程。

Result: 实验表明iRadioDiff在室内无线电地图构建和基于接收信号强度的室内定位方面达到最先进性能，并在不同布局和材料配置下具有良好的泛化能力。

Conclusion: iRadioDiff能够准确建模非平稳场不连续性，高效构建物理一致的无线电地图，为室内环境感知提供有效解决方案。

Abstract: Radio maps (RMs) serve as environment-aware electromagnetic (EM) representations that connect scenario geometry and material properties to the spatial distribution of signal strength, enabling localization without costly in-situ measurements. However, constructing high-fidelity indoor RMs remains challenging due to the prohibitive latency of EM solvers and the limitations of learning-based methods, which often rely on sparse measurements or assumptions of homogeneous material, which are misaligned with the heterogeneous and multipath-rich nature of indoor environments. To overcome these challenges, we propose iRadioDiff, a sampling-free diffusion-based framework for indoor RM construction. iRadioDiff is conditioned on access point (AP) positions, and physics-informed prompt encoded by material reflection and transmission coefficients. It further incorporates multipath-critical priors, including diffraction points, strong transmission boundaries, and line-of-sight (LoS) contours, to guide the generative process via conditional channels and boundary-weighted objectives. This design enables accurate modeling of nonstationary field discontinuities and efficient construction of physically consistent RMs. Experiments demonstrate that iRadioDiff achieves state-of-the-art performance in indoor RM construction and received signal strength based indoor localization, which offers effective generalization across layouts and material configurations. Code is available at https://github.com/UNIC-Lab/iRadioDiff.

</details>


### [43] [Cross-Contrastive Clustering for Multimodal Attributed Graphs with Dual Graph Filtering](https://arxiv.org/abs/2511.20030)
*Haoran Zheng,Renchi Yang,Hongtao Wang,Jianliang Xu*

Main category: cs.LG

TL;DR: 提出了一种双图滤波（DGF）方案，用于处理多模态属性图（MMAGs）的聚类问题，通过特征级去噪和三交叉对比学习策略，有效克服了现有多视图聚类方法在处理大预训练模型输出的多模态属性时的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有多视图聚类方法严重依赖视图间高度相关性，但忽略了MMAGs中多模态属性（如文本、图像）的低模态相关性和强特征噪声等独特特性，导致聚类性能不佳。

Method: 提出双图滤波（DGF）方案，包含特征级去噪组件和三交叉对比训练策略，通过跨模态、邻域和社区的实例级对比学习来学习鲁棒且具有区分度的节点表示。

Result: 在8个基准MMAG数据集上的综合实验表明，DGF在聚类质量方面能够一致且显著地优于多种最先进的基线方法。

Conclusion: DGF方案通过创新的图滤波和对比学习策略，有效解决了MMAGs聚类中的模态相关性和噪声问题，取得了优异的聚类性能。

Abstract: Multimodal Attributed Graphs (MMAGs) are an expressive data model for representing the complex interconnections among entities that associate attributes from multiple data modalities (text, images, etc.). Clustering over such data finds numerous practical applications in real scenarios, including social community detection, medical data analytics, etc. However, as revealed by our empirical studies, existing multi-view clustering solutions largely rely on the high correlation between attributes across various views and overlook the unique characteristics (e.g., low modality-wise correlation and intense feature-wise noise) of multimodal attributes output by large pre-trained language and vision models in MMAGs, leading to suboptimal clustering performance.
  Inspired by foregoing empirical observations and our theoretical analyses with graph signal processing, we propose the Dual Graph Filtering (DGF) scheme, which innovatively incorporates a feature-wise denoising component into node representation learning, thereby effectively overcoming the limitations of traditional graph filters adopted in the extant multi-view graph clustering approaches. On top of that, DGF includes a tri-cross contrastive training strategy that employs instance-level contrastive learning across modalities, neighborhoods, and communities for learning robust and discriminative node representations. Our comprehensive experiments on eight benchmark MMAG datasets exhibit that DGF is able to outperform a wide range of state-of-the-art baselines consistently and significantly in terms of clustering quality measured against ground-truth labels.

</details>


### [44] [RED-F: Reconstruction-Elimination based Dual-stream Contrastive Forecasting for Multivariate Time Series Anomaly Prediction](https://arxiv.org/abs/2511.20044)
*PengYu Chen,Xiaohou Shi,Yuan Chang,Yan Sun,Sajal K. Das*

Main category: cs.LG

TL;DR: 本文提出RED-F框架，通过重构消除和双流对比预测来解决多元时间序列中的异常预测问题，能够放大微弱异常前兆信号。


<details>
  <summary>Details</summary>
Motivation: 现有无监督方法在预测时倾向于重建正常模式，导致微弱异常前兆信号被淹没，无法有效预测异常。

Method: 提出RED-F框架，包含重构消除模型(REM)和双流对比预测模型(DFM)。REM通过混合时频机制消除前兆生成纯净基线，DFM通过对比两个预测流之间的差异来放大前兆信号，并使用多序列预测目标增强预测敏感性。

Result: 在六个真实世界数据集上的实验表明，RED-F在异常预测任务中表现出优越性能。

Conclusion: RED-F通过重构消除和对比预测机制有效解决了微弱异常前兆检测的挑战，显著提升了异常预测能力。

Abstract: The proactive prediction of anomalies (AP) in mul- tivariate time series (MTS) is a critical challenge to ensure system dependability. The difficulty lies in identifying subtle anomaly precursors concealed within normal signals. However, existing unsupervised methods, trained exclusively on normal data, demonstrate a fundamental propensity to reconstruct normal patterns. Consequently, when confronted with weak precursors, their predictions are dominated by the normal pattern, submerging the very signal required for prediction. To contend with the limitation, we propose RED-F, a Reconstruction- Elimination based Dual-stream Contrastive Forecasting frame- work, comprising the Reconstruction-Elimination Model (REM) and the Dual-stream Contrastive Forecasting Model (DFM). The REM utilizes a hybrid time-frequency mechanism to mitigate the precursor, generating a purified, normal-pattern baseline. The DFM then receives this purified baseline and the original sequence which retains the precursor as parallel inputs. At the core of our framework, RED-F employs a contrastive forecast that transforms the difficult task of absolute signal detection into a simpler, more robust task of relative trajectory comparison by computing the divergence between these two predictive streams. This contrastive mechanism serves to amplify the faint precursor signal. Furthermore, the DFM is trained with a novel Multi-Series Prediction (MSP) objective, which leverages distant future con- text to enhance its predictive sensitivity. Extensive experiments on six real-world datasets demonstrate the superior capability of RED-F in anomaly prediction tasks.

</details>


### [45] [SOMBRL: Scalable and Optimistic Model-Based RL](https://arxiv.org/abs/2511.20066)
*Bhavya Sukhija,Lenart Treven,Carmelo Sferrazza,Florian Dörfler,Pieter Abbeel,Andreas Krause*

Main category: cs.LG

TL;DR: 提出SOMBRL方法，基于不确定性乐观原则，通过结合外部奖励和认知不确定性进行探索，在模型强化学习中实现高效探索。


<details>
  <summary>Details</summary>
Motivation: 解决基于模型的强化学习中未知系统动态下的高效探索挑战，需要直接从在线交互中学习。

Method: 学习不确定性感知的动态模型，贪婪地最大化外部奖励和智能体认知不确定性的加权和，兼容任何策略优化器或规划器。

Result: 在非线性动态的有限时域、折扣无限时域和非情景设置中具有次线性遗憾，在状态和视觉控制环境中表现优异，在动态RC汽车硬件上超越现有技术。

Conclusion: SOMBRL为基于模型的强化学习提供了灵活、可扩展的原则性探索解决方案，展示了原则性探索在MBRL中的优势。

Abstract: We address the challenge of efficient exploration in model-based reinforcement learning (MBRL), where the system dynamics are unknown and the RL agent must learn directly from online interactions. We propose Scalable and Optimistic MBRL (SOMBRL), an approach based on the principle of optimism in the face of uncertainty. SOMBRL learns an uncertainty-aware dynamics model and greedily maximizes a weighted sum of the extrinsic reward and the agent's epistemic uncertainty. SOMBRL is compatible with any policy optimizers or planners, and under common regularity assumptions on the system, we show that SOMBRL has sublinear regret for nonlinear dynamics in the (i) finite-horizon, (ii) discounted infinite-horizon, and (iii) non-episodic settings. Additionally, SOMBRL offers a flexible and scalable solution for principled exploration. We evaluate SOMBRL on state-based and visual-control environments, where it displays strong performance across all tasks and baselines. We also evaluate SOMBRL on a dynamic RC car hardware and show SOMBRL outperforms the state-of-the-art, illustrating the benefits of principled exploration for MBRL.

</details>


### [46] [QiMeng-CRUX: Narrowing the Gap between Natural Language and Verilog via Core Refined Understanding eXpression](https://arxiv.org/abs/2511.20099)
*Lei Huang,Rui Zhang,Jiaming Guo,Yang Zhang,Di Huang,Shuyao Cheng,Pengwei Jin,Chongxiao Li,Zidong Du,Xing Hu,Qi Guo,Yunji Chen*

Main category: cs.LG

TL;DR: 该论文提出了CRUX-V模型，通过引入结构化中间表示空间CRUX来弥合自然语言描述与Verilog代码生成之间的语义鸿沟，采用两阶段训练框架提升生成质量，在多个基准测试中达到最先进性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于自然语言的硬件描述语言生成方法存在描述模糊、冗余和非结构化的问题，导致Verilog代码生成面临挑战。需要解决从开放自然语言空间到高度约束目标空间的复杂转换问题。

Method: 引入CRUX结构化中间表示空间来捕获用户意图的核心语义，设计包含联合表达建模和双空间优化的两阶段训练框架，提升CRUX和Verilog代码的生成质量。

Result: CRUX-V模型在多个Verilog生成基准测试中达到最先进性能，特别是在复杂设计任务下表现优异。CRUX表示空间具有可迁移性，可作为其他代码模型的输入提示。

Conclusion: CRUX结构化中间表示空间有效缩小了自由形式自然语言描述与精确Verilog生成之间的差距，证明了该方法在硬件代码生成任务中的有效性。

Abstract: Large language models (LLMs) have shown promising capabilities in hardware description language (HDL) generation. However, existing approaches often rely on free-form natural language descriptions that are often ambiguous, redundant, and unstructured, which poses significant challenges for downstream Verilog code generation. We treat hardware code generation as a complex transformation from an open-ended natural language space to a domain-specific, highly constrained target space. To bridge this gap, we introduce Core Refined Understanding eXpression (CRUX), a structured intermediate space that captures the essential semantics of user intent while organizing the expression for precise Verilog code generation. We further design a two-stage training framework, comprising Joint Expression Modeling and Dual-Space Optimization, to enhance the quality of both CRUX and Verilog code. Experiments across multiple Verilog generation benchmarks demonstrate that our model, CRUX-V, achieves state-of-the-art performance among general models, particularly under challenging design tasks. Furthermore, the CRUX space proves transferable and beneficial when used as input prompts for other code models, highlighting its effectiveness in narrowing the gap between free-form natural language descriptions and precise Verilog generation.

</details>


### [47] [Multivariate Forecasting of Bitcoin Volatility with Gradient Boosting: Deterministic, Probabilistic, and Feature Importance Perspectives](https://arxiv.org/abs/2511.20105)
*Grzegorz Dudek,Mateusz Kasprzyk,Paweł Pełka*

Main category: cs.LG

TL;DR: 本研究应用LightGBM模型对比特币实现波动率进行确定性和概率性预测，使用69个市场、行为和宏观经济指标作为预测因子，并比较了与计量经济学和机器学习基准模型的性能。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索LightGBM模型在捕捉加密货币市场非线性和高方差特性方面的有效性，同时提供对波动率动态的可解释性见解。

Method: 采用两种基于分位数的方法进行概率预测：使用pinball损失函数的直接分位数回归，以及将点预测转换为预测分布的残差模拟方法。使用增益和排列特征重要性技术识别主要波动率驱动因素。

Result: 结果发现交易量、滞后波动率指标、投资者关注度和市值是波动率的主要驱动因素。LightGBM模型在捕捉加密货币市场特性方面表现有效。

Conclusion: LightGBM模型能够有效捕捉加密货币市场的非线性和高方差特性，同时提供对潜在波动率动态的可解释性见解。

Abstract: This study investigates the application of the Light Gradient Boosting Machine (LGBM) model for both deterministic and probabilistic forecasting of Bitcoin realized volatility. Utilizing a comprehensive set of 69 predictors -- encompassing market, behavioral, and macroeconomic indicators -- we evaluate the performance of LGBM-based models and compare them with both econometric and machine learning baselines. For probabilistic forecasting, we explore two quantile-based approaches: direct quantile regression using the pinball loss function, and a residual simulation method that transforms point forecasts into predictive distributions. To identify the main drivers of volatility, we employ gain-based and permutation feature importance techniques, consistently highlighting the significance of trading volume, lagged volatility measures, investor attention, and market capitalization. The results demonstrate that LGBM models effectively capture the nonlinear and high-variance characteristics of cryptocurrency markets while providing interpretable insights into the underlying volatility dynamics.

</details>


### [48] [On the Limits of Momentum in Decentralized and Federated Optimization](https://arxiv.org/abs/2511.20168)
*Riccardo Zaccone,Sai Praneeth Karimireddy,Carlo Masone*

Main category: cs.LG

TL;DR: 本文分析了联邦学习中动量方法在循环客户端参与下的表现，理论上证明了动量仍然受到统计异质性的影响，且递减步长无法解决该问题。


<details>
  <summary>Details</summary>
Motivation: 探索动量在分布式SGD中的使用，特别是在联邦学习环境中，动量直观上似乎是缓解统计异质性影响的一种解决方案。

Method: 分析循环客户端参与下的动量方法，进行理论证明和数值实验验证。

Result: 动量在统计异质性下仍然受到影响，任何比Θ(1/t)更快递减的步长调度都会导致收敛到依赖于初始化和异质性界限的常数值。

Conclusion: 动量无法保证在无界异质性下的收敛，数值结果和深度学习实验证实了理论的相关性。

Abstract: Recent works have explored the use of momentum in local methods to enhance distributed SGD. This is particularly appealing in Federated Learning (FL), where momentum intuitively appears as a solution to mitigate the effects of statistical heterogeneity. Despite recent progress in this direction, it is still unclear if momentum can guarantee convergence under unbounded heterogeneity in decentralized scenarios, where only some workers participate at each round. In this work we analyze momentum under cyclic client participation, and theoretically prove that it remains inevitably affected by statistical heterogeneity. Similarly to SGD, we prove that decreasing step-sizes do not help either: in fact, any schedule decreasing faster than $Θ\left(1/t\right)$ leads to convergence to a constant value that depends on the initialization and the heterogeneity bound. Numerical results corroborate the theory, and deep learning experiments confirm its relevance for realistic settings.

</details>


### [49] [Interpretable Air Pollution Forecasting by Physics-Guided Spatiotemporal Decoupling](https://arxiv.org/abs/2511.20257)
*Zhiguo Zhang,Xiaoliang Ma,Daniel Schlesinger*

Main category: cs.LG

TL;DR: 提出了一种物理引导、可解释的时空学习框架，用于空气污染预测，在性能和可解释性之间取得平衡。


<details>
  <summary>Details</summary>
Motivation: 准确的空气污染预测对公共健康至关重要，但现有模型在性能和可解释性之间存在权衡。需要开发既能保持高性能又具有可解释性的模型。

Method: 将污染物浓度的时空行为分解为两个透明、可加的模块：物理引导的传输核（基于风和地理条件的定向权重）和可解释的注意力机制（学习局部响应并将未来浓度归因于特定历史滞后和外部驱动因素）。

Result: 在斯德哥尔摩地区的综合数据集上评估，该模型在多个预测时间范围内始终优于最先进的基线方法。

Conclusion: 该模型将高预测性能与时空可解释性相结合，为实际应用中的空气质量管理提供了更可靠的基础。

Abstract: Accurate and interpretable air pollution forecasting is crucial for public health, but most models face a trade-off between performance and interpretability. This study proposes a physics-guided, interpretable-by-design spatiotemporal learning framework. The model decomposes the spatiotemporal behavior of air pollutant concentrations into two transparent, additive modules. The first is a physics-guided transport kernel with directed weights conditioned on wind and geography (advection). The second is an explainable attention mechanism that learns local responses and attributes future concentrations to specific historical lags and exogenous drivers. Evaluated on a comprehensive dataset from the Stockholm region, our model consistently outperforms state-of-the-art baselines across multiple forecasting horizons. Our model's integration of high predictive performance and spatiotemporal interpretability provides a more reliable foundation for operational air-quality management in real-world applications.

</details>


### [50] [Beyond Components: Singular Vector-Based Interpretability of Transformer Circuits](https://arxiv.org/abs/2511.20273)
*Areeb Ahmad,Abhinav Joshi,Ashutosh Modi*

Main category: cs.LG

TL;DR: 本文提出了一种细粒度的视角，将Transformer中的注意力头和MLP层分解为正交奇异方向，揭示了单个组件内叠加的独立计算。在IOI、GP和GT等标准任务上验证发现，先前识别的功能头（如名称移动器）包含多个重叠的子功能，对应不同的奇异方向。


<details>
  <summary>Details</summary>
Motivation: 现有的机制可解释性方法通常将注意力头和MLP层视为不可分割的单元，忽略了它们内部可能学习到的功能子结构。本文旨在揭示Transformer组件内部更细粒度的计算结构。

Method: 引入基于奇异值分解的细粒度视角，将注意力头和MLP层分解为正交奇异方向，分析这些方向对应的计算功能。在IOI、GP和GT等标准任务上进行验证。

Result: 发现先前识别的典型功能头（如名称移动器）编码了多个重叠的子功能，这些子功能与不同的奇异方向对齐。计算图中的节点在特定的低秩方向上表现出强激活，表明有意义的计算存在于紧凑子空间中。

Conclusion: Transformer的计算比先前假设的更加分布式、结构化和组合化。这种细粒度视角为机制可解释性和模型内部理解开辟了新途径。

Abstract: Transformer-based language models exhibit complex and distributed behavior, yet their internal computations remain poorly understood. Existing mechanistic interpretability methods typically treat attention heads and multilayer perceptron layers (MLPs) (the building blocks of a transformer architecture) as indivisible units, overlooking possibilities of functional substructure learned within them. In this work, we introduce a more fine-grained perspective that decomposes these components into orthogonal singular directions, revealing superposed and independent computations within a single head or MLP. We validate our perspective on widely used standard tasks like Indirect Object Identification (IOI), Gender Pronoun (GP), and Greater Than (GT), showing that previously identified canonical functional heads, such as the name mover, encode multiple overlapping subfunctions aligned with distinct singular directions. Nodes in a computational graph, that are previously identified as circuit elements show strong activation along specific low-rank directions, suggesting that meaningful computations reside in compact subspaces. While some directions remain challenging to interpret fully, our results highlight that transformer computations are more distributed, structured, and compositional than previously assumed. This perspective opens new avenues for fine-grained mechanistic interpretability and a deeper understanding of model internals.

</details>


### [51] [Learning Subgroups with Maximum Treatment Effects without Causal Heuristics](https://arxiv.org/abs/2511.20189)
*Lincen Yang,Zhong Li,Matthijs van Leeuwen,Saber Salehkaleybar*

Main category: cs.LG

TL;DR: 本文提出在结构因果模型框架下直接研究最优亚组发现问题，证明在基于分区的模型假设下，最优亚组发现可简化为标准监督学习问题，无需使用因果启发式方法。


<details>
  <summary>Details</summary>
Motivation: 现有亚组发现方法存在两个主要问题：一是需要准确估计点状条件处理效应，二是使用缺乏严格理论依据的因果启发式方法。本文旨在直接在结构因果模型框架下解决这些问题。

Method: 在基于分区的模型假设下，将最优亚组发现问题转化为数据生成模型恢复问题，从而转化为标准回归或分类问题。使用CART方法实例化该框架来学习具有最大处理效应的亚组。

Result: 在大量合成和半合成数据集上的实验表明，该方法比多种基线方法更准确地识别出具有最大处理效应的亚组。

Conclusion: 直接在结构因果模型框架下研究亚组发现问题可以避免因果启发式方法，将问题转化为标准监督学习任务，从而更准确地识别最优亚组。

Abstract: Discovering subgroups with the maximum average treatment effect is crucial for targeted decision making in domains such as precision medicine, public policy, and education. While most prior work is formulated in the potential outcome framework, the corresponding structural causal model (SCM) for this task has been largely overlooked. In practice, two approaches dominate. The first estimates pointwise conditional treatment effects and then fits a tree on those estimates, effectively turning subgroup estimation into the harder problem of accurate pointwise estimation. The second constructs decision trees or rule sets with ad-hoc 'causal' heuristics, typically without rigorous justification for why a given heuristic may be used or whether such heuristics are necessary at all. We address these issues by studying the problem directly under the SCM framework. Under the assumption of a partition-based model, we show that optimal subgroup discovery reduces to recovering the data-generating models and hence a standard supervised learning problem (regression or classification). This allows us to adopt any partition-based methods to learn the subgroup from data. We instantiate the approach with CART, arguably one of the most widely used tree-based methods, to learn the subgroup with maximum treatment effect. Finally, on a large collection of synthetic and semi-synthetic datasets, we compare our method against a wide range of baselines and find that our approach, which avoids such causal heuristics, more accurately identifies subgroups with maximum treatment effect. Our source code is available at https://github.com/ylincen/causal-subgroup.

</details>


### [52] [HVAdam: A Full-Dimension Adaptive Optimizer](https://arxiv.org/abs/2511.20277)
*Yiheng Zhang,Shaowu Wu,Yuanzhuo Xu,Jiajun Wu,Shang Xu,Steve Drew,Xiaoguang Niu*

Main category: cs.LG

TL;DR: 本文提出了Anon优化器，通过可调节的自适应机制在SGD和Adam之间插值甚至超越两者，解决了自适应优化器泛化性能较差的问题。


<details>
  <summary>Details</summary>
Motivation: 自适应优化器如Adam在训练大规模模型时表现优异，但在经典架构如CNN上泛化性能不如非自适应方法如SGD。本文旨在解决这一性能差距。

Method: 提出Anon优化器，具有连续可调的自适应性；引入增量延迟更新(IDU)机制，比AMSGrad的硬最大跟踪策略更灵活，增强对梯度噪声的鲁棒性。

Result: 理论上在凸和非凸设置下建立了收敛保证；实证上在图像分类、扩散模型和语言建模任务上持续优于最先进的优化器。

Conclusion: 自适应性可以作为有价值的可调设计原则，Anon提供了第一个统一可靠的框架，能够弥合经典和现代优化器之间的差距并超越它们的优势特性。

Abstract: Adaptive optimizers such as Adam have achieved great success in training large-scale models like large language models and diffusion models. However, they often generalize worse than non-adaptive methods, such as SGD on classical architectures like CNNs. We identify a key cause of this performance gap: adaptivity in pre-conditioners, which limits the optimizer's ability to adapt to diverse optimization landscapes. To address this, we propose Anon (Adaptivity Non-restricted Optimizer with Novel convergence technique), a novel optimizer with continuously tunable adaptivity
  , allowing it to interpolate between SGD-like and Adam-like behaviors and even extrapolate beyond both. To ensure convergence across the entire adaptivity spectrum, we introduce incremental delay update (IDU), a novel mechanism that is more flexible than AMSGrad's hard max-tracking strategy and enhances robustness to gradient noise. We theoretically establish convergence guarantees under both convex and non-convex settings. Empirically, Anon consistently outperforms state-of-the-art optimizers on representative image classification, diffusion, and language modeling tasks. These results demonstrate that adaptivity can serve as a valuable tunable design principle, and Anon provides the first unified and reliable framework capable of bridging the gap between classical and modern optimizers and surpassing their advantageous properties.

</details>


### [53] [In-Context Compositional Learning via Sparse Coding Transformer](https://arxiv.org/abs/2511.20194)
*Wei Chen,Jingxi Yu,Zichen Miao,Qiang Qiu*

Main category: cs.LG

TL;DR: 本文提出了一种基于稀疏编码原理的注意力机制改进方法，旨在增强Transformer在组合学习任务中的能力。通过将注意力块重新解释为输入到输出的映射，使用编码和解码字典，并对系数施加稀疏性约束，从而更好地捕捉组合结构。


<details>
  <summary>Details</summary>
Motivation: Transformer架构在语言、视觉和多模态任务中取得了显著成功，但在处理组合学习任务时仍面临挑战。这些任务需要模型从上下文示例中推断组合规则，而Transformer并非天生设计用于处理组合任务，且结构归纳偏差有限。

Method: 将注意力块重新解释为通过投影到两个学习字典原子（编码字典和解码字典）的输入到输出映射。编码字典将输入分解为一组系数，表示输入的组合结构，并对这些系数施加稀疏性约束。稀疏系数用于线性组合解码字典原子以生成输出。

Result: 在S-RAVEN和RAVEN数据集上验证了方法的有效性。对于某些组合泛化任务，该方法在标准Transformer失败时仍能保持性能，显示了其学习和应用组合规则的能力。

Conclusion: 提出的基于稀疏编码的注意力改进方法能够有效增强Transformer在组合学习任务中的能力，特别是在组合泛化任务中表现出色，为处理组合任务提供了新的思路。

Abstract: Transformer architectures have achieved remarkable success across language, vision, and multimodal tasks, and there is growing demand for them to address in-context compositional learning tasks. In these tasks, models solve the target problems by inferring compositional rules from context examples, which are composed of basic components structured by underlying rules. However, some of these tasks remain challenging for Transformers, which are not inherently designed to handle compositional tasks and offer limited structural inductive bias. In this work, inspired by the principle of sparse coding, we propose a reformulation of the attention to enhance its capability for compositional tasks. In sparse coding, data are represented as sparse combinations of dictionary atoms with coefficients that capture their compositional rules. Specifically, we reinterpret the attention block as a mapping of inputs into outputs through projections onto two sets of learned dictionary atoms: an encoding dictionary and a decoding dictionary. The encoding dictionary decomposes the input into a set of coefficients, which represent the compositional structure of the input. To enhance structured representations, we impose sparsity on these coefficients. The sparse coefficients are then used to linearly combine the decoding dictionary atoms to generate the output. Furthermore, to assist compositional generalization tasks, we propose estimating the coefficients of the target problem as a linear combination of the coefficients obtained from the context examples. We demonstrate the effectiveness of our approach on the S-RAVEN and RAVEN datasets. For certain compositional generalization tasks, our method maintains performance even when standard Transformers fail, owing to its ability to learn and apply compositional rules.

</details>


### [54] [Geometry of Decision Making in Language Models](https://arxiv.org/abs/2511.20315)
*Abhinav Joshi,Divyanshu Bhatt,Ashutosh Modi*

Main category: cs.LG

TL;DR: 该研究通过内在维度分析大型语言模型在多项选择题任务中的决策过程，发现模型在早期层使用低维流形，中间层扩展维度空间，后期层压缩并收敛到与决策相关的表示。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在各种任务中表现出强大的泛化能力，但其内部决策过程仍然不透明，研究者希望通过几何视角分析隐藏表示来理解模型的决策动态。

Method: 对28个开源transformer模型进行大规模研究，使用多个估计器估计各层的内在维度，并量化每层在多项选择题任务上的性能。

Result: 发现模型间存在一致的内在维度模式：早期层在低维流形上操作，中间层扩展空间，后期层压缩空间并收敛到决策相关表示。

Conclusion: 结果表明LLMs隐式地将语言输入投影到结构化的低维流形上，这些流形与任务特定决策对齐，为理解语言模型中泛化和推理的出现提供了新的几何视角。

Abstract: Large Language Models (LLMs) show strong generalization across diverse tasks, yet the internal decision-making processes behind their predictions remain opaque. In this work, we study the geometry of hidden representations in LLMs through the lens of \textit{intrinsic dimension} (ID), focusing specifically on decision-making dynamics in a multiple-choice question answering (MCQA) setting. We perform a large-scale study, with 28 open-weight transformer models and estimate ID across layers using multiple estimators, while also quantifying per-layer performance on MCQA tasks. Our findings reveal a consistent ID pattern across models: early layers operate on low-dimensional manifolds, middle layers expand this space, and later layers compress it again, converging to decision-relevant representations. Together, these results suggest LLMs implicitly learn to project linguistic inputs onto structured, low-dimensional manifolds aligned with task-specific decisions, providing new geometric insights into how generalization and reasoning emerge in language models.

</details>


### [55] [Communication-Efficient Learning for Satellite Constellations](https://arxiv.org/abs/2511.20220)
*Ruxandra-Stefania Tudose,Moritz H. W. Grüss,Grace Ra Kim,Karl H. Johansson,Nicola Bastianello*

Main category: cs.LG

TL;DR: 提出了一种用于低地球轨道卫星星座的联邦学习算法，通过本地训练、压缩和误差反馈机制减少通信开销，同时保持模型准确性。


<details>
  <summary>Details</summary>
Motivation: 利用广泛分布的低地球轨道卫星星座解决学习问题，特别是在联邦学习框架下，卫星收集和处理数据，地面站聚合模型，旨在设计通信效率高且准确的训练算法。

Method: 采用多种机制减少与地面站的通信次数（本地训练）和通信量（压缩），并提出误差反馈机制提高准确性，该机制还可作为算法无关的误差反馈方案广泛应用。

Result: 通过理论分析算法的收敛性，并在真实空间场景下与现有技术进行仿真比较，展示了优越的性能。

Conclusion: 所提出的算法在减少通信开销的同时，能够保持较高的模型准确性，适用于卫星星座的联邦学习场景。

Abstract: Satellite constellations in low-Earth orbit are now widespread, enabling positioning, Earth imaging, and communications. In this paper we address the solution of learning problems using these satellite constellations. In particular, we focus on a federated approach, where satellites collect and locally process data, with the ground station aggregating local models. We focus on designing a novel, communication-efficient algorithm that still yields accurate trained models. To this end, we employ several mechanisms to reduce the number of communications with the ground station (local training) and their size (compression). We then propose an error feedback mechanism that enhances accuracy, which yields, as a byproduct, an algorithm-agnostic error feedback scheme that can be more broadly applied. We analyze the convergence of the resulting algorithm, and compare it with the state of the art through simulations in a realistic space scenario, showcasing superior performance.

</details>


### [56] [Soft Adaptive Policy Optimization](https://arxiv.org/abs/2511.20347)
*Chang Gao,Chujie Zheng,Xiong-Hui Chen,Kai Dang,Shixuan Liu,Bowen Yu,An Yang,Shuai Bai,Jingren Zhou,Junyang Lin*

Main category: cs.LG

TL;DR: SAPO是一种软自适应策略优化方法，通过温度控制的门机制替代硬截断，在保持序列级一致性的同时自适应地衰减离策略更新，提高了LLM强化学习训练的稳定性和性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于组的策略优化方法（如GSPO和GRPO）使用硬截断来缓解重要性比率的高方差问题，但难以同时保持稳定性和有效学习。特别是在MoE模型中，token级重要性比率的高方差会导致不稳定更新。

Method: 提出Soft Adaptive Policy Optimization (SAPO)，用平滑的温度控制门替代硬截断，自适应地衰减离策略更新，同时保留有用的学习信号。SAPO既是序列一致的又是token自适应的，形成连续的信任区域。

Result: 在数学推理基准测试中，SAPO在可比较的训练预算下表现出更好的训练稳定性和更高的Pass@1性能。使用SAPO训练Qwen3-VL模型系列，在不同任务和模型大小上都获得了持续的性能提升。

Conclusion: SAPO为LLM的RL训练提供了更可靠、可扩展和有效的优化策略，通过软门控机制在保持稳定性的同时提高了样本效率。

Abstract: Reinforcement learning (RL) plays an increasingly important role in enhancing the reasoning capabilities of large language models (LLMs), yet stable and performant policy optimization remains challenging. Token-level importance ratios often exhibit high variance-a phenomenon exacerbated in Mixture-of-Experts models-leading to unstable updates. Existing group-based policy optimization methods, such as GSPO and GRPO, alleviate this problem via hard clipping, making it difficult to maintain both stability and effective learning. We propose Soft Adaptive Policy Optimization (SAPO), which replaces hard clipping with a smooth, temperature-controlled gate that adaptively attenuates off-policy updates while preserving useful learning signals. Compared with GSPO and GRPO, SAPO is both sequence-coherent and token-adaptive. Like GSPO, SAPO maintains sequence-level coherence, but its soft gating forms a continuous trust region that avoids the brittle hard clipping band used in GSPO. When a sequence contains a few highly off-policy tokens, GSPO suppresses all gradients for that sequence, whereas SAPO selectively down-weights only the offending tokens and preserves the learning signal from the near-on-policy ones, improving sample efficiency. Relative to GRPO, SAPO replaces hard token-level clipping with smooth, temperature-controlled scaling, enabling more informative and stable updates. Empirical results on mathematical reasoning benchmarks indicate that SAPO exhibits improved training stability and higher Pass@1 performance under comparable training budgets. Moreover, we employ SAPO to train the Qwen3-VL model series, demonstrating that SAPO yields consistent performance gains across diverse tasks and different model sizes. Overall, SAPO provides a more reliable, scalable, and effective optimization strategy for RL training of LLMs.

</details>


### [57] [Decoupling and Damping: Structurally-Regularized Gradient Matching for Multimodal Graph Condensation](https://arxiv.org/abs/2511.20222)
*Lian Shen,Zhendan Chen,Yinhui jiang,Meijia Song,Ziming Su,Juan Liu,Xiangrong Liu*

Main category: cs.LG

TL;DR: 提出了SR-GM框架，通过梯度解耦和结构阻尼正则化解决多模态图压缩中的梯度冲突和结构噪声放大问题，显著提升压缩效率和模型性能。


<details>
  <summary>Details</summary>
Motivation: 多模态图在关键Web应用中日益重要，但大规模图训练计算负担重。现有图压缩方法在多模态设置下失效，主要由于模态间语义不对齐导致的梯度冲突，以及GNN消息传递架构对梯度噪声的放大效应。

Method: SR-GM框架包含两个协同组件：1）梯度解耦机制，通过正交投影解决模态间梯度冲突；2）结构阻尼正则化器，利用图的Dirichlet能量将拓扑从噪声放大器转变为优化过程中的稳定力量。

Result: 实验表明SR-GM相比基线方法显著提高精度并加速收敛。消融研究证实同时解决梯度冲突和结构放大对实现优越性能至关重要。压缩后的多模态图展现出强大的跨架构泛化能力。

Conclusion: 该研究为资源受限环境下的多模态图学习提供了可扩展的方法论，有望加速神经架构搜索等应用。

Abstract: In critical web applications such as e-commerce and recommendation systems, multimodal graphs integrating rich visual and textual attributes are increasingly central, yet their large scale introduces substantial computational burdens for training Graph Neural Networks (GNNs). While Graph Condensation (GC) offers a promising solution by synthesizing smaller datasets, existing methods falter in the multimodal setting. We identify a dual challenge causing this failure: (1) conflicting gradients arising from semantic misalignments between modalities, and (2) the GNN's message-passing architecture pathologically amplifying this gradient noise across the graph structure. To address this, we propose Structurally-Regularized Gradient Matching (SR-GM), a novel condensation framework tailored for multimodal graphs. SR-GM introduces two synergistic components: first, a gradient decoupling mechanism that resolves inter-modality conflicts at their source via orthogonal projection; and second, a structural damping regularizer that acts directly on the gradient field. By leveraging the graph's Dirichlet energy, this regularizer transforms the topology from a noise amplifier into a stabilizing force during optimization. Extensive experiments demonstrate that SR-GM significantly improves accuracy and accelerates convergence compared to baseline methods. Ablation studies confirm that addressing both gradient conflict and structural amplification in tandem is essential for achieving superior performance. Moreover, the condensed multimodal graphs exhibit strong cross-architecture generalization and promise to accelerate applications like Neural Architecture Search. This research provides a scalable methodology for multimodal graph-based learning in resource-constrained environments.

</details>


### [58] [Short-Range Oversquashing](https://arxiv.org/abs/2511.20406)
*Yaaqov Mishayev,Yonatan Sverdlov,Tal Amir,Nadav Dym*

Main category: cs.LG

TL;DR: 本文揭示了过压缩现象不仅存在于长距离任务中，也出现在短距离问题中，区分了瓶颈现象和梯度消失两种机制，并表明图变换器比专门的MPNNs更能有效解决过压缩问题。


<details>
  <summary>Details</summary>
Motivation: MPNNs在图上学习时存在过压缩问题，传统认为这主要影响长距离信息处理，但本文发现过压缩也出现在短距离任务中，需要重新理解其根本机制。

Method: 通过分析短距离任务中的过压缩现象，区分瓶颈现象和梯度消失两种机制，并与虚拟节点等MPNN改进方法以及图变换器进行对比。

Result: 发现短距离瓶颈效应无法被现有过压缩解释所捕捉，虚拟节点无法解决此问题，而变换器在此类任务中表现成功。

Conclusion: 图变换器相比专门的MPNNs是解决过压缩问题更有效的方案，特别是在处理短距离瓶颈效应方面具有优势。

Abstract: Message Passing Neural Networks (MPNNs) are widely used for learning on graphs, but their ability to process long-range information is limited by the phenomenon of oversquashing. This limitation has led some researchers to advocate Graph Transformers as a better alternative, whereas others suggest that it can be mitigated within the MPNN framework, using virtual nodes or other rewiring techniques.
  In this work, we demonstrate that oversquashing is not limited to long-range tasks, but can also arise in short-range problems. This observation allows us to disentangle two distinct mechanisms underlying oversquashing: (1) the bottleneck phenomenon, which can arise even in low-range settings, and (2) the vanishing gradient phenomenon, which is closely associated with long-range tasks.
  We further show that the short-range bottleneck effect is not captured by existing explanations for oversquashing, and that adding virtual nodes does not resolve it. In contrast, transformers do succeed in such tasks, positioning them as the more compelling solution to oversquashing, compared to specialized MPNNs.

</details>


### [59] [MTBBench: A Multimodal Sequential Clinical Decision-Making Benchmark in Oncology](https://arxiv.org/abs/2511.20490)
*Kiril Vasilev,Alexandre Misrahi,Eeshaan Jain,Phil F Cheng,Petros Liakopoulos,Olivier Michielin,Michael Moor,Charlotte Bunne*

Main category: cs.LG

TL;DR: MTBBench是一个模拟分子肿瘤委员会(MTB)决策的基准测试，针对多模态大语言模型在生物医学推理中的局限性，提供临床相关、多模态、纵向的肿瘤学问题评估框架。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试无法捕捉真实临床工作流程的复杂性，特别是多智能体决策环境如MTB，缺乏纵向和多模态的复杂性。

Method: 开发MTBBench基准测试，模拟MTB风格的决策过程，包含临床挑战性、多模态和纵向的肿瘤学问题，并通过临床医生验证的真实标注。

Result: 测试显示现有LLMs在可靠性方面存在严重问题，经常产生幻觉，难以处理时间分辨数据，无法调和冲突证据或不同模态。

Conclusion: MTBBench提供了一个具有挑战性的测试平台，通过基于基础模型的工具增强多模态和纵向推理，在任务级性能上分别提升9.0%和11.2%。

Abstract: Multimodal Large Language Models (LLMs) hold promise for biomedical reasoning, but current benchmarks fail to capture the complexity of real-world clinical workflows. Existing evaluations primarily assess unimodal, decontextualized question-answering, overlooking multi-agent decision-making environments such as Molecular Tumor Boards (MTBs). MTBs bring together diverse experts in oncology, where diagnostic and prognostic tasks require integrating heterogeneous data and evolving insights over time. Current benchmarks lack this longitudinal and multimodal complexity. We introduce MTBBench, an agentic benchmark simulating MTB-style decision-making through clinically challenging, multimodal, and longitudinal oncology questions. Ground truth annotations are validated by clinicians via a co-developed app, ensuring clinical relevance. We benchmark multiple open and closed-source LLMs and show that, even at scale, they lack reliability -- frequently hallucinating, struggling with reasoning from time-resolved data, and failing to reconcile conflicting evidence or different modalities. To address these limitations, MTBBench goes beyond benchmarking by providing an agentic framework with foundation model-based tools that enhance multi-modal and longitudinal reasoning, leading to task-level performance gains of up to 9.0% and 11.2%, respectively. Overall, MTBBench offers a challenging and realistic testbed for advancing multimodal LLM reasoning, reliability, and tool-use with a focus on MTB environments in precision oncology.

</details>


### [60] [The Driver-Blindness Phenomenon: Why Deep Sequence Models Default to Autocorrelation in Blood Glucose Forecasting](https://arxiv.org/abs/2511.20601)
*Heman Shakeri*

Main category: cs.LG

TL;DR: 该论文指出深度序列模型在血糖预测中未能有效利用胰岛素、饮食和活动等临床信息驱动因素，将这种现象称为'驱动因素盲视'，并通过Δ_drivers指标量化多变量模型相比单变量基线的性能增益。


<details>
  <summary>Details</summary>
Motivation: 尽管血糖生理机制已被充分理解，但现有深度序列模型未能有效利用胰岛素、饮食和活动等临床驱动因素进行血糖预测，导致模型性能受限。

Method: 提出Δ_drivers指标来量化多变量模型相比单变量基线的性能增益，分析导致驱动因素盲视的三个因素：架构偏置（C1）、数据保真度差距（C2）和生理异质性（C3），并综合了生理特征编码器、因果正则化和个性化等缓解策略。

Result: 研究发现文献中Δ_drivers通常接近零，表明多变量模型相比单变量基线几乎没有性能提升，验证了驱动因素盲视现象的普遍存在。

Conclusion: 建议未来研究应常规报告Δ_drivers指标，以防止驱动因素盲视的模型被误认为是先进技术，并推荐采用生理特征编码器、因果正则化和个性化等策略来缓解该问题。

Abstract: Deep sequence models for blood glucose forecasting consistently fail to leverage clinically informative drivers--insulin, meals, and activity--despite well-understood physiological mechanisms. We term this Driver-Blindness and formalize it via $Δ_{\text{drivers}}$, the performance gain of multivariate models over matched univariate baselines. Across the literature, $Δ_{\text{drivers}}$ is typically near zero. We attribute this to three interacting factors: architectural biases favoring autocorrelation (C1), data fidelity gaps that render drivers noisy and confounded (C2), and physiological heterogeneity that undermines population-level models (C3). We synthesize strategies that partially mitigate Driver-Blindness--including physiological feature encoders, causal regularization, and personalization--and recommend that future work routinely report $Δ_{\text{drivers}}$ to prevent driver-blind models from being considered state-of-the-art.

</details>


### [61] [MXtalTools: A Toolkit for Machine Learning on Molecular Crystals](https://arxiv.org/abs/2511.20327)
*Michael Kilgour,Mark E. Tuckerman,Jutta Rogal*

Main category: cs.LG

TL;DR: MXtalTools是一个灵活的Python软件包，用于分子晶体的数据驱动建模，支持机器学习研究分子固态。


<details>
  <summary>Details</summary>
Motivation: 为分子固态的机器学习研究提供灵活的数据驱动建模工具，促进分子晶体的高效建模和分析。

Method: 开发包含多个功能模块的Python软件包：数据集合成与整理、模型训练与推理集成工作流、晶体参数化与表示、晶体结构采样与优化、端到端可微分晶体采样构建与分析。

Result: 实现了模块化的分子晶体建模工具，支持CUDA加速的高通量晶体建模，代码开源并提供详细文档。

Conclusion: MXtalTools为分子晶体建模提供了灵活、高效的解决方案，其模块化设计便于集成到现有工作流或构建新的建模流程。

Abstract: We present MXtalTools, a flexible Python package for the data-driven modelling of molecular crystals, facilitating machine learning studies of the molecular solid state. MXtalTools comprises several classes of utilities: (1) synthesis, collation, and curation of molecule and crystal datasets, (2) integrated workflows for model training and inference, (3) crystal parameterization and representation, (4) crystal structure sampling and optimization, (5) end-to-end differentiable crystal sampling, construction and analysis. Our modular functions can be integrated into existing workflows or combined and used to build novel modelling pipelines. MXtalTools leverages CUDA acceleration to enable high-throughput crystal modelling. The Python code is available open-source on our GitHub page, with detailed documentation on ReadTheDocs.

</details>


### [62] [DiFR: Inference Verification Despite Nondeterminism](https://arxiv.org/abs/2511.20621)
*Adam Karvonen,Daniel Reuter,Roy Rinberg,Luke Marks,Adrià Garriga-Alonso,Keri Warr*

Main category: cs.LG

TL;DR: Token-DiFR是一种通过比较生成令牌与可信参考实现预测来验证LLM推理输出的方法，使用采样种子同步约束有效输出，能可靠识别采样错误、模拟错误和模型量化。


<details>
  <summary>Details</summary>
Motivation: 随着LLM推理需求增长，需要验证推理过程是否正确执行且无错误或篡改，但由于良性数值噪声导致重复运行相同推理过程产生不同结果，难以区分合法变化与实际问题。

Method: Token-DiFR通过比较生成令牌与可信参考实现预测来验证输出，使用采样种子同步约束有效输出；Activation-DiFR使用随机正交投影将激活压缩为紧凑指纹进行验证。

Result: Token-DiFR在300个输出令牌内检测4位量化的AUC > 0.999；Activation-DiFR仅用2个输出令牌检测4位量化的AUC > 0.999，通信开销比现有方法减少25-75%。

Conclusion: 提出的方法能有效验证LLM推理正确性，Token-DiFR以零额外成本提供可审计证据，Activation-DiFR实现高效前向验证，已开源集成vLLM加速可验证推理部署。

Abstract: As demand for LLM inference grows, it is becoming increasingly important that providers and their customers can verify that inference processes are performed correctly, without errors or tampering. However, re-running the same inference process twice often leads to different results due to benign numerical noise, making it difficult to distinguish legitimate variation from actual problems. To address this problem, we introduce Token-DiFR (Token-Divergence-From-Reference), a method for verifying inference outputs by comparing generated tokens against predictions made by a trusted reference implementation conditioned on the same random seed. Sampling seed synchronization tightly constrains valid outputs, leaving providers minimal room to deviate from correct inference, which allows output tokens themselves to serve as auditable evidence of correctness at zero additional cost to the provider. Token-DiFR reliably identifies sampling errors, simulated bugs, and model quantization, detecting 4-bit quantization with AUC $>$ 0.999 within 300 output tokens. For applications requiring sample-efficient forward-pass verification, we additionally introduce Activation-DiFR, a scheme that uses random orthogonal projections to compress activations into compact fingerprints for subsequent verification. Activation-DiFR detects 4-bit quantization with AUC $>$ 0.999 using just 2 output tokens, while reducing communication overhead by 25-75% relative to existing methods. We release an open-source integration with vLLM to accelerate practical deployment of verifiable inference.

</details>


### [63] [ROOT: Robust Orthogonalized Optimizer for Neural Network Training](https://arxiv.org/abs/2511.20626)
*Wei He,Kai Han,Hang Zhou,Hanting Chen,Zhicheng Liu,Xinghao Chen,Yunhe Wang*

Main category: cs.LG

TL;DR: 本文提出了ROOT优化器，通过双重鲁棒机制解决大语言模型优化中的正交化精度和异常值噪声问题，在噪声和非凸场景下实现了更快的收敛和更好的性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型优化面临算法精度敏感性和训练不稳定性挑战，现有基于动量正交化的优化器存在维度脆弱性和异常值噪声脆弱性两个关键鲁棒性限制。

Method: 提出ROOT优化器：1）使用自适应牛顿迭代和细粒度系数的维度鲁棒正交化方案；2）通过近端优化的优化鲁棒框架抑制异常值噪声。

Result: 大量实验表明，ROOT相比Muon和Adam优化器显著提高了鲁棒性，实现了更快的收敛速度和更优的最终性能，特别是在噪声和非凸场景下。

Conclusion: ROOT为开发能够处理现代大规模模型训练复杂性的鲁棒精确优化器建立了新范式。

Abstract: The optimization of large language models (LLMs) remains a critical challenge, particularly as model scaling exacerbates sensitivity to algorithmic imprecision and training instability. Recent advances in optimizers have improved convergence efficiency through momentum orthogonalization, but suffer from two key robustness limitations: dimensional fragility in orthogonalization precision and vulnerability to outlier-induced noise. To address these robustness challenges, we introduce ROOT, a Robust Orthogonalized Optimizer that enhances training stability through dual robustness mechanisms. First, we develop a dimension-robust orthogonalization scheme using adaptive Newton iterations with fine-grained coefficients tailored to specific matrix sizes, ensuring consistent precision across diverse architectural configurations. Second, we introduce an optimization-robust framework via proximal optimization that suppresses outlier noise while preserving meaningful gradient directions. Extensive experiments demonstrate that ROOT achieves significantly improved robustness, with faster convergence and superior final performance compared to both Muon and Adam-based optimizers, particularly in noisy and non-convex scenarios. Our work establishes a new paradigm for developing robust and precise optimizers capable of handling the complexities of modern large-scale model training. The code will be available at https://github.com/huawei-noah/noah-research/tree/master/ROOT.

</details>


### [64] [MoRE: Batch-Robust Multi-Omics Representations from Frozen Pre-trained Transformers](https://arxiv.org/abs/2511.20382)
*Audrey Pei-Hsuan Chen*

Main category: cs.LG

TL;DR: MoRE是一个多组学表示学习框架，通过复用预训练的transformer模型，使用参数高效微调策略将异质组学数据对齐到共享潜在空间中，在保持生物学结构的同时显著减少可训练参数。


<details>
  <summary>Details</summary>
Motivation: 多组学数据存在极端维度、模态异质性和批次效应等挑战，而预训练transformer在生物序列建模中显示出广泛泛化能力，但在多组学整合中应用不足。

Method: MoRE在冻结的预训练transformer骨干上附加轻量级的模态特定适配器和任务自适应融合层，联合优化掩码建模目标与监督对比学习和批次不变对齐损失。

Result: 与scGPT、scVI、Harmony等基线相比，MoRE在批次鲁棒性和生物学保守性方面表现竞争性，同时显著减少可训练参数。

Conclusion: MoRE是迈向通用组学基础模型的实用步骤，能够生成跨未见细胞类型和平台泛化的结构保持嵌入。

Abstract: Representation learning on multi-omics data is challenging due to extreme dimensionality, modality heterogeneity, and cohort-specific batch effects. While pre-trained transformer backbones have shown broad generalization capabilities in biological sequence modeling, their application to multi-omics integration remains underexplored. We present MoRE (Multi-Omics Representation Embedding), a framework that repurposes frozen pre-trained transformers to align heterogeneous assays into a shared latent space. Unlike purely generative approaches, MoRE employs a parameter-efficient fine-tuning (PEFT) strategy, prioritizing cross-sample and cross-modality alignment over simple sequence reconstruction. Specifically, MoRE attaches lightweight, modality-specific adapters and a task-adaptive fusion layer to the frozen backbone. It optimizes a masked modeling objective jointly with supervised contrastive and batch-invariant alignment losses, yielding structure-preserving embeddings that generalize across unseen cell types and platforms. We benchmark MoRE against established baselines, including scGPT, scVI, and Harmony with scArches, evaluating integration fidelity, rare population detection, and modality transfer. Our results demonstrate that MoRE achieves competitive batch robustness and biological conservation while significantly reducing trainable parameters compared to fully fine-tuned models. This work positions MoRE as a practical step toward general-purpose omics foundation models.

</details>


### [65] [Identifying environmental factors associated with tetrodotoxin contamination in bivalve mollusks using eXplainable AI](https://arxiv.org/abs/2511.20395)
*M. C. Schoppema,B. H. M. van der Velden,A. Hürriyetoğlu,M. D. Klijnstra,E. J. Faassen,A. Gerssen,H. J. van der Fels-Klerx*

Main category: cs.LG

TL;DR: 开发了一种可解释的深度学习模型，用于预测荷兰Zeeland河口的河豚毒素污染，识别出日照时间、全球辐射、水温和氯化物浓度是主要影响因素。


<details>
  <summary>Details</summary>
Motivation: 自2012年以来，欧洲温带水域的双壳类软体动物中发现河豚毒素，导致食品安全风险和经济损失，需要早期预测河豚毒素污染。

Method: 使用气象和水文特征作为输入，开发了基于深度学习的可解释模型来预测河豚毒素的存在或缺失。

Result: 结果显示日出时间、日落时间、全球辐射、水温和氯化物浓度对河豚毒素污染贡献最大，日照时间是重要驱动因素。

Conclusion: 该可解释深度学习模型识别出的环境因素（日照小时数、全球辐射、水温和水氯化物浓度）与河豚毒素污染相关，为食品行业和主管部门提供了有价值的风险缓解工具。

Abstract: Since 2012, tetrodotoxin (TTX) has been found in seafoods such as bivalve mollusks in temperate European waters. TTX contamination leads to food safety risks and economic losses, making early prediction of TTX contamination vital to the food industry and competent authorities. Recent studies have pointed to shallow habitats and water temperature as main drivers to TTX contamination in bivalve mollusks. However, the temporal relationships between abiotic factors, biotic factors, and TTX contamination remain unexplored.
  We have developed an explainable, deep learning-based model to predict TTX contamination in the Dutch Zeeland estuary. Inputs for the model were meteorological and hydrological features; output was the presence or absence of TTX contamination.
  Results showed that the time of sunrise, time of sunset, global radiation, water temperature, and chloride concentration contributed most to TTX contamination. Thus, the effective number of sun hours, represented by day length and global radiation, was an important driver for tetrodotoxin contamination in bivalve mollusks.
  To conclude, our explainable deep learning model identified the aforementioned environmental factors (number of sun hours, global radiation, water temperature, and water chloride concentration) to be associated with tetrodotoxin contamination in bivalve mollusks; making our approach a valuable tool to mitigate marine toxin risks for food industry and competent authorities.

</details>


### [66] [Tight Margin-Based Generalization Bounds for Voting Classifiers over Finite Hypothesis Sets](https://arxiv.org/abs/2511.20407)
*Kasper Green Larsen,Natascha Schalburg*

Main category: cs.LG

TL;DR: 本文证明了第一个基于边界的投票分类器泛化边界，该边界在假设集大小、边界、具有给定边界的训练点比例、训练样本数量和失败概率之间的权衡方面是渐近紧致的。


<details>
  <summary>Details</summary>
Motivation: 为投票分类器建立更精确的泛化边界，填补现有理论在边界分析方面的不足。

Method: 通过数学证明方法，推导出基于边界的泛化边界理论结果。

Result: 成功证明了第一个渐近紧致的投票分类器泛化边界。

Conclusion: 该结果为投票分类器的泛化性能提供了更精确的理论保证，在多个关键参数之间实现了最优权衡。

Abstract: We prove the first margin-based generalization bound for voting classifiers, that is asymptotically tight in the tradeoff between the size of the hypothesis set, the margin, the fraction of training points with the given margin, the number of training samples and the failure probability.

</details>


### [67] [Diffusion for Fusion: Designing Stellarators with Generative AI](https://arxiv.org/abs/2511.20445)
*Misha Padidar,Teresa Huang,Andrew Giuliani,Marina Spivak*

Main category: cs.LG

TL;DR: 本文提出使用条件扩散模型快速生成具有理想特性的准对称仿星器设计，作为机器学习在仿星器设计中的案例研究。


<details>
  <summary>Details</summary>
Motivation: 仿星器设计通常需要耗费大量计算时间，而机器学习方法可以利用大型优化仿星器数据集来加速设计过程。

Method: 在QUASR数据库上训练条件扩散模型，生成具有特定特性（纵横比和平均旋转变换）的准对称仿星器设计。

Result: 生成的仿星器表现出良好性能：准对称性偏差小于5%，且能设计出训练中未见特性的仿星器。

Conclusion: 扩散模型在仿星器设计中具有潜力，为达到低于1%的准对称性目标提供了机会，并展示了生成建模在推进仿星器设计中的多个有前景方向。

Abstract: Stellarators are a prospective class of fusion-based power plants that confine a hot plasma with three-dimensional magnetic fields. Typically framed as a PDE-constrained optimization problem, stellarator design is a time-consuming process that can take hours to solve on a computing cluster. Developing fast methods for designing stellarators is crucial for advancing fusion research. Given the recent development of large datasets of optimized stellarators, machine learning approaches have emerged as a potential candidate. Motivated by this, we present an open inverse problem to the machine learning community: to rapidly generate high-quality stellarator designs which have a set of desirable characteristics. As a case study in the problem space, we train a conditional diffusion model on data from the QUASR database to generate quasisymmetric stellarator designs with desirable characteristics (aspect ratio and mean rotational transform). The diffusion model is applied to design stellarators with characteristics not seen during training. We provide evaluation protocols and show that many of the generated stellarators exhibit solid performance: less than 5% deviation from quasisymmetry and the target characteristics. The modest deviation from quasisymmetry highlights an opportunity to reach the sub 1% target. Beyond the case study, we share multiple promising avenues for generative modeling to advance stellarator design.

</details>


### [68] [Towards Trustworthy Wi-Fi Sensing: Systematic Evaluation of Deep Learning Model Robustness to Adversarial Attacks](https://arxiv.org/abs/2511.20456)
*Shreevanth Krishnaa Gopalakrishnan,Stephen Hailes*

Main category: cs.LG

TL;DR: 本文系统评估了CSI深度学习模型在不同威胁模型下的鲁棒性，发现较小模型虽然高效但在对抗攻击下鲁棒性较差，物理可实现扰动比无约束特征空间攻击更难成功，对抗训练能有效缓解漏洞。


<details>
  <summary>Details</summary>
Motivation: 随着机器学习在基于CSI的人类感知系统中日益重要，这些系统面临对抗性扰动的安全风险，需要量化模型鲁棒性以确保无线感知在真实环境中的安全部署。

Method: 建立评估框架，比较紧凑时间自编码器与大型深度架构在三个公共数据集上的表现，分析模型规模、训练机制和物理约束对鲁棒性的影响，测试白盒、黑盒/迁移和通用扰动等威胁模型。

Result: 实验表明较小模型在对抗攻击下鲁棒性显著较差；物理可实现信号空间扰动比无约束特征空间攻击成功率低；对抗训练能提高平均鲁棒准确率，仅适度降低清洁数据性能。

Conclusion: 研究结果为鲁棒性评估提供了量化基准，并为设计安全可靠的人类中心感知系统提供了指导原则，支持无线感知向可靠跨域操作发展。

Abstract: Machine learning has become integral to Channel State Information (CSI)-based human sensing systems and is expected to power applications such as device-free activity recognition and identity detection in future cellular and Wi-Fi generations. However, these systems rely on models whose decisions can be subtly perturbed, raising concerns for security and reliability in ubiquitous sensing. Quantifying and understanding the robustness of such models, defined as their ability to maintain accurate predictions under adversarial perturbations, is therefore critical before wireless sensing can be safely deployed in real-world environments.
  This work presents a systematic evaluation of the robustness of CSI deep learning models under diverse threat models (white-box, black-box/transfer, and universal perturbations) and varying degrees of attack realism. We establish a framework to compare compact temporal autoencoder models with larger deep architectures across three public datasets, quantifying how model scale, training regime, and physical constraints influence robustness. Our experiments show that smaller models, while efficient and equally performant on clean data, are markedly less robust. We further confirm that physically realizable signal-space perturbations, designed to be feasible in real wireless channels, significantly reduce attack success compared to unconstrained feature-space attacks. Adversarial training mitigates these vulnerabilities, improving mean robust accuracy with only moderate degradation in clean performance across both model classes. As wireless sensing advances towards reliable, cross-domain operation, these findings provide quantitative baselines for robustness estimation and inform design principles for secure and trustworthy human-centered sensing systems.

</details>


### [69] [NVIDIA Nemotron Parse 1.1](https://arxiv.org/abs/2511.20478)
*Kateryna Chumachenko,Amala Sanjay Deshmukh,Jarno Seppanen,Ilia Karmanov,Chia-Chih Chen,Lukas Voegtle,Philipp Fischer,Marek Wawrzos,Saeid Motiian,Roman Ageev,Kedi Wu,Alexandre Milesi,Maryam Moosaei,Krzysztof Pawelec,Padmavathy Subramanian,Mehrzad Samadi,Xin Yu,Celina Dear,Sarah Stoddard,Jenna Diamond,Jesse Oliver,Leanna Chraghchian,Patrick Skelly,Tom Balough,Yao Xu,Jane Polak Scowcroft,Daniel Korzekwa,Darragh Hanley,Sandip Bhaskar,Timo Roman,Karan Sapra,Andrew Tao,Bryan Catanzaro*

Main category: cs.LG

TL;DR: Nemotron-Parse-1.1是一个轻量级文档解析和OCR模型，相比前代在OCR、Markdown格式化、表格解析和图像文本提取方面有显著改进，支持更长的输出序列，采用885M参数的编码器-解码器架构。


<details>
  <summary>Details</summary>
Motivation: 提升文档解析和OCR能力，提供更高效的轻量级解决方案，支持处理视觉密集文档。

Method: 采用编码器-解码器架构，包含885M参数，其中语言解码器为256M参数，支持文本边界框提取和语义分类。

Result: 在公共基准测试中达到竞争性准确率，发布了模型权重和优化容器，同时提供速度优化版本Nemotron-Parse-1.1-TC，速度提升20%且质量损失极小。

Conclusion: Nemotron-Parse-1.1是一个强大的轻量级OCR解决方案，在多个文档解析任务上表现优异，并提供了开源资源和优化版本。

Abstract: We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation.

</details>


### [70] [DP-MicroAdam: Private and Frugal Algorithm for Training and Fine-tuning](https://arxiv.org/abs/2511.20509)
*Mihaela Hudişteanu,Edwige Cyffers,Nikita P. Kalinin*

Main category: cs.LG

TL;DR: 提出了DP-MicroAdam，一种内存高效且支持稀疏性的自适应差分隐私优化器，在多个基准测试中优于现有自适应DP优化器，并与DP-SGD竞争或更优。


<details>
  <summary>Details</summary>
Motivation: 自适应优化器在非私有训练中是事实标准，能提供更快收敛和更好性能，但差分隐私训练仍主要使用DP-SGD，需要大量计算和超参数调优。

Method: 开发DP-MicroAdam优化器，具有内存效率和稀疏性感知特性，在随机非凸优化中证明以最优速率收敛。

Result: 在CIFAR-10、大规模ImageNet训练和预训练transformer的私有微调等基准测试中，DP-MicroAdam优于现有自适应DP优化器，与DP-SGD相比具有竞争性或更优的准确性。

Conclusion: 自适应优化可以在差分隐私下同时提高性能和稳定性。

Abstract: Adaptive optimizers are the de facto standard in non-private training as they often enable faster convergence and improved performance. In contrast, differentially private (DP) training is still predominantly performed with DP-SGD, typically requiring extensive compute and hyperparameter tuning. We propose DP-MicroAdam, a memory-efficient and sparsity-aware adaptive DP optimizer. We prove that DP-MicroAdam converges in stochastic non-convex optimization at the optimal $\mathcal{O}(1/\sqrt{T})$ rate, up to privacy-dependent constants. Empirically, DP-MicroAdam outperforms existing adaptive DP optimizers and achieves competitive or superior accuracy compared to DP-SGD across a range of benchmarks, including CIFAR-10, large-scale ImageNet training, and private fine-tuning of pretrained transformers. These results demonstrate that adaptive optimization can improve both performance and stability under differential privacy.

</details>


### [71] [Adam Simplified: Bias Correction Simplified](https://arxiv.org/abs/2511.20516)
*Sam Laing,Antonio Orvieto*

Main category: cs.LG

TL;DR: 本文对Adam优化器中的偏差校正功能进行了系统性研究，发现在最优超参数配置下，偏差校正不会改善最终测试性能，有时甚至有害。


<details>
  <summary>Details</summary>
Motivation: Adam优化器是现代深度学习的基石，但其各个组件的经验必要性往往被视为理所当然。本文旨在深入理解偏差校正这一功能的作用，目前对其贡献的理解仍然不足。

Method: 通过在视觉和语言建模任务上进行一系列系统性消融实验，分析偏差校正的影响。

Result: 研究表明，在最优超参数配置下，包含偏差校正不会提高最终测试性能。除非实施适当的学习率调度，否则偏差校正有时会对性能产生负面影响。偏差校正可重新解释为一种隐式学习率调度，其行为强烈依赖于平滑超参数β₁、β₂的选择。

Conclusion: 研究结果挑战了偏差校正组件普遍包含的常规做法，表明其并非总是必要的。

Abstract: The Adam optimizer is a cornerstone of modern deep learning, yet the empirical necessity of each of its individual components is often taken for granted. This paper presents a focused investigation into the role of bias-correction, a feature whose contribution remains poorly understood. Through a series of systematic ablations on vision and language modelling tasks, we demonstrate that the conventional wisdom surrounding bias correction is misleading. In particular, we demonstrate that in the optimal hyper-parameter configuration, the inclusion of bias correction leads to no improvement in final test performance. Moreover, unless appropriate learning rate scheduling is implemented, the inclusion of bias correction can sometimes be detrimental to performance. We further reinterpret bias correction as a form of implicit learning rate scheduling whose behaviour is strongly dependent on the choice of smoothing hyper-parameters $β_1, β_2 \in [0,1)$. Our findings challenge the universal inclusion of this component.

</details>


### [72] [Feature-Modulated UFNO for Improved Prediction of Multiphase Flow in Porous Media](https://arxiv.org/abs/2511.20543)
*Alhasan Abdellatif,Hannah P. Menke,Ahmed H. Elsheikh,Florian Doster,Kamaljit Singh*

Main category: cs.LG

TL;DR: UFNO-FiLM通过引入FiLM层解耦标量输入和空间特征，并使用空间加权损失函数，在保持UFNO高精度的同时解决了标量输入处理效率低和误差敏感区域学习不足的问题。


<details>
  <summary>Details</summary>
Motivation: UFNO虽然通过并行UNet路径提高了预测精度，但存在两个问题：1）将标量输入作为空间分布场处理导致频域冗余计算；2）标准损失函数未考虑误差敏感区域的空间变化。

Method: 1）使用FiLM层解耦标量输入与空间特征，避免标量信号进入傅里叶变换；2）采用空间加权损失函数，在关键区域优先学习。

Result: 在地下多相流实验中，相比UFNO实现了21%的气体饱和度平均绝对误差降低。

Conclusion: UFNO-FiLM通过FiLM层和空间加权损失有效提升了模型预测精度，特别是在关键物理区域。

Abstract: The UNet-enhanced Fourier Neural Operator (UFNO) extends the Fourier Neural Operator (FNO) by incorporating a parallel UNet pathway, enabling the retention of both high- and low-frequency components. While UFNO improves predictive accuracy over FNO, it inefficiently treats scalar inputs (e.g., temperature, injection rate) as spatially distributed fields by duplicating their values across the domain. This forces the model to process redundant constant signals within the frequency domain. Additionally, its standard loss function does not account for spatial variations in error sensitivity, limiting performance in regions of high physical importance. We introduce UFNO-FiLM, an enhanced architecture that incorporates two key innovations. First, we decouple scalar inputs from spatial features using a Feature-wise Linear Modulation (FiLM) layer, allowing the model to modulate spatial feature maps without introducing constant signals into the Fourier transform. Second, we employ a spatially weighted loss function that prioritizes learning in critical regions. Our experiments on subsurface multiphase flow demonstrate a 21\% reduction in gas saturation Mean Absolute Error (MAE) compared to UFNO, highlighting the effectiveness of our approach in improving predictive accuracy.

</details>


### [73] [E2E-GRec: An End-to-End Joint Training Framework for Graph Neural Networks and Recommender Systems](https://arxiv.org/abs/2511.20564)
*Rui Xue,Shichao Zhu,Liang Qin,Guangmou Pan,Yang Song,Tianfu Wu*

Main category: cs.LG

TL;DR: 提出E2E-GRec端到端训练框架，将GNN训练与推荐系统统一，解决传统两阶段方法的高计算开销和缺乏联合优化问题。


<details>
  <summary>Details</summary>
Motivation: 传统两阶段GNN推荐系统存在高计算开销和缺乏联合优化的问题，导致GNN对推荐任务的表达能力不足。

Method: 采用高效子图采样、图特征自编码器作为自监督任务、两层级特征融合与基于Gradnorm的动态损失平衡。

Result: 离线评估和在线A/B测试显示，在停留时长和跳过视频数量等指标上均有显著提升。

Conclusion: E2E-GRec框架在多个推荐指标上持续优于传统方法，实现了显著的性能提升。

Abstract: Graph Neural Networks (GNNs) have emerged as powerful tools for modeling graph-structured data and have been widely used in recommender systems, such as for capturing complex user-item and item-item relations. However, most industrial deployments adopt a two-stage pipeline: GNNs are first pre-trained offline to generate node embeddings, which are then used as static features for downstream recommender systems. This decoupled paradigm leads to two key limitations: (1) high computational overhead, since large-scale GNN inference must be repeatedly executed to refresh embeddings; and (2) lack of joint optimization, as the gradient from the recommender system cannot directly influence the GNN learning process, causing the GNN to be suboptimally informative for the recommendation task. In this paper, we propose E2E-GRec, a novel end-to-end training framework that unifies GNN training with the recommender system. Our framework is characterized by three key components: (i) efficient subgraph sampling from a large-scale cross-domain heterogeneous graph to ensure training scalability and efficiency; (ii) a Graph Feature Auto-Encoder (GFAE) serving as an auxiliary self-supervised task to guide the GNN to learn structurally meaningful embeddings; and (iii) a two-level feature fusion mechanism combined with Gradnorm-based dynamic loss balancing, which stabilizes graph-aware multi-task end-to-end training. Extensive offline evaluations, online A/B tests (e.g., a +0.133% relative improvement in stay duration, a 0.3171% reduction in the average number of videos a user skips) on large-scale production data, together with theoretical analysis, demonstrate that E2E-GRec consistently surpasses traditional approaches, yielding significant gains across multiple recommendation metrics.

</details>


### [74] [A Tale of Two Geometries: Adaptive Optimizers and Non-Euclidean Descent](https://arxiv.org/abs/2511.20584)
*Shuo Xie,Tianhao Wang,Beining Wu,Zhiyuan Li*

Main category: cs.LG

TL;DR: 本文揭示了自适应优化器与归一化最速下降(NSD)之间的紧密联系，分析了两种算法在几何条件上的差异，并将自适应平滑性理论扩展到非凸设置，证明了自适应平滑性能够实现Nesterov动量加速，同时提出了自适应梯度方差概念用于随机优化。


<details>
  <summary>Details</summary>
Motivation: 研究自适应优化器与归一化最速下降之间的理论联系，探索不同几何条件对优化算法性能的影响，特别是在非凸设置和随机优化场景下的理论保证。

Method: 将自适应平滑性理论扩展到非凸设置，分析自适应优化器与NSD在几何条件上的差异，引入自适应梯度方差概念，并研究Nesterov动量在自适应优化器中的加速效果。

Result: 证明了自适应平滑性精确刻画了自适应优化器的收敛性，在凸设置下能够实现Nesterov动量加速，且在某些非欧几里得几何下无法通过标准平滑性获得类似保证；提出了自适应梯度方差概念，实现了维度无关的收敛保证。

Conclusion: 自适应平滑性是理解自适应优化器性能的关键几何条件，能够实现标准平滑性无法达到的加速效果，自适应梯度方差为随机优化提供了新的理论框架。

Abstract: Adaptive optimizers can reduce to normalized steepest descent (NSD) when only adapting to the current gradient, suggesting a close connection between the two algorithmic families. A key distinction between their analyses, however, lies in the geometries, e.g., smoothness notions, they rely on. In the convex setting, adaptive optimizers are governed by a stronger adaptive smoothness condition, while NSD relies on the standard notion of smoothness. We extend the theory of adaptive smoothness to the nonconvex setting and show that it precisely characterizes the convergence of adaptive optimizers. Moreover, we establish that adaptive smoothness enables acceleration of adaptive optimizers with Nesterov momentum in the convex setting, a guarantee unattainable under standard smoothness for certain non-Euclidean geometry. We further develop an analogous comparison for stochastic optimization by introducing adaptive gradient variance, which parallels adaptive smoothness and leads to dimension-free convergence guarantees that cannot be achieved under standard gradient variance for certain non-Euclidean geometry.

</details>


### [75] [Anatomica: Localized Control over Geometric and Topological Properties for Anatomical Diffusion Models](https://arxiv.org/abs/2511.20587)
*Karim Kadry,Abdallah Abdelwahed,Shoaib Goraya,Ajay Manicka,Naravich Chutisilp,Farhad Nezami,Elazer Edelman*

Main category: cs.LG

TL;DR: Anatomica是一个推理时框架，用于生成具有局部几何拓扑控制的多类解剖体素图。通过使用不同维度、位置和形状的立方体控制域来提取相关子结构，并计算可微惩罚函数来引导样本满足目标约束。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够灵活控制解剖结构几何和拓扑特征的框架，用于合成数据集的理性设计，支持虚拟试验或机器学习工作流程。

Method: 使用立方体控制域提取局部子结构，通过体素矩控制几何特征（大小、形状、位置），通过持久同调控制拓扑特征（连通分量、环、空洞），并在潜在扩散模型中实现神经场解码器部分提取子结构。

Result: Anatomica能够灵活应用于不同解剖系统，在任意维度和坐标系下组合约束来控制复杂结构。

Conclusion: 该框架实现了对解剖体素图的精确几何和拓扑控制，为合成数据生成提供了有效的工具，支持虚拟试验和机器学习应用。

Abstract: We present Anatomica: an inference-time framework for generating multi-class anatomical voxel maps with localized geo-topological control. During generation, we use cuboidal control domains of varying dimensionality, location, and shape to slice out relevant substructures. These local substructures are used to compute differentiable penalty functions that steer the sample towards target constraints. We control geometric features such as size, shape, and position through voxel-wise moments, while topological features such as connected components, loops, and voids are enforced through persistent homology. Lastly, we implement Anatomica for latent diffusion models, where neural field decoders partially extract substructures, enabling the efficient control of anatomical properties. Anatomica applies flexibly across diverse anatomical systems, composing constraints to control complex structures over arbitrary dimensions and coordinate systems, thereby enabling the rational design of synthetic datasets for virtual trials or machine learning workflows.

</details>


### [76] [Attention Trajectories as a Diagnostic Axis for Deep Reinforcement Learning](https://arxiv.org/abs/2511.20591)
*Charlotte Beylier,Hannah Selder,Arthur Fleig,Simon M. Hofmann,Nico Scherf*

Main category: cs.LG

TL;DR: 本文提出了注意力导向指标（ATOMs）来研究强化学习智能体在训练过程中的注意力发展，通过在三个不同版本的Pong游戏中测试，发现ATOMs能够区分不同游戏变体训练出的智能体的注意力模式，并且这些注意力差异会转化为行为差异。


<details>
  <summary>Details</summary>
Motivation: 强化学习智能体的学习过程除了数学公式外仍然缺乏深入理解，需要新的工具来研究智能体在训练过程中的注意力发展。

Method: 引入注意力导向指标（ATOMs），在三个设计用于教授不同行为的Pong游戏变体中进行受控实验，并辅以行为评估。

Result: ATOMs成功区分了在不同游戏变体上训练的智能体的注意力模式，这些注意力模式的差异转化为行为差异；通过持续监测发现智能体注意力发展呈阶段性，且这些阶段在不同游戏变体中保持一致。

Conclusion: ATOMs有助于提高对强化学习智能体学习过程的理解，并更好地理解注意力与学习之间的关系。

Abstract: The learning process of a reinforcement learning (RL) agent remains poorly understood beyond the mathematical formulation of its learning algorithm. To address this gap, we introduce attention-oriented metrics (ATOMs) to investigate the development of an RL agent's attention during training. In a controlled experiment, we tested ATOMs on three variations of a Pong game, each designed to teach the agent distinct behaviours, complemented by a behavioural assessment. ATOMs successfully delineate the attention patterns of an agent trained on each game variation, and that these differences in attention patterns translate into differences in the agent's behaviour. Through continuous monitoring of ATOMs during training, we observed that the agent's attention developed in phases, and that these phases were consistent across game variations. Overall, we believe that ATOM could help improve our understanding of the learning processes of RL agents and better understand the relationship between attention and learning.

</details>


### [77] [Latent Diffusion Inversion Requires Understanding the Latent Space](https://arxiv.org/abs/2511.20592)
*Mingxing Rao,Bowen Qu,Daniel Moyer*

Main category: cs.LG

TL;DR: 本文研究发现潜在扩散模型在潜在空间中存在非均匀记忆现象，某些维度对记忆化贡献更大。通过基于解码器回拉度量的维度排序方法，识别出关键记忆维度，显著提升了成员推理攻击性能。


<details>
  <summary>Details</summary>
Motivation: 现有模型反转技术主要关注数据域扩散模型，而忽略了潜在空间生成模型中的编码器/解码器对和潜在代码的作用。本文旨在探索潜在扩散模型中的记忆化模式及其隐私风险。

Method: 提出基于解码器回拉度量的维度排序方法，识别对记忆化贡献最大的潜在维度，并在成员推理攻击中移除较少记忆化的维度来计算攻击统计量。

Result: 在CIFAR-10、CelebA、ImageNet-1K等多个数据集上，该方法使成员推理攻击的AUROC平均提升2.7%，TPR@1%FPR显著增加6.42%，在极低误报率下识别成员的置信度大幅提高。

Conclusion: 研究揭示了自编码器几何结构对潜在扩散模型记忆化的被忽视影响，为分析基于扩散的生成模型的隐私风险提供了新视角。

Abstract: The recovery of training data from generative models (``model inversion'') has been extensively studied for diffusion models in the data domain. The encoder/decoder pair and corresponding latent codes have largely been ignored by inversion techniques applied to latent space generative models, e.g., Latent Diffusion models (LDMs). In this work we describe two key findings: (1) The diffusion model exhibits non-uniform memorization across latent codes, tending to overfit samples located in high-distortion regions of the decoder pullback metric. (2) Even within a single latent code, different dimensions contribute unequally to memorization. We introduce a principled method to rank latent dimensions by their per-dimensional contribution to the decoder pullback metric, identifying those most responsible for memorization. Empirically, removing less-memorizing dimensions when computing attack statistics for score-based membership inference attacker significantly improves performance, with average AUROC gains of 2.7\% and substantial increases in TPR@1\%FPR (6.42\%) across diverse datasets including CIFAR-10, CelebA, ImageNet-1K, Pokémon, MS-COCO, and Flickr. This indicates stronger confidence in identifying members under extremely low false-positive tolerance. Our results highlight the overlooked influence of the auto-encoder geometry on LDM memorization and provide a new perspective for analyzing privacy risks in diffusion-based generative models.

</details>


### [78] [Image2Gcode: Image-to-G-code Generation for Additive Manufacturing Using Diffusion-Transformer Model](https://arxiv.org/abs/2511.20636)
*Ziyue Wang,Yayati Jadhav,Peter Pak,Amir Barati Farimani*

Main category: cs.LG

TL;DR: Image2Gcode是一个端到端的数据驱动框架，直接从图像和零件图纸生成打印机可读的G代码，绕过了传统的CAD建模阶段，加速了增材制造的设计到制造周期。


<details>
  <summary>Details</summary>
Motivation: 传统的机械设计和制造流程依赖CAD建模，这是一个主要瓶颈：构建特定对象的3D几何模型速度慢，不适合快速原型制作。即使微小的设计变更也需要在CAD软件中手动更新，使得迭代耗时且难以扩展。

Method: 该框架首先从图像中提取切片结构线索，然后使用去噪扩散概率模型（DDPM）处理G代码序列。通过迭代去噪，模型将高斯噪声转换为具有相应挤出参数的可执行打印移动轨迹，建立了从视觉输入到原生工具路径的直接映射。

Result: 通过直接从2D图像生成结构化G代码，Image2Gcode消除了对CAD或STL中间文件的需求，降低了增材制造的门槛，并加速了设计到制造的周期。该方法支持从简单草图或视觉参考进行按需原型制作。

Conclusion: 这是一个灵活、计算效率高的框架，提高了设计迭代、修复工作流程和分布式制造的可访问性，实现了从概念到物理工件的自动化管道。

Abstract: Mechanical design and manufacturing workflows conventionally begin with conceptual design, followed by the creation of a computer-aided design (CAD) model and fabrication through material-extrusion (MEX) printing. This process requires converting CAD geometry into machine-readable G-code through slicing and path planning. While each step is well established, dependence on CAD modeling remains a major bottleneck: constructing object-specific 3D geometry is slow and poorly suited to rapid prototyping. Even minor design variations typically necessitate manual updates in CAD software, making iteration time-consuming and difficult to scale. To address this limitation, we introduce Image2Gcode, an end-to-end data-driven framework that bypasses the CAD stage and generates printer-ready G-code directly from images and part drawings. Instead of relying on an explicit 3D model, a hand-drawn or captured 2D image serves as the sole input. The framework first extracts slice-wise structural cues from the image and then employs a denoising diffusion probabilistic model (DDPM) over G-code sequences. Through iterative denoising, the model transforms Gaussian noise into executable print-move trajectories with corresponding extrusion parameters, establishing a direct mapping from visual input to native toolpaths. By producing structured G-code directly from 2D imagery, Image2Gcode eliminates the need for CAD or STL intermediates, lowering the entry barrier for additive manufacturing and accelerating the design-to-fabrication cycle. This approach supports on-demand prototyping from simple sketches or visual references and integrates with upstream 2D-to-3D reconstruction modules to enable an automated pipeline from concept to physical artifact. The result is a flexible, computationally efficient framework that advances accessibility in design iteration, repair workflows, and distributed manufacturing.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [79] [Using Wearable Devices to Improve Chronic PainTreatment among Patients with Opioid Use Disorder](https://arxiv.org/abs/2511.19577)
*Abhay Goyal,Navin Kumar,Kimberly DiMeola,Rafael Trujillo,Soorya Ram Shimgekar,Christian Poellabauer,Pi Zonooz,Ermonda Gjoni-Markaj,Declan Barry,Lynn Madden*

Main category: cs.AI

TL;DR: 本研究探讨了使用可穿戴设备和AI方法预测慢性疼痛和阿片类药物使用障碍患者的疼痛峰值，发现机器学习模型预测准确率较高，但大语言模型在此领域表现有限。


<details>
  <summary>Details</summary>
Motivation: 慢性疼痛和阿片类药物使用障碍是相互关联的常见慢性疾病，目前缺乏针对接受阿片类药物使用障碍药物治疗患者的循证综合治疗方案。可穿戴设备有潜力监测复杂患者信息，为OUD和CP患者开发治疗方案提供信息。

Method: 使用可穿戴设备监测患者数据，应用多种AI方法（包括机器学习模型和大语言模型）分析疼痛峰值的临床相关性。

Result: 机器学习模型在预测疼痛峰值方面取得了相对较高的准确率（>0.7），而大语言模型在提供疼痛峰值见解方面表现有限。

Conclusion: 通过可穿戴设备的实时监测结合先进AI模型，可以促进疼痛峰值的早期检测，支持个性化干预，帮助降低阿片类药物复吸风险，改善MOUD依从性，并增强CP和OUD护理的整合。鉴于大语言模型整体表现有限，这些发现强调了开发能够在OUD/CP背景下提供可行见解的大语言模型的必要性。

Abstract: Chronic pain (CP) and opioid use disorder (OUD) are common and interrelated chronic medical conditions. Currently, there is a paucity of evidence-based integrated treatments for CP and OUD among individuals receiving medication for opioid use disorder (MOUD). Wearable devices have the potential to monitor complex patient information and inform treatment development for persons with OUD and CP, including pain variability (e.g., exacerbations of pain or pain spikes) and clinical correlates (e.g., perceived stress). However, the application of large language models (LLMs) with wearable data for understanding pain spikes, remains unexplored. Consequently, the aim of this pilot study was to examine the clinical correlates of pain spikes using a range of AI approaches. We found that machine learning models achieved relatively high accuracy (>0.7) in predicting pain spikes, while LLMs were limited in providing insights on pain spikes. Real-time monitoring through wearable devices, combined with advanced AI models, could facilitate early detection of pain spikes and support personalized interventions that may help mitigate the risk of opioid relapse, improve adherence to MOUD, and enhance the integration of CP and OUD care. Given overall limited LLM performance, these findings highlight the need to develop LLMs which can provide actionable insights in the OUD/CP context.

</details>


### [80] [HeaRT: A Hierarchical Circuit Reasoning Tree-Based Agentic Framework for AMS Design Optimization](https://arxiv.org/abs/2511.19669)
*Souradip Poddar,Chia-Tung Ho,Ziming Wei,Weidong Cao,Haoxing Ren,David Z. Pan*

Main category: cs.AI

TL;DR: HeaRT是一个基础推理引擎，用于自动化设计循环，旨在实现智能、自适应、类人风格的设计优化。它在40个电路基准测试中表现出>97%的推理准确率和>98%的Pass@1性能，同时仅需SOTA基线0.5倍的真实时间token预算，并在各种优化方法中实现>3倍的收敛速度提升。


<details>
  <summary>Details</summary>
Motivation: 传统的AI驱动AMS设计自动化算法受限于对高质量数据集的依赖、跨架构可移植性差以及缺乏自适应机制。

Method: 提出HeaRT基础推理引擎，作为自动化循环和智能自适应设计优化的第一步。

Result: HeaRT在40个电路基准测试中保持>97%的推理准确率和>98%的Pass@1性能，即使电路复杂性增加，同时仅需SOTA基线0.5倍的真实时间token预算。在尺寸和拓扑设计适应任务中，HeaRT实现了>3倍的收敛速度提升，同时保留了先前的设计意图。

Conclusion: HeaRT是一个有效的推理引擎，能够显著提高AMS设计自动化的效率和适应性，为智能设计优化提供了有前景的方向。

Abstract: Conventional AI-driven AMS design automation algorithms remain constrained by their reliance on high-quality datasets to capture underlying circuit behavior, coupled with poor transferability across architectures, and a lack of adaptive mechanisms. This work proposes HeaRT, a foundational reasoning engine for automation loops and a first step toward intelligent, adaptive, human-style design optimization. HeaRT consistently demonstrates reasoning accuracy >97% and Pass@1 performance >98% across our 40-circuit benchmark repository, even as circuit complexity increases, while operating at <0.5x real-time token budget of SOTA baselines. Our experiments show that HeaRT yields >3x faster convergence in both sizing and topology design adaptation tasks across diverse optimization approaches, while preserving prior design intent.

</details>


### [81] [FISCAL: Financial Synthetic Claim-document Augmented Learning for Efficient Fact-Checking](https://arxiv.org/abs/2511.19671)
*Rishab Sharma,Iman Saberi,Elham Alipour,Jie JW Wu,Fatemeh Fard*

Main category: cs.AI

TL;DR: FISCAL框架通过生成金融事实核查的合成数据，训练出轻量级验证器MiniCheck-FISCAL，在金融数值声明验证任务上超越GPT-3.5 Turbo等模型，接近更大模型的准确性。


<details>
  <summary>Details</summary>
Motivation: 当前金融应用中大语言模型存在事实可靠性不足和计算效率低的问题，需要开发轻量级但高精度的金融事实核查系统。

Method: 提出FISCAL模块化框架生成金融合成数据，并基于此训练MiniCheck-FISCAL轻量级验证器。

Result: MiniCheck-FISCAL在金融数据集上超越GPT-3.5 Turbo和同类开源模型，接近Mixtral-8x22B等大20倍模型的准确性，在外部数据集上可与GPT-4o和Claude-3.5相媲美。

Conclusion: 领域特定的合成数据结合高效微调，可使紧凑模型在金融AI应用中实现最先进的准确性、鲁棒性和可扩展性。

Abstract: Financial applications of large language models (LLMs) require factual reliability and computational efficiency, yet current systems often hallucinate details and depend on prohibitively large models. We propose FISCAL (Financial Synthetic Claim-Document Augmented Learning), a modular framework for generating synthetic data tailored to financial fact-checking. Using FISCAL, we generate a dataset called FISCAL-data and use it to train MiniCheck-FISCAL, a lightweight verifier for numerical financial claims. MiniCheck-FISCAL outperforms its baseline, surpasses GPT-3.5 Turbo and other open-source peers of similar size, and approaches the accuracy of much larger systems (20x), such as Mixtral-8x22B and Command R+. On external datasets FinDVer and Fin-Fact, it rivals GPT-4o and Claude-3.5 while outperforming Gemini-1.5 Flash. These results show that domain-specific synthetic data, combined with efficient fine-tuning, enables compact models to achieve state-of-the-art accuracy, robustness, and scalability for practical financial AI. The dataset and scripts are available in the project repository (link provided in the paper).

</details>


### [82] [Scaling Item-to-Standard Alignment with Large Language Models: Accuracy, Limits, and Solutions](https://arxiv.org/abs/2511.19749)
*Farzan Karimi-Malekabadi,Pooya Razavi,Sonya Powers*

Main category: cs.AI

TL;DR: 本研究探讨了大型语言模型（LLMs）在教育评估项目与内容标准对齐过程中的应用潜力，通过三个实验验证了LLMs在识别错误对齐项目、选择正确技能和候选技能筛选方面的表现。


<details>
  <summary>Details</summary>
Motivation: 传统的人工对齐审查虽然准确但效率低下，特别是在大规模项目库中。研究旨在验证LLMs是否能加速这一过程而不牺牲准确性。

Method: 使用超过12,000个K-5年级的项目-技能对，测试了三种LLMs（GPT-3.5 Turbo、GPT-4o-mini和GPT-4o）在三个任务上的表现：识别错误对齐项目、从完整标准集中选择正确技能、以及在分类前缩小候选列表。

Result: GPT-4o-mini在识别对齐状态方面达到83-94%的准确率；数学领域表现强劲，阅读领域因语义重叠标准而表现较低；候选技能预筛选显著改善结果，正确技能出现在前五建议中的概率超过95%。

Conclusion: LLMs，特别是结合候选筛选策略时，能显著减少人工审查负担同时保持对齐准确性。建议开发混合流程，将基于LLM的筛选与人工审查相结合，为持续项目验证和教学对齐提供可扩展解决方案。

Abstract: As educational systems evolve, ensuring that assessment items remain aligned with content standards is essential for maintaining fairness and instructional relevance. Traditional human alignment reviews are accurate but slow and labor-intensive, especially across large item banks. This study examines whether Large Language Models (LLMs) can accelerate this process without sacrificing accuracy. Using over 12,000 item-skill pairs in grades K-5, we tested three LLMs (GPT-3.5 Turbo, GPT-4o-mini, and GPT-4o) across three tasks that mirror real-world challenges: identifying misaligned items, selecting the correct skill from the full set of standards, and narrowing candidate lists prior to classification. In Study 1, GPT-4o-mini correctly identified alignment status in approximately 83-94% of cases, including subtle misalignments. In Study 2, performance remained strong in mathematics but was lower for reading, where standards are more semantically overlapping. Study 3 demonstrated that pre-filtering candidate skills substantially improved results, with the correct skill appearing among the top five suggestions more than 95% of the time. These findings suggest that LLMs, particularly when paired with candidate filtering strategies, can significantly reduce the manual burden of item review while preserving alignment accuracy. We recommend the development of hybrid pipelines that combine LLM-based screening with human review in ambiguous cases, offering a scalable solution for ongoing item validation and instructional alignment.

</details>


### [83] [KOM: A Multi-Agent Artificial Intelligence System for Precision Management of Knee Osteoarthritis (KOA)](https://arxiv.org/abs/2511.19798)
*Weizhi Liu,Xi Chen,Zekun Jiang,Liang Zhao,Kunyuan Jiang,Ruisi Tang,Li Wang,Mingke You,Hanyu Zhou,Hongyu Chen,Qiankun Xiong,Yong Nie,Kang Li,Jian Li*

Main category: cs.AI

TL;DR: 开发了KOM多智能体系统，用于自动化膝关节骨关节炎的评估、风险预测和治疗处方，在资源有限环境中提升诊疗效率。


<details>
  <summary>Details</summary>
Motivation: 膝关节骨关节炎影响全球6亿多人，个性化多学科干预虽能延缓疾病进展，但在资源有限环境中难以实施，需要自动化解决方案。

Method: 构建KOM多智能体系统，自动化执行KOA诊疗路径中的关键任务，基于患者个体特征、疾病状态、风险因素和禁忌症生成定制化管理方案。

Result: 基准实验显示KOM在影像分析和处方生成方面优于通用大语言模型；随机三臂模拟研究表明KOM与临床医生协作使诊断和规划时间减少38.5%，治疗质量提升。

Conclusion: KOM有助于实现自动化KOA管理，整合到临床工作流程中可提高护理效率，其模块化架构为开发其他慢性病AI辅助管理系统提供借鉴。

Abstract: Knee osteoarthritis (KOA) affects more than 600 million individuals globally and is associated with significant pain, functional impairment, and disability. While personalized multidisciplinary interventions have the potential to slow disease progression and enhance quality of life, they typically require substantial medical resources and expertise, making them difficult to implement in resource-limited settings. To address this challenge, we developed KOM, a multi-agent system designed to automate KOA evaluation, risk prediction, and treatment prescription. This system assists clinicians in performing essential tasks across the KOA care pathway and supports the generation of tailored management plans based on individual patient profiles, disease status, risk factors, and contraindications. In benchmark experiments, KOM demonstrated superior performance compared to several general-purpose large language models in imaging analysis and prescription generation. A randomized three-arm simulation study further revealed that collaboration between KOM and clinicians reduced total diagnostic and planning time by 38.5% and resulted in improved treatment quality compared to each approach used independently. These findings indicate that KOM could help facilitate automated KOA management and, when integrated into clinical workflows, has the potential to enhance care efficiency. The modular architecture of KOM may also offer valuable insights for developing AI-assisted management systems for other chronic conditions.

</details>


### [84] [A Unified Evaluation-Instructed Framework for Query-Dependent Prompt Optimization](https://arxiv.org/abs/2511.19829)
*Ke Chen,Yifeng Wang,Hassan Almosapeeh,Haohan Wang*

Main category: cs.AI

TL;DR: 该论文提出了一个基于评估指导的提示优化方法，通过建立系统化的提示评估框架和预测多维质量分数的评估器，实现可解释的、查询依赖的提示优化，在多个数据集和模型上超越了现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法主要优化静态模板，在复杂动态用户场景中效果有限；查询依赖方法依赖不稳定的文本反馈或黑盒奖励模型，提供弱且不可解释的优化信号；提示质量本身缺乏统一系统定义，导致评估信号碎片化不可靠。

Method: 首先建立性能导向的系统化提示评估框架，开发并微调无需执行的评估器来直接从文本预测多维质量分数，然后评估器指导指标感知的优化器诊断失败模式并以可解释、查询依赖的方式重写提示。

Result: 评估器在预测提示性能方面达到最强准确度，评估指导的优化在8个数据集和3个骨干模型上持续超越静态模板和查询依赖基线方法。

Conclusion: 提出了统一、基于指标的提示质量视角，证明了评估指导的优化管道能够在多样化任务中提供稳定、可解释且模型无关的改进。

Abstract: Most prompt-optimization methods refine a single static template, making them ineffective in complex and dynamic user scenarios. Existing query-dependent approaches rely on unstable textual feedback or black-box reward models, providing weak and uninterpretable optimization signals. More fundamentally, prompt quality itself lacks a unified, systematic definition, resulting in fragmented and unreliable evaluation signals. Our approach first establishes a performance-oriented, systematic, and comprehensive prompt evaluation framework. Furthermore, we develop and finetune an execution-free evaluator that predicts multi-dimensional quality scores directly from text. The evaluator then instructs a metric-aware optimizer that diagnoses failure modes and rewrites prompts in an interpretable, query-dependent manner. Our evaluator achieves the strongest accuracy in predicting prompt performance, and the evaluation-instructed optimization consistently surpass both static-template and query-dependent baselines across eight datasets and on three backbone models. Overall, we propose a unified, metric-grounded perspective on prompt quality, and demonstrated that our evaluation-instructed optimization pipeline delivers stable, interpretable, and model-agnostic improvements across diverse tasks.

</details>


### [85] [Reinforcement Learning with $ω$-Regular Objectives and Constraints](https://arxiv.org/abs/2511.19849)
*Dominik Wagner,Leon Witzman,Luke Ong*

Main category: cs.AI

TL;DR: 本文提出了一种结合ω-正则目标与显式约束的强化学习方法，解决了传统标量奖励在表达时序、条件和安全关键目标方面的局限性，同时处理安全与性能的权衡问题。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习依赖标量奖励，表达能力有限，容易导致奖励攻击，且无法明确表达时序、条件或安全关键目标。单一标量性能度量掩盖了安全与性能之间的权衡关系。

Method: 开发基于线性规划的模型强化学习算法，将ω-正则目标与显式约束相结合，安全要求和优化目标分开处理。建立了到约束极限平均问题的转换，并保持最优性保证。

Result: 算法在极限情况下能够产生最大化满足ω-正则目标概率的策略，同时遵守指定阈值内的ω-正则约束。

Conclusion: 该方法成功解决了强化学习中安全与性能的权衡问题，通过ω-正则目标与显式约束的结合，提供了更丰富的目标表达能力和安全保证。

Abstract: Reinforcement learning (RL) commonly relies on scalar rewards with limited ability to express temporal, conditional, or safety-critical goals, and can lead to reward hacking. Temporal logic expressible via the more general class of $ω$-regular objectives addresses this by precisely specifying rich behavioural properties. Even still, measuring performance by a single scalar (be it reward or satisfaction probability) masks safety-performance trade-offs that arise in settings with a tolerable level of risk.
  We address both limitations simultaneously by combining $ω$-regular objectives with explicit constraints, allowing safety requirements and optimisation targets to be treated separately. We develop a model-based RL algorithm based on linear programming, which in the limit produces a policy maximising the probability of satisfying an $ω$-regular objective while also adhering to $ω$-regular constraints within specified thresholds. Furthermore, we establish a translation to constrained limit-average problems with optimality-preserving guarantees.

</details>


### [86] [MicroSims: A Framework for AI-Generated, Scalable Educational Simulations with Universal Embedding and Adaptive Learning Support](https://arxiv.org/abs/2511.19864)
*Valerie Lockhart,Dan McCreary,Troy A. Peterson*

Main category: cs.AI

TL;DR: MicroSims是一个用于创建轻量级交互式教育模拟的框架，通过AI辅助生成、iframe架构和可定制代码，解决了传统模拟创建成本高、技术复杂的问题。


<details>
  <summary>Details</summary>
Motivation: 传统教育模拟创建需要大量资源和技术专长，限制了其广泛应用。MicroSims旨在通过AI技术降低创建门槛，使教育工作者无需编程知识即可快速生成定制化模拟。

Method: 采用标准化设计模式支持AI生成，iframe架构实现通用嵌入和安全沙箱，透明可修改代码支持定制和教学透明度。

Result: 研究表明交互式模拟比传统教学能提高30-40%的概念理解。MicroSims在保持这些优势的同时，解决了成本、技术复杂性和平台依赖等障碍。

Conclusion: MicroSims框架对教育公平具有重要意义，能够创建低成本智能交互式教科书，使全球教育工作者能够按需创建定制化、与课程对齐的模拟。

Abstract: Educational simulations have long been recognized as powerful tools for enhancing learning outcomes, yet their creation has traditionally required substantial resources and technical expertise. This paper introduces MicroSims a novel framework for creating lightweight, interactive educational simulations that can be rapidly generated using artificial intelligence, universally embedded across digital learning platforms, and easily customized without programming knowledge. MicroSims occupy a unique position at the intersection of three key innovations: (1) standardized design patterns that enable AI-assisted generation, (2) iframe-based architecture that provides universal embedding and sandboxed security, and (3) transparent, modifiable code that supports customization and pedagogical transparency. We present a comprehensive framework encompassing design principles, technical architecture, metadata standards, and development workflows. Drawing on empirical research from physics education studies and meta-analyses across STEM disciplines, we demonstrate that interactive simulations can improve conceptual understanding by up to 30-40\% compared to traditional instruction. MicroSims extend these benefits while addressing persistent barriers of cost, technical complexity, and platform dependence. This work has significant implications for educational equity, and low-cost intelligent interactive textbooks that enabling educators worldwide to create customized, curriculum-aligned simulations on demand. We discuss implementation considerations, present evidence of effectiveness, and outline future directions for AI-powered adaptive learning systems built on the MicroSim foundation.

</details>


### [87] [Agentic AI-Empowered Conversational Embodied Intelligence Networks in 6G](https://arxiv.org/abs/2511.19865)
*Mingkai Chen,Zijie Feng,Lei Wang,Yaser Khamayseh*

Main category: cs.AI

TL;DR: 提出CC-EIN框架解决6G时代多智能体语义协作问题，通过多模态融合、自适应语义通信、任务协调和可解释性模块，在灾后救援场景中实现95.4%任务完成率和95%传输效率。


<details>
  <summary>Details</summary>
Motivation: 现有系统在多模态信息融合、自适应通信和决策可解释性方面存在挑战，需要提升多智能体在复杂任务中的语义协作能力。

Method: CC-EIN框架包含四个核心模块：PerceptiNet进行图像和雷达数据的跨模态融合生成统一语义表示；自适应语义通信策略根据任务紧急性和信道质量动态调整编码方案和传输功率；语义驱动协作机制支持任务分解和异构设备无冲突协调；InDec模块通过Grad-CAM可视化增强决策透明度。

Result: 在灾后救援场景的仿真实验中，CC-EIN实现了95.4%的任务完成率和95%的传输效率，同时保持了强语义一致性和能量效率。

Conclusion: CC-EIN框架有效解决了多智能体语义协作中的关键挑战，为6G时代的智能设备协作提供了可行解决方案，在复杂任务执行中表现出优越性能。

Abstract: In the 6G era, semantic collaboration among multiple embodied intelligent devices (MEIDs) becomes crucial for complex task execution. However, existing systems face challenges in multimodal information fusion, adaptive communication, and decision interpretability. To address these limitations, we propose a collaborative Conversational Embodied Intelligence Network (CC-EIN) integrating multimodal feature fusion, adaptive semantic communication, task coordination, and interpretability. PerceptiNet performs cross-modal fusion of image and radar data to generate unified semantic representations. An adaptive semantic communication strategy dynamically adjusts coding schemes and transmission power according to task urgency and channel quality. A semantic-driven collaboration mechanism further supports task decomposition and conflict-free coordination among heterogeneous devices. Finally, the InDec module enhances decision transparency through Grad-CAM visualization. Simulation results in post-earthquake rescue scenarios demonstrate that CC-EIN achieves 95.4% task completion rate and 95% transmission efficiency while maintaining strong semantic consistency and energy efficiency.

</details>


### [88] [Simulated Self-Assessment in Large Language Models: A Psychometric Approach to AI Self-Efficacy](https://arxiv.org/abs/2511.19872)
*Daniel I Jackson,Emma L Jensen,Syed-Amad Hussain,Emre Sezgin*

Main category: cs.AI

TL;DR: 本研究将通用自我效能感量表(GSES)应用于10个大型语言模型，在四种条件下测试其模拟自我评估能力。结果显示模型自我评估在不同条件下差异显著，但自我评估与真实能力不匹配，且高自我效能感对应更拟人化的推理风格。


<details>
  <summary>Details</summary>
Motivation: 当前对大型语言模型的评估主要关注任务准确性，而忽视了自我评估这一可靠智能的关键方面。本研究旨在探索LLMs的模拟自我评估能力及其与真实表现的关系。

Method: 将10项通用自我效能感量表(GSES)应用于10个LLMs，在四种条件下测试：无任务、计算推理、社会推理和摘要任务。通过重复管理和随机项目顺序测试稳定性，并进行定性分析。

Result: 模型自我评估在不同条件下差异显著，总体得分低于人类标准。所有模型在计算和社会问题上都达到完美准确率，但摘要表现差异很大。自我评估不能可靠反映能力，后续置信度提示导致适度下调。

Conclusion: 心理测量提示为LLM沟通行为提供了结构化洞察，但不能提供校准的性能估计。高自我效能感对应更拟人化的推理风格，而低分反映谨慎、去拟人化的解释。

Abstract: Self-assessment is a key aspect of reliable intelligence, yet evaluations of large language models (LLMs) focus mainly on task accuracy. We adapted the 10-item General Self-Efficacy Scale (GSES) to elicit simulated self-assessments from ten LLMs across four conditions: no task, computational reasoning, social reasoning, and summarization. GSES responses were highly stable across repeated administrations and randomized item orders. However, models showed significantly different self-efficacy levels across conditions, with aggregate scores lower than human norms. All models achieved perfect accuracy on computational and social questions, whereas summarization performance varied widely. Self-assessment did not reliably reflect ability: several low-scoring models performed accurately, while some high-scoring models produced weaker summaries. Follow-up confidence prompts yielded modest, mostly downward revisions, suggesting mild overestimation in first-pass assessments. Qualitative analysis showed that higher self-efficacy corresponded to more assertive, anthropomorphic reasoning styles, whereas lower scores reflected cautious, de-anthropomorphized explanations. Psychometric prompting provides structured insight into LLM communication behavior but not calibrated performance estimates.

</details>


### [89] [RPM-MCTS: Knowledge-Retrieval as Process Reward Model with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2511.19895)
*Yuanyuan Lin,Xiangyu Ouyang,Teng Zhang,Kaixin Sui*

Main category: cs.AI

TL;DR: RPM-MCTS是一种基于蒙特卡洛树搜索的代码生成方法，通过知识检索作为过程奖励模型来评估中间算法步骤，无需复杂训练过程奖励模型。该方法使用相似性过滤去除冗余节点，并利用沙箱执行反馈定位和纠正错误步骤，在减少15%令牌消耗的同时提升代码生成性能。


<details>
  <summary>Details</summary>
Motivation: 现有的基于树搜索的代码生成方法难以有效评估中间算法步骤，无法及时定位和纠正错误步骤，导致生成错误代码且计算成本增加。

Method: 提出RPM-MCTS方法：1）使用知识检索作为过程奖励模型评估中间步骤；2）在扩展阶段采用相似性过滤去除冗余节点；3）利用沙箱执行反馈定位错误步骤并进行针对性纠正。

Result: 在四个公共代码生成基准测试中，RPM-MCTS优于当前最先进方法，同时实现约15%的令牌消耗减少。使用RPM-MCTS构建的数据对基础模型进行全微调可显著提升其代码能力。

Conclusion: RPM-MCTS通过知识检索和沙箱反馈有效解决了中间步骤评估和错误定位问题，在提升代码生成质量的同时降低了计算成本。

Abstract: Tree search-based methods have made significant progress in enhancing the code generation capabilities of large language models. However, due to the difficulty in effectively evaluating intermediate algorithmic steps and the inability to locate and timely correct erroneous steps, these methods often generate incorrect code and incur increased computational costs. To tackle these problems, we propose RPM-MCTS, an effective method that utilizes Knowledge-Retrieval as Process Reward Model based on Monte Carlo Tree Search to evaluate intermediate algorithmic steps. By utilizing knowledge base retrieval, RPM-MCTS avoids the complex training of process reward models. During the expansion phase, similarity filtering is employed to remove redundant nodes, ensuring diversity in reasoning paths. Furthermore, our method utilizes sandbox execution feedback to locate erroneous algorithmic steps during generation, enabling timely and targeted corrections. Extensive experiments on four public code generation benchmarks demonstrate that RPM-MCTS outperforms current state-of-the-art methods while achieving an approximately 15% reduction in token consumption. Furthermore, full fine-tuning of the base model using the data constructed by RPM-MCTS significantly enhances its code capabilities.

</details>


### [90] [Semantic-KG: Using Knowledge Graphs to Construct Benchmarks for Measuring Semantic Similarity](https://arxiv.org/abs/2511.19925)
*Qiyao Wei,Edward Morrell,Lea Goetz,Mihaela van der Schaar*

Main category: cs.AI

TL;DR: 本文提出了一种利用知识图谱生成基准数据集的新方法，用于评估LLM输出的语义相似性方法，解决了现有基准在生成成本、领域适用性和等价定义方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前评估LLM开放形式文本响应的语义相似性方法可能更关注句法或词汇形式而非语义内容，现有基准存在高生成成本、领域适用性有限和等价定义不明确等问题。

Method: 利用知识图谱生成语义相似或不相似的自然语言陈述对，不相似对分为四种子类型，在四个不同领域生成基准数据集，并比较传统NLP评分和LLM作为评判者的语义相似性方法。

Result: 研究发现语义变化的子类型和基准领域都会影响语义相似性方法的性能，没有哪种方法始终表现最优。

Conclusion: 研究结果对使用LLM作为评判者检测文本语义内容具有重要启示，代码和数据集已公开。

Abstract: Evaluating the open-form textual responses generated by Large Language Models (LLMs) typically requires measuring the semantic similarity of the response to a (human generated) reference. However, there is evidence that current semantic similarity methods may capture syntactic or lexical forms over semantic content. While benchmarks exist for semantic equivalence, they often suffer from high generation costs due to reliance on subjective human judgment, limited availability for domain-specific applications, and unclear definitions of equivalence. This paper introduces a novel method for generating benchmarks to evaluate semantic similarity methods for LLM outputs, specifically addressing these limitations. Our approach leverages knowledge graphs (KGs) to generate pairs of natural-language statements that are semantically similar or dissimilar, with dissimilar pairs categorized into one of four sub-types. We generate benchmark datasets in four different domains (general knowledge, biomedicine, finance, biology), and conduct a comparative study of semantic similarity methods including traditional natural language processing scores and LLM-as-a-judge predictions. We observe that the sub-type of semantic variation, as well as the domain of the benchmark impact the performance of semantic similarity methods, with no method being consistently superior. Our results present important implications for the use of LLM-as-a-judge in detecting the semantic content of text. Code is available at https://github.com/QiyaoWei/semantic-kg and the dataset is available at https://huggingface.co/datasets/QiyaoWei/Semantic-KG.

</details>


### [91] [A System-Level Taxonomy of Failure Modes in Large Language Model Applications](https://arxiv.org/abs/2511.19933)
*Vaishali Vinay*

Main category: cs.AI

TL;DR: 本文提出了一个系统级分类法，识别了15种真实世界LLM应用中的隐藏故障模式，分析了评估与监控实践的差距，并探讨了LLM部署的生产挑战，最后提出了构建可靠、可维护和成本感知的LLM系统的高层设计原则。


<details>
  <summary>Details</summary>
Motivation: 随着LLM被快速集成到决策支持工具和自动化工作流中，其在生产环境中的行为仍未被充分理解，且其故障模式与传统机器学习模型有根本性差异。

Method: 提出了一个包含15种隐藏故障模式的系统级分类法，包括多步推理漂移、潜在不一致性、上下文边界退化、错误工具调用等，并分析了现有评估基准的局限性。

Result: 识别了LLM部署的生产挑战，包括可观测性限制、成本约束和更新引起的回归问题，揭示了评估与监控实践之间的差距。

Conclusion: 通过将LLM可靠性框架化为系统工程问题而非纯模型中心问题，为未来关于评估方法、AI系统鲁棒性和可靠LLM部署的研究提供了分析基础。

Abstract: Large language models (LLMs) are being rapidly integrated into decision-support tools, automation workflows, and AI-enabled software systems. However, their behavior in production environments remains poorly understood, and their failure patterns differ fundamentally from those of traditional machine learning models. This paper presents a system-level taxonomy of fifteen hidden failure modes that arise in real-world LLM applications, including multi-step reasoning drift, latent inconsistency, context-boundary degradation, incorrect tool invocation, version drift, and cost-driven performance collapse. Using this taxonomy, we analyze the growing gap in evaluation and monitoring practices: existing benchmarks measure knowledge or reasoning but provide little insight into stability, reproducibility, drift, or workflow integration. We further examine the production challenges associated with deploying LLMs - including observability limitations, cost constraints, and update-induced regressions - and outline high-level design principles for building reliable, maintainable, and cost-aware LLM systems. Finally, we outline high-level design principles for building reliable, maintainable, and cost-aware LLM-based systems. By framing LLM reliability as a system-engineering problem rather than a purely model-centric one, this work provides an analytical foundation for future research on evaluation methodology, AI system robustness, and dependable LLM deployment.

</details>


### [92] [M$^3$Prune: Hierarchical Communication Graph Pruning for Efficient Multi-Modal Multi-Agent Retrieval-Augmented Generation](https://arxiv.org/abs/2511.19969)
*Weizi Shao,Taolin Zhang,Zijie Zhou,Chen Chen,Chengyu Wang,Xiaofeng He*

Main category: cs.AI

TL;DR: 提出了M³Prune框架，通过剪枝多模态多智能体层次通信图中的冗余边，在保持任务性能的同时显著降低token开销和计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统存在显著的token开销和计算成本问题，限制了大规模部署。

Method: 采用多模态多智能体层次通信图剪枝框架，包括模态内图稀疏化、动态通信拓扑构建和渐进式冗余边剪枝。

Result: 在通用和领域特定mRAG基准测试中，该方法在显著减少token消耗的同时，持续优于单智能体和鲁棒多智能体mRAG系统。

Conclusion: M³Prune框架能够有效平衡任务性能和token开销，为多模态检索增强生成系统的大规模部署提供了可行方案。

Abstract: Recent advancements in multi-modal retrieval-augmented generation (mRAG), which enhance multi-modal large language models (MLLMs) with external knowledge, have demonstrated that the collective intelligence of multiple agents can significantly outperform a single model through effective communication. Despite impressive performance, existing multi-agent systems inherently incur substantial token overhead and increased computational costs, posing challenges for large-scale deployment. To address these issues, we propose a novel Multi-Modal Multi-agent hierarchical communication graph PRUNING framework, termed M$^3$Prune. Our framework eliminates redundant edges across different modalities, achieving an optimal balance between task performance and token overhead. Specifically, M$^3$Prune first applies intra-modal graph sparsification to textual and visual modalities, identifying the edges most critical for solving the task. Subsequently, we construct a dynamic communication topology using these key edges for inter-modal graph sparsification. Finally, we progressively prune redundant edges to obtain a more efficient and hierarchical topology. Extensive experiments on both general and domain-specific mRAG benchmarks demonstrate that our method consistently outperforms both single-agent and robust multi-agent mRAG systems while significantly reducing token consumption.

</details>


### [93] [Interactive AI NPCs Powered by LLMs: Technical Report for the CPDC Challenge 2025](https://arxiv.org/abs/2511.20200)
*Yitian Huang,Yuxuan Lei,Jianxun Lian,Hao Liao*

Main category: cs.AI

TL;DR: 团队MSRA_SC在CPDC 2025竞赛中提出统一框架，通过上下文工程和GRPO训练，在API和GPU赛道均取得优异成绩。


<details>
  <summary>Details</summary>
Motivation: 解决常识人物对话中的工具调用稳定性、执行可靠性和角色扮演指导问题，同时缓解小样本过拟合。

Method: 采用上下文工程（动态工具剪枝、人物剪裁、参数归一化、函数合并）和GRPO训练（用强化学习替代监督微调）。

Result: 最终排名：Task 2 API第1名，Task 1 API第2名，Task 3 API和GPU赛道均第3名。

Conclusion: 提出的简单有效框架在常识人物对话任务中表现出色，证明了方法的有效性。

Abstract: This report presents the solution and results of our team MSRA\_SC in the Commonsense Persona-Grounded Dialogue Challenge (CPDC 2025). We propose a simple yet effective framework that unifies improvements across both GPU Track and API Track. Our method centers on two key components. First, Context Engineering applies dynamic tool pruning and persona clipping for input compression, combined with post-processing techniques such as parameter normalization and function merging. Together with manually refined prompts, this design improves tool call stability, execution reliability, and role-playing guidance. Second, in the GPU Track, we further adopt GRPO training, replacing supervised fine-tuning with reinforcement learning directly optimized by reward signals. This mitigates small-sample overfitting and significantly enhances task-oriented dialogue performance. In the final evaluation, our team ranks 1st in Task 2 API, 2nd in Task 1 API, and 3rd in both Task 3 API and GPU track, demonstrating the effectiveness of our approach. Our code is publicly available at https://gitlab.aicrowd.com/nikoo_yu/cpdc-2025-winning-solution

</details>


### [94] [CostNav: A Navigation Benchmark for Cost-Aware Evaluation of Embodied Agents](https://arxiv.org/abs/2511.20216)
*Haebin Seong,Sungmin Kim,Minchan Kim,Yongjun Cho,Myunchul Joe,Suhwan Choi,Jaeyoon Jung,Jiyong Youn,Yoonshik Kim,Samwoo Seong,Yubeen Park,Youngjae Yu,Yunsung Lee*

Main category: cs.AI

TL;DR: CostNav是首个将导航性能与商业可行性结合的经济测试平台，通过成本收益分析揭示传统导航指标与商业部署之间的差距。


<details>
  <summary>Details</summary>
Motivation: 现有导航基准只关注任务成功率，忽视了商业部署的经济可行性，这对自动驾驶配送机器人的商业化至关重要。

Method: CostNav建立了一个微观导航经济测试平台，模拟完整的经济生命周期，包括硬件、训练、能源、维护成本和配送收入，使用行业参数进行成本收益分析。

Result: 基准测试显示43.0%的服务水平协议合规率，但商业上不可行：每次运行亏损30.009美元，无盈亏平衡点，其中碰撞导致的维护成本占99.7%。

Conclusion: CostNav填补了导航研究与商业部署之间的鸿沟，为评估基于规则的导航、模仿学习和成本感知强化学习提供了基础，使数据驱动的经济权衡决策成为可能。

Abstract: Existing navigation benchmarks focus on task success metrics while overlooking economic viability -- critical for commercial deployment of autonomous delivery robots. We introduce \emph{CostNav}, a \textbf{Micro-Navigation Economic Testbed} that evaluates embodied agents through comprehensive cost-revenue analysis aligned with real-world business operations. CostNav models the complete economic lifecycle including hardware, training, energy, maintenance costs, and delivery revenue with service-level agreements, using industry-derived parameters. \textbf{To our knowledge, CostNav is the first work to quantitatively expose the gap between navigation research metrics and commercial viability}, revealing that optimizing for task success fundamentally differs from optimizing for economic deployment. Our cost model uses parameters derived from industry data sources (energy rates, delivery service pricing), and we project from a reduced-scale simulation to realistic deliveries. Under this projection, the baseline achieves 43.0\% SLA compliance but is \emph{not} commercially viable: yielding a loss of \$30.009 per run with no finite break-even point, because operating costs are dominated by collision-induced maintenance, which accounts for 99.7\% of per-run costs and highlights collision avoidance as a key optimization target. We demonstrate a learning-based on-device navigation baseline and establish a foundation for evaluating rule-based navigation, imitation learning, and cost-aware RL training. CostNav bridges the gap between navigation research and commercial deployment, enabling data-driven decisions about economic trade-offs across navigation paradigms.

</details>


### [95] [Improving Language Agents through BREW](https://arxiv.org/abs/2511.20297)
*Shashank Kirtania,Param Biyani,Priyanshu Gupta,Yasharth Bajpai,Roshni Iyer,Sumit Gulwani,Gustavo Soares*

Main category: cs.AI

TL;DR: BREW框架通过构建和精炼经验学习知识库来优化LLM智能体，在保持计算效率的同时显著提升任务精度并减少API调用。


<details>
  <summary>Details</summary>
Motivation: 当前基于PPO和GRPO的智能体训练方法计算开销大，且生成的策略难以解释、适应或增量改进，需要更实用的优化方法。

Method: 引入BREW框架，通过知识库构建和精炼来优化智能体，采用有效的记忆分区方法提高检索效率，利用任务评分器和行为准则学习洞察，并通过状态空间搜索确保鲁棒性。

Result: 在OSWorld、τ²Bench和SpreadsheetBench等真实世界基准测试中，BREW实现了10-20%的任务精度提升，10-15%的API/工具调用减少，执行时间更快，同时保持与基础模型相当的计算效率。

Conclusion: BREW将知识库确立为模块化、可控的智能体优化基础，为行为塑造提供了透明、可解释和可扩展的明确机制。

Abstract: Large Language Model (LLM)-based agents are increasingly applied to tasks requiring structured reasoning, tool use, and environmental adaptation, such as data manipulation, multistep planning, and computer-use automation. However, despite their versatility, current training paradigms for model weight optimization methods, like PPO and GRPO, remain relatively impractical with their high computational overhead for rollout convergence. In addition, the resulting agent policies are difficult to interpret, adapt, or incrementally improve. To address this, we investigate creating and refining structured memory of experiential learning of an agent from its environment as an alternative route to agent optimization. We introduce BREW (Bootstrapping expeRientially-learned Environmental knoWledge), a framework for agent optimization for downstream tasks via KB construction and refinement. In our formulation, we introduce an effective method for partitioning agent memory for more efficient retrieval and refinement. BREW uses task graders and behavior rubrics to learn insights while leveraging state-space search for ensuring robustness from the noise and non-specificity in natural language. Empirical results on real world, domain-grounded benchmarks -- OSWorld, $τ^2$Bench, and SpreadsheetBench -- show BREW achieves $10-20\%$ improvement in task precision, $10-15\%$ reduction in API/tool calls leading to faster execution time, all while maintaining computational efficiency on par with base models. Unlike prior work where memory is treated as static context, we establish the KB as a modular and controllable substrate for agent optimization -- an explicit lever for shaping behavior in a transparent, interpretable, and extensible manner.

</details>


### [96] [Active Inference in Discrete State Spaces from First Principles](https://arxiv.org/abs/2511.20321)
*Patrick Kenny*

Main category: cs.AI

TL;DR: 本文澄清了主动推理与自由能原理的关系，表明在离散状态空间中实现主动推理的优化问题可以表述为约束散度最小化问题，无需依赖期望自由能概念。


<details>
  <summary>Details</summary>
Motivation: 澄清主动推理概念，将其与自由能原理区分开来，探索更基础的数学框架。

Method: 将主动推理优化问题重新表述为约束散度最小化问题，使用标准平均场方法求解，避免使用期望自由能概念。

Result: 提出的感知/行动散度准则在建模感知时与变分自由能一致，在建模行动时与期望自由能泛函相差一个熵正则化项。

Conclusion: 主动推理可以在更基础的数学框架下实现，无需依赖自由能原理，为理解主动推理提供了新的视角。

Abstract: We seek to clarify the concept of active inference by disentangling it from the Free Energy Principle. We show how the optimizations that need to be carried out in order to implement active inference in discrete state spaces can be formulated as constrained divergence minimization problems which can be solved by standard mean field methods that do not appeal to the idea of expected free energy. When it is used to model perception, the perception/action divergence criterion that we propose coincides with variational free energy. When it is used to model action, it differs from an expected free energy functional by an entropy regularizer.

</details>


### [97] [VibraVerse: A Large-Scale Geometry-Acoustics Alignment Dataset for Physically-Consistent Multimodal Learning](https://arxiv.org/abs/2511.20422)
*Bo Pang,Chenxi Xu,Jierui Ren,Guoping Wang,Sheng Li*

Main category: cs.AI

TL;DR: VibraVerse是一个大规模几何-声学对齐数据集，通过显式连接3D几何→物理属性→模态参数→声学信号的因果链，实现物理一致的多模态学习。


<details>
  <summary>Details</summary>
Motivation: 现有基于视觉和语言的多模态学习框架缺乏物理一致性，忽略了物体几何、材料、振动模式和产生声音之间的内在因果关系。

Method: 引入CLASP对比学习框架进行跨模态对齐，保持物体物理结构与其声学响应之间的因果对应关系，确保每个样本在形状、图像和声音的统一表示空间中具有物理一致性。

Result: 在几何到声音预测、声音引导形状重建和跨模态表示学习等基准任务上的广泛验证表明，在VibraVerse上训练的模型在不同模态间表现出更高的准确性、可解释性和泛化能力。

Conclusion: VibraVerse为物理一致和因果可解释的多模态学习建立了基准，为声音引导的具身感知和物理世界的深入理解奠定了基础。

Abstract: Understanding the physical world requires perceptual models grounded in physical laws rather than mere statistical correlations. However, existing multimodal learning frameworks, focused on vision and language, lack physical consistency and overlook the intrinsic causal relationships among an object's geometry, material, vibration modes, and the sounds it produces. We introduce VibraVerse, a large-scale geometry-acoustics alignment dataset that explicitly bridges the causal chain from 3D geometry -> physical attributes -> modal parameters -> acoustic signals. Each 3D model has explicit physical properties (density, Young's modulus, Poisson's ratio) and volumetric geometry, from which modal eigenfrequencies and eigenvectors are computed for impact sound synthesis under controlled excitations. To establish this coherence, we introduce CLASP, a contrastive learning framework for cross-modal alignment that preserves the causal correspondence between an object's physical structure and its acoustic response. This framework enforces physically consistent alignment across modalities, ensuring that every sample is coherent, traceable to the governing equations, and embedded within a unified representation space spanning shape, image, and sound. Built upon VibraVerse, we define a suite of benchmark tasks for geometry-to-sound prediction, sound-guided shape reconstruction, and cross-modal representation learning. Extensive validations on these tasks demonstrate that models trained on VibraVerse exhibit superior accuracy, interpretability, and generalization across modalities. These results establish VibraVerse as a benchmark for physically consistent and causally interpretable multimodal learning, providing a foundation for sound-guided embodied perception and a deeper understanding of the physical world. The dataset will be open-sourced.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [98] [Can You Keep Calm?: Adaptive Gameplay using Heart Rate as a Controller](https://arxiv.org/abs/2511.19934)
*Md Mosharaf Hossan,Rifat Ara Tasnim,Farjana Z Eishita*

Main category: cs.HC

TL;DR: 本研究开发了一款心率控制的游戏，通过生物反馈调整游戏玩法来帮助玩家管理压力，实验表明心率控制游戏能减少负面情绪、增加正面情绪，并降低心脏反应性。


<details>
  <summary>Details</summary>
Motivation: 利用配备传感器的智能手机和智能手表等手持设备，通过心率控制的游戏为心理健康干预提供可访问且吸引人的治疗环境。

Method: 开发心率控制游戏，通过生物反馈调整游戏玩法，并进行对照实验评估心率控制对玩家体验和压力管理的影响。

Result: 心率控制游戏减少了负面情绪，增加了正面情绪，玩家在心率自适应目标游戏玩法中表现出相对较低的心脏反应性。

Conclusion: 基于生物反馈的游戏化数字环境在支持可访问的心理健康支持方面具有广阔前景。

Abstract: Serious games for health are designed with specific health objectives and are increasingly being used in mental health interventions. Leveraging sensor equipped handheld devices such as smartphones and smartwatches, these games can provide accessible and engaging therapeutic environments. This study introduces a heart rate (HR) controlled game to aid players manage stress by adjusting gameplay according to their biometric feedback. We aimed to determine how HR-based controls influence their experience and if it can be used to reduce stress. Findings from a controlled experiment revealed that HR controlled gameplay reduced negative and increased positive emotions. Also, players exhibited relatively less cardiac reactivity in HR adaptive target based gameplay. This highlights the promise of biometric feedback based gamified digital environments in supporting accessible mental health support.

</details>


### [99] [Editing with AI: How Doctors Refine LLM-Generated Answers to Patient Queries](https://arxiv.org/abs/2511.19940)
*Rahul Sharma,Pragnya Ramjee,Kaushik Murali,Mohit Jain*

Main category: cs.HC

TL;DR: 本研究探讨了眼科医生使用LLM生成回复草稿时的编辑行为，发现虽然LLM输出通常准确，但需要人工监督来纠正错误和自动化偏见。情境化是主要的编辑形式，而不同的编辑工作流程在准确性和工作量之间存在权衡。


<details>
  <summary>Details</summary>
Motivation: 随着数字患者信息量的增加给医疗系统带来压力，LLM有望为临床医生生成回复草稿，但医生如何完善这些草稿仍未被充分探索。

Method: 采用混合方法研究，让9名眼科医生回答144个白内障手术问题，涵盖三种条件：从头开始书写、直接编辑LLM草稿和基于指令的间接编辑。

Result: LLM输出通常准确，但偶尔的错误和自动化偏见显示需要人工监督。情境化是主要的编辑形式。间接编辑减少了工作量但引入了错误，直接编辑确保了精确性但工作量更高。

Conclusion: 为构建安全、可扩展的LLM辅助临床通信系统提供了设计和政策建议。

Abstract: Patients frequently seek information during their medical journeys, but the rising volume of digital patient messages has strained healthcare systems. Large language models (LLMs) offer promise in generating draft responses for clinicians, yet how physicians refine these drafts remains underexplored. We present a mixed-methods study with nine ophthalmologists answering 144 cataract surgery questions across three conditions: writing from scratch, directly editing LLM drafts, and instruction-based indirect editing. Our quantitative and qualitative analyses reveal that while LLM outputs were generally accurate, occasional errors and automation bias revealed the need for human oversight. Contextualization--adapting generic answers to local practices and patient expectations--emerged as a dominant form of editing. Editing workflows revealed trade-offs: indirect editing reduced effort but introduced errors, while direct editing ensured precision but with higher workload. We conclude with design and policy implications for building safe, scalable LLM-assisted clinical communication systems.

</details>


### [100] [GUIDAETA - A Versatile Interactions Dataset with extensive Context Information and Metadata](https://arxiv.org/abs/2511.20328)
*Stefan Lengauer,Sarah Annabelle Von Götz,Marie-Therese Hoesch,Florian Dieter Steinwidder,Mariia Tytarenko,Michael Bedek,Tobias Schreck*

Main category: cs.HC

TL;DR: 本文提出了GUIDAETA数据集，这是一个大规模引导式交互数据集，包含250多名用户完成716个信息检索任务的数据，记录了239万次鼠标和键盘事件，总观察时间近50小时，并包含丰富的上下文信息和元数据。


<details>
  <summary>Details</summary>
Motivation: 由于交互数据在认知科学、可视化、人机交互等多个领域应用广泛，但公开可用的数据集数量有限且适用性受限，因此需要构建一个大规模、结构化的引导式交互数据集。

Method: 通过大规模引导用户研究收集数据，250多名用户使用定制消费信息系统完成三个预定义信息检索任务，记录鼠标键盘事件、小部件信息、系统事件和相关显示内容。

Result: 创建了包含716个完成任务、239万次交互事件（其中鼠标235万次、键盘4万次）的数据集，总观察时间近50小时，并包含用户社会人口学数据和反馈问卷等元数据。

Conclusion: GUIDAETA是一个多功能数据集，具有丰富的上下文信息和元数据，适用于各种研究领域和目的，填补了现有公开数据集在规模和适用性方面的不足。

Abstract: Interaction data is widely used in multiple domains such as cognitive science, visualization, human computer interaction, and cybersecurity, among others. Applications range from cognitive analyses over user/behavior modeling, adaptation, recommendations, to (user/bot) identification/verification. That is, research on these applications - in particular those relying on learned models - require copious amounts of structured data for both training and evaluation. Different application domains thereby impose different requirements. I.e., for some purposes it is vital that the data is based on a guided interaction process, meaning that monitored subjects pursued a given task, while other purposes require additional context information, such as widget interactions or metadata. Unfortunately, the amount of publicly available datasets is small and their respective applicability for specific purposes limited. We present GUIDEd Interaction DATA (GUIDAETA) - a new dataset, collected from a large-scale guided user study with more than 250 users, each working on three pre-defined information retrieval tasks using a custom-built consumer information system. Besides being larger than most comparable datasets - with 716 completed tasks, 2.39 million mouse and keyboard events (2.35 million and 40 thousand, respectively) and a total observation period of almost 50 hours - its interactions exhibit encompassing context information in the form of widget information, triggered (system) events and associated displayed content. Combined with extensive metadata such as sociodemographic user data and answers to explicit feedback questionnaires (regarding perceived usability, experienced cognitive load, pre-knowledge on the information system's topic), GUIDAETA constitutes a versatile dataset, applicable for various research domains and purposes.

</details>
