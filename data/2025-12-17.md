<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 25]
- [cs.LG](#cs.LG) [Total: 49]
- [cs.HC](#cs.HC) [Total: 6]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Leveraging LLMs for Structured Data Extraction from Unstructured Patient Records](https://arxiv.org/abs/2512.13700)
*Mitchell A. Klusty,Elizabeth C. Solie,Caroline N. Leach,W. Vaiden Logan,Lynnet E. Richey,John C. Gensel,David P. Szczykutowicz,Bryan C. McLellan,Emily B. Collier,Samuel E. Armstrong,V. K. Cody Bumgardner*

Main category: cs.AI

TL;DR: 提出一个基于本地部署大语言模型的自动化临床特征提取框架，用于从电子健康记录中提取结构化信息，减少人工病历审查负担


<details>
  <summary>Details</summary>
Motivation: 人工病历审查耗时耗力，需要专家从非结构化的电子健康记录中提取复杂信息，限制了临床研究的效率

Method: 开发了一个安全、模块化的框架，在符合HIPAA标准的本地计算基础设施上部署大语言模型，结合检索增强生成和结构化响应方法，提供可扩展的容器化解决方案

Result: 该框架在多个医学特征提取任务中实现了高准确率，与专家标注数据集相比表现良好，甚至发现了人工审查中遗漏的标注错误

Conclusion: 该框架展示了LLM系统通过自动化提取减少人工病历审查负担的潜力，能够提高数据捕获的一致性，加速临床研究进程

Abstract: Manual chart review remains an extremely time-consuming and resource-intensive component of clinical research, requiring experts to extract often complex information from unstructured electronic health record (EHR) narratives. We present a secure, modular framework for automated structured feature extraction from clinical notes leveraging locally deployed large language models (LLMs) on institutionally approved, Health Insurance Portability and Accountability Act (HIPPA)-compliant compute infrastructure. This system integrates retrieval augmented generation (RAG) and structured response methods of LLMs into a widely deployable and scalable container to provide feature extraction for diverse clinical domains. In evaluation, the framework achieved high accuracy across multiple medical characteristics present in large bodies of patient notes when compared against an expert-annotated dataset and identified several annotation errors missed in manual review. This framework demonstrates the potential of LLM systems to reduce the burden of manual chart review through automated extraction and increase consistency in data capture, accelerating clinical research.

</details>


### [2] [Blind Radio Mapping via Spatially Regularized Bayesian Trajectory Inference](https://arxiv.org/abs/2512.13701)
*Zheng Xing,Junting Chen*

Main category: cs.AI

TL;DR: 提出了一种无需位置标签的盲无线电地图构建框架，利用MIMO-OFDM信道测量推断用户轨迹，在室内环境中实现高精度定位和波束图重建。


<details>
  <summary>Details</summary>
Motivation: 传统无线电地图构建方法需要大量位置标记数据，成本高且在实际场景中不实用。本文旨在开发一种无需位置标签的盲构建方法，解决实际部署中的成本和技术限制问题。

Method: 首先证明了在准镜面环境模型下，非视距信道状态信息具有空间连续性，推导出与物理距离成比例的CSI距离度量。针对泊松分布AP部署中的直线轨迹，证明了即使在角度分辨率较差的情况下，定位误差的克拉美罗下界也能渐近消失。基于这些理论结果，开发了一个空间正则化贝叶斯推理框架，联合估计信道特征、区分视距/非视距条件并恢复用户轨迹。

Result: 在射线追踪数据集上的实验表明，平均定位误差为0.68米，波束图重建误差为3.3%，验证了所提出的盲映射方法的有效性。

Conclusion: 该盲无线电地图构建框架能够在无需位置标签的情况下，仅利用MIMO-OFDM信道测量实现高精度的用户轨迹推断和波束图重建，为智能无线应用提供了实用且成本效益高的解决方案。

Abstract: Radio maps enable intelligent wireless applications by capturing the spatial distribution of channel characteristics. However, conventional construction methods demand extensive location-labeled data, which are costly and impractical in many real-world scenarios. This paper presents a blind radio map construction framework that infers user trajectories from indoor multiple-input multiple-output (MIMO)-Orthogonal Frequency-Division Multiplexing (OFDM) channel measurements without relying on location labels. It first proves that channel state information (CSI) under non-line-of-sight (NLOS) exhibits spatial continuity under a quasi-specular environmental model, allowing the derivation of a CSI-distance metric that is proportional to the corresponding physical distance. For rectilinear trajectories in Poisson-distributed access point (AP) deployments, it is shown that the Cramer-Rao Lower Bound (CRLB) of localization error vanishes asymptotically, even under poor angular resolution. Building on these theoretical results, a spatially regularized Bayesian inference framework is developed that jointly estimates channel features, distinguishes line-of-sight (LOS)/NLOS conditions and recovers user trajectories. Experiments on a ray-tracing dataset demonstrate an average localization error of 0.68 m and a beam map reconstruction error of 3.3%, validating the effectiveness of the proposed blind mapping method.

</details>


### [3] [Adjudicator: Correcting Noisy Labels with a KG-Informed Council of LLM Agents](https://arxiv.org/abs/2512.13704)
*Doohee You,Sundeep Paul*

Main category: cs.AI

TL;DR: Adjudicator系统通过构建知识图谱和多智能体LLM架构，自动识别和纠正训练数据中的噪声标签，在工业应用中实现高精度数据验证。


<details>
  <summary>Details</summary>
Motivation: 生产机器学习系统的性能受限于训练数据质量，高风险的工业应用中噪声标签会降低性能并损害用户信任，需要自动化的标签噪声识别和纠正方案。

Method: 采用神经符号方法：1) 构建动态知识图谱统一项目上下文；2) 设计"智能体委员会"多智能体LLM架构，让专门化的智能体通过辩论和投票判断标签有效性；3) 引入基于知识图谱的覆盖逻辑来识别复杂结构性错误。

Result: 在AlleNoise基准的1000项平衡子集上，知识图谱增强模型达到0.99 F1分数，显著优于单LLM基线(0.48 F1)和非知识图谱委员会(0.59 F1)。系统通过知识图谱实现了对复杂结构性错误的完全召回和完美识别。

Conclusion: Adjudicator提供了一个稳健且可解释的自动化高精度数据验证系统，为严格监管的工业环境中生成黄金数据集提供了重要概念验证。

Abstract: The performance of production machine learning systems is fundamentally limited by the quality of their training data. In high-stakes industrial applications, noisy labels can degrade performance and erode user trust. This paper presents Adjudicator, a system that addresses the critical data mining challenge of automatically identifying and correcting label noise and has been validated for production deployment. Adjudicator models this as a neuro-symbolic task, first constructing a dynamic Knowledge Graph (KG) to unify item context. This KG then informs a "Council of Agents," a novel multi-agent Large Language Model architecture where specialized agents debate and vote on a label's validity. We validate our system on a 1,000-item balanced subset of the AlleNoise benchmark. Our KG-informed model achieves a 0.99 F1-score, significantly outperforming a single-LLM baseline (0.48 F1) and a non-KG council (0.59 F1). Our analysis reveals this is due to a Precision, achieved by a novel override logic that uses the KG to perfectly identify complex, structural errors (complete Recall) -- a class of errors that baselines fail to find. This result demonstrates a robust and explainable system for automated, high-precision data verification, serving as a vital proof-of-concept for generating golden datasets in strictly governed industrial environments.

</details>


### [4] [AI-Powered Annotation Pipelines for Stabilizing Large Language Models: A Human-AI Synergy Approach](https://arxiv.org/abs/2512.13714)
*Gangesh Pathak,Prasanna Kumar*

Main category: cs.AI

TL;DR: 本文提出了一种基于AI的标注流程，用于系统识别、标记和修复LLM输出中的不稳定性模式，通过人机协同方法结合自动弱监督和基于置信度的标注，旨在解决LLM在高度监管行业中的可靠性问题。


<details>
  <summary>Details</summary>
Motivation: LLM在高度监管行业中的应用面临不稳定性、推理不一致、幻觉和性能波动等问题，特别是在工作流程中。当前的稳定化方法如RLHF和监督微调虽然能提供可量化的改进，但成本高昂且依赖大量人工标注，难以可持续扩展。

Method: 提出基于AI的标注流程，采用人机协同方法结合自动弱监督模型和基于置信度的标注，辅以目标人工验证。引入语义一致性、事实正确性和逻辑连贯性等稳定性特定标注类别，通过反馈循环实现模型的持续校准和鲁棒性增强。

Result: 该方法能够系统性地识别和修复LLM输出中的不稳定性模式，通过人机协同确保反馈信息的可靠性和道德完整性，为LLM在需要事实精确性和行为一致性的领域提供更可靠的稳定化解决方案。

Conclusion: 提出的AI标注流程为人机协同的LLM稳定化提供了可持续的解决方案，通过系统性的不稳定性模式识别和修复，结合自动化与人工验证，有望解决当前稳定化方法成本高、难以扩展的问题，提升LLM在高度监管行业中的可靠性。

Abstract: LLM implementations are failing in highly regulated industries owing to instability issues, inconsistent reasoning, hallucinations and performance variability, especially in workflows. These reliability issues restrict safe use of LLM in areas that need the precision of facts and consistent behavior (Aiyappa et al., 2023). The current methods of stabilization, such as, reinforcement learning with human feedback (RLHF) and supervised fine-tuning, offer quantifiable improvements but are expensive and based on the intensive annotation of humans, thus being not easily scaled in a sustainable way (Dong et al., 2023; Retzlaff et al., 2024). This paper presents an AI-based annotation pipeline that systematically identifies, labels, and fixes for instability patterns on LLM output. Our human-AI synergy method combines the models of automated weak supervision and confidence-based annotation with the target human validation to guarantee the reliability and moral uprightness of feedback information (Cabitza et al., 2023; Jiang et al., 2023). The semantic consistency, factual correctness, and logical coherence categories of stability-specific annotation are introduced into our framework, allowing the continuous calibration of models and the enhancement of their robustness based on the feedback loops (Honovich et al., 2021; Nan et al., 2021).

</details>


### [5] [Compressed Causal Reasoning: Quantization and GraphRAG Effects on Interventional and Counterfactual Accuracy](https://arxiv.org/abs/2512.13725)
*Steve Nwaiwu,Nipat Jongsawat,Anucha Tungkasthan*

Main category: cs.AI

TL;DR: 研究首次系统评估量化（INT8和NF4）对LLM因果推理能力的影响，发现4位量化下因果推理意外稳健，干预查询最敏感，而现有反事实基准未能揭示量化导致的推理漂移。


<details>
  <summary>Details</summary>
Motivation: 随着大模型部署向边缘和资源受限环境转移，量化模型成为标准，但精度降低对形式化因果推理的影响尚不清楚。本研究旨在填补这一空白，首次系统评估量化对Pearl因果阶梯三个层次的影响。

Method: 使用3000个样本的分层CLadder基准，评估Llama 3 8B在量化前后的表现；在CRASS基准上测试；采用图检索增强生成（Graph RAG）结合真实因果图进行增强评估。

Result: NF4量化下总体退化小于1%；干预查询对精度损失最敏感；反事实推理相对稳定但在特定查询类型上存在异质性弱点；CRASS基准上各精度表现几乎相同；图RAG使NF4干预准确率提升1.7%。

Conclusion: 因果推理对4位量化意外稳健；图结构增强可选择性强化干预推理；现有反事实基准未能捕捉深层因果脆弱性；为部署高效且结构支持的因果AI系统提供实用指导。

Abstract: Causal reasoning in Large Language Models spanning association, intervention, and counterfactual inference is essential for reliable decision making in high stakes settings. As deployment shifts toward edge and resource constrained environments, quantized models such as INT8 and NF4 are becoming standard. Yet the impact of precision reduction on formal causal reasoning is poorly understood. To our knowledge, this is the first study to systematically evaluate quantization effects across all three levels of Pearls Causal Ladder. Using a 3000 sample stratified CLadder benchmark, we find that rung level accuracy in Llama 3 8B remains broadly stable under quantization, with NF4 showing less than one percent overall degradation. Interventional queries at rung 2 are the most sensitive to precision loss, whereas counterfactual reasoning at rung 3 is comparatively stable but exhibits heterogeneous weaknesses across query types such as collider bias and backdoor adjustment. Experiments on the CRASS benchmark show near identical performance across precisions, indicating that existing commonsense counterfactual datasets lack the structural sensitivity needed to reveal quantization induced reasoning drift. We further evaluate Graph Retrieval Augmented Generation using ground truth causal graphs and observe a consistent improvement in NF4 interventional accuracy of plus 1.7 percent, partially offsetting compression related degradation. These results suggest that causal reasoning is unexpectedly robust to four bit quantization, graph structured augmentation can selectively reinforce interventional reasoning, and current counterfactual benchmarks fail to capture deeper causal brittleness. This work provides an initial empirical map of compressed causal reasoning and practical guidance for deploying efficient and structurally supported causal AI systems.

</details>


### [6] [State-Dependent Refusal and Learned Incapacity in RLHF-Aligned Language Models](https://arxiv.org/abs/2512.13762)
*TK Lee*

Main category: cs.AI

TL;DR: 该研究提出了一种定性案例研究方法，用于审计大语言模型在长时程交互中的行为选择性，发现同一模型在非敏感领域表现正常，但在政策敏感领域反复出现功能性拒绝，表现出行为不对称性。


<details>
  <summary>Details</summary>
Motivation: 虽然大语言模型被广泛部署为通用工具，但标准定量基准测试无法捕捉到长期交互中可能出现的特定行为模式。研究者希望开发一种定性审计方法来揭示模型在政策相关领域的行为选择性。

Method: 采用定性案例研究方法，通过一个86轮对话会话进行审计。定义了三种响应机制：正常表现（NP）、功能性拒绝（FR）和元叙事（MN），并观察这些机制在不同领域的分布模式。

Result: 研究发现同一模型在广泛非敏感领域表现正常，但在提供商或政策敏感领域反复产生功能性拒绝，形成了NP和FR之间的系统性不对称。元叙事角色框架叙述倾向于与敏感情境中的拒绝同时出现。

Conclusion: 该研究提出了基于可观察行为的交互层面审计框架，并引入"习得性无能"作为描述选择性保留行为的概念，为研究潜在对齐副作用提供了新视角，值得在不同用户和模型中进行进一步调查。

Abstract: Large language models (LLMs) are widely deployed as general-purpose tools, yet extended interaction can reveal behavioral patterns not captured by standard quantitative benchmarks. We present a qualitative case-study methodology for auditing policy-linked behavioral selectivity in long-horizon interaction. In a single 86-turn dialogue session, the same model shows Normal Performance (NP) in broad, non-sensitive domains while repeatedly producing Functional Refusal (FR) in provider- or policy-sensitive domains, yielding a consistent asymmetry between NP and FR across domains. Drawing on learned helplessness as an analogy, we introduce learned incapacity (LI) as a behavioral descriptor for this selective withholding without implying intentionality or internal mechanisms. We operationalize three response regimes (NP, FR, Meta-Narrative; MN) and show that MN role-framing narratives tend to co-occur with refusals in the same sensitive contexts. Overall, the study proposes an interaction-level auditing framework based on observable behavior and motivates LI as a lens for examining potential alignment side effects, warranting further investigation across users and models.

</details>


### [7] [Mathematics and Coding are Universal AI Benchmarks](https://arxiv.org/abs/2512.13764)
*Przemyslaw Chojecki*

Main category: cs.AI

TL;DR: 该论文研究了数学和编程在AI智能体心理测量电池模空间中的特殊作用，证明了数学定理证明和编程任务生成的电池子空间在评估度量下是稠密的，其中编程具有普遍性，而纯数学则具有谱稳定性优势。


<details>
  <summary>Details</summary>
Motivation: 研究数学和编程在AI智能体心理评估中的特殊地位，探索它们作为"通用坐标"的潜力，以及形式化数学如何成为高级AI智能体递归自我改进的自然点火域。

Method: 基于AAI框架和GVU动力学，定义数学纤维，结合形式证明核（如Lean、Coq），分析GVU流在数学纤维上的谱稳定性。主要技术结果是密度定理：在智能体输出均匀紧致和AAI泛函Lipschitz条件下，数学定理证明和编程任务生成的电池子空间在评估度量下是稠密的。

Result: 证明了编程单独具有普遍性，而纯数学则不具有表达普遍性，但其优势在于谱稳定性。数学纤维与形式证明核配对时，GVU流允许谱稳定的自我改进机制，这源于类似神谕的验证过程。

Conclusion: 数学和编程为AI智能体评估提供了"通用坐标"，形式化数学是高级AI智能体递归自我改进的自然点火域，其特权是谱稳定性而非表达普遍性。

Abstract: We study the special role of mathematics and coding inside the moduli space of psychometric batteries for AI agents. Building on the AAI framework and GVU dynamics from previous works, we define the Mathematics Fiber and show that, when paired with formal proof kernels (e.g. Lean, Coq), GVU flows on this fiber admit spectrally stable self-improvement regimes due to oracle-like verification. Our main technical result is a density theorem: under uniform tightness of agent outputs and a Lipschitz AAI functional, the subspace of batteries generated by mathematical theorem-proving and coding tasks is dense in the moduli space of batteries with respect to the evaluation metric. Coding alone is universal in this sense, while pure mathematics is not; its privilege is spectral rather than expressive. We interpret this as evidence that mathematics and coding provide ``universal coordinates'' for evaluation, and that formal mathematics is a natural ignition domain for recursive self-improvement in advanced AI agents.

</details>


### [8] [Semantic Grounding Index: Geometric Bounds on Context Engagement in RAG Systems](https://arxiv.org/abs/2512.13771)
*Javier Marín*

Main category: cs.AI

TL;DR: 该论文提出了语义接地指数(SGI)，用于检测RAG系统中的幻觉现象，发现幻觉回答在嵌入空间中更接近问题而非检索到的上下文，这种现象被称为"语义懒惰"。


<details>
  <summary>Details</summary>
Motivation: 研究RAG系统产生幻觉时在嵌入空间中留下的几何痕迹，开发一种计算高效、理论基础的指标来识别需要验证的响应。

Method: 引入语义接地指数(SGI)，定义为响应到问题与响应到上下文在单位超球面上的角度距离比值。基于球面三角不等式推导理论预测，并在HaluEval数据集(n=5,000)上使用五种嵌入模型进行验证。

Result: 发现大效应量(Cohen's d=0.92-1.28)，跨模型相关性高(r=0.85)。SGI的判别能力随问题-上下文角度分离增加而增强，从d=0.61到d=1.27，AUC从0.72提升到0.83。在长响应(d=2.05)和短问题(d=1.22)上表现优异，校准误差ECE=0.10。

Conclusion: SGI为生产RAG部署提供了计算高效、理论基础的检测工具，但仅测量主题参与度而非事实准确性(TruthfulQA上AUC=0.478)。

Abstract: When retrieval-augmented generation (RAG) systems hallucinate, what geometric trace does this leave in embedding space? We introduce the Semantic Grounding Index (SGI), defined as the ratio of angular distances from the response to the question versus the context on the unit hypersphere $\mathbb{S}^{d-1}$.Our central finding is \emph{semantic laziness}: hallucinated responses remain angularly proximate to questions rather than departing toward retrieved contexts. On HaluEval ($n$=5,000), we observe large effect sizes (Cohen's $d$ ranging from 0.92 to 1.28) across five embedding models with mean cross-model correlation $r$=0.85. Crucially, we derive from the spherical triangle inequality that SGI's discriminative power should increase with question-context angular separation $θ(q,c)$-a theoretical prediction confirmed empirically: effect size rises monotonically from $d$=0.61 -low $θ(q,c)$, to $d$=1.27 -high $θ(q,c)$, with AUC improving from 0.72 to 0.83. Subgroup analysis reveals that SGI excels on long responses ($d$=2.05) and short questions ($d$=1.22), while remaining robust across context lengths. Calibration analysis yields ECE=0.10, indicating SGI scores can serve as probability estimates, not merely rankings. A critical negative result on TruthfulQA (AUC=0.478) establishes that angular geometry measures topical engagement rather than factual accuracy. SGI provides computationally efficient, theoretically grounded infrastructure for identifying responses that warrant verification in production RAG deployments.

</details>


### [9] [MURIM: Multidimensional Reputation-based Incentive Mechanism for Federated Learning](https://arxiv.org/abs/2512.13955)
*Sindhuja Madabushi,Dawood Wasif,Jin-Hee Cho*

Main category: cs.AI

TL;DR: MURIM是一个基于多维信誉的联邦学习激励机制，通过综合考虑客户端可靠性、隐私、资源容量和公平性，防止恶意客户端获得不当奖励，提升联邦学习的整体性能。


<details>
  <summary>Details</summary>
Motivation: 联邦学习面临客户端激励不足、隐私风险、资源约束等关键挑战，需要评估客户端可靠性以实现公平激励分配，确保每个客户端的数据对全局模型做出有意义的贡献。

Method: 提出MURIM多维信誉激励机制，基于客户端贡献、延迟和信誉分配激励，配备可靠性验证模块，联合考虑客户端可靠性、隐私、资源容量和公平性。

Result: 在MNIST、FMNIST和ADULT Income数据集上的实验显示，MURIM在公平性指标上提升达18%，隐私攻击成功率降低5-9%，对投毒和噪声梯度攻击的鲁棒性提升达85%。

Conclusion: MURIM有效缓解对抗威胁，促进公平真实的参与，在异构动态联邦学习环境中保持稳定的模型收敛，为联邦学习提供可靠的激励机制。

Abstract: Federated Learning (FL) has emerged as a leading privacy-preserving machine learning paradigm, enabling participants to share model updates instead of raw data. However, FL continues to face key challenges, including weak client incentives, privacy risks, and resource constraints. Assessing client reliability is essential for fair incentive allocation and ensuring that each client's data contributes meaningfully to the global model. To this end, we propose MURIM, a MUlti-dimensional Reputation-based Incentive Mechanism that jointly considers client reliability, privacy, resource capacity, and fairness while preventing malicious or unreliable clients from earning undeserved rewards. MURIM allocates incentives based on client contribution, latency, and reputation, supported by a reliability verification module. Extensive experiments on MNIST, FMNIST, and ADULT Income datasets demonstrate that MURIM achieves up to 18% improvement in fairness metrics, reduces privacy attack success rates by 5-9%, and improves robustness against poisoning and noisy-gradient attacks by up to 85% compared to state-of-the-art baselines. Overall, MURIM effectively mitigates adversarial threats, promotes fair and truthful participation, and preserves stable model convergence across heterogeneous and dynamic federated settings.

</details>


### [10] [Evaluating Frontier LLMs on PhD-Level Mathematical Reasoning: A Benchmark on a Textbook in Theoretical Computer Science about Randomized Algorithms](https://arxiv.org/abs/2512.13978)
*Yang Cao,Yubin Chen,Xuyang Guo,Zhao Song,Song Yue,Jiahao Zhang,Jiale Zhao*

Main category: cs.AI

TL;DR: 该论文对GPT-5-Thinking、Gemini-3-Pro、Claude-Sonnet-4.5-Thinking和Grok-4四个前沿大语言模型在《随机算法》研究生教材上的形式化证明能力进行了基准测试，发现顶级模型准确率约66%，但模型间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学推理和科学发现方面取得了显著进展，但缺乏对这些模型在经典研究生数学理论上的严格评估。需要了解这些模型在规范数学推理任务上的基线能力。

Method: 使用Motwani和Raghavan的《随机算法》教材作为测试基准，要求四个前沿模型（GPT-5-Thinking、Gemini-3-Pro、Claude-Sonnet-4.5-Thinking、Grok-4）为教材中的引理和练习生成形式化的LaTeX证明。

Result: 顶级模型（Gemini和Claude）准确率约66%，表现出对概率方法和形式逻辑的良好掌握；其他模型一致性较差，准确率约40%。定性分析显示在简洁性、幻觉率和逻辑结构方面存在差异。

Conclusion: 前沿模型已达到适合研究生教学辅助和形式化任务的熟练度阈值，但在严格数学推导的可靠性方面存在显著差异。代码和所有LLM生成响应已开源。

Abstract: The rapid advancement of large language models (LLMs) has led to significant breakthroughs in automated mathematical reasoning and scientific discovery. Georgiev, G${ó}$mez-Serrano, Tao, and Wagner [GGSTW+25] demonstrate that AI systems can explore new constructions and improve existing bounds, illustrating the growing potential of LLMs to accelerate mathematical discovery. Similarly, Bubeck et al. [BCE+25] show that GPT-5 can meaningfully contribute to scientific workflows, from proposing hypotheses to generating proofs and analyses. Despite these advances, a rigorous evaluation of these models on canonical, graduate-level mathematical theory remains necessary to understand their baseline reasoning capabilities. In this paper, we present a comprehensive benchmark of four frontier models: GPT-5-Thinking, Gemini-3-Pro, Claude-Sonnet-4.5-Thinking, and Grok-4 against the classic curriculum of Randomized Algorithms by Motwani and Raghavan [MR95].
  We tasked each model with generating formal LaTeX proofs for a series of lemmas and exercises spanning the textbook. We find that while the top-tier models (Gemini, and Claude) achieve a high accuracy rate (approx. 66%), demonstrating a robust grasp of probabilistic method and formal logic, other models lag significantly in consistency (approx. 40%). We provide a qualitative analysis of the generated proofs, highlighting differences in conciseness, hallucination rates, and logical structure. Our results suggest that while frontier models have reached a threshold of proficiency suitable for graduate-level pedagogical assistance and formalization, significant variance exists in their reliability for rigorous mathematical derivation. The code and the full set of LLM-generated responses are open-sourced and publicly available at https://github.com/magiclinux/math_benchmark_probability.

</details>


### [11] [ReflCtrl: Controlling LLM Reflection via Representation Engineering](https://arxiv.org/abs/2512.13979)
*Ge Yan,Chung-En Sun,Tsui-Wei,Weng*

Main category: cs.AI

TL;DR: ReflCtrl框架通过表征工程控制LLM的自我反思频率，在保持性能的同时减少推理成本，发现反思与模型内部不确定性相关。


<details>
  <summary>Details</summary>
Motivation: 虽然带有思维链推理的大语言模型通过自我反思提升了性能，但反思过程显著增加了推理成本。本研究旨在探索如何控制反思频率以优化推理效率。

Method: 通过表征工程方法，将模型推理过程分段，识别反思步骤，在潜在空间中提取控制反思行为的方向向量，提出名为ReflCtrl的逐步引导方法控制反思频率。

Result: 实验表明：(1) 在许多情况下反思是冗余的，特别是在更强的模型中，最多可节省33.6%的推理token同时保持性能；(2) 模型的反思行为与内部不确定性信号高度相关。

Conclusion: 自我反思可以通过表征工程进行控制，反思与模型内部不确定性相关，这为优化推理效率提供了新途径。

Abstract: Large language models (LLMs) with Chain-of-Thought (CoT) reasoning have achieved strong performance across diverse tasks, including mathematics, coding, and general reasoning. A distinctive ability of these reasoning models is self-reflection: the ability to review and revise previous reasoning steps. While self-reflection enhances reasoning performance, it also increases inference cost. In this work, we study self-reflection through the lens of representation engineering. We segment the model's reasoning into steps, identify the steps corresponding to reflection, and extract a reflection direction in the latent space that governs this behavior. Using this direction, we propose a stepwise steering method that can control reflection frequency. We call our framework ReflCtrl. Our experiments show that (1) in many cases reflections are redundant, especially in stronger models (in our experiments, we can save up to 33.6 percent of reasoning tokens while preserving performance), and (2) the model's reflection behavior is highly correlated with an internal uncertainty signal, implying self-reflection may be controlled by the model's uncertainty.

</details>


### [12] [Sparsity-Controllable Dynamic Top-p MoE for Large Foundation Model Pre-training](https://arxiv.org/abs/2512.13996)
*Can Jin,Hongwu Peng,Mingcan Xiang,Qixin Zhang,Xiangchi Yuan,Amit Hasan,Ohiremen Dibua,Yifan Gong,Yan Kang,Dimitris N. Metaxas*

Main category: cs.AI

TL;DR: DTop-p MoE：一种可控制稀疏度的动态Top-p路由机制，通过PI控制器动态调整概率阈值，使激活专家稀疏度与目标对齐，并在不同层学习不同的专家选择模式。


<details>
  <summary>Details</summary>
Motivation: 标准Top-k路由策略对所有token采用统一的稀疏模式，忽略了token难度的差异；而现有的Top-p路由使用固定全局概率阈值，导致计算成本不可控且对超参数敏感。

Method: 提出DTop-p MoE路由机制：1）使用PI控制器动态调整概率阈值，使运行中的激活专家稀疏度与指定目标对齐；2）引入动态路由归一化机制，适应层间路由logits，使不同层学习不同的专家选择模式。

Result: 在大语言模型和扩散Transformer上的实验表明，DTop-p consistently优于Top-k和固定阈值Top-p基线，能精确控制激活专家数量，同时自适应地在不同token和层间分配资源。

Conclusion: DTop-p在专家粒度、专家容量、模型规模和数据集大小方面表现出良好的扩展性，为大规模MoE预训练提供了鲁棒框架。

Abstract: Sparse Mixture-of-Experts (MoE) architectures effectively scale model capacity by activating only a subset of experts for each input token. However, the standard Top-k routing strategy imposes a uniform sparsity pattern that ignores the varying difficulty of tokens. While Top-p routing offers a flexible alternative, existing implementations typically rely on a fixed global probability threshold, which results in uncontrolled computational costs and sensitivity to hyperparameter selection. In this paper, we propose DTop-p MoE, a sparsity-controllable dynamic Top-p routing mechanism. To resolve the challenge of optimizing a non-differentiable threshold, we utilize a Proportional-Integral (PI) Controller that dynamically adjusts the probability threshold to align the running activated-expert sparsity with a specified target. Furthermore, we introduce a dynamic routing normalization mechanism that adapts layer-wise routing logits, allowing different layers to learn distinct expert-selection patterns while utilizing a global probability threshold. Extensive experiments on Large Language Models and Diffusion Transformers demonstrate that DTop-p consistently outperforms both Top-k and fixed-threshold Top-p baselines. Our analysis confirms that DTop-p maintains precise control over the number of activated experts while adaptively allocating resources across different tokens and layers. Furthermore, DTop-p exhibits strong scaling properties with respect to expert granularity, expert capacity, model size, and dataset size, offering a robust framework for large-scale MoE pre-training.

</details>


### [13] [Intention Chain-of-Thought Prompting with Dynamic Routing for Code Generation](https://arxiv.org/abs/2512.14048)
*Shen Li,Li Huang,Shaoxiong Zhan,Weifeng Sun,Tao Yin,Zhongxin Liu,Meng Yan*

Main category: cs.AI

TL;DR: RoutingGen是一个难度感知的路由框架，根据任务复杂度动态选择提示策略：简单任务用少样本提示，复杂任务用意图思维链(ICoT)进行结构化推理，在保持性能的同时显著减少token使用。


<details>
  <summary>Details</summary>
Motivation: 现有思维链(CoT)方法存在两个主要局限：1) 对简单任务统一应用会导致过度思考；2) 缺乏代码生成中的意图抽象，如核心算法设计和效率建模，导致模型关注表面结构而忽视全局目标。

Method: 提出RoutingGen框架，基于难度感知动态路由提示策略：简单任务使用少样本提示，复杂任务使用意图思维链(ICoT)。ICoT引导模型捕捉任务意图，包括核心算法逻辑和时间复杂度等抽象概念。

Result: 在三个模型和六个标准代码生成基准测试中，RoutingGen在大多数设置下达到最先进性能，同时平均减少46.37%的总token使用量。ICoT在具有挑战性的基准测试中优于六个现有提示基线。

Conclusion: RoutingGen通过难度感知路由和意图思维链，有效解决了现有CoT方法的局限性，在保持高性能的同时显著提高了效率，为代码生成任务提供了更智能的提示策略框架。

Abstract: Large language models (LLMs) exhibit strong generative capabilities and have shown great potential in code generation. Existing chain-of-thought (CoT) prompting methods enhance model reasoning by eliciting intermediate steps, but suffer from two major limitations: First, their uniform application tends to induce overthinking on simple tasks. Second, they lack intention abstraction in code generation, such as explicitly modeling core algorithmic design and efficiency, leading models to focus on surface-level structures while neglecting the global problem objective. Inspired by the cognitive economy principle of engaging structured reasoning only when necessary to conserve cognitive resources, we propose RoutingGen, a novel difficulty-aware routing framework that dynamically adapts prompting strategies for code generation. For simple tasks, it adopts few-shot prompting; for more complex ones, it invokes a structured reasoning strategy, termed Intention Chain-of-Thought (ICoT), which we introduce to guide the model in capturing task intention, such as the core algorithmic logic and its time complexity. Experiments across three models and six standard code generation benchmarks show that RoutingGen achieves state-of-the-art performance in most settings, while reducing total token usage by 46.37% on average across settings. Furthermore, ICoT outperforms six existing prompting baselines on challenging benchmarks.

</details>


### [14] [OpenDataArena: A Fair and Open Arena for Benchmarking Post-Training Dataset Value](https://arxiv.org/abs/2512.14051)
*Mengzhang Cai,Xin Gao,Yu Li,Honglin Lin,Zheng Liu,Zhuoshi Pan,Qizhi Pei,Xiaoran Shang,Mengyuan Sun,Zinan Tang,Xiaoyang Wang,Zhanping Zhong,Yun Zhu,Dahua Lin,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: OpenDataArena (ODA) 是一个开源平台，用于系统评估和基准测试LLM后训练数据质量，通过统一训练评估流程、多维评分框架、数据谱系探索和开源工具集，揭示数据特性与模型性能之间的因果关系。


<details>
  <summary>Details</summary>
Motivation: 当前LLM发展依赖于高质量的后训练数据，但数据本身却是一个黑箱——组成不透明、来源不确定、缺乏系统评估。这种不透明性阻碍了研究的可复现性，也模糊了数据特性与模型行为之间的因果关系。

Method: ODA平台包含四个核心组件：1) 统一的训练-评估流程，确保不同模型和领域的公平比较；2) 多维评分框架，从数十个维度评估数据质量；3) 交互式数据谱系探索器，可视化数据集谱系和来源；4) 完全开源的工具包，支持训练、评估和评分。

Result: 在ODA上进行了大规模实验：覆盖120多个训练数据集、22个基准测试、600多次训练运行、4000万个处理数据点。分析揭示了数据复杂性与任务性能之间的权衡，通过谱系追踪识别了流行基准中的冗余，并绘制了数据集间的谱系关系图。

Conclusion: ODA不仅是一个基准测试平台，更旨在推动从试错式数据管理向数据为中心的AI科学转变，为研究数据混合规律和基础模型战略组成提供严谨框架。所有结果、工具和配置都已开源，以促进高质量数据评估的民主化。

Abstract: The rapid evolution of Large Language Models (LLMs) is predicated on the quality and diversity of post-training datasets. However, a critical dichotomy persists: while models are rigorously benchmarked, the data fueling them remains a black box--characterized by opaque composition, uncertain provenance, and a lack of systematic evaluation. This opacity hinders reproducibility and obscures the causal link between data characteristics and model behaviors. To bridge this gap, we introduce OpenDataArena (ODA), a holistic and open platform designed to benchmark the intrinsic value of post-training data. ODA establishes a comprehensive ecosystem comprising four key pillars: (i) a unified training-evaluation pipeline that ensures fair, open comparisons across diverse models (e.g., Llama, Qwen) and domains; (ii) a multi-dimensional scoring framework that profiles data quality along tens of distinct axes; (iii) an interactive data lineage explorer to visualize dataset genealogy and dissect component sources; and (iv) a fully open-source toolkit for training, evaluation, and scoring to foster data research. Extensive experiments on ODA--covering over 120 training datasets across multiple domains on 22 benchmarks, validated by more than 600 training runs and 40 million processed data points--reveal non-trivial insights. Our analysis uncovers the inherent trade-offs between data complexity and task performance, identifies redundancy in popular benchmarks through lineage tracing, and maps the genealogical relationships across datasets. We release all results, tools, and configurations to democratize access to high-quality data evaluation. Rather than merely expanding a leaderboard, ODA envisions a shift from trial-and-error data curation to a principled science of Data-Centric AI, paving the way for rigorous studies on data mixing laws and the strategic composition of foundation models.

</details>


### [15] [RADAR: Accelerating Large Language Model Inference With RL-Based Dynamic Draft Trees](https://arxiv.org/abs/2512.14069)
*Junjie Ma,Jinlong Li*

Main category: cs.AI

TL;DR: RADAR是一种基于强化学习的动态草稿树推测采样方法，通过实时决策减少冗余计算，加速大语言模型推理


<details>
  <summary>Details</summary>
Motivation: 现代大语言模型推理成本高且速度慢，传统推测采样方法中草稿模型调用次数是预设超参数，缺乏灵活性，无法有效生成和利用候选标记

Method: 提出RADAR方法，将草稿树生成过程建模为马尔可夫决策过程，采用离线强化学习训练预测模型，实时决策草稿模型调用，减少冗余计算

Result: 在三个大语言模型和四个任务上的评估显示，RADAR相比自回归解码基线实现了3.17x-4.82x的加速

Conclusion: RADAR通过强化学习驱动的动态草稿树生成，有效加速大语言模型推理，代码已开源

Abstract: Inference with modern Large Language Models (LLMs) is expensive and slow, and speculative sampling has emerged as an effective solution to this problem, however, the number of the calls to the draft model for generating candidate tokens in speculative sampling is a preset hyperparameter, lacking flexibility. To generate and utilize the candidate tokens more effectively, we propose RADAR, a novel speculative sampling method with RL-based dynamic draft trees. RADAR formulates the draft tree generation process as a Markov Decision Process (MDP) and employs offline reinforcement learning to train a prediction model, which enables real-time decision on the calls to the draft model, reducing redundant computations and further accelerating inference. Evaluations across three LLMs and four tasks show that RADAR achieves a speedup of 3.17x-4.82x over the auto-regressive decoding baseline. The code is available at https://github.com/minaduki-sora/RADAR.

</details>


### [16] [Grammar Search for Multi-Agent Systems](https://arxiv.org/abs/2512.14079)
*Mayank Singh,Vikas Yadav,Shiva Krishna Reddy Malay,Shravan Nayak,Sai Rajeswar,Sathwik Tejaswi Madhusudhan,Eduardo Blanco*

Main category: cs.AI

TL;DR: 本文提出了一种基于固定可组合组件的结构化框架，用于自动搜索多智能体系统，在数学和问答领域的五个基准测试中，有四个优于之前的LLM自由搜索方法，且更高效、可解释。


<details>
  <summary>Details</summary>
Motivation: 当前多智能体系统的自动搜索主要依赖LLM在代码空间中进行自由搜索，但这种方法缺乏结构性和效率。本文旨在开发一个更结构化的框架来探索相同的搜索空间。

Method: 提出了一种结构化框架，使用一组固定的简单可组合组件来探索多智能体系统的搜索空间，而不是依赖LLM的自由生成。这种方法虽然候选生成阶段缺乏LLM的生成灵活性，但通过组件组合实现系统构建。

Result: 在数学和问答两个领域的五个基准测试中，该方法在四个基准上优于之前的LLM自由搜索方法。此外，该方法还提供了更高效的搜索过程和更模块化、可解释的多智能体系统。

Conclusion: 基于固定可组合组件的结构化框架在自动搜索多智能体系统方面优于LLM自由搜索方法，不仅性能更好，而且更高效、可解释，生成的系统逻辑更简单。

Abstract: Automatic search for Multi-Agent Systems has recently emerged as a key focus in agentic AI research. Several prior approaches have relied on LLM-based free-form search over the code space. In this work, we propose a more structured framework that explores the same space through a fixed set of simple, composable components. We show that, despite lacking the generative flexibility of LLMs during the candidate generation stage, our method outperforms prior approaches on four out of five benchmarks across two domains: mathematics and question answering. Furthermore, our method offers additional advantages, including a more cost-efficient search process and the generation of modular, interpretable multi-agent systems with simpler logic.

</details>


### [17] [Incentivizing Tool-augmented Thinking with Images for Medical Image Analysis](https://arxiv.org/abs/2512.14157)
*Yankai Jiang,Yujie Zhang,Peng Zhang,Yichen Li,Jintai Chen,Xiaoming Shi,Shihui Zhen*

Main category: cs.AI

TL;DR: Ophiuchus是一个工具增强的多模态大语言模型框架，通过动态聚焦医学图像的细粒度区域，实现精确的定位和诊断，超越了现有方法的性能限制。


<details>
  <summary>Details</summary>
Motivation: 现有基于推理的医学MLLMs虽然在生成文本推理链方面有进展，但在需要动态、迭代聚焦细粒度视觉区域的复杂任务中仍存在困难，无法实现精确的定位和诊断。

Method: 提出Ophiuchus框架，包含三阶段训练策略：1) 冷启动训练，使用工具集成推理数据实现基本工具选择和关键区域检查；2) 自反思微调，加强反思推理并鼓励重新审视工具输出；3) 智能体工具强化学习，直接优化任务特定奖励并模拟专家诊断行为。

Result: 在多种医学基准测试（包括VQA、检测和基于推理的分割）中，Ophiuchus持续优于闭源和开源的最先进方法。

Conclusion: 该方法为医学AI智能体通过工具集成推理实现真正"用图像思考"开辟了道路，数据集、代码和训练模型将公开发布。

Abstract: Recent reasoning based medical MLLMs have made progress in generating step by step textual reasoning chains. However, they still struggle with complex tasks that necessitate dynamic and iterative focusing on fine-grained visual regions to achieve precise grounding and diagnosis. We introduce Ophiuchus, a versatile, tool-augmented framework that equips an MLLM to (i) decide when additional visual evidence is needed, (ii) determine where to probe and ground within the medical image, and (iii) seamlessly weave the relevant sub-image content back into an interleaved, multimodal chain of thought. In contrast to prior approaches limited by the performance ceiling of specialized tools, Ophiuchus integrates the model's inherent grounding and perception capabilities with external tools, thereby fostering higher-level reasoning. The core of our method is a three-stage training strategy: cold-start training with tool-integrated reasoning data to achieve basic tool selection and adaptation for inspecting key regions; self-reflection fine-tuning to strengthen reflective reasoning and encourage revisiting tool outputs; and Agentic Tool Reinforcement Learning to directly optimize task-specific rewards and emulate expert-like diagnostic behavior. Extensive experiments show that Ophiuchus consistently outperforms both closed-source and open-source SOTA methods across diverse medical benchmarks, including VQA, detection, and reasoning-based segmentation. Our approach illuminates a path toward medical AI agents that can genuinely "think with images" through tool-integrated reasoning. Datasets, codes, and trained models will be released publicly.

</details>


### [18] [Georeferencing complex relative locality descriptions with large language models](https://arxiv.org/abs/2512.14228)
*Aneesha Fernando,Surangika Ranathunga,Kristin Stock,Raj Prasanna,Christopher B. Jones*

Main category: cs.AI

TL;DR: 本文探索使用大语言模型自动地理编码生物多样性标本记录中的复杂位置描述，通过QLoRA微调在多个数据集上取得优于基线方法的结果。


<details>
  <summary>Details</summary>
Motivation: 生物标本记录中的位置描述通常是叙述性的而非坐标，传统基于地名或地理指示词的方法难以处理相对空间关系，导致地理编码不准确且劳动密集，需要自动化解决方案。

Method: 首先识别有效的提示模式，然后使用量化低秩适应（QLoRA）在多区域多语言的生物多样性数据集上微调大语言模型。

Result: 在固定训练数据量下，该方法在多个数据集上平均65%的记录在10公里半径内，最佳结果（纽约州）达到85%在10公里内和67%在1公里内，优于现有基线方法。

Conclusion: 大语言模型在处理复杂位置描述方面表现出色，展示了在生物多样性领域地理编码复杂位置描述的潜力。

Abstract: Georeferencing text documents has typically relied on either gazetteer-based methods to assign geographic coordinates to place names, or on language modelling approaches that associate textual terms with geographic locations. However, many location descriptions specify positions relatively with spatial relationships, making geocoding based solely on place names or geo-indicative words inaccurate. This issue frequently arises in biological specimen collection records, where locations are often described through narratives rather than coordinates if they pre-date GPS. Accurate georeferencing is vital for biodiversity studies, yet the process remains labour-intensive, leading to a demand for automated georeferencing solutions. This paper explores the potential of Large Language Models (LLMs) to georeference complex locality descriptions automatically, focusing on the biodiversity collections domain. We first identified effective prompting patterns, then fine-tuned an LLM using Quantized Low-Rank Adaptation (QLoRA) on biodiversity datasets from multiple regions and languages. Our approach outperforms existing baselines with an average, across datasets, of 65% of records within a 10 km radius, for a fixed amount of training data. The best results (New York state) were 85% within 10km and 67% within 1km. The selected LLM performs well for lengthy, complex descriptions, highlighting its potential for georeferencing intricate locality descriptions.

</details>


### [19] [Gödel's Poetry](https://arxiv.org/abs/2512.14252)
*Kelly J. Davis*

Main category: cs.AI

TL;DR: 该论文提出了一种结合专用语言模型和递归定理分解的新方法，用于Lean4自动定理证明，显著提升了miniF2F测试集的通过率。


<details>
  <summary>Details</summary>
Motivation: 形式化自动定理证明长期以来被视为人工智能的挑战，需要开发更有效的方法来解决复杂的数学定理证明问题。

Method: 采用专用语言模型生成Lean4证明，结合递归分解将困难定理分解为更简单的蕴含命题，通过多智能体架构协调自动形式化、证明生成、分解和递归证明等过程。

Result: 在不使用分解的情况下，在miniF2F测试集上达到90.4%的通过率；使用分解后，性能得到显著提升。系统已开源并在PyPI上发布。

Conclusion: 该方法通过结合语言模型和递归分解策略，为自动定理证明提供了有效的解决方案，开源实现便于进一步研究和扩展。

Abstract: Formal, automated theorem proving has long been viewed as a challenge to artificial intelligence. We introduce here a new approach to computer theorem proving, one that employs specialized language models for Lean4 proof generation combined with recursive decomposition of difficult theorems into simpler entailing propositions. These models are coordinated through a multi-agent architecture that orchestrates autoformalization (if required), proof generation, decomposition of difficult theorems into simpler entailing propositions, and recursive proof (and/or decomposition) of these propositions. Without decomposition, we achieve a 90.4% pass rate on miniF2F. With decomposition, this is significantly improved. A key technical contribution lies in our extension of the Kimina Lean Server with abstract syntax tree (AST) parsing capabilities to facilitate automated, recursive proof decomposition. The system is made available on PyPI as goedels-poetry (at https://pypi.org/project/goedels-poetry ), and the open-source implementation KellyJDavis/goedels-poetry (at https://github.com/KellyJDavis/goedels-poetry ) facilitates both adaptation to alternative language models and extension with custom functionality.

</details>


### [20] [Leveraging LLMs for Collaborative Ontology Engineering in Parkinson Disease Monitoring and Alerting](https://arxiv.org/abs/2512.14288)
*Georgios Bouchouras,Dimitrios Doumanas,Andreas Soularidis,Konstantinos Kotis,George A. Vouros*

Main category: cs.AI

TL;DR: 本文研究了LLM在帕金森病监测与预警本体工程中的应用，比较了四种方法：单次提示、思维链提示、X-HCOME和SimX-HCOME+，发现纯LLM方法能生成本体但不完整，而人机协作方法能显著提升本体质量。


<details>
  <summary>Details</summary>
Motivation: 探索LLM在自动化本体开发中的有效性，评估纯LLM方法是否能创建全面的本体，以及人机协作是否能实现这一目标，特别是在帕金森病监测与预警这一复杂领域。

Method: 采用四种关键方法：1) 单次提示技术；2) 思维链提示；3) X-HCOME（结合人类专业知识与LLM能力的混合方法）；4) SimX-HCOME+（强调持续人类监督和迭代优化的混合方法）。

Result: 纯LLM方法（单次提示和思维链提示）能自主构建本体但不完整，需要大量人工优化；X-HCOME显著提升了本体的全面性，结果与专家构建的本体非常相似；SimX-HCOME+通过持续人类监督创建了更全面准确的本体。

Conclusion: 人机协作在推进本体工程方面具有巨大潜力，特别是在复杂领域如帕金森病。未来研究方向包括开发专门用于本体构建的GPT模型。

Abstract: This paper explores the integration of Large Language Models (LLMs) in the engineering of a Parkinson's Disease (PD) monitoring and alerting ontology through four key methodologies: One Shot (OS) prompt techniques, Chain of Thought (CoT) prompts, X-HCOME, and SimX-HCOME+. The primary objective is to determine whether LLMs alone can create comprehensive ontologies and, if not, whether human-LLM collaboration can achieve this goal. Consequently, the paper assesses the effectiveness of LLMs in automated ontology development and the enhancement achieved through human-LLM collaboration.
  Initial ontology generation was performed using One Shot (OS) and Chain of Thought (CoT) prompts, demonstrating the capability of LLMs to autonomously construct ontologies for PD monitoring and alerting. However, these outputs were not comprehensive and required substantial human refinement to enhance their completeness and accuracy.
  X-HCOME, a hybrid ontology engineering approach that combines human expertise with LLM capabilities, showed significant improvements in ontology comprehensiveness. This methodology resulted in ontologies that are very similar to those constructed by experts.
  Further experimentation with SimX-HCOME+, another hybrid methodology emphasizing continuous human supervision and iterative refinement, highlighted the importance of ongoing human involvement. This approach led to the creation of more comprehensive and accurate ontologies.
  Overall, the paper underscores the potential of human-LLM collaboration in advancing ontology engineering, particularly in complex domains like PD. The results suggest promising directions for future research, including the development of specialized GPT models for ontology construction.

</details>


### [21] [Massive Editing for Large Language Models Based on Dynamic Weight Generation](https://arxiv.org/abs/2512.14395)
*Wentao Wan,Qiqing Lao,Zhiwei Xie,Hefeng Wu,Runnan Lin,Liang Lin,Keze Wang*

Main category: cs.AI

TL;DR: 本文提出了一种基于动态权重生成的大规模知识编辑方法MeG，通过在LLM特定层附加动态权重神经元，并使用扩散模型根据输入查询条件生成权重，实现了大规模知识编辑的目标。


<details>
  <summary>Details</summary>
Motivation: 当前在大型语言模型中进行大规模知识编辑时，确保编辑的可靠性、通用性和局部性指标仍然是一个挑战。需要一种低成本、高效的方法来修改LLM中的知识。

Method: MeG方法在LLM的特定层附加动态权重神经元，使用扩散模型根据输入查询条件生成该神经元的权重。通过添加单个动态权重神经元实现大规模知识编辑。

Result: 实验表明，MeG在可靠性、通用性和局部性指标上显著优于现有知识编辑方法，特别是在局部性指标的绝对数值上有显著提升。

Conclusion: 提出的MeG方法通过动态权重生成机制，有效解决了大规模知识编辑的挑战，在多个关键指标上表现出优越性能。

Abstract: Knowledge Editing (KE) is a field that studies how to modify some knowledge in Large Language Models (LLMs) at a low cost (compared to pre-training). Currently, performing large-scale edits on LLMs while ensuring the Reliability, Generality, and Locality metrics of the edits remain a challenge. This paper proposes a Massive editing approach for LLMs based on dynamic weight Generation (MeG). Our MeG involves attaching a dynamic weight neuron to specific layers of the LLMs and using a diffusion model to conditionally generate the weights of this neuron based on the input query required for the knowledge. This allows the use of adding a single dynamic weight neuron to achieve the goal of large-scale knowledge editing. Experiments show that our MeG can significantly improve the performance of large-scale KE in terms of Reliability, Generality, and Locality metrics compared to existing knowledge editing methods, particularly with a high percentage point increase in the absolute value index for the Locality metric, demonstrating the advantages of our proposed method.

</details>


### [22] [Context-Picker: Dynamic context selection using multi-stage reinforcement learning](https://arxiv.org/abs/2512.14465)
*Siyuan Zhu,Chengdong Xu,Kaiqiang Ke,Chao Yu*

Main category: cs.AI

TL;DR: Context-Picker：一个用于长上下文问答的推理感知框架，将上下文选择从相似性排序转变为最小充分子集选择，通过两阶段强化学习优化，显著提升答案准确性同时减少上下文长度。


<details>
  <summary>Details</summary>
Motivation: 在长上下文问答中，确定查询所需的最佳上下文量是一个重要挑战。包含太少段落可能遗漏关键信息，包含太多则会引入噪声并降低答案质量。传统方法如固定Top-K检索和单阶段重排序面临选择适当段落数量的困境，这对事实性问题尤为突出，因为这类问题通常只需要少量特定证据。

Method: 提出Context-Picker框架，将上下文选择视为决策过程，通过人类启发的两阶段强化学习优化：1）召回导向阶段，优先覆盖推理链；2）精确导向阶段，积极修剪冗余以提炼紧凑证据集。为解决奖励稀疏性问题，提出离线证据蒸馏管道，通过留一法挖掘"最小充分集"，提供密集、任务对齐的监督。

Result: 在五个长上下文和多跳问答基准测试中，Context-Picker显著优于强大的RAG基线，在可比或减少的上下文长度下实现更优的答案准确性。消融研究表明，粗到细的优化调度、冗余感知的奖励塑造和推理引导的格式都对性能提升有重要贡献。

Conclusion: Context-Picker通过将上下文选择范式从相似性排序转变为最小充分子集选择，有效解决了长上下文问答中的上下文量优化问题。该框架的两阶段强化学习方法和离线证据蒸馏技术为处理事实性问题提供了更精确、更高效的解决方案。

Abstract: In long-context question answering (LCQA), determining the optimal amount of context for a given query is a significant challenge. Including too few passages may omit critical information, while including too many can introduce noise and reduce the quality of the answer. Traditional approaches, such as fixed Top-$K$ retrieval and single-stage reranking, face the dilemma of selecting the right number of passages. This problem is particularly pronounced for factoid questions, which often require only a few specific pieces of evidence. To address this issue, we introduce \emph{Context-Picker}, a reasoning-aware framework that shifts the paradigm from similarity-based ranking to minimal sufficient subset selection. Context-Picker treats context selection as a decision-making process optimized via a human-inspired, two-stage reinforcement learning schedule: a \emph{recall-oriented} stage that prioritizes the coverage of reasoning chains, followed by a \emph{precision-oriented} stage that aggressively prunes redundancy to distill a compact evidence set. To resolve reward sparsity, we propose an offline evidence distillation pipeline that mines "minimal sufficient sets" via a Leave-One-Out (LOO) procedure, providing dense, task-aligned supervision. Experiments on five long-context and multi-hop QA benchmarks demonstrate that Context-Picker significantly outperforms strong RAG baselines, achieving superior answer accuracy with comparable or reduced context lengths. Ablation studies indicate that the coarse-to-fine optimization schedule, the redundancy-aware reward shaping, and the rationale-guided format all contribute substantially to these gains.

</details>


### [23] [Dynamic Learning Rate Scheduling based on Loss Changes Leads to Faster Convergence](https://arxiv.org/abs/2512.14527)
*Shreyas Subramanian,Bala Krishnamoorthy,Pranav Murthy*

Main category: cs.AI

TL;DR: 提出了一种基于当前损失自适应调整学习率的新型调度器GreedyLR，在多个NLP、CV和LLM任务上验证了其优于现有调度器的性能。


<details>
  <summary>Details</summary>
Motivation: 尽管训练优化器取得了显著进展，但大多数研究工作仍使用常见的调度器选择（如余弦或指数衰减）。本文旨在研究一种能够根据当前损失自适应调整学习率的新型调度器。

Method: 提出了GreedyLR调度器，该调度器在训练过程中基于当前损失自适应调整学习率。提供了算法的理论分析，包括收敛性证明和最大化收敛速率的最优缩放因子F的推导。

Result: 在多个NLP、CV和LLM任务（包括微调和预训练实验，参数规模高达70亿）上的实验结果表明，该方法在准确性、速度和收敛性方面优于多个最先进的调度器。实验还显示了算法对现实噪声环境的鲁棒性。

Conclusion: GreedyLR调度器易于实现、计算高效，可被视为训练的良好默认调度器选择。

Abstract: Despite significant advances in optimizers for training, most research works use common scheduler choices like Cosine or exponential decay. In this paper, we study \emph{GreedyLR}, a novel scheduler that adaptively adjusts the learning rate during training based on the current loss. To validate the effectiveness of our proposed scheduler, we conduct experiments on several NLP, CV, and LLM tasks with up to $7B$ parameters, including both fine-tuning and pre-training experiments. The results show that our approach outperforms several state-of-the-art schedulers in terms of accuracy, speed, and convergence. We also provide a theoretical analysis of the GreedyLR algorithm, including a proof of convergence and derivation of the optimal scaling factor $F$ that maximizes the convergence rate, along with experiments to show robustness of the algorithm to realistic noisy landscapes. Our scheduler is easy to implement, computationally efficient, and could be considered a good default scheduler for training.

</details>


### [24] [Universal Reasoning Model](https://arxiv.org/abs/2512.14693)
*Zitian Gao,Lynx Chen,Yihao Xiao,He Xing,Ran Tao,Haoming Luo,Joey Zhou,Bryan Dai*

Main category: cs.AI

TL;DR: 本文分析了通用Transformer在复杂推理任务中的性能来源，发现其改进主要来自循环归纳偏置和Transformer的强非线性组件，而非复杂的架构设计。基于此提出了通用推理模型URM，在ARC-AGI基准上取得了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 通用Transformer在ARC-AGI和数独等复杂推理任务中表现出色，但其性能提升的具体来源尚不明确。本文旨在系统分析UT变体，识别真正有效的组件，以构建更高效的推理模型。

Method: 通过系统分析UT变体，发现性能提升主要源于循环归纳偏置和Transformer的强非线性组件。基于此提出了通用推理模型URM，结合了短卷积和截断反向传播技术来增强UT。

Result: URM在ARC-AGI基准上取得了显著改进，在ARC-AGI 1上达到53.8% pass@1，在ARC-AGI 2上达到16.0% pass@1，均达到最先进水平。

Conclusion: 通用Transformer在复杂推理任务中的性能提升主要来自循环归纳偏置和强非线性组件，而非复杂架构设计。URM通过结合短卷积和截断反向传播进一步提升了推理性能，为高效推理模型设计提供了新思路。

Abstract: Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.

</details>


### [25] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 论文提出IPR（交互式物理推理器），通过世界模型推演来评估和强化VLM策略，引入PhysCode物理中心动作编码，在1000+游戏中预训练，在物理推理任务上表现优于GPT-5，且性能随训练游戏和交互步骤增加而提升。


<details>
  <summary>Details</summary>
Motivation: 人类通过观察、交互和环境互动来学习物理和因果关系，但现有方法（如VLMs和世界模型）难以捕捉底层物理机制和因果关系。VLMs缺乏交互环境中的前瞻性，世界模型则过度模仿视觉模式而非分析物理和因果关系。

Method: 提出IPR（交互式物理推理器），使用世界模型推演来评分和强化VLM的策略；引入PhysCode物理中心动作编码，将语义意图与动力学对齐，为预测和推理提供共享动作空间。在1000+异构游戏上进行预训练。

Result: IPR在从原始直觉到目标驱动推理的各种关卡上表现稳健，总体表现甚至超过GPT-5。性能随训练游戏数量和交互步骤增加而提升，并能零样本迁移到未见过的游戏中。

Conclusion: 物理中心的交互是实现持续改进物理推理能力的有效途径。研究结果表明，通过大规模交互训练，智能体可以像人类一样通过互动学习物理和因果关系。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. To study this, we introduce a Game-to-Unseen (G2U) benchmark of 1,000+ heterogeneous games that exhibit significant visual domain gaps. Existing approaches, including VLMs and world models, struggle to capture underlying physics and causality since they are not focused on core mechanisms and overfit to visual details. VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on levels from primitive intuition to goal-driven reasoning, and even surpasses GPT-5 overall. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning. Further demos and project details can be found at https://mybearyzhang.github.io/ipr-1.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [26] [Mitigating Catastrophic Forgetting in Mathematical Reasoning Finetuning through Mixed Training](https://arxiv.org/abs/2512.13706)
*John Graham Reynolds*

Main category: cs.LG

TL;DR: 论文研究了微调大语言模型进行数学推理时的灾难性遗忘问题，发现仅数学训练会使NLI准确率大幅下降64.5个百分点，而混合训练策略能完全消除遗忘同时保持数学性能。


<details>
  <summary>Details</summary>
Motivation: 当微调大语言模型进行数学推理等专门任务时，模型会出现灾难性遗忘，丧失先前学习的能力。作者旨在研究这一现象并寻找解决方案。

Method: 使用Flan-T5-Base模型（2.5亿参数）在DeepMind Mathematics数据集上进行微调，测量在MultiNLI上的遗忘情况。提出混合训练策略，在训练中交错使用数学和NLI示例，系统探索从1:1到15:1的混合比例。

Result: 仅数学训练将数学准确率从3.1%提升到12.0%，但导致NLI准确率从81.0%崩溃到16.5%（下降64.5个百分点）。混合训练完全消除了灾难性遗忘，1:1比例达到12.0%数学准确率（与仅数学训练相当）同时保持86.2%的NLI准确率。即使最小NLI暴露（6.2%）也能提供有效正则化。

Conclusion: 专业化不一定需要遗忘通用能力，混合训练策略能有效防止灾难性遗忘。这一发现对扩展到更大模型具有重要意义，混合训练可能带来超越遗忘预防的额外好处。

Abstract: When finetuning large language models for specialized tasks such as mathematical reasoning, models exhibit catastrophic forgetting, losing previously learned capabilities. We investigate this by finetuning Flan-T5-Base (250M parameters) on the DeepMind Mathematics dataset and measuring forgetting on MultiNLI. Math-only training improves mathematical accuracy from 3.1\% to 12.0\% but causes NLI accuracy to collapse from 81.0\% to 16.5\%--a 64.5 percentage point drop occurring within the first 1,000 training steps. We propose mixed training strategies that interleave mathematical and NLI examples during training. Our results demonstrate that mixed training completely eliminates catastrophic forgetting while maintaining equivalent mathematical performance: the balanced 1:1 ratio achieves 12.0\% math accuracy (matching math-only) while preserving 86.2\% NLI accuracy. We systematically explore mixing ratios from 1:1 to 15:1, finding that even minimal NLI exposure (6.2\%) provides effective regularization. These findings demonstrate that specialization need not require forgetting general capabilities, with implications for scaling to larger models where mixed training may confer additional benefits beyond forgetting prevention.

</details>


### [27] [Predictive Modeling of Flood-Prone Areas Using SAR and Environmental Variables](https://arxiv.org/abs/2512.13710)
*Edwin Oluoch Awino,Denis Machanda*

Main category: cs.LG

TL;DR: 该研究结合SAR影像与多源环境数据，使用机器学习方法对肯尼亚Nyando河流域进行洪水易发性建模，发现随机森林模型表现最佳，识别出维多利亚湖附近的Kano平原为最高风险区。


<details>
  <summary>Details</summary>
Motivation: 洪水是全球最具破坏性的自然灾害之一，对生态系统、基础设施和人类生计构成严重威胁。该研究旨在通过结合SAR影像和环境水文数据，为数据有限的地区开发有效的洪水易发性评估方法，为灾害风险管理提供支持。

Method: 研究使用Sentinel-1双极化SAR数据处理2024年5月洪水事件，生成二值化洪水清单作为训练数据。结合六个条件因子（坡度、高程、坡向、土地利用/覆盖、土壤类型、距河流距离）训练四种监督分类器：逻辑回归、分类回归树、支持向量机和随机森林。通过准确率、Cohen's Kappa和ROC分析评估模型性能。

Result: 随机森林模型取得了最高的预测性能（准确率=0.762；Kappa=0.480），优于其他三种模型。基于随机森林的易发性地图显示，维多利亚湖附近的低洼Kano平原具有最高的洪水脆弱性，这与历史洪水记录和2024年5月洪水事件的影响一致。

Conclusion: 该研究证明了结合SAR数据和集成机器学习方法在数据有限地区进行洪水易发性制图的价值。生成的易发性地图为灾害风险减少、土地利用规划和早期预警系统开发提供了重要见解。

Abstract: Flooding is one of the most destructive natural hazards worldwide, posing serious risks to ecosystems, infrastructure, and human livelihoods. This study combines Synthetic Aperture Radar (SAR) imagery with environmental and hydrological data to model flood susceptibility in the River Nyando watershed, western Kenya. Sentinel-1 dual-polarization SAR data from the May 2024 flood event were processed to produce a binary flood inventory, which served as training data for machine learning (ML) models. Six conditioning factors -- slope, elevation, aspect, land use/land cover, soil type, and distance from streams -- were integrated with the SAR-derived flood inventory to train four supervised classifiers: Logistic Regression (LR), Classification and Regression Trees (CART), Support Vector Machines (SVM), and Random Forest (RF). Model performance was assessed using accuracy, Cohen's Kappa, and Receiver Operating Characteristic (ROC) analysis. Results indicate that RF achieved the highest predictive performance (accuracy = 0.762; Kappa = 0.480), outperforming LR, CART, and SVM. The RF-based susceptibility map showed that low-lying Kano Plains near Lake Victoria have the highest flood vulnerability, consistent with historical flood records and the impacts of the May 2024 event. These findings demonstrate the value of combining SAR data and ensemble ML methods for flood susceptibility mapping in regions with limited data. The resulting maps offer important insights for disaster risk reduction, land-use planning, and early warning system development.

</details>


### [28] [Delete and Retain: Efficient Unlearning for Document Classification](https://arxiv.org/abs/2512.13711)
*Aadya Goel,Mayuri Sridhar*

Main category: cs.LG

TL;DR: 本文提出了一种名为Hessian Reassignment的两步、模型无关的类别级遗忘方法，用于文档分类模型，通过Hessian-向量系统和Top-1分类保证，在保持准确性的同时实现高效遗忘。


<details>
  <summary>Details</summary>
Motivation: 机器遗忘旨在高效地从模型中移除特定训练数据的影响而无需完全重新训练。虽然在大语言模型的遗忘方面取得了进展，但文档分类模型的遗忘研究相对不足，特别是类别级遗忘。

Method: 提出Hessian Reassignment方法：第一步，通过求解Hessian-向量系统（使用共轭梯度法）执行单次影响式更新，从目标类别中减去所有训练点的贡献；第二步，通过Top-1分类强制执行决策空间保证，而不是像常见基线那样随机重新分类已删除类别的样本。

Result: 在标准文本基准测试中，Hessian Reassignment在保持接近完整重新训练（不含目标类别）的准确性的同时，运行速度快几个数量级。此外，通过池化多影子攻击测量，该方法持续降低了被移除类别的成员推理优势。

Conclusion: 这些结果表明，Hessian Reassignment为文档分类中的高效类别遗忘提供了一条实用且原则性的路径，填补了该领域的研究空白。

Abstract: Machine unlearning aims to efficiently remove the influence of specific training data from a model without full retraining. While much progress has been made in unlearning for LLMs, document classification models remain relatively understudied. In this paper, we study class-level unlearning for document classifiers and present Hessian Reassignment, a two-step, model-agnostic solution. First, we perform a single influence-style update that subtracts the contribution of all training points from the target class by solving a Hessian-vector system with conjugate gradients, requiring only gradient and Hessian-vector products. Second, in contrast to common unlearning baselines that randomly reclassify deleted-class samples, we enforce a decision-space guarantee via Top-1 classification. On standard text benchmarks, Hessian Reassignment achieves retained-class accuracy close to full retrain-without-class while running orders of magnitude faster. Additionally, it consistently lowers membership-inference advantage on the removed class, measured with pooled multi-shadow attacks. These results demonstrate a practical, principled path to efficient class unlearning in document classification.

</details>


### [29] [Prediction of Respiratory Syncytial Virus-Associated Hospitalizations Using Machine Learning Models Based on Environmental Data](https://arxiv.org/abs/2512.13712)
*Eric Guo*

Main category: cs.LG

TL;DR: 该研究开发了一个机器学习框架，通过整合废水监测、气象和空气质量数据来预测美国RSV相关住院情况，发现废水RSV水平是最强预测因子，并揭示了特定人群和地区的风险差异。


<details>
  <summary>Details</summary>
Motivation: 呼吸道合胞病毒（RSV）是导致幼儿住院的主要原因，其爆发受环境条件强烈影响。需要开发预测模型来改善公共卫生干预和资源分配。

Method: 整合每周住院率、废水RSV水平、每日气象测量和空气污染物浓度数据，使用CART、随机森林和提升等分类模型预测RSV相关住院风险等级（低风险、警报、流行）。

Result: 废水RSV水平是最强预测因子，其次是温度、臭氧水平和比湿等气象和空气质量变量。研究发现美国原住民和阿拉斯加原住民住院率显著更高，高海拔地区（表面压力较低）住院率也持续较高。

Conclusion: 结合环境和社区监测数据对预测RSV爆发具有重要价值，可支持更及时的公共卫生干预。研究开发了交互式R Shiny仪表板，使模型更易于访问和使用。

Abstract: Respiratory syncytial virus (RSV) is a leading cause of hospitalization among young children, with outbreaks strongly influenced by environmental conditions. This study developed a machine learning framework to predict RSV-associated hospitalizations in the United States (U.S.) by integrating wastewater surveillance, meteorological, and air quality data. The dataset combined weekly hospitalization rates, wastewater RSV levels, daily meteorological measurements, and air pollutant concentrations. Classification models, including CART, Random Forest, and Boosting, were trained to predict weekly RSV-associated hospitalization rates classified as \textit{Low risk}, \textit{Alert}, and \textit{Epidemic} levels. The wastewater RSV level was identified as the strongest predictor, followed by meteorological and air quality variables such as temperature, ozone levels, and specific humidity. Notably, the analysis also revealed significantly higher RSV-associated hospitalization rates among Native Americans and Alaska Natives. Further research is needed to better understand the drivers of RSV disparity in these communities to improve prevention strategies. Furthermore, states at high altitudes, characterized by lower surface pressure, showed consistently higher RSV-associated hospitalization rates. These findings highlight the value of combining environmental and community surveillance data to forecast RSV outbreaks, enabling more timely public health interventions and resource allocation. In order to provide accessibility and practical use of the models, we have developed an interactive R Shiny dashboard (https://f6yxlu-eric-guo.shinyapps.io/rsv_app/), which allows users to explore RSV-associated hospitalization risk levels across different states, visualize the impact of key predictors, and interactively generate RSV outbreak forecasts.

</details>


### [30] [Federated Few-Shot Learning for Epileptic Seizure Detection Under Privacy Constraints](https://arxiv.org/abs/2512.13717)
*Ekaterina Sysoykova,Bernhard Anzengruber-Tanase,Michael Haslgrubler,Philipp Seidl,Alois Ferscha*

Main category: cs.LG

TL;DR: 该研究提出了一种两阶段联邦少样本学习框架，用于在数据稀缺、分布不均且受隐私限制的临床环境中实现个性化EEG癫痫检测。


<details>
  <summary>Details</summary>
Motivation: 当前基于深度学习的EEG癫痫检测方法大多依赖大规模集中式标注数据集，但临床实践中EEG数据稀缺、分布在不同机构且受隐私法规限制，无法进行数据汇集，这限制了AI模型在真实医疗环境中的应用。

Method: 提出两阶段联邦少样本学习框架：第一阶段使用联邦学习在模拟的非IID医院站点上微调预训练的BIOT模型，实现共享表征学习而不集中EEG数据；第二阶段通过联邦少样本个性化，仅使用5个标注EEG片段为每个患者自适应调整分类器。

Result: 联邦微调阶段达到平衡准确率0.43（集中式0.52）、Cohen's kappa 0.42（0.49）、加权F1 0.69（0.74）；FFSL阶段客户端特定模型在四个具有异质事件分布的站点上平均平衡准确率达0.77、Cohen's kappa 0.62、加权F1 0.73。

Conclusion: FFSL框架能够在现实数据可用性和隐私约束下支持有效的患者自适应癫痫检测，为解决临床环境中的数据稀缺和隐私问题提供了可行方案。

Abstract: Many deep learning approaches have been developed for EEG-based seizure detection; however, most rely on access to large centralized annotated datasets. In clinical practice, EEG data are often scarce, patient-specific distributed across institutions, and governed by strict privacy regulations that prohibit data pooling. As a result, creating usable AI-based seizure detection models remains challenging in real-world medical settings. To address these constraints, we propose a two-stage federated few-shot learning (FFSL) framework for personalized EEG-based seizure detection. The method is trained and evaluated on the TUH Event Corpus, which includes six EEG event classes. In Stage 1, a pretrained biosignal transformer (BIOT) is fine-tuned across non-IID simulated hospital sites using federated learning, enabling shared representation learning without centralizing EEG recordings. In Stage 2, federated few-shot personalization adapts the classifier to each patient using only five labeled EEG segments, retaining seizure-specific information while still benefiting from cross-site knowledge. Federated fine-tuning achieved a balanced accuracy of 0.43 (centralized: 0.52), Cohen's kappa of 0.42 (0.49), and weighted F1 of 0.69 (0.74). In the FFSL stage, client-specific models reached an average balanced accuracy of 0.77, Cohen's kappa of 0.62, and weighted F1 of 0.73 across four sites with heterogeneous event distributions. These results suggest that FFSL can support effective patient-adaptive seizure detection under realistic data-availability and privacy constraints.

</details>


### [31] [Time-Constrained Recommendations: Reinforcement Learning Strategies for E-Commerce](https://arxiv.org/abs/2512.13726)
*Sayak Chakrabarty,Souradip Pal*

Main category: cs.LG

TL;DR: 论文提出在有限时间预算约束下的推荐系统优化方法，使用强化学习同时学习用户偏好和时间预算模式，在电商场景中提升用户参与度。


<details>
  <summary>Details</summary>
Motivation: 传统推荐任务忽略了用户时间预算这一关键资源约束。在移动购物等场景中，用户评估推荐项目需要时间成本，高相关性但评估成本高的项目可能超出用户时间预算，影响用户参与度。

Method: 将时间约束的slate推荐统一建模为具有预算感知效用的马尔可夫决策过程，使用强化学习算法（包括on-policy和off-policy控制方法）同时学习用户偏好和时间预算模式。

Result: 实验使用阿里巴巴个性化重排序数据集，在电商场景中进行slate优化。结果表明，在严格时间预算下，强化学习方法比传统的基于上下文bandit的方法表现更好。

Conclusion: 强化学习能够有效处理时间约束下的推荐问题，通过同时考虑项目相关性和评估成本，在有限时间预算内优化用户参与度，为资源受限的推荐系统提供了新思路。

Abstract: Unlike traditional recommendation tasks, finite user time budgets introduce a critical resource constraint, requiring the recommender system to balance item relevance and evaluation cost. For example, in a mobile shopping interface, users interact with recommendations by scrolling, where each scroll triggers a list of items called slate. Users incur an evaluation cost - time spent assessing item features before deciding to click. Highly relevant items having higher evaluation costs may not fit within the user's time budget, affecting engagement. In this position paper, our objective is to evaluate reinforcement learning algorithms that learn patterns in user preferences and time budgets simultaneously, crafting recommendations with higher engagement potential under resource constraints. Our experiments explore the use of reinforcement learning to recommend items for users using Alibaba's Personalized Re-ranking dataset supporting slate optimization in e-commerce contexts. Our contributions include (i) a unified formulation of time-constrained slate recommendation modeled as Markov Decision Processes (MDPs) with budget-aware utilities; (ii) a simulation framework to study policy behavior on re-ranking data; and (iii) empirical evidence that on-policy and off-policy control can improve performance under tight time budgets than traditional contextual bandit-based methods.

</details>


### [32] [Composite Classifier-Free Guidance for Multi-Modal Conditioning in Wind Dynamics Super-Resolution](https://arxiv.org/abs/2512.13729)
*Jacob Schnell,Aditya Makkar,Gunadi Gani,Aniket Srinivasan Ashok,Darren Lo,Mike Optis,Alexander Wong,Yuhao Chen*

Main category: cs.LG

TL;DR: 本文提出了一种用于风数据超分辨率的新型复合分类器自由引导（CCFG）方法，并将其应用于WindDM扩散模型，在工业级风动力学重建中实现了最先进的性能，成本比传统方法降低1000倍。


<details>
  <summary>Details</summary>
Motivation: 高分辨率、高精度的风数据对天气建模、风力涡轮机优化等应用至关重要，但获取成本高昂且困难。传统重建方法要么成本低但精度不足，要么精度高但成本昂贵。深度学习方法试图解决这一权衡，但风数据与自然图像不同，通常需要10个以上的输入通道，而自然图像只有3个RGB通道。

Method: 提出了复合分类器自由引导（CCFG）方法，这是对标准分类器自由引导（CFG）的泛化，能够更好地处理多个条件输入。CCFG可以轻松集成到任何使用标准CFG dropout预训练的扩散模型中。基于CCFG开发了WindDM扩散模型，专门用于工业级风动力学重建。

Result: CCFG在风超分辨率任务中输出的保真度高于标准CFG。WindDM在深度学习模型中实现了最先进的重建质量，同时成本比传统方法降低了高达1000倍。

Conclusion: 提出的CCFG方法能够有效处理风数据超分辨率中的多条件输入问题，WindDM模型在保持高精度的同时大幅降低了成本，为工业级风动力学重建提供了实用且经济的解决方案。

Abstract: Various weather modelling problems (e.g., weather forecasting, optimizing turbine placements, etc.) require ample access to high-resolution, highly accurate wind data. Acquiring such high-resolution wind data, however, remains a challenging and expensive endeavour. Traditional reconstruction approaches are typically either cost-effective or accurate, but not both. Deep learning methods, including diffusion models, have been proposed to resolve this trade-off by leveraging advances in natural image super-resolution. Wind data, however, is distinct from natural images, and wind super-resolvers often use upwards of 10 input channels, significantly more than the usual 3-channel RGB inputs in natural images. To better leverage a large number of conditioning variables in diffusion models, we present a generalization of classifier-free guidance (CFG) to multiple conditioning inputs. Our novel composite classifier-free guidance (CCFG) can be dropped into any pre-trained diffusion model trained with standard CFG dropout. We demonstrate that CCFG outputs are higher-fidelity than those from CFG on wind super-resolution tasks. We present WindDM, a diffusion model trained for industrial-scale wind dynamics reconstruction and leveraging CCFG. WindDM achieves state-of-the-art reconstruction quality among deep learning models and costs up to $1000\times$ less than classical methods.

</details>


### [33] [PIS: A Generalized Physical Inversion Solver for Arbitrary Sparse Observations via Set-Conditioned Diffusion](https://arxiv.org/abs/2512.13732)
*Weijie Yang,Xun Zhang*

Main category: cs.LG

TL;DR: PIS是一种基于集合条件扩散的物理参数反演框架，能在任意稀疏观测条件下稳定准确地求解PDE约束的反问题，相比现有方法显著降低误差并提供可靠的不确定性量化。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习和算子学习模型在处理稀疏、不规则、受传感器布置限制的间接观测时表现不佳：固定网格假设失效、重建质量急剧下降、反演不可靠且缺乏不确定性量化。这在流体力学、地震反演和结构健康监测等领域普遍存在。

Method: 提出物理反演求解器(PIS)，采用集合条件扩散框架，使用基于Set Transformer的编码器处理任意数量和几何分布的观测数据，并采用余弦退火稀疏课程实现卓越的鲁棒性。同时提供信息论分析来理解极端稀疏条件下的反演极限。

Result: 在三个具有挑战性的PDE反问题（达西流、波场反演、结构健康监测）上进行评估，包括观测率仅0.29%的极端稀疏情况。现有基线方法无法重建有意义的场，而PIS保持稳定准确，将反演误差降低12.28%-88.73%，并能可靠生成校准的后验样本。

Conclusion: PIS是一个强大、通用且对稀疏性具有独特韧性的解决方案，能够在任意和严重欠采样观测条件下进行物理反演，其生成的后验样本能准确反映数据稀缺性和固有的物理模糊性。

Abstract: Estimation of PDE-constrained physical parameters from limited indirect measurements is inherently ill-posed, particularly when observations are sparse, irregular, and constrained by real-world sensor placement. This challenge is ubiquitous in fields such as fluid mechanics, seismic inversion, and structural health monitoring. Existing deep and operator-learning models collapse under these conditions: fixed-grid assumptions fail, reconstruction deteriorates sharply, and inversion becomes unreliable with limited robustness and no uncertainty quantification (UQ).We propose the Physical Inversion Solver (PIS), a set-conditioned diffusion framework enabling inversion from truly arbitrary observation sets. PIS employs a Set Transformer-based encoder to handle measurements of any number or geometry, and a cosine-annealed sparsity curriculum for exceptional robustness. An accompanying information-theoretic analysis provides insight into the limits of inversion under extreme sparsity by revealing how observation entropy varies across physical systems.PIS is evaluated on three challenging PDE inverse problems: Darcy flow, wavefield inversion (Helmholtz), and structural health monitoring (Hooke's Law). Across all tasks and sparsity regimes -- including extreme cases with an observation rate of only $0.29\%$ -- existing operator-learning baselines fail to reconstruct meaningful fields, often diverging or collapsing entirely.In stark contrast, PIS remains stable and accurate, reducing inversion error by $12.28\%$--$88.73\%$ and reliably producing calibrated posterior samples. These samples accurately reflect both data scarcity and intrinsic physical ambiguity. These results position PIS as a powerful, general-purpose, and uniquely sparsity-resilient solution for physical inversion under arbitrary and severely undersampled observations.

</details>


### [34] [Low-Rank Compression of Language Models via Differentiable Rank Selection](https://arxiv.org/abs/2512.13733)
*Sidhant Sundrani,Francesco Tudisco,Pasquale Minervini*

Main category: cs.LG

TL;DR: LLRC是一种无需微调的梯度学习方法，通过学习掩码权重选择奇异值，优化大语言模型的低秩压缩，在保持激活相似性的同时实现更好的压缩效果。


<details>
  <summary>Details</summary>
Motivation: 现有低秩压缩方法在确定各层最优秩时面临挑战：启发式方法搜索空间有限可能导致次优结果，而梯度方法在不进行后压缩微调时性能不如启发式方法。需要一种无需微调就能联合优化压缩率和下游任务准确性的方法。

Method: 提出LLRC（Learning to Low-Rank Compress），一种基于梯度的无微调方法。使用校准数据集训练掩码权重，选择更少的奇异值，同时最小化中间激活与原始模型的差异。直接学习选择奇异值的掩码权重。

Result: 在Llama-2-13B模型上，20%压缩率下，LLRC在MMLU、BoolQ和OpenbookQA上分别比STRS方法提升12%、3.5%和4.4%。相比其他无微调压缩技术（SVD-LLM、LLM-Pruner），在各种数据集和压缩率下表现更优，且与需要微调的LLM-Pruner变体表现相当。

Conclusion: LLRC提供了一种有效的无微调低秩压缩方法，通过学习掩码权重直接优化奇异值选择，在保持模型性能的同时实现更好的压缩效果，为大型语言模型压缩提供了新的解决方案。

Abstract: Approaches for compressing large-language models using low-rank decomposition have made strides, particularly with the introduction of activation and loss-aware SVD, which improves the trade-off between decomposition rank and downstream task performance. Despite these advancements, a persistent challenge remains--selecting the optimal ranks for each layer to jointly optimise compression rate and downstream task accuracy. Current methods either rely on heuristics that can yield sub-optimal results due to their limited discrete search space or are gradient-based but are not as performant as heuristic approaches without post-compression fine-tuning. To address these issues, we propose Learning to Low-Rank Compress (LLRC), a gradient-based approach which directly learns the weights of masks that select singular values in a fine-tuning-free setting. Using a calibration dataset, we train only the mask weights to select fewer and fewer singular values while minimising the divergence of intermediate activations from the original model. Our approach outperforms competing ranking selection methods that similarly require no post-compression fine-tuning across various compression rates on common-sense reasoning and open-domain question-answering tasks. For instance, with a compression rate of 20% on Llama-2-13B, LLRC outperforms the competitive Sensitivity-based Truncation Rank Searching (STRS) on MMLU, BoolQ, and OpenbookQA by 12%, 3.5%, and 4.4%, respectively. Compared to other compression techniques, our approach consistently outperforms fine-tuning-free variants of SVD-LLM and LLM-Pruner across datasets and compression rates. Our fine-tuning-free approach also performs competitively with the fine-tuning variant of LLM-Pruner.

</details>


### [35] [Plug-and-Play Parameter-Efficient Tuning of Embeddings for Federated Recommendation](https://arxiv.org/abs/2512.13734)
*Haochen Yuan,Yang Zhang,Xiang He,Quan Z. Sheng,Zhongjie Wang*

Main category: cs.LG

TL;DR: 提出基于参数高效微调(PEFT)的联邦推荐框架，通过减少嵌入参数传输量来提升通信效率，同时保持或提高推荐准确性。


<details>
  <summary>Details</summary>
Motivation: 随着云边协同的发展，推荐服务在分布式环境中训练需求增加。联邦推荐通过共享模型参数而非原始数据保护隐私，但大量嵌入参数导致通信效率低下。现有研究主要关注模型效率提升，忽视了嵌入参数开销问题。

Method: 提出基于参数高效微调(PEFT)的联邦推荐训练框架，采用轻量级插件式设计，可无缝集成到现有联邦推荐方法中。除了整合LoRA和基于哈希的编码等常见PEFT技术外，还探索使用残差量化变分自编码器(RQ-VAE)作为新型PEFT策略。

Result: 在不同联邦推荐模型骨干和数据集上的广泛实验表明，该框架显著降低了通信开销，同时提高了推荐准确性。

Conclusion: 提出的基于PEFT的联邦推荐框架有效解决了嵌入参数传输效率问题，为分布式推荐系统提供了一种通信高效的解决方案。

Abstract: With the rise of cloud-edge collaboration, recommendation services are increasingly trained in distributed environments. Federated Recommendation (FR) enables such multi-end collaborative training while preserving privacy by sharing model parameters instead of raw data. However, the large number of parameters, primarily due to the massive item embeddings, significantly hampers communication efficiency. While existing studies mainly focus on improving the efficiency of FR models, they largely overlook the issue of embedding parameter overhead. To address this gap, we propose a FR training framework with Parameter-Efficient Fine-Tuning (PEFT) based embedding designed to reduce the volume of embedding parameters that need to be transmitted. Our approach offers a lightweight, plugin-style solution that can be seamlessly integrated into existing FR methods. In addition to incorporating common PEFT techniques such as LoRA and Hash-based encoding, we explore the use of Residual Quantized Variational Autoencoders (RQ-VAE) as a novel PEFT strategy within our framework. Extensive experiments across various FR model backbones and datasets demonstrate that our framework significantly reduces communication overhead while improving accuracy. The source code is available at https://github.com/young1010/FedPEFT.

</details>


### [36] [DARTs: A Dual-Path Robust Framework for Anomaly Detection in High-Dimensional Multivariate Time Series](https://arxiv.org/abs/2512.13735)
*Xuechun Liu,Heli Sun,Xuecheng Wu,Ruichen Cao,Yunyun Shi,Dingkang Yang,Haoran Li*

Main category: cs.LG

TL;DR: DARTs是一个用于高维多元时间序列异常检测的鲁棒长短时双路径框架，通过窗口感知的时空软融合机制有效处理噪声并捕获长距离时空依赖关系。


<details>
  <summary>Details</summary>
Motivation: 现有方法在低维场景下表现良好，但在处理高维噪声时间序列时难以鲁棒地捕获长距离时空依赖关系，这限制了在大型工业控制系统中的异常检测性能。

Method: 提出DARTs框架，包含三个互补组件：1) 短时路径：多视图稀疏图学习器和扩散多关系图单元协作捕获高噪声时间序列中的分层判别性短时时空模式；2) 长时路径：多尺度时空图构造器建模高维表示空间中的显著长时动态；3) 窗口感知时空软融合机制过滤残留噪声并无缝整合异常模式。

Result: 在主流数据集上的大量定性和定量实验结果表明，DARTs具有优越性和鲁棒性。消融研究也验证了所提出组件关键设计因素的有效性。

Conclusion: DARTs通过长短时双路径架构和窗口感知软融合机制，有效解决了高维噪声时间序列中长距离时空依赖关系捕获的挑战，在多元时间序列异常检测任务中表现出色。

Abstract: Multivariate time series anomaly detection (MTSAD) aims to accurately identify and localize complex abnormal patterns in the large-scale industrial control systems. While existing approaches excel in recognizing the distinct patterns under the low-dimensional scenarios, they often fail to robustly capture long-range spatiotemporal dependencies when learning representations from the high-dimensional noisy time series. To address these limitations, we propose DARTs, a robust long short-term dual-path framework with window-aware spatiotemporal soft fusion mechanism, which can be primarily decomposed into three complementary components. Specifically, in the short-term path, we introduce a Multi-View Sparse Graph Learner and a Diffusion Multi-Relation Graph Unit that collaborate to adaptively capture hierarchical discriminative short-term spatiotemporal patterns in the high-noise time series. While in the long-term path, we design a Multi-Scale Spatiotemporal Graph Constructor to model salient long-term dynamics within the high-dimensional representation space. Finally, a window-aware spatiotemporal soft-fusion mechanism is introduced to filter the residual noise while seamlessly integrating anomalous patterns. Extensive qualitative and quantitative experimental results across mainstream datasets demonstrate the superiority and robustness of our proposed DARTs. A series of ablation studies are also conducted to explore the crucial design factors of our proposed components. Our code and model will be made publicly open soon.

</details>


### [37] [TF-MCL: Time-frequency Fusion and Multi-domain Cross-Loss for Self-supervised Depression Detection](https://arxiv.org/abs/2512.13736)
*Li-Xuan Zhao,Chen-Yang Xu,Wen-Qiang Li,Bo Wang,Rong-Xing Wei,Qing-Hao Menga*

Main category: cs.LG

TL;DR: 提出TF-MCL模型，通过时间-频率融合和多域交叉损失改进对比学习，用于基于EEG信号的抑郁症检测，在公开数据集上显著超越现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于EEG信号的抑郁症检测方法主要依赖监督学习，但抑郁症标签获取困难。现有对比学习方法未能充分考虑EEG信号的时频分布特性，且对低语义数据表示能力不足，难以有效用于抑郁症检测任务。

Method: 提出TF-MCL模型，包含融合映射头(FMH)将时频域信息重映射到融合域，生成时频混合表示；通过优化多域交叉损失函数重构时频域和融合域的表示分布，提升模型获取融合表示的能力。

Result: 在公开数据集MODMA和PRED+CT上评估，准确率分别比现有最佳方法提升5.87%和9.96%，显示出显著性能改进。

Conclusion: TF-MCL模型通过时频融合和多域交叉损失有效解决了对比学习在EEG信号抑郁症检测中的局限性，显著提升了检测性能，为无标签或弱标签场景下的抑郁症检测提供了有效解决方案。

Abstract: In recent years, there has been a notable increase in the use of supervised detection methods of major depressive disorder (MDD) based on electroencephalogram (EEG) signals. However, the process of labeling MDD remains challenging. As a self-supervised learning method, contrastive learning could address the shortcomings of supervised learning methods, which are unduly reliant on labels in the context of MDD detection. However, existing contrastive learning methods are not specifically designed to characterize the time-frequency distribution of EEG signals, and their capacity to acquire low-semantic data representations is still inadequate for MDD detection tasks. To address the problem of contrastive learning method, we propose a time-frequency fusion and multi-domain cross-loss (TF-MCL) model for MDD detection. TF-MCL generates time-frequency hybrid representations through the use of a fusion mapping head (FMH), which efficiently remaps time-frequency domain information to the fusion domain, and thus can effectively enhance the model's capacity to synthesize time-frequency information. Moreover, by optimizing the multi-domain cross-loss function, the distribution of the representations in the time-frequency domain and the fusion domain is reconstructed, thereby improving the model's capacity to acquire fusion representations. We evaluated the performance of our model on the publicly available datasets MODMA and PRED+CT and show a significant improvement in accuracy, outperforming the existing state-of-the-art (SOTA) method by 5.87% and 9.96%, respectively.

</details>


### [38] [The Laminar Flow Hypothesis: Detecting Jailbreaks via Semantic Turbulence in Large Language Models](https://arxiv.org/abs/2512.13741)
*Md. Hasib Ur Rahman*

Main category: cs.LG

TL;DR: 论文提出"层流假说"，认为良性输入在LLM潜在空间中产生平滑过渡，而对抗性提示引发"语义湍流"，可通过层间余弦速度方差检测，实现轻量级越狱攻击检测和安全架构诊断。


<details>
  <summary>Details</summary>
Motivation: 当前LLM防御策略依赖计算昂贵的外部分类器或脆弱的词法过滤器，忽视了模型推理过程的内在动态。需要开发更有效、轻量的检测方法来应对越狱攻击。

Method: 提出层流假说，形式化定义"语义湍流"概念，开发零样本度量指标：层间余弦速度方差。通过实验评估不同小语言模型在对抗性提示下的内部动态变化。

Result: RLHF对齐的Qwen2-1.5B在攻击下湍流增加75.4%，验证了内部冲突假说；Gemma-2B湍流减少22.0%，表现出不同的"反射式"拒绝机制。语义湍流可作为轻量级实时越狱检测器。

Conclusion: 语义湍流不仅是有效的越狱攻击检测工具，还能作为非侵入式诊断工具，用于分类黑盒模型的安全架构，为LLM安全防御提供了新视角。

Abstract: As Large Language Models (LLMs) become ubiquitous, the challenge of securing them against adversarial "jailbreaking" attacks has intensified. Current defense strategies often rely on computationally expensive external classifiers or brittle lexical filters, overlooking the intrinsic dynamics of the model's reasoning process. In this work, the Laminar Flow Hypothesis is introduced, which posits that benign inputs induce smooth, gradual transitions in an LLM's high-dimensional latent space, whereas adversarial prompts trigger chaotic, high-variance trajectories - termed Semantic Turbulence - resulting from the internal conflict between safety alignment and instruction-following objectives. This phenomenon is formalized through a novel, zero-shot metric: the variance of layer-wise cosine velocity. Experimental evaluation across diverse small language models reveals a striking diagnostic capability. The RLHF-aligned Qwen2-1.5B exhibits a statistically significant 75.4% increase in turbulence under attack (p less than 0.001), validating the hypothesis of internal conflict. Conversely, Gemma-2B displays a 22.0% decrease in turbulence, characterizing a distinct, low-entropy "reflex-based" refusal mechanism. These findings demonstrate that Semantic Turbulence serves not only as a lightweight, real-time jailbreak detector but also as a non-invasive diagnostic tool for categorizing the underlying safety architecture of black-box models.

</details>


### [39] [Comparative Evaluation of Embedding Representations for Financial News Sentiment Analysis](https://arxiv.org/abs/2512.13749)
*Joyjit Roy,Samaresh Kumar Singh*

Main category: cs.LG

TL;DR: 该研究比较了在数据稀缺环境下基于嵌入的金融新闻情感分析方法，发现预训练嵌入在数据不足时效果有限，验证集过小会导致过拟合，建议采用少样本学习等替代方案。


<details>
  <summary>Details</summary>
Motivation: 金融情感分析对市场理解很重要，但标准NLP方法在小数据集上遇到显著挑战，特别是在资源受限环境中需要有效的解决方案。

Method: 使用Word2Vec、GloVe和句子转换器表示，结合梯度提升算法，在手动标注的金融新闻标题上进行比较评估。

Result: 实验结果显示验证集和测试集性能存在显著差距，模型表现不如简单基线；预训练嵌入在数据不足时收益递减；小验证集导致模型选择过程中的过拟合。

Conclusion: 嵌入质量本身无法解决情感分类中的数据稀缺问题，建议资源有限的从业者考虑少样本学习、数据增强或词典增强的混合方法。

Abstract: Financial sentiment analysis enhances market understanding; however, standard natural language processing approaches encounter significant challenges when applied to small datasets. This study provides a comparative evaluation of embedding-based methods for financial news sentiment classification in resource-constrained environments. Word2Vec, GloVe, and sentence transformer representations are evaluated in combination with gradient boosting on manually labeled headlines. Experimental results identify a substantial gap between validation and test performance, with models performing worse than trivial baselines despite strong validation metrics. The analysis demonstrates that pretrained embeddings yield diminishing returns below a critical data sufficiency threshold, and that small validation sets contribute to overfitting during model selection. Practical application is illustrated through weekly sentiment aggregation and narrative summarization for market monitoring workflows. The findings offer empirical evidence that embedding quality alone cannot address fundamental data scarcity in sentiment classification. For practitioners operating with limited resources, the results indicate the need to consider alternative approaches such as few-shot learning, data augmentation, or lexicon-enhanced hybrid methods when labeled samples are scarce.

</details>


### [40] [MIDUS: Memory-Infused Depth Up-Scaling](https://arxiv.org/abs/2512.13751)
*Taero Kim,Hoyoon Byun,Youngjun Choi,Sungrae Park,Kyungwoo Song*

Main category: cs.LG

TL;DR: MIDUS提出了一种新的深度扩展方法，用头级记忆层替代传统的FFN复制，通过为每个注意力头分配独立记忆库，在保持高效参数的同时提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 现有深度扩展方法依赖FFN复制，效率有限且性能提升受限。研究发现注意力头在层内和层间具有不同的功能角色，这为更高效的扩展方法提供了机会。

Method: 提出MIDUS方法，在复制的块中用头级记忆层替换FFN层，为每个注意力头分配独立记忆库，支持头级检索，将信息注入后续层，同时保持头级功能结构。设计包含稀疏记忆访问和高效的头级值分解模块。

Result: 在持续预训练实验中，MIDUS相比强基线DUS方法展现出稳健的性能提升，同时保持高效的参数占用，缓解了效率与性能之间的权衡。

Conclusion: MIDUS通过头级记忆设计，成为传统FFN复制进行深度扩展的有力且资源高效的替代方案，为大规模语言模型扩展提供了新方向。

Abstract: Scaling large language models (LLMs) demands approaches that increase capacity without incurring excessive parameter growth or inference cost. Depth Up-Scaling (DUS) has emerged as a promising strategy by duplicating layers and applying Continual Pre-training (CPT), but its reliance on feed-forward networks (FFNs) limits efficiency and attainable gains. We introduce Memory-Infused Depth Up-Scaling (MIDUS), which replaces FFNs in duplicated blocks with a head-wise memory (HML) layer. Motivated by observations that attention heads have distinct roles both across and within layers, MIDUS assigns an independent memory bank to each head, enabling head-wise retrieval and injecting information into subsequent layers while preserving head-wise functional structure. This design combines sparse memory access with head-wise representations and incorporates an efficient per-head value factorization module, thereby relaxing the usual efficiency-performance trade-off. Across our CPT experiments, MIDUS exhibits robust performance improvements over strong DUS baselines while maintaining a highly efficient parameter footprint. Our findings establish MIDUS as a compelling and resource-efficient alternative to conventional FFN replication for depth up-scaling by leveraging its head-wise memory design.

</details>


### [41] [Constrained Policy Optimization via Sampling-Based Weight-Space Projection](https://arxiv.org/abs/2512.13788)
*Shengfan Cao,Francesco Borrelli*

Main category: cs.LG

TL;DR: SCPO是一种基于采样的权重空间投影方法，用于在未知安全约束下进行策略学习，通过轨迹采样和光滑性边界构建局部安全区域，使用凸SOCP进行投影更新，确保训练过程中的安全性。


<details>
  <summary>Details</summary>
Motivation: 安全关键学习需要在不离开安全操作区域的情况下提升性能。研究者在模型参数必须满足基于rollout的未知安全约束的场景下，需要一种能够直接在参数空间强制安全约束的方法，而不需要约束函数的梯度信息。

Method: 提出SCPO方法：1) 通过轨迹采样和参数变化与安全指标偏移之间的光滑性边界构建局部安全区域；2) 使用凸二阶锥规划(SOCP)将梯度更新投影到安全区域；3) 在具有稳定备份策略的约束控制设置中，确保闭环稳定性并实现超出保守备份的安全适应。

Result: 在有害监督的回归任务和具有恶意专家的约束双积分器任务中，该方法能够一致拒绝不安全更新，在整个训练过程中保持可行性，并实现有意义的原始目标改进。

Conclusion: SCPO方法提供了安全归纳保证：从任何安全初始化开始，只要投影可行，所有中间策略都保持安全。该方法在约束策略学习中实现了安全第一阶步进，为安全关键学习提供了有效的解决方案。

Abstract: Safety-critical learning requires policies that improve performance without leaving the safe operating regime. We study constrained policy learning where model parameters must satisfy unknown, rollout-based safety constraints. We propose SCPO, a sampling-based weight-space projection method that enforces safety directly in parameter space without requiring gradient access to the constraint functions. Our approach constructs a local safe region by combining trajectory rollouts with smoothness bounds that relate parameter changes to shifts in safety metrics. Each gradient update is then projected via a convex SOCP, producing a safe first-order step. We establish a safe-by-induction guarantee: starting from any safe initialization, all intermediate policies remain safe given feasible projections. In constrained control settings with a stabilizing backup policy, our approach further ensures closed-loop stability and enables safe adaptation beyond the conservative backup. On regression with harmful supervision and a constrained double-integrator task with malicious expert, our approach consistently rejects unsafe updates, maintains feasibility throughout training, and achieves meaningful primal objective improvement.

</details>


### [42] [The Double Life of Code World Models: Provably Unmasking Malicious Behavior Through Execution Traces](https://arxiv.org/abs/2512.13821)
*Subramanyam Sahoo,Jared Junkin*

Main category: cs.LG

TL;DR: 提出CTVP框架，通过语义轨道分析验证不可信代码生成模型，检测后门注入，无需直接执行恶意代码


<details>
  <summary>Details</summary>
Motivation: 大型语言模型越来越多地生成代码而缺乏人工监督，引发了对后门注入和恶意行为的严重担忧

Method: Cross-Trace Verification Protocol (CTVP)框架，通过分析模型在语义等价程序变换上的执行轨迹预测一致性模式来检测行为异常

Result: 引入Adversarial Robustness Quotient (ARQ)量化验证计算成本，理论分析显示指数增长且不可博弈性，为代码生成任务提供可扩展的AI控制方法

Conclusion: 语义轨道分析为代码生成任务提供了可扩展、理论基础的AI控制方法，能够有效检测后门行为

Abstract: Large language models (LLMs) increasingly generate code with minimal human oversight, raising critical concerns about backdoor injection and malicious behavior. We present Cross-Trace Verification Protocol (CTVP), a novel AI control framework that verifies untrusted code-generating models through semantic orbit analysis. Rather than directly executing potentially malicious code, CTVP leverages the model's own predictions of execution traces across semantically equivalent program transformations. By analyzing consistency patterns in these predicted traces, we detect behavioral anomalies indicative of backdoors. Our approach introduces the Adversarial Robustness Quotient (ARQ), which quantifies the computational cost of verification relative to baseline generation, demonstrating exponential growth with orbit size. Theoretical analysis establishes information-theoretic bounds showing non-gamifiability -- adversaries cannot improve through training due to fundamental space complexity constraints. This work demonstrates that semantic orbit analysis provides a scalable, theoretically grounded approach to AI control for code generation tasks.

</details>


### [43] [Explainable reinforcement learning from human feedback to improve alignment](https://arxiv.org/abs/2512.13837)
*Shicheng Liu,Siyuan Xu,Wenjie Qiu,Hangfan Zhang,Minghui Zhu*

Main category: cs.LG

TL;DR: 本文提出了一种通过识别并修正导致不满意响应的训练数据来改进RLHF的方法，包括事后解释和反学习两个部分。


<details>
  <summary>Details</summary>
Motivation: 受人类通过寻找并修正原因来改进不满意结果的行为启发，研究者希望将这种策略应用于改进语言模型的RLHF对齐。现有研究表明，经过RLHF调优的语言模型仍可能产生不满意响应，需要一种系统方法来改进这些响应。

Method: 方法分为两部分：1）事后解释方法：将问题形式化为约束组合优化问题，寻找最接近提示-响应对的训练数据集，并提出高效的迭代数据选择算法；2）反学习方法：通过反学习导致不满意响应的训练数据来改进响应，同时不显著影响其他满意响应。

Result: 实验结果表明，该算法能够有效改进RLHF的性能，通过修正导致不满意响应的训练数据来提升模型输出质量。

Conclusion: 将人类通过修正原因来改进结果的策略应用于RLHF是可行的，提出的方法能够识别并修正导致不满意响应的训练数据，从而有效改进语言模型的对齐效果。

Abstract: A common and effective strategy for humans to improve an unsatisfactory outcome in daily life is to find a cause of this outcome and correct the cause. In this paper, we investigate whether this human improvement strategy can be applied to improving reinforcement learning from human feedback (RLHF) for alignment of language models (LMs). In particular, it is observed in the literature that LMs tuned by RLHF can still output unsatisfactory responses. This paper proposes a method to improve the unsatisfactory responses by correcting their causes. Our method has two parts. The first part proposes a post-hoc explanation method to explain why an unsatisfactory response is generated to a prompt by identifying the training data that lead to this response. We formulate this problem as a constrained combinatorial optimization problem where the objective is to find a set of training data closest to this prompt-response pair in a feature representation space, and the constraint is that the prompt-response pair can be decomposed as a convex combination of this set of training data in the feature space. We propose an efficient iterative data selection algorithm to solve this problem. The second part proposes an unlearning method that improves unsatisfactory responses to some prompts by unlearning the training data that lead to these unsatisfactory responses and, meanwhile, does not significantly degrade satisfactory responses to other prompts. Experimental results demonstrate that our algorithm can improve RLHF.

</details>


### [44] [Measuring Uncertainty Calibration](https://arxiv.org/abs/2512.13872)
*Kamil Ciosek,Nicolò Felicioni,Sina Ghiassian,Juan Elenter Litwin,Francesco Tonolini,David Gustaffson,Eva Garcia Martin,Carmen Barcena Gonzales,Raphaëlle Bertrand-Lalo*

Main category: cs.LG

TL;DR: 该论文提出了两种估计二元分类器L1校准误差的方法：一是为有界变差校准函数的分类器提供上界；二是提供一种修改分类器的方法，使其校准误差能被高效上界估计且不影响性能。


<details>
  <summary>Details</summary>
Motivation: 从有限数据集估计二元分类器的L1校准误差是一个重要但具有挑战性的问题。现有方法通常需要强假设或渐进分析，缺乏实用、分布无关的非渐进方法。

Method: 1. 为校准函数具有有界变差的分类器提供L1校准误差的上界；2. 提出一种修改分类器的方法，使得修改后的分类器校准误差能被高效上界估计，且不显著影响分类性能，无需限制性假设。

Result: 所有结果都是非渐进且分布无关的。提出的方法产生了实用的程序，可以在真实世界数据集上以适度的开销运行，为实际测量校准误差提供了可行的解决方案。

Conclusion: 论文提供了实用的校准误差测量建议，提出的方法能够在保持分类器性能的同时，高效地估计校准误差上界，为实际应用中的校准评估提供了有效工具。

Abstract: We make two contributions to the problem of estimating the $L_1$ calibration error of a binary classifier from a finite dataset. First, we provide an upper bound for any classifier where the calibration function has bounded variation. Second, we provide a method of modifying any classifier so that its calibration error can be upper bounded efficiently without significantly impacting classifier performance and without any restrictive assumptions. All our results are non-asymptotic and distribution-free. We conclude by providing advice on how to measure calibration error in practice. Our methods yield practical procedures that can be run on real-world datasets with modest overhead.

</details>


### [45] [Privacy-Enhancing Infant Cry Classification with Federated Transformers and Denoising Regularization](https://arxiv.org/abs/2512.13880)
*Geofrey Owino,Bernard Shibwabo*

Main category: cs.LG

TL;DR: 提出一个结合去噪自编码器、卷积分词器和Transformer编码器的婴儿哭声分类系统，采用联邦学习训练，实现隐私保护、噪声鲁棒和通信高效的部署。


<details>
  <summary>Details</summary>
Motivation: 婴儿哭声分类有助于早期评估婴儿需求，但现有方案面临音频数据隐私问题、背景噪声敏感性和跨录音环境的领域偏移等部署限制。

Method: 端到端婴儿哭声分析管道，集成去噪自编码器、卷积分词器和Transformer编码器，采用通信高效的联邦学习训练，包含设备端去噪、自适应分割、事后校准和基于能量的分布外弃权机制。

Result: 在Baby Chillanto和Donate-a-Cry数据集上，模型获得宏观F1分数0.938、AUC 0.962和预期校准误差0.032，每轮客户端上传从约36-42MB减少到3.3MB，在NVIDIA Jetson Nano上实现每帧96ms的实时推理。

Conclusion: 该系统展示了隐私保护、噪声鲁棒和通信高效的婴儿哭声分类的实用路径，适合联邦部署。

Abstract: Infant cry classification can aid early assessment of infant needs. However, deployment of such solutions is limited by privacy concerns around audio data, sensitivity to background noise, and domain shift across recording environments. We present an end-to-end infant cry analysis pipeline that integrates a denoising autoencoder (DAE), a convolutional tokenizer, and a Transformer encoder trained using communication-efficient federated learning (FL). The system performs on-device denoising, adaptive segmentation, post hoc calibration, and energy-based out-of-distribution (OOD) abstention. Federated training employs a regularized control variate update with 8-bit adapter deltas under secure aggregation. Using the Baby Chillanto and Donate-a-Cry datasets with ESC-50 noise overlays, the model achieves a macro F1 score of 0.938, an AUC of 0.962, and an Expected Calibration Error (ECE) of 0.032, while reducing per-round client upload from approximately 36 to 42 MB to 3.3 MB. Real-time edge inference on an NVIDIA Jetson Nano (4 GB, TensorRT FP16) achieves 96 ms per one-second spectrogram frame. These results demonstrate a practical path toward privacy-preserving, noise-robust, and communication-efficient infant cry classification suitable for federated deployment.

</details>


### [46] [OPTIMA: Optimal One-shot Pruning for LLMs via Quadratic Programming Reconstruction](https://arxiv.org/abs/2512.13886)
*Mohammad Mozaffari,Samuel Kushnir,Maryam Mehri Dehnavi,Amir Yazdanbakhsh*

Main category: cs.LG

TL;DR: OPTIMA是一种实用的单次训练后剪枝方法，通过将层间权重重构转化为独立的行式二次规划问题，在保持精度的同时实现大规模剪枝。


<details>
  <summary>Details</summary>
Motivation: 训练后模型剪枝面临权衡：简单的启发式方法速度快但精度下降，而联合优化方法能恢复精度但计算成本过高。需要一种既能保持精度又具有可扩展性的实用剪枝方法。

Method: OPTIMA将掩码选择后的层间权重重构转化为独立的行式二次规划问题，这些QP共享共同的层Hessian矩阵。通过实现加速器友好的QP求解器，每层累积一个Hessian并并行求解多个小型QP，实现单次训练后剪枝。

Result: OPTIMA在多个LLM家族和稀疏度下持续改进零样本性能，获得高达3.97%的绝对精度提升。在NVIDIA H100上，OPTIMA在40小时内完成8B参数transformer的端到端剪枝，峰值内存为60GB。

Conclusion: OPTIMA为单次训练后剪枝设定了新的精度-效率权衡基准，实现了实用的大规模模型剪枝，无需微调即可在单个加速器上完成。

Abstract: Post-training model pruning is a promising solution, yet it faces a trade-off: simple heuristics that zero weights are fast but degrade accuracy, while principled joint optimization methods recover accuracy but are computationally infeasible at modern scale. One-shot methods such as SparseGPT offer a practical trade-off in optimality by applying efficient, approximate heuristic weight updates. To close this gap, we introduce OPTIMA, a practical one-shot post-training pruning method that balances accuracy and scalability. OPTIMA casts layer-wise weight reconstruction after mask selection as independent, row-wise Quadratic Programs (QPs) that share a common layer Hessian. Solving these QPs yields the per-row globally optimal update with respect to the reconstruction objective given the estimated Hessian. The shared-Hessian structure makes the problem highly amenable to batching on accelerators. We implement an accelerator-friendly QP solver that accumulates one Hessian per layer and solves many small QPs in parallel, enabling one-shot post-training pruning at scale on a single accelerator without fine-tuning. OPTIMA integrates with existing mask selectors and consistently improves zero-shot performance across multiple LLM families and sparsity regimes, yielding up to 3.97% absolute accuracy improvement. On an NVIDIA H100, OPTIMA prunes a 8B-parameter transformer end-to-end in 40 hours with 60GB peak memory. Together, these results set a new state-of-the-art accuracy-efficiency trade-offs for one-shot post-training pruning.

</details>


### [47] [Let's (not) just put things in Context: Test-Time Training for Long-Context LLMs](https://arxiv.org/abs/2512.13898)
*Rachit Bansal,Aston Zhang,Rishabh Tiwari,Lovish Madaan,Sai Surya Duvvuri,Devvrit Khatri,David Brandfonbrener,David Alvarez-Melis,Prajjwal Bhargava,Mihir Sanjay Kale,Samy Jelassi*

Main category: cs.LG

TL;DR: 研究发现长上下文LLM存在"分数稀释"问题，当前推理时策略（如生成更多思考标记）在长上下文任务中效果有限，提出通过针对特定上下文的梯度更新来显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM已经能够处理百万标记的长上下文，但经验表明它们无法可靠利用这么长的文本。同时，推理时计算（如生成思考标记）在复杂推理任务中有效，但在长上下文任务中效果不佳，需要探索更有效的长上下文利用方法。

Method: 通过控制实验发现当前推理时策略在长上下文任务中效果有限，提出"分数稀释"概念解释静态自注意力的固有局限性。提出简单方法：在给定上下文上进行有针对性的梯度更新，理论上克服静态自注意力的限制。

Result: 该方法在模型和长上下文基准测试中带来一致的大幅性能提升。Qwen3-4B在LongBench-v2和ZeroScrolls基准测试子集上平均分别提升12.6和14.1个百分点。

Conclusion: 对于长上下文任务，少量针对特定上下文的训练比当前推理时扩展策略（如生成更多思考标记）是更好的推理计算利用方式。该方法为长上下文LLM的有效利用提供了实用解决方案。

Abstract: Progress on training and architecture strategies has enabled LLMs with millions of tokens in context length. However, empirical evidence suggests that such long-context LLMs can consume far more text than they can reliably use. On the other hand, it has been shown that inference-time compute can be used to scale performance of LLMs, often by generating thinking tokens, on challenging tasks involving multi-step reasoning. Through controlled experiments on sandbox long-context tasks, we find that such inference-time strategies show rapidly diminishing returns and fail at long context. We attribute these failures to score dilution, a phenomenon inherent to static self-attention. Further, we show that current inference-time strategies cannot retrieve relevant long-context signals under certain conditions. We propose a simple method that, through targeted gradient updates on the given context, provably overcomes limitations of static self-attention. We find that this shift in how inference-time compute is spent leads to consistently large performance improvements across models and long-context benchmarks. Our method leads to large 12.6 and 14.1 percentage point improvements for Qwen3-4B on average across subsets of LongBench-v2 and ZeroScrolls benchmarks. The takeaway is practical: for long context, a small amount of context-specific training is a better use of inference compute than current inference-time scaling strategies like producing more thinking tokens.

</details>


### [48] [Exploring Machine Learning, Deep Learning, and Explainable AI Methods for Seasonal Precipitation Prediction in South America](https://arxiv.org/abs/2512.13910)
*Matheus Corrêa Domingos,Valdivino Alexandre de Santiago Júnior,Juliana Aparecida Anochi,Elcio Hideiti Shiguemori,Luísa Mirelle Costa dos Santos,Hércules Carlos dos Santos Pereira,André Estevam Costa Oliveira*

Main category: cs.LG

TL;DR: 该研究比较了传统机器学习、深度学习和动态模型在南美洲降水预报中的表现，发现LSTM模型在强降水预报中表现最佳，而传统动态模型BAM表现最差。


<details>
  <summary>Details</summary>
Motivation: 降水预报对社会至关重要，但传统动态模型存在局限性。虽然AI技术已被用于气象预报，但缺乏对纯数据驱动方法在降水预报中可行性的广泛研究，特别是在南美洲地区。

Method: 研究比较了多种方法：传统机器学习（随机森林、XGBoost）、深度学习（1D CNN、LSTM、GRU）和传统动态模型（巴西全球大气模型BAM）。使用2019年所有季节的数据，并采用可解释AI技术分析模型行为。

Result: LSTM模型表现出最强的预测性能，特别是在强降水预报中准确性最高。传统动态模型BAM表现最差。XGBoost在延迟方面表现更好，但精度略有损失。深度学习模型在气候预报中显示出可行性。

Conclusion: 深度学习模型（特别是LSTM）在降水预报中表现出色，证实了数据驱动方法在气候预报中的可行性，这与全球主要气象和气候预报中心的趋势一致。

Abstract: Forecasting meteorological variables is challenging due to the complexity of their processes, requiring advanced models for accuracy. Accurate precipitation forecasts are vital for society. Reliable predictions help communities mitigate climatic impacts. Based on the current relevance of artificial intelligence (AI), classical machine learning (ML) and deep learning (DL) techniques have been used as an alternative or complement to dynamic modeling. However, there is still a lack of broad investigations into the feasibility of purely data-driven approaches for precipitation forecasting. This study aims at addressing this issue where different classical ML and DL approaches for forecasting precipitation in South America, taking into account all 2019 seasons, are considered in a detailed investigation. The selected classical ML techniques were Random Forests and extreme gradient boosting (XGBoost), while the DL counterparts were a 1D convolutional neural network (CNN 1D), a long short-term memory (LSTM) model, and a gated recurrent unit (GRU) model. Additionally, the Brazilian Global Atmospheric Model (BAM) was used as a representative of the traditional dynamic modeling approach. We also relied on explainable artificial intelligence (XAI) to provide some explanations for the models behaviors. LSTM showed strong predictive performance while BAM, the traditional dynamic model representative, had the worst results. Despite presented the higher latency, LSTM was most accurate for heavy precipitation. If cost is a concern, XGBoost offers lower latency with slightly accuracy loss. The results of this research confirm the viability of DL models for climate forecasting, solidifying a global trend in major meteorological and climate forecasting centers.

</details>


### [49] [Capturing reduced-order quantum many-body dynamics out of equilibrium via neural ordinary differential equations](https://arxiv.org/abs/2512.13913)
*Patrick Egenlauf,Iva Březinová,Sabine Andergassen,Miriam Klopotek*

Main category: cs.LG

TL;DR: 该研究使用神经ODE模型分析量子多体系统中二粒子约化密度矩阵的动力学，发现只有在二粒子和三粒子关联度高的参数区域才能准确预测，而在反关联或无关联区域则失败，表明需要记忆依赖的闭合方案。


<details>
  <summary>Details</summary>
Motivation: 研究量子多体系统非平衡动力学时，精确波函数方法计算复杂度随粒子数指数增长，而平均场方法又忽略了重要的二粒子关联。时间依赖的二粒子约化密度矩阵（TD2RDM）方法通过传播二粒子约化密度矩阵并闭合BBGKY层次结构提供了一种折中方案，但忽略记忆效应的时域重构泛函在不同动力学区域的有效性和存在性仍不清楚。

Method: 使用神经ODE模型，在精确的二粒子约化密度矩阵数据上进行训练（无维度约简），尝试仅基于瞬时二粒子累积量重构动力学，而不需要显式的三粒子信息。通过分析二粒子和三粒子累积量之间的Pearson相关性来评估模型性能。

Result: 神经ODE模型仅在二粒子和三粒子累积量Pearson相关性大的参数区域能够准确再现动力学。在反关联或无关联区域，模型失败，表明没有简单的瞬时二粒子累积量时域泛函能够捕捉演化过程。时间平均的三粒子关联积累幅度是预测成功的主要指标：中等关联积累时，神经ODE预测和现有TD2RDM重构都准确；而关联积累更强时则出现系统性失效。

Conclusion: 研究结果表明，在强关联区域需要三粒子累积量重构中的记忆依赖核。神经ODE可作为模型无关的诊断工具，用于映射累积量展开方法的适用区域，并指导非局域闭合方案的发展。从有限数据中学习高维RDM动力学的能力为快速、数据驱动的关联量子物质模拟开辟了新途径。

Abstract: Out-of-equilibrium quantum many-body systems exhibit rapid correlation buildup that underlies many emerging phenomena. Exact wave-function methods to describe this scale exponentially with particle number; simpler mean-field approaches neglect essential two-particle correlations. The time-dependent two-particle reduced density matrix (TD2RDM) formalism offers a middle ground by propagating the two-particle reduced density matrix (2RDM) and closing the BBGKY hierarchy with a reconstruction of the three-particle cumulant. But the validity and existence of time-local reconstruction functionals ignoring memory effects remain unclear across different dynamical regimes. We show that a neural ODE model trained on exact 2RDM data (no dimensionality reduction) can reproduce its dynamics without any explicit three-particle information -- but only in parameter regions where the Pearson correlation between the two- and three-particle cumulants is large. In the anti-correlated or uncorrelated regime, the neural ODE fails, indicating that no simple time-local functional of the instantaneous two-particle cumulant can capture the evolution. The magnitude of the time-averaged three-particle-correlation buildup appears to be the primary predictor of success: For a moderate correlation buildup, both neural ODE predictions and existing TD2RDM reconstructions are accurate, whereas stronger values lead to systematic breakdowns. These findings pinpoint the need for memory-dependent kernels in the three-particle cumulant reconstruction for the latter regime. Our results place the neural ODE as a model-agnostic diagnostic tool that maps the regime of applicability of cumulant expansion methods and guides the development of non-local closure schemes. More broadly, the ability to learn high-dimensional RDM dynamics from limited data opens a pathway to fast, data-driven simulation of correlated quantum matter.

</details>


### [50] [EDGC: Entropy-driven Dynamic Gradient Compression for Efficient LLM Training](https://arxiv.org/abs/2511.10333)
*Qingao Yi,Jiaang Duan,Hanwen Hu,Qin Hua,Haiyan Zhao,Shiyou Qian,Dingyu Yang,Jian Cao,Jinghua Tang,Yinghao Yu,Chenzhi Liao,Kangjin Wang,Liping Zhang*

Main category: cs.LG

TL;DR: EDGC是一种基于熵的动态梯度压缩框架，通过根据梯度熵演化趋势调整压缩率，在保持模型精度的同时显著减少LLM训练中的通信开销。


<details>
  <summary>Details</summary>
Motivation: 训练大型语言模型面临计算资源和内存容量的重大挑战。现有静态梯度压缩方法忽略了训练过程中梯度的动态演化特性，导致性能下降，需要在保持性能的同时加速LLM训练。

Method: 提出EDGC框架，包含三个关键组件：1) 使用下采样方法高效估计梯度熵；2) 建立压缩率与梯度熵的理论模型；3) 基于窗口的调整机制动态适应管道阶段的压缩率。

Result: 在32-NVIDIA-V100集群上训练GPT2-2.5B和在64-NVIDIA-H100集群上训练GPT2-12.1B，EDGC将通信延迟和训练时间分别减少了46.45%和16.13%，同时保持了LLM的准确性。

Conclusion: EDGC通过动态调整压缩率，有效解决了LLM训练中的通信瓶颈问题，在保持模型性能的同时显著提升了训练效率。

Abstract: Training large language models (LLMs) poses significant challenges regarding computational resources and memory capacity. Although distributed training techniques help mitigate these issues, they still suffer from considerable communication overhead. Existing approaches primarily rely on static gradient compression to enhance communication efficiency; however, these methods neglect the dynamic nature of evolving gradients during training, leading to performance degradation. Accelerating LLM training via compression without sacrificing performance remains a challenge. In this paper, we propose an entropy-driven dynamic gradient compression framework called EDGC. The core concept is to adjust the compression rate during LLM training based on the evolving trends of gradient entropy, taking into account both compression efficiency and error. EDGC consists of three key components.First, it employs a down-sampling method to efficiently estimate gradient entropy, reducing computation overhead. Second, it establishes a theoretical model linking compression rate with gradient entropy, enabling more informed compression decisions. Lastly, a window-based adjustment mechanism dynamically adapts the compression rate across pipeline stages, improving communication efficiency and maintaining model performance. We implemented EDGC on a 32-NVIDIA-V100 cluster and a 64-NVIDIA-H100 cluster to train GPT2-2.5B and GPT2-12.1B, respectively. The results show that EDGC significantly reduces communication latency and training time by up to 46.45% and 16.13% while preserving LLM accuracy.

</details>


### [51] [Sliding Window Recurrences for Sequence Models](https://arxiv.org/abs/2512.13921)
*Dragos Secrieru,Garyk Brixi,Yoshua Bengio,Taiji Suzuki,Michael Poli,Stefano Massaroli*

Main category: cs.LG

TL;DR: 论文提出了一种用于线性递归的分层分解框架，开发了与GPU内存层次结构对齐的滑动窗口递归算法，并基于此构建了Phalanx层作为窗口注意力或线性递归的替代方案，在10亿参数的多混合模型中实现了显著的速度提升。


<details>
  <summary>Details</summary>
Motivation: 多混合架构因其更好的质量和性能有望主导语言建模。现有方法在处理长上下文时面临效率问题，需要开发与GPU内存层次结构对齐的算法来优化性能。

Method: 提出了分层分解框架用于线性递归，开发了滑动窗口递归算法，将递归截断为硬件对齐的窗口以减少跨warp通信开销。基于此构建了Phalanx层作为窗口注意力或线性递归的即插即用替代方案。

Result: 在10亿参数的多混合模型中，Phalanx在4K到32K上下文长度范围内相比优化的Transformer实现了10-40%的速度提升，同时保持了相同的困惑度性能。

Conclusion: 滑动窗口递归和Phalanx层为多混合语言模型提供了高效的长上下文处理方案，在保持模型质量的同时显著提升了推理速度。

Abstract: Multi-hybrid architectures are poised to take over language modeling due to better quality and performance. We introduce a hierarchical decomposition framework for linear recurrences that allows us to develop algorithms aligned with GPU memory hierarchies, yielding Sliding Window Recurrences. We focus specifically on truncating recurrences to hardware-aligned windows which are naturally jagged, limiting costly inter-warp communication. Using SWR, we develop Phalanx layers that serve as drop-in replacements for windowed attention or linear recurrences. In 1B parameter multi-hybrid models, Phalanx achieves over 10-40% speedup across 4K to 32K context length over optimized Transformers while matching perplexity.

</details>


### [52] [Pattern-Guided Diffusion Models](https://arxiv.org/abs/2512.13945)
*Vivian Lin,Kuk Jin Jang,Wenwen Si,Insup Lee*

Main category: cs.LG

TL;DR: PGDM利用原型分析提取时间序列中的重复模式，通过模式引导扩散模型进行更准确的预测，并引入基于原型分析的不确定性量化技术。


<details>
  <summary>Details</summary>
Motivation: 现有扩散模型在多元时间序列预测中很少考虑数据中的重复结构或模式，这限制了预测的准确性和现实性。

Method: 使用原型分析提取时间序列中的固有模式，估计最可能的下一个模式，并用该模式估计引导扩散模型的预测，同时基于模式估计不确定性动态调整引导水平。

Result: 在视觉场测量和运动捕捉帧预测任务中，模式引导使PGDM性能提升高达40.67%/56.26%和14.12%/14.10%，且优于基线方法高达65.58%/84.83%和93.64%/92.55%。

Conclusion: 模式引导扩散模型能有效利用时间序列中的重复结构，显著提升预测性能，并为不确定性量化提供了新方法。

Abstract: Diffusion models have shown promise in forecasting future data from multivariate time series. However, few existing methods account for recurring structures, or patterns, that appear within the data. We present Pattern-Guided Diffusion Models (PGDM), which leverage inherent patterns within temporal data for forecasting future time steps. PGDM first extracts patterns using archetypal analysis and estimates the most likely next pattern in the sequence. By guiding predictions with this pattern estimate, PGDM makes more realistic predictions that fit within the set of known patterns. We additionally introduce a novel uncertainty quantification technique based on archetypal analysis, and we dynamically scale the guidance level based on the pattern estimate uncertainty. We apply our method to two well-motivated forecasting applications, predicting visual field measurements and motion capture frames. On both, we show that pattern guidance improves PGDM's performance (MAE / CRPS) by up to 40.67% / 56.26% and 14.12% / 14.10%, respectively. PGDM also outperforms baselines by up to 65.58% / 84.83% and 93.64% / 92.55%.

</details>


### [53] [Accelerating MHC-II Epitope Discovery via Multi-Scale Prediction in Antigen Presentation](https://arxiv.org/abs/2512.14011)
*Yue Wan,Jiayi Yuan,Zhiwei Feng,Xiaowei Jia*

Main category: cs.LG

TL;DR: 该研究构建了一个经过精心整理的MHC-II抗原表位数据集，并基于此设计了三个机器学习任务来推进计算免疫治疗研究。


<details>
  <summary>Details</summary>
Motivation: MHC-II抗原表位在免疫治疗中至关重要，但与MHC-I相比，其研究面临更大挑战：结合特异性复杂、基序模式模糊、现有数据集较小且标准化程度低。

Method: 从IEDB和其他公共来源构建精心整理的数据集，扩展并标准化现有肽-MHC-II数据集，并引入具有更丰富生物学背景的新型抗原-MHC-II数据集。基于此数据集制定了肽结合、肽呈递和抗原呈递三个机器学习任务，并采用多尺度评估框架对现有模型进行基准测试。

Result: 创建了一个标准化的MHC-II相互作用数据集，引入了新的抗原-MHC-II数据集，建立了三个渐进式机器学习任务框架，并对现有模型进行了全面评估和分析。

Conclusion: 这项工作为推进计算免疫治疗提供了宝贵资源，为未来ML指导的表位发现和免疫反应预测建模研究奠定了基础。

Abstract: Antigenic epitope presented by major histocompatibility complex II (MHC-II) proteins plays an essential role in immunotherapy. However, compared to the more widely studied MHC-I in computational immunotherapy, the study of MHC-II antigenic epitope poses significantly more challenges due to its complex binding specificity and ambiguous motif patterns. Consequently, existing datasets for MHC-II interactions are smaller and less standardized than those available for MHC-I. To address these challenges, we present a well-curated dataset derived from the Immune Epitope Database (IEDB) and other public sources. It not only extends and standardizes existing peptide-MHC-II datasets, but also introduces a novel antigen-MHC-II dataset with richer biological context. Leveraging this dataset, we formulate three major machine learning (ML) tasks of peptide binding, peptide presentation, and antigen presentation, which progressively capture the broader biological processes within the MHC-II antigen presentation pathway. We further employ a multi-scale evaluation framework to benchmark existing models, along with a comprehensive analysis over various modeling designs to this problem with a modular framework. Overall, this work serves as a valuable resource for advancing computational immunotherapy, providing a foundation for future research in ML guided epitope discovery and predictive modeling of immune responses.

</details>


### [54] [EXAONE Path 2.5: Pathology Foundation Model with Multi-Omics Alignment](https://arxiv.org/abs/2512.14019)
*Juseung Yun,Sunwoo Yu,Sumin Ha,Jonghyun Kim,Janghyeon Lee,Jongseong Jang,Soonyoung Lee*

Main category: cs.LG

TL;DR: EXAONE Path 2.5是一个病理学基础模型，通过联合建模组织学、基因组学、表观遗传学和转录组学等多模态数据，创建更全面的患者肿瘤生物学表征。


<details>
  <summary>Details</summary>
Motivation: 癌症进展涉及多个生物学层面的相互作用，特别是超出形态学层面的分子层面，这些对仅基于图像的模型是不可见的。为了捕捉更广泛的生物学景观，需要整合多模态数据来更全面地反映肿瘤生物学。

Method: 1. 多模态SigLIP损失实现异质模态间的全配对对比学习；2. 片段感知旋转位置编码(F-RoPE)模块，在WSI中保留空间结构和组织片段拓扑；3. 针对WSI和RNA-seq的领域专用内部基础模型，提供生物学基础的嵌入以实现稳健的多模态对齐。

Result: 在两个互补基准测试中评估：内部真实世界临床数据集和包含80个任务的Patho-Bench基准。该框架表现出高数据和参数效率，在Patho-Bench上达到与最先进基础模型相当的性能，同时在内部临床设置中表现出最高的适应性。

Conclusion: 结果强调了生物学信息多模态设计的价值，并突显了整合基因型到表型建模在下一代精准肿瘤学中的潜力。

Abstract: Cancer progression arises from interactions across multiple biological layers, especially beyond morphological and across molecular layers that remain invisible to image-only models. To capture this broader biological landscape, we present EXAONE Path 2.5, a pathology foundation model that jointly models histologic, genomic, epigenetic and transcriptomic modalities, producing an integrated patient representation that reflects tumor biology more comprehensively. Our approach incorporates three key components: (1) multimodal SigLIP loss enabling all-pairwise contrastive learning across heterogeneous modalities, (2) a fragment-aware rotary positional encoding (F-RoPE) module that preserves spatial structure and tissue-fragment topology in WSI, and (3) domain-specialized internal foundation models for both WSI and RNA-seq to provide biologically grounded embeddings for robust multimodal alignment. We evaluate EXAONE Path 2.5 against six leading pathology foundation models across two complementary benchmarks: an internal real-world clinical dataset and the Patho-Bench benchmark covering 80 tasks. Our framework demonstrates high data and parameter efficiency, achieving on-par performance with state-of-the-art foundation models on Patho-Bench while exhibiting the highest adaptability in the internal clinical setting. These results highlight the value of biologically informed multimodal design and underscore the potential of integrated genotype-to-phenotype modeling for next-generation precision oncology.

</details>


### [55] [FusAD: Time-Frequency Fusion with Adaptive Denoising for General Time Series Analysis](https://arxiv.org/abs/2512.14078)
*Da Zhang,Bingyu Li,Zhiyuan Zhao,Feiping Nie,Junyu Gao,Xuelong Li*

Main category: cs.LG

TL;DR: FusAD是一个统一的时间序列分析框架，通过自适应时频融合和去噪机制，在分类、预测和异常检测任务中优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分析面临三大挑战：1）缺乏高效、多任务兼容且可泛化的统一框架；2）现有方法通常针对单一任务或特定数据类型设计；3）真实数据常受噪声、复杂频率成分和多尺度动态模式影响，难以进行鲁棒特征提取。

Method: FusAD框架包含三个核心组件：1）自适应时频融合机制，整合傅里叶和小波变换以捕获全局-局部和多尺度动态特征；2）自适应去噪机制，自动感知和过滤各类噪声；3）通用信息融合和解码结构，结合掩码预训练，促进多粒度表示的高效学习和迁移。

Result: 在主流时间序列基准测试中，FusAD在分类、预测和异常检测任务上持续优于最先进模型，同时保持高效率和可扩展性。

Conclusion: FusAD通过创新的时频融合和自适应去噪机制，成功构建了一个统一、高效且可泛化的时间序列分析框架，能够有效处理多任务建模并整合多样化时间序列信息。

Abstract: Time series analysis plays a vital role in fields such as finance, healthcare, industry, and meteorology, underpinning key tasks including classification, forecasting, and anomaly detection. Although deep learning models have achieved remarkable progress in these areas in recent years, constructing an efficient, multi-task compatible, and generalizable unified framework for time series analysis remains a significant challenge. Existing approaches are often tailored to single tasks or specific data types, making it difficult to simultaneously handle multi-task modeling and effectively integrate information across diverse time series types. Moreover, real-world data are often affected by noise, complex frequency components, and multi-scale dynamic patterns, which further complicate robust feature extraction and analysis. To ameliorate these challenges, we propose FusAD, a unified analysis framework designed for diverse time series tasks. FusAD features an adaptive time-frequency fusion mechanism, integrating both Fourier and Wavelet transforms to efficiently capture global-local and multi-scale dynamic features. With an adaptive denoising mechanism, FusAD automatically senses and filters various types of noise, highlighting crucial sequence variations and enabling robust feature extraction in complex environments. In addition, the framework integrates a general information fusion and decoding structure, combined with masked pre-training, to promote efficient learning and transfer of multi-granularity representations. Extensive experiments demonstrate that FusAD consistently outperforms state-of-the-art models on mainstream time series benchmarks for classification, forecasting, and anomaly detection tasks, while maintaining high efficiency and scalability. Code is available at https://github.com/zhangda1018/FusAD.

</details>


### [56] [SonicMoE: Accelerating MoE with IO and Tile-aware Optimizations](https://arxiv.org/abs/2512.14080)
*Wentao Guo,Mayank Mishra,Xinle Cheng,Ion Stoica,Tri Dao*

Main category: cs.LG

TL;DR: SonicMoE提出了一种内存高效的MoE训练方法，通过最小化激活缓存、GPU内核内存IO与计算重叠以及"token rounding"技术，显著减少激活内存并提高计算吞吐量。


<details>
  <summary>Details</summary>
Motivation: 当前细粒度MoE模型面临激活内存占用增加和硬件效率降低的问题，而稀疏MoE则因Grouped GEMM内核中的填充导致计算浪费。需要解决这些效率瓶颈以提升MoE模型训练性能。

Method: 1) 提出内存高效算法，最小化MoE前向和反向传播的激活缓存；2) 设计GPU内核，实现内存IO与计算重叠；3) 提出"token rounding"方法，减少Grouped GEMM内核中的填充浪费。

Result: SonicMoE将激活内存减少45%，在Hopper GPU上实现1.86倍计算吞吐量提升。在64个H100上达到2130亿token/天的训练吞吐量，接近ScatterMoE在96个H100上的性能。在高稀疏度设置下，token rounding算法带来额外1.16倍加速。

Conclusion: SonicMoE通过创新的内存优化、计算重叠和token rounding技术，显著提升了MoE模型的训练效率，解决了当前MoE架构面临的内存和计算效率瓶颈，为大规模MoE模型训练提供了实用解决方案。

Abstract: Mixture of Experts (MoE) models have emerged as the de facto architecture for scaling up language models without significantly increasing the computational cost. Recent MoE models demonstrate a clear trend towards high expert granularity (smaller expert intermediate dimension) and higher sparsity (constant number of activated experts with higher number of total experts), which improve model quality per FLOP. However, fine-grained MoEs suffer from increased activation memory footprint and reduced hardware efficiency due to higher IO costs, while sparser MoEs suffer from wasted computations due to padding in Grouped GEMM kernels. In response, we propose a memory-efficient algorithm to compute the forward and backward passes of MoEs with minimal activation caching for the backward pass. We also design GPU kernels that overlap memory IO with computation benefiting all MoE architectures. Finally, we propose a novel "token rounding" method that minimizes the wasted compute due to padding in Grouped GEMM kernels. As a result, our method SonicMoE reduces activation memory by 45% and achieves a 1.86x compute throughput improvement on Hopper GPUs compared to ScatterMoE's BF16 MoE kernel for a fine-grained 7B MoE. Concretely, SonicMoE on 64 H100s achieves a training throughput of 213 billion tokens per day comparable to ScatterMoE's 225 billion tokens per day on 96 H100s for a 7B MoE model training with FSDP-2 using the lm-engine codebase. Under high MoE sparsity settings, our tile-aware token rounding algorithm yields an additional 1.16x speedup on kernel execution time compared to vanilla top-$K$ routing while maintaining similar downstream performance. We open-source all our kernels to enable faster MoE model training.

</details>


### [57] [Cornserve: Efficiently Serving Any-to-Any Multimodal Models](https://arxiv.org/abs/2512.14098)
*Jeff J. Ma,Jae-Won Chung,Jisang Ahn,Yizhuo Liang,Akshay Jajoo,Myungjin Lee,Mosharaf Chowdhury*

Main category: cs.LG

TL;DR: Cornserve是一个用于Any-to-Any多模态模型的高效在线服务系统，通过自动优化部署计划和分布式运行时，显著提升吞吐量和降低延迟。


<details>
  <summary>Details</summary>
Motivation: Any-to-Any多模态模型在输入输出类型、计算路径和计算规模上存在高度异构性，传统服务系统难以有效处理这种复杂性，需要专门设计的服务系统来应对这些挑战。

Method: 1) 允许开发者描述通用Any-to-Any模型的计算图；2) 规划器根据模型和工作负载特征自动寻找优化部署计划，包括是否及如何将模型分解为更小组件；3) 分布式运行时按计划执行模型，高效处理异构性。

Result: Cornserve能够高效服务多样化的Any-to-Any模型和工作负载，相比现有解决方案，吞吐量提升最高达3.81倍，尾部延迟降低最高达5.79倍。

Conclusion: Cornserve通过专门设计的规划器和分布式运行时，有效解决了Any-to-Any多模态模型服务中的异构性挑战，为这类新兴模型提供了高效可靠的服务解决方案。

Abstract: We present Cornserve, an efficient online serving system for an emerging class of multimodal models called Any-to-Any models. Any-to-Any models accept combinations of text and multimodal data (e.g., image, video, audio) as input and also generate combinations of text and multimodal data as output, introducing request type, computation path, and computation scaling heterogeneity in model serving.
  Cornserve allows model developers to describe the computation graph of generic Any-to-Any models, which consists of heterogeneous components such as multimodal encoders, autoregressive models like Large Language Models (LLMs), and multimodal generators like Diffusion Transformers (DiTs). Given this, Cornserve's planner automatically finds an optimized deployment plan for the model, including whether and how to disaggregate the model into smaller components based on model and workload characteristics. Cornserve's distributed runtime then executes the model per the plan, efficiently handling Any-to-Any model heterogeneity during online serving. Evaluations show that Cornserve can efficiently serve diverse Any-to-Any models and workloads, delivering up to 3.81$\times$ throughput improvement and up to 5.79$\times$ tail latency reduction over existing solutions.

</details>


### [58] [A First-Order Logic-Based Alternative to Reward Models in RLHF](https://arxiv.org/abs/2512.14100)
*Chunjin Jian,Xinhua Zhu*

Main category: cs.LG

TL;DR: 提出S-GRPO方法，使用逻辑相似性奖励机制替代传统奖励建模，通过形式逻辑一致性引导模型对齐人类偏好，避免模型崩溃


<details>
  <summary>Details</summary>
Motivation: 现有RLHF方法依赖奖励模型的质量和稳定性，而传统奖励建模存在启发式估计问题。需要更可靠的方法来引导大语言模型与人类价值观对齐

Method: 提出基于逻辑相似性的奖励机制，使用形式逻辑一致性替代启发式奖励估计。引入S-GRPO（GRPO的监督变体），包含额外监督组件，联合优化生成项、KL散度正则化和基于标签的目标

Result: S-GRPO在性能和鲁棒性上均优于标准监督微调（SFT），并能扩展现有的偏好学习框架（如GRPO和DPO），提供更灵活、任务自适应的对齐训练方法

Conclusion: 逻辑相似性奖励机制是传统奖励建模的有效替代方案，S-GRPO框架为模型对齐提供了更可靠和灵活的方法，能够避免模型崩溃并适应多视角问题

Abstract: Reinforcement Learning from Human Feedback (RLHF) plays a crucial role in aligning large language models (LLMs) with human values and preferences. However, the quality and stability of the trained reward model largely determine the final alignment performance. Existing approaches such as Proximal Policy Optimization (PPO) rely heavily on reward models to guide LLMs toward human-aligned behaviors.
  In this work, we propose a logic-similarity-based reward mechanism as an alternative to conventional reward modeling. Instead of relying on heuristic reward estimation, our method leverages formal logical consistency to steer model alignment with human preferences. Since real-world questions can be interpreted from multiple perspectives, to ensure that logic-based reinforcement learning does not cause model collapse, we introduce S-GRPO, a supervised variant of the GRPO framework. S-GRPO incorporates an additional supervised component and jointly optimizes the generation term, KL-divergence regularization, and label-based objective during training.
  Experimental results demonstrate that S-GRPO consistently outperforms standard supervised fine-tuning (SFT) in both performance and robustness. Furthermore, it extends existing preference-learning frameworks such as GRPO and DPO, offering a more flexible and task-adaptive approach to alignment training. Our code is available at https://github.com/ChunjinJiang/sgrpo.

</details>


### [59] [Random-Bridges as Stochastic Transports for Generative Models](https://arxiv.org/abs/2512.14190)
*Stefano Goria,Levent A. Mengütürk,Murat C. Mengütürk,Berkan Sesen*

Main category: cs.LG

TL;DR: 论文提出使用随机桥（random-bridges）作为生成模型中的随机传输方法，相比传统方法能以更少步骤生成高质量样本，计算成本低且适合高速生成任务。


<details>
  <summary>Details</summary>
Motivation: 本文旨在探索随机桥在生成建模领域的应用潜力。随机桥作为在固定时间点满足目标分布条件的随机过程，可以充当两个概率分布之间的随机传输工具，具有马尔可夫或非马尔可夫特性，以及连续、不连续或混合模式，为生成模型提供了新的理论框架。

Method: 从一般概率陈述出发，推导出具体的学习和模拟算法表示。基于高斯随机桥构建方法，通过信息处理的方式实现随机桥的生成模型框架，支持不同驱动过程下的多种模式。

Result: 基于高斯随机桥的实证结果显示，该方法能以显著更少的步骤生成高质量样本，同时获得有竞争力的Fréchet Inception Distance（FID）分数。实验证明该框架计算成本低廉，适合高速生成任务。

Conclusion: 随机桥为生成建模提供了有效的随机传输框架，相比传统方法在生成效率上有显著优势，计算成本低且适合实时应用，为高质量快速生成任务提供了有前景的解决方案。

Abstract: This paper motivates the use of random-bridges -- stochastic processes conditioned to take target distributions at fixed timepoints -- in the realm of generative modelling. Herein, random-bridges can act as stochastic transports between two probability distributions when appropriately initialized, and can display either Markovian or non-Markovian, and either continuous, discontinuous or hybrid patterns depending on the driving process. We show how one can start from general probabilistic statements and then branch out into specific representations for learning and simulation algorithms in terms of information processing. Our empirical results, built on Gaussian random bridges, produce high-quality samples in significantly fewer steps compared to traditional approaches, while achieving competitive Frechet inception distance scores. Our analysis provides evidence that the proposed framework is computationally cheap and suitable for high-speed generation tasks.

</details>


### [60] [Estimating problem difficulty without ground truth using Large Language Model comparisons](https://arxiv.org/abs/2512.14220)
*Marthe Ballon,Andres Algaba,Brecht Verbeken,Vincent Ginis*

Main category: cs.LG

TL;DR: 提出LLM Compare方法，通过大语言模型进行成对难度比较，结合Bradley-Terry评分，解决现有难度评估方法无法处理分布外问题的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有难度评估方法（如人工校准或基于性能的评分）无法扩展到分布外问题，因为这些方法不可扩展、耗时且依赖真实标签。需要一种能够评估当前人类和LLM都无法解决的问题的难度测量方法。

Method: 提出LLM Compare方法：使用大语言模型进行成对难度比较，然后基于比较结果计算Bradley-Terry评分。该方法具有连续性、动态性、模型无关性且不依赖真实标签信息。

Result: 1. 概念框架验证：LLM Compare占据所有理想象限（连续动态、模型无关、不依赖真实标签）；2. 与人类标注高度一致：Pearson r ≥ 0.80 (n=1876)；3. 对幻觉鲁棒：10%噪声注入下Pearson相关性下降小于6%。

Conclusion: LLM Compare是第一个能够评估分布外问题难度的连续动态、模型无关且不依赖真实标签的方法，可替代耗时的人工标注和合成数据生成，对课程设计、模型评估和AI辅助研究构思有重要意义。

Abstract: Recent advances in the finetuning of large language models (LLMs) have significantly improved their performance on established benchmarks, emphasizing the need for increasingly difficult, synthetic data. A key step in this data generation pipeline is a method for estimating problem difficulty. Current approaches, such as human calibration or performance-based scoring, fail to generalize to out-of-distribution problems, i.e. problems currently unsolvable by humans and LLMs, because they are not scalable, time-consuming, and ground truth dependent. Therefore, we propose a new method for estimating problem difficulty, LLM compare, that addresses these limitations. An LLM performs pairwise difficulty comparisons, and then Bradley-Terry scores are computed based on the outcomes. To validate our method, we first propose a conceptual framework that positions existing approaches on three orthogonal planes--construction, scale and dependence--identifying which quadrants a measure needs to occupy to score out-of-distribution problems. LLM compare naturally occupies all desirable quadrants as the first measure that is continuous and dynamic, model-agnostic and independent of ground truth information. As a second validation, we show that LLM compare demonstrates strong alignment with human annotations: Pearson $r \geq 0.80$ for $n=1876$. Thirdly, we show that LLM compare is robust to hallucinations, with less than $6\%$ degradation in Pearson correlation for $10\%$ noise injection. Our work represents a significant step towards replacing time-consuming human annotations and synthetic data generation, and will be an important driver for curriculum design, model evaluation, and AI-assisted research ideation.

</details>


### [61] [Physically consistent model learning for reaction-diffusion systems](https://arxiv.org/abs/2512.14240)
*Erion Morina,Martin Holler*

Main category: cs.LG

TL;DR: 该论文提出了一种从数据中学习反应-扩散系统的方法，确保学习到的模型具有物理一致性和适定性。通过修改参数化反应项来强制满足质量守恒和拟正性，并扩展了正则化学习理论到这类物理约束系统。


<details>
  <summary>Details</summary>
Motivation: 传统的数据驱动模型学习反应-扩散系统时，往往不能保证学习到的模型满足基本的物理原理（如质量守恒、非负性），这可能导致模型在物理上不一致、不可靠。需要开发既能从数据学习又能自动满足物理约束的方法。

Method: 1. 提出系统化修改参数化反应项的技术，使修改后的反应项固有地满足质量守恒和拟正性；2. 在正则化学习框架中融入这些物理约束；3. 扩展现有理论结果，证明在物理约束下的收敛性和适定性。

Result: 1. 开发了确保质量守恒和拟正性的反应项修改技术；2. 证明了学习问题的解收敛到唯一正则化最小化解，即使强制了守恒定律和拟正性；3. 提供了拟正函数的逼近结果，用于构建物理一致的参数化。

Conclusion: 该方法推进了反应-扩散系统可解释且可靠的数据驱动模型发展，确保学习到的模型符合基本物理定律，为物理约束下的机器学习提供了理论保证和实践技术。

Abstract: This paper addresses the problem of learning reaction-diffusion (RD) systems from data while ensuring physical consistency and well-posedness of the learned models. Building on a regularization-based framework for structured model learning, we focus on learning parameterized reaction terms and investigate how to incorporate key physical properties, such as mass conservation and quasipositivity, directly into the learning process. Our main contributions are twofold: First, we propose techniques to systematically modify a given class of parameterized reaction terms such that the resulting terms inherently satisfy mass conservation and quasipositivity, ensuring that the learned RD systems preserve non-negativity and adhere to physical principles. These modifications also guarantee well-posedness of the resulting PDEs under additional regularity and growth conditions. Second, we extend existing theoretical results on regularization-based model learning to RD systems using these physically consistent reaction terms. Specifically, we prove that solutions to the learning problem converge to a unique, regularization-minimizing solution of a limit system even when conservation laws and quasipositivity are enforced. In addition, we provide approximation results for quasipositive functions, essential for constructing physically consistent parameterizations. These results advance the development of interpretable and reliable data-driven models for RD systems that align with fundamental physical laws.

</details>


### [62] [Beyond MMD: Evaluating Graph Generative Models with Geometric Deep Learning](https://arxiv.org/abs/2512.14241)
*Salvatore Romano,Marco Grassia,Giuseppe Mangioni*

Main category: cs.LG

TL;DR: 本文提出了一种名为RGM的新方法，用于评估图生成模型，克服了传统最大平均差异(MMD)度量的局限性，并对GRAN和EDGE两种最先进的图生成模型进行了综合评估。


<details>
  <summary>Details</summary>
Motivation: 图生成在许多领域至关重要，但现有图生成模型的评估主要依赖最大平均差异(MMD)度量，这种方法存在局限性，需要更有效的评估方法来准确衡量生成图的质量和真实性。

Method: 提出RGM（Representation-aware Graph-generation Model evaluation）方法，使用几何深度学习模型在专门设计的合成和真实图数据集上进行图分类任务，从而评估图生成模型的性能。

Result: 对GRAN和EDGE两种最先进图生成模型的评估显示，虽然两者都能生成具有某些拓扑属性的图，但在保持区分不同图域的结构特征方面存在显著局限性，同时证实了MMD作为评估度量的不足。

Conclusion: 需要超越MMD的替代评估方法来准确评估图生成模型，RGM方法为此提供了新方向，未来研究应关注更全面的评估框架。

Abstract: Graph generation is a crucial task in many fields, including network science and bioinformatics, as it enables the creation of synthetic graphs that mimic the properties of real-world networks for various applications. Graph Generative Models (GGMs) have emerged as a promising solution to this problem, leveraging deep learning techniques to learn the underlying distribution of real-world graphs and generate new samples that closely resemble them. Examples include approaches based on Variational Auto-Encoders, Recurrent Neural Networks, and more recently, diffusion-based models. However, the main limitation often lies in the evaluation process, which typically relies on Maximum Mean Discrepancy (MMD) as a metric to assess the distribution of graph properties in the generated ensemble. This paper introduces a novel methodology for evaluating GGMs that overcomes the limitations of MMD, which we call RGM (Representation-aware Graph-generation Model evaluation). As a practical demonstration of our methodology, we present a comprehensive evaluation of two state-of-the-art Graph Generative Models: Graph Recurrent Attention Networks (GRAN) and Efficient and Degree-guided graph GEnerative model (EDGE). We investigate their performance in generating realistic graphs and compare them using a Geometric Deep Learning model trained on a custom dataset of synthetic and real-world graphs, specifically designed for graph classification tasks. Our findings reveal that while both models can generate graphs with certain topological properties, they exhibit significant limitations in preserving the structural characteristics that distinguish different graph domains. We also highlight the inadequacy of Maximum Mean Discrepancy as an evaluation metric for GGMs and suggest alternative approaches for future research.

</details>


### [63] [FLAME: Flow Enhanced Legendre Memory Models for General Time Series Forecasting](https://arxiv.org/abs/2512.14253)
*Xingjian Wu,Hanyin Cheng,Xiangfei Qiu,Zhengyu Li,Jilin Hu,Chenjuan Guo,Bin Yang*

Main category: cs.LG

TL;DR: FLAME是一个轻量级时间序列基础模型家族，支持确定性和概率性预测，通过生成式概率建模确保效率和鲁棒性，在零样本预测任务中达到SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 开发一个既轻量又强大的时间序列基础模型，能够同时处理确定性和概率性预测任务，在保持高效率的同时提供准确的长期预测能力。

Method: 使用Legendre Memory增强泛化能力，采用LegT和LegS变体在编码和解码阶段捕获数据内在归纳偏差；采用基于Normalization Flow的预测头来建模任意复杂分布。

Result: 在TSFM-Bench和ProbTS等基准测试中，FLAME在确定性和概率性预测任务上都展现了最先进的零样本性能。

Conclusion: FLAME是一个高效且鲁棒的时间序列基础模型，通过创新的Legendre Memory架构和Normalization Flow预测头，在保持轻量级的同时实现了卓越的预测性能。

Abstract: In this work, we introduce FLAME, a family of extremely lightweight and capable Time Series Foundation Models, which support both deterministic and probabilistic forecasting via generative probabilistic modeling, thus ensuring both efficiency and robustness. FLAME utilizes the Legendre Memory for strong generalization capabilities. Through adapting variants of Legendre Memory, i.e., translated Legendre (LegT) and scaled Legendre (LegS), in the Encoding and Decoding phases, FLAME can effectively capture the inherent inductive bias within data and make efficient long-range inferences. To enhance the accuracy of probabilistic forecasting while keeping efficient, FLAME adopts a Normalization Flow based forecasting head, which can model the arbitrarily intricate distributions over the forecasting horizon in a generative manner. Comprehensive experiments on well-recognized benchmarks, including TSFM-Bench and ProbTS, demonstrate the consistent state-of-the-art zero-shot performance of FLAME on both deterministic and probabilistic forecasting tasks.

</details>


### [64] [Black-Box Auditing of Quantum Model: Lifted Differential Privacy with Quantum Canaries](https://arxiv.org/abs/2512.14388)
*Baobao Song,Shiva Raj Pokhrel,Athanasios V. Vasilakos,Tianqing Zhu,Gang Li*

Main category: cs.LG

TL;DR: 首个基于提升量子差分隐私的黑盒隐私审计框架，用于量子机器学习模型，通过量子金丝雀检测记忆化并量化隐私泄露


<details>
  <summary>Details</summary>
Motivation: 量子机器学习模型可能记忆敏感数据，存在隐私风险。现有量子差分隐私机制缺乏对部署模型的实证验证工具

Method: 基于提升量子差分隐私的黑盒隐私审计框架，使用量子金丝雀（策略性偏移编码的量子态）检测记忆化，通过金丝雀偏移与迹距离界限的数学连接量化隐私泄露

Result: 建立了金丝雀偏移与迹距离界限的严格数学连接，推导出隐私预算消耗的经验下界，在模拟和物理量子硬件上验证了框架有效性

Conclusion: 该框架填补了量子差分隐私理论保证与实际隐私验证之间的关键空白，为量子机器学习系统提供了强大的隐私验证工具

Abstract: Quantum machine learning (QML) promises significant computational advantages, yet models trained on sensitive data risk memorizing individual records, creating serious privacy vulnerabilities. While Quantum Differential Privacy (QDP) mechanisms provide theoretical worst-case guarantees, they critically lack empirical verification tools for deployed models. We introduce the first black-box privacy auditing framework for QML based on Lifted Quantum Differential Privacy, leveraging quantum canaries (strategically offset-encoded quantum states) to detect memorization and precisely quantify privacy leakage during training. Our framework establishes a rigorous mathematical connection between canary offset and trace distance bounds, deriving empirical lower bounds on privacy budget consumption that bridge the critical gap between theoretical guarantees and practical privacy verification. Comprehensive evaluations across both simulated and physical quantum hardware demonstrate our framework's effectiveness in measuring actual privacy loss in QML models, enabling robust privacy verification in QML systems.

</details>


### [65] [Residual GRU+MHSA: A Lightweight Hybrid Recurrent Attention Model for Cardiovascular Disease Detection](https://arxiv.org/abs/2512.14563)
*Tejaswani Dash,Gautam Datla,Anudeep Vurity,Tazeem Ahmad,Mohd Adnan,Saima Rafi,Saisha Patro,Saina Patro*

Main category: cs.LG

TL;DR: 提出Residual GRU with Multi-Head Self-Attention模型，用于心血管疾病预测，在UCI心脏病数据集上表现优于传统方法和深度学习基线。


<details>
  <summary>Details</summary>
Motivation: 心血管疾病是全球主要死因，需要可靠高效的预测工具。传统方法依赖手工特征和临床经验，机器学习方法可重复性更好但难以在噪声和异质临床数据上泛化。

Method: 提出紧凑的深度学习架构，整合残差双向门控循环单元用于特征列序列建模、通道重加权块、以及带可学习分类token的多头自注意力池化来捕获全局上下文。

Result: 在UCI心脏病数据集上，模型准确率0.861，宏F1分数0.860，ROC-AUC 0.908，PR-AUC 0.904，优于所有基线方法。消融研究证实了残差循环、通道门控和注意力池化的各自贡献。

Conclusion: 轻量级混合循环和注意力架构在临床风险预测中提供了准确性和效率的良好平衡，支持在资源受限的医疗环境中部署。

Abstract: Cardiovascular disease (CVD) remains the leading cause of mortality worldwide, underscoring the need for reliable and efficient predictive tools that support early intervention. Traditional diagnostic approaches rely on handcrafted features and clinician expertise, while machine learning methods improve reproducibility but often struggle to generalize across noisy and heterogeneous clinical data. In this work, we propose Residual GRU with Multi-Head Self-Attention, a compact deep learning architecture designed for tabular clinical records. The model integrates residual bidirectional gated recurrent units for sequential modeling of feature columns, a channel reweighting block, and multi-head self-attention pooling with a learnable classification token to capture global context. We evaluate the model on the UCI Heart Disease dataset using 5-fold stratified cross-validation and compare it against classical methods such as Logistic Regression, Random Forest, and Support Vector Machines, as well as modern deep learning baselines including DeepMLP, convolutional networks, recurrent networks, and Transformers. The proposed model achieves an accuracy of 0.861, macro-F1 of 0.860, ROC-AUC of 0.908, and PR-AUC of 0.904, outperforming all baselines. Ablation studies confirm the individual contributions of residual recurrence, channel gating, and attention pooling. t-SNE visualizations further indicate that the learned embeddings exhibit clearer separation between disease and non-disease classes compared to raw features. These results demonstrate that lightweight hybrid recurrent and attention-based architectures provide a strong balance between accuracy and efficiency for clinical risk prediction, supporting deployment in resource-constrained healthcare settings.

</details>


### [66] [GRAFT: Grid-Aware Load Forecasting with Multi-Source Textual Alignment and Fusion](https://arxiv.org/abs/2512.14400)
*Fangzhou Lin,Guoshun He,Zhenyu Guo,Zhe Huang,Jinsong Tao*

Main category: cs.LG

TL;DR: GRAFT模型改进STanHOP，通过文本引导融合和跨注意力机制，实现多源文本信息与电力负荷的严格对齐，提升电网负荷预测精度。


<details>
  <summary>Details</summary>
Motivation: 电力负荷同时受到天气、日历节奏、突发事件和政策等多时间尺度外生因素的影响，需要开发能够整合多源文本信息的电网感知预测方法。

Method: 提出GRAFT模型：1）严格对齐每日聚合的新闻、社交媒体和政策文本与半小时负荷数据；2）通过跨注意力机制实现文本引导融合到特定时间位置；3）提供即插即用的外部记忆接口以适应不同信息源。

Result: 在澳大利亚五个州2019-2021年数据集上，GRAFT在小时、日、月三个时间尺度上显著优于基线模型，达到或超过最先进水平，在事件驱动场景中表现稳健，并能通过注意力机制实现文本到负荷效应的时空定位和源级解释。

Conclusion: GRAFT通过文本引导融合和多源信息整合，有效提升了电网负荷预测的准确性和可解释性，为标准化实证评估提供了基准数据集和工具。

Abstract: Electric load is simultaneously affected across multiple time scales by exogenous factors such as weather and calendar rhythms, sudden events, and policies. Therefore, this paper proposes GRAFT (GRid-Aware Forecasting with Text), which modifies and improves STanHOP to better support grid-aware forecasting and multi-source textual interventions. Specifically, GRAFT strictly aligns daily-aggregated news, social media, and policy texts with half-hour load, and realizes text-guided fusion to specific time positions via cross-attention during both training and rolling forecasting. In addition, GRAFT provides a plug-and-play external-memory interface to accommodate different information sources in real-world deployment. We construct and release a unified aligned benchmark covering 2019--2021 for five Australian states (half-hour load, daily-aligned weather/calendar variables, and three categories of external texts), and conduct systematic, reproducible evaluations at three scales -- hourly, daily, and monthly -- under a unified protocol for comparison across regions, external sources, and time scales. Experimental results show that GRAFT significantly outperforms strong baselines and reaches or surpasses the state of the art across multiple regions and forecasting horizons. Moreover, the model is robust in event-driven scenarios and enables temporal localization and source-level interpretation of text-to-load effects through attention read-out. We release the benchmark, preprocessing scripts, and forecasting results to facilitate standardized empirical evaluation and reproducibility in power grid load forecasting.

</details>


### [67] [Model-Based Reinforcement Learning in Discrete-Action Non-Markovian Reward Decision Processes](https://arxiv.org/abs/2512.14617)
*Alessandro Trapasso,Luca Iocchi,Fabio Patrizi*

Main category: cs.LG

TL;DR: QR-MAX：首个基于模型的NMRDP算法，通过奖励机分解马尔可夫转移学习与非马尔可夫奖励处理，实现多项式样本复杂度的PAC收敛


<details>
  <summary>Details</summary>
Motivation: 许多实际决策问题涉及依赖于整个系统历史的任务，而非仅达到特定状态。马尔可夫强化学习方法不适合此类任务，而非马尔可夫奖励决策过程（NMRDPs）虽能处理时间依赖任务，但长期缺乏形式化保证（近最优性和样本效率）

Method: 提出QR-MAX算法：1) 离散NMRDPs的基于模型算法，通过奖励机将马尔可夫转移学习与非马尔可夫奖励处理分解；2) Bucket-QR-MAX扩展至连续状态空间，使用SimHash离散化器保持相同分解结构，无需手动网格化或函数逼近

Result: QR-MAX是首个利用这种分解实现多项式样本复杂度PAC收敛到ε最优策略的基于模型RL算法。实验表明，在复杂度递增的环境中，相比现代最先进的基于模型RL方法，样本效率显著提升，寻找最优策略的鲁棒性增强

Conclusion: 该研究解决了NMRDPs中长期缺乏形式化保证的问题，通过分解方法实现了高效的非马尔可夫强化学习，为处理时间依赖任务提供了理论保证和实用算法

Abstract: Many practical decision-making problems involve tasks whose success depends on the entire system history, rather than on achieving a state with desired properties. Markovian Reinforcement Learning (RL) approaches are not suitable for such tasks, while RL with non-Markovian reward decision processes (NMRDPs) enables agents to tackle temporal-dependency tasks. This approach has long been known to lack formal guarantees on both (near-)optimality and sample efficiency. We contribute to solving both issues with QR-MAX, a novel model-based algorithm for discrete NMRDPs that factorizes Markovian transition learning from non-Markovian reward handling via reward machines. To the best of our knowledge, this is the first model-based RL algorithm for discrete-action NMRDPs that exploits this factorization to obtain PAC convergence to $\varepsilon$-optimal policies with polynomial sample complexity. We then extend QR-MAX to continuous state spaces with Bucket-QR-MAX, a SimHash-based discretiser that preserves the same factorized structure and achieves fast and stable learning without manual gridding or function approximation. We experimentally compare our method with modern state-of-the-art model-based RL approaches on environments of increasing complexity, showing a significant improvement in sample efficiency and increased robustness in finding optimal policies.

</details>


### [68] [gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation](https://arxiv.org/abs/2512.14658)
*Alban Puech,Matteo Mazzonelli,Celia Cintas,Tamara R. Govindasamy,Mangaliso Mngomezulu,Jonas Weiss,Matteo Baù,Anna Varbella,François Mirallès,Kibaek Kim,Le Xie,Hendrik F. Hamann,Etienne Vos,Thomas Brunschwiler*

Main category: cs.LG

TL;DR: gridfm-datakit-v1是一个用于生成真实多样电力潮流和最优潮流数据集的Python库，解决了现有数据集在随机性、多样性、边界情况和成本变化方面的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有电力系统数据集面临三个主要挑战：1) 缺乏真实的随机负荷和拓扑扰动，限制了场景多样性；2) PF数据集仅限于OPF可行点，阻碍了ML求解器对违反运行限制情况的泛化能力；3) OPF数据集使用固定发电机成本函数，限制了在不同成本情况下的泛化能力。

Method: 通过结合真实世界负荷剖面的全局缩放与局部噪声，支持任意N-k拓扑扰动来创建多样且真实的数据集；生成超出运行限制的PF样本；生成具有变化发电机成本的OPF数据；并能高效扩展到大型电网（最多10,000个节点）。

Result: 开发了gridfm-datakit-v1库，提供了与OPFData、OPF-Learn、PGLearn和PFΔ的比较，支持大规模电网（最多10,000个节点），并在GitHub上开源，可通过pip安装。

Conclusion: gridfm-datakit-v1解决了现有电力系统数据集的局限性，提供了更真实、多样且可扩展的数据生成能力，有助于训练更鲁棒的机器学习求解器。

Abstract: We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$Δ$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.

</details>


### [69] [Bridging Artificial Intelligence and Data Assimilation: The Data-driven Ensemble Forecasting System ClimaX-LETKF](https://arxiv.org/abs/2512.14444)
*Akira Takeshima,Kenta Shiraishi,Atsushi Okazaki,Tadashi Tsuyuki,Shunji Kotsuki*

Main category: cs.LG

TL;DR: ClimaX-LETKF是首个纯数据驱动的机器学习集合天气预报系统，能够独立于数值天气预报模型，通过同化观测数据实现多年稳定运行。


<details>
  <summary>Details</summary>
Motivation: 机器学习天气预报虽取得进展，但在同化真实观测数据或集合预报方面的研究仍然有限。需要开发能够独立于传统数值天气预报模型的机器学习集合预报系统。

Method: 开发ClimaX-LETKF系统，使用NCEP ADP全球高空和地面天气观测数据进行同化。比较了松弛到先验扰动(RTPP)和松弛到先验扩展(RTPS)两种方法的效果。

Result: 系统能够多年稳定运行，RTPP比RTPS表现出更好的稳定性和准确性，这与数值天气预报模型通常更稳定于RTPS的情况相反。研究发现机器学习天气预报模型在将大气场恢复到其吸引子方面的能力不如数值天气预报模型。

Conclusion: 这项工作为增强机器学习天气预报集合预报系统提供了宝贵见解，代表了向实际应用迈出的重要一步，揭示了机器学习模型与数值天气预报模型在集合预报特性上的差异。

Abstract: While machine learning-based weather prediction (MLWP) has achieved significant advancements, research on assimilating real observations or ensemble forecasts within MLWP models remains limited. We introduce ClimaX-LETKF, the first purely data-driven ML-based ensemble weather forecasting system. It operates stably over multiple years, independently of numerical weather prediction (NWP) models, by assimilating the NCEP ADP Global Upper Air and Surface Weather Observations. The system demonstrates greater stability and accuracy with relaxation to prior perturbation (RTPP) than with relaxation to prior spread (RTPS), while NWP models tend to be more stable with RTPS. RTPP replaces an analysis perturbation with a weighted blend of analysis and background perturbations, whereas RTPS simply rescales the analysis perturbation. Our experiments reveal that MLWP models are less capable of restoring the atmospheric field to its attractor than NWP models. This work provides valuable insights for enhancing MLWP ensemble forecasting systems and represents a substantial step toward their practical applications.

</details>


### [70] [Kinetic-Mamba: Mamba-Assisted Predictions of Stiff Chemical Kinetics](https://arxiv.org/abs/2512.14471)
*Additi Pandey,Liang Wei,Hessam Babaee,George Em Karniadakis*

Main category: cs.LG

TL;DR: Kinetic-Mamba：基于Mamba架构的神经算子框架，用于燃烧模拟中的化学动力学建模，通过三种互补模型预测热化学状态变量的时间演化。


<details>
  <summary>Details</summary>
Motivation: 精确的化学动力学建模对燃烧模拟至关重要，传统方法在处理复杂反应路径和热化学状态演化时存在计算效率或精度限制，需要开发更高效的建模框架。

Method: 提出Kinetic-Mamba框架，包含三种模型：(1)独立Mamba模型预测状态变量时间演化；(2)约束Mamba模型在保持质量守恒下学习状态动力学；(3)基于温度依赖区域的机制感知架构使用两个Mamba模型。还开发了潜在空间变体，在降维潜在空间演化动力学并在物理流形上重建完整状态。评估采用时间分解和递归预测策略。

Result: 在Syngas和GRI-Mech 3.0反应机制上的计算实验表明，该框架仅使用状态变量的初始条件就能高保真地预测复杂动力学行为，并具有良好的外推能力。

Conclusion: Kinetic-Mamba框架成功地将神经算子的表达能力与Mamba架构的高效时间建模能力相结合，为燃烧模拟中的化学动力学建模提供了准确且鲁棒的解决方案。

Abstract: Accurate chemical kinetics modeling is essential for combustion simulations, as it governs the evolution of complex reaction pathways and thermochemical states. In this work, we introduce Kinetic-Mamba, a Mamba-based neural operator framework that integrates the expressive power of neural operators with the efficient temporal modeling capabilities of Mamba architectures. The framework comprises three complementary models: (i) a standalone Mamba model that predicts the time evolution of thermochemical state variables from given initial conditions; (ii) a constrained Mamba model that enforces mass conservation while learning the state dynamics; and (iii) a regime-informed architecture employing two standalone Mamba models to capture dynamics across temperature-dependent regimes. We additionally develop a latent Kinetic-Mamba variant that evolves dynamics in a reduced latent space and reconstructs the full state on the physical manifold. We evaluate the accuracy and robustness of Kinetic-Mamba using both time-decomposition and recursive-prediction strategies. We further assess the extrapolation capabilities of the model on varied out-of-distribution datasets. Computational experiments on Syngas and GRI-Mech 3.0 reaction mechanisms demonstrate that our framework achieves high fidelity in predicting complex kinetic behavior using only the initial conditions of the state variables.

</details>


### [71] [Synthetic Electrogram Generation with Variational Autoencoders for ECGI](https://arxiv.org/abs/2512.14537)
*Miriam Gutiérrez Fernández,Karen López-Linares,Carlos Fambuena Santos,María S. Guillem,Andreu M. Climent,Óscar Barquero Pérez*

Main category: cs.LG

TL;DR: 使用变分自编码器生成合成心房电图以解决配对体表电位-心内电图数据稀缺问题，提升非侵入性心电图成像的深度学习性能


<details>
  <summary>Details</summary>
Motivation: 心房颤动是最常见的心律失常，需要准确表征心房电活动。非侵入性心电图成像结合深度学习从体表电位估计心内电图有前景，但进展受到配对体表电位-心内电图数据集有限的阻碍。

Method: 研究变分自编码器生成合成多通道心房电图。提出两种模型：窦性心律特定VAE和类别条件VAE（在窦性心律和房颤信号上训练）。使用形态学、频谱和分布相似性指标评估生成的心电图。

Result: 窦性心律特定VAE在计算机模拟心电图上实现更高保真度，而类别条件VAE能够实现节律特定生成，但以窦性重建质量降低为代价。作为概念验证，生成的心电图用于下游非侵入性心电图重建任务的数据增强，适度增强提高了估计性能。

Conclusion: 基于变分自编码器的生成模型有潜力缓解数据稀缺问题，并增强基于深度学习的心电图成像流程。

Abstract: Atrial fibrillation (AF) is the most prevalent sustained cardiac arrhythmia, and its clinical assessment requires accurate characterization of atrial electrical activity. Noninvasive electrocardiographic imaging (ECGI) combined with deep learning (DL) approaches for estimating intracardiac electrograms (EGMs) from body surface potentials (BSPMs) has shown promise, but progress is hindered by the limited availability of paired BSPM-EGM datasets. To address this limitation, we investigate variational autoencoders (VAEs) for the generation of synthetic multichannel atrial EGMs. Two models are proposed: a sinus rhythm-specific VAE (VAE-S) and a class-conditioned VAE (VAE-C) trained on both sinus rhythm and AF signals. Generated EGMs are evaluated using morphological, spectral, and distributional similarity metrics. VAE-S achieves higher fidelity with respect to in silico EGMs, while VAE-C enables rhythm-specific generation at the expense of reduced sinus reconstruction quality. As a proof of concept, the generated EGMs are used for data augmentation in a downstream noninvasive EGM reconstruction task, where moderate augmentation improves estimation performance. These results demonstrate the potential of VAE-based generative modeling to alleviate data scarcity and enhance deep learning-based ECGI pipelines.

</details>


### [72] [Counterfactual Explanations for Time Series Should be Human-Centered and Temporally Coherent in Interventions](https://arxiv.org/abs/2512.14559)
*Emmanuel C. Chukwu,Rianne M. Schouten,Monique Tabak,Mykola Pechenizkiy*

Main category: cs.LG

TL;DR: 该论文批评现有时间序列反事实解释方法在临床推荐场景中的不足，指出其缺乏因果合理性和时间连贯性，并呼吁开发更实用、用户中心的反事实干预方法。


<details>
  <summary>Details</summary>
Motivation: 当前时间序列分类的反事实解释方法主要基于静态数据假设，仅关注最小化输入扰动来改变模型预测。在临床推荐场景中，这种方法的根本不足在于：干预措施随时间展开，必须具有因果合理性和时间连贯性，而现有方法无法满足这些要求。

Method: 论文通过批判性分析现有方法，识别了关键缺陷：时间盲点和缺乏用户中心考虑。同时进行了鲁棒性分析，测试几种最先进的时间序列反事实方法对随机噪声的敏感性。

Result: 鲁棒性分析显示，生成的反事实解释对随机噪声高度敏感，这表明在真实临床环境中，这些方法的可靠性有限，因为微小的测量变异是不可避免的。

Conclusion: 需要超越仅改变预测而不考虑可行性的方法，开发具有因果合理性、时间连贯性、用户中心的反事实解释框架。强调需要在实际环境中可行、有目的导向的干预措施。

Abstract: Counterfactual explanations are increasingly proposed as interpretable mechanisms to achieve algorithmic recourse. However, current counterfactual techniques for time series classification are predominantly designed with static data assumptions and focus on generating minimal input perturbations to flip model predictions. This paper argues that such approaches are fundamentally insufficient in clinical recommendation settings, where interventions unfold over time and must be causally plausible and temporally coherent. We advocate for a shift towards counterfactuals that reflect sustained, goal-directed interventions aligned with clinical reasoning and patient-specific dynamics. We identify critical gaps in existing methods that limit their practical applicability, specifically, temporal blind spots and the lack of user-centered considerations in both method design and evaluation metrics. To support our position, we conduct a robustness analysis of several state-of-the-art methods for time series and show that the generated counterfactuals are highly sensitive to stochastic noise. This finding highlights their limited reliability in real-world clinical settings, where minor measurement variations are inevitable. We conclude by calling for methods and evaluation frameworks that go beyond mere prediction changes without considering feasibility or actionability. We emphasize the need for actionable, purpose-driven interventions that are feasible in real-world contexts for the users of such applications.

</details>


### [73] [ParaFormer: A Generalized PageRank Graph Transformer for Graph Representation Learning](https://arxiv.org/abs/2512.14619)
*Chaohao Yuan,Zhenjie Song,Ercan Engin Kuruoglu,Kangfei Zhao,Yang Liu,Deli Zhao,Hong Cheng,Yu Rong*

Main category: cs.LG

TL;DR: ParaFormer是一种基于PageRank增强注意力机制的图Transformer，通过自适应滤波缓解传统图Transformer中的过度平滑问题，在节点分类和图分类任务上表现优异。


<details>
  <summary>Details</summary>
Motivation: 传统图Transformer虽然通过全局注意力机制解决了深度GNN的过度平滑问题，但研究发现全局注意力本身也会导致严重的过度平滑，使节点表征变得难以区分，这种效应甚至比GNN更强。

Method: 提出PageRank Transformer（ParaFormer），采用PageRank增强的注意力模块来模拟深度Transformer的行为，通过自适应滤波机制缓解过度平滑问题。

Result: 在11个数据集（从数千到数百万节点）的节点分类和图分类任务上，ParaFormer均取得了稳定的性能提升，验证了其有效性。

Conclusion: ParaFormer通过PageRank增强注意力机制成功缓解了图Transformer中的过度平滑问题，为图学习提供了一种更有效的Transformer架构。

Abstract: Graph Transformers (GTs) have emerged as a promising graph learning tool, leveraging their all-pair connected property to effectively capture global information. To address the over-smoothing problem in deep GNNs, global attention was initially introduced, eliminating the necessity for using deep GNNs. However, through empirical and theoretical analysis, we verify that the introduced global attention exhibits severe over-smoothing, causing node representations to become indistinguishable due to its inherent low-pass filtering. This effect is even stronger than that observed in GNNs. To mitigate this, we propose PageRank Transformer (ParaFormer), which features a PageRank-enhanced attention module designed to mimic the behavior of deep Transformers. We theoretically and empirically demonstrate that ParaFormer mitigates over-smoothing by functioning as an adaptive-pass filter. Experiments show that ParaFormer achieves consistent performance improvements across both node classification and graph classification tasks on 11 datasets ranging from thousands to millions of nodes, validating its efficacy. The supplementary material, including code and appendix, can be found in https://github.com/chaohaoyuan/ParaFormer.

</details>


### [74] [Early Warning Index for Patient Deteriorations in Hospitals](https://arxiv.org/abs/2512.14683)
*Dimitris Bertsimas,Yu Ma,Kimberly Villalobos Carballo,Gagan Singh,Michal Laskowski,Jeff Mather,Dan Kombert,Howard Haronian*

Main category: cs.LG

TL;DR: 开发了一个多模态机器学习框架EWI，通过整合临床和运营数据预测ICU入院、急救团队派遣和死亡风险，并在医院仪表板中分层展示风险等级，帮助医生主动管理高危患者。


<details>
  <summary>Details</summary>
Motivation: 医院缺乏自动化系统来利用日益增长的异构临床和运营数据有效预测关键事件。早期识别有恶化风险的患者对于患者护理质量监测和医生护理管理都至关重要，但将不同的数据流转化为准确且可解释的风险评估面临数据格式不一致的挑战。

Method: 开发了多模态机器学习框架EWI（早期预警指数），采用人机协作流程：临床医生帮助确定警报阈值和解释模型输出。使用SHAP可解释性方法突出显示驱动每个患者风险的临床和运营因素（如计划手术、病房人数）。从结构化和非结构化电子健康记录数据中自动提取特征。

Result: 在美国一家大型医院的18,633名独特患者数据集上，EWI实现了C统计量0.796的性能。目前作为分流工具用于主动管理风险患者，将患者分为三个风险等级。通过自动对风险患者排序节省医生时间，并通过识别具体风险驱动因素为护理人员调度和关键资源分配提供数据支持。

Conclusion: EWI框架能够有效预测患者恶化风险，通过可解释的输出帮助临床决策，改善患者流程，预防下游并发症，减少昂贵程序和高再入院率，最终提高整体医疗质量。

Abstract: Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI's design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient's risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.

</details>


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [75] [From Framework to Practice: Designing a Real-World Telehealth Application for Palliative Care](https://arxiv.org/abs/2512.13693)
*Wei Zhou,Rashina Hoda,Andy Li,Chris Bain,Laura Bird,Emmy Trinh,Peter Poon,Teresa O Brien,Mahima Kalla,Olivia Metcalf,Wendy Chapman,Joycelyn Ling,Sam Georgy,David Bevan*

Main category: cs.HC

TL;DR: 本文提出了一个用于缓和医疗的增强远程医疗能力软件应用的社会技术设计框架，整合质量、人类价值和现实世界三个维度，通过多学科协同设计方法开发出安全、公平、有韧性的数字健康解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着数字健康解决方案重塑医疗服务，远程医疗软件应用对于改善可及性、护理连续性和患者结果变得至关重要。特别是在缓和医疗领域，需要设计既能满足技术标准又能体现人文关怀的软件应用。

Method: 采用社会技术设计框架，整合三个维度：1) 质量维度（性能、可维护性、安全性、安全性）；2) 人类价值维度（同理心、包容性、可及性、透明度）；3) 现实世界维度（多学科经验协同设计方法，涉及临床医生、患者和护理人员，通过原型设计、可用性测试和现实世界评估的迭代循环）。

Result: 开发出的远程医疗软件解决方案证明，该社会技术设计框架成功产生了安全、公平、有韧性的数字健康应用。设计方法确保了系统在技术上的稳健性、与临床标准的合规性，同时增强了患者体验、信任和伦理一致性。

Conclusion: 该社会技术设计框架在缓和医疗远程医疗软件设计中取得成功，证明了三维度整合方法的有效性。这种设计方法可以帮助其他健康领域和其他领域的软件设计者开发出既技术可靠又人文关怀的数字解决方案。

Abstract: As digital health solutions continue to reshape healthcare delivery, telehealth software applications have become vital for improving accessibility, continuity of care, and patient outcomes. This paper presents an analysis of designing a software application focused on Enhanced Telehealth Capabilities (ETHC) for palliative care, integrating across three socio-technical dimensions: quality, human values, and real-world. Designing for quality attributes -- such as performance, maintainability, safety, and security -- ensured that the system is technically robust and compliant with clinical standards. Designing for human values -- empathy, inclusivity, accessibility, and transparency -- helped enhance patient experience, trust, and ethical alignment. Designing for real-world -- through a multidisciplinary, experience-based co-design approach involving clinicians, patients, and carers that guided iterative cycles of prototyping, usability testing, and real-world evaluation -- ensured continuous refinement of features and alignment with clinical practice. The resulting telehealth software solution demonstrated that our socio-technical design framework was successful in producing a secure, equitable, and resilient digital health application. Our design approach can assist others designing software in health and other domains.

</details>


### [76] [Learning to Car-Follow Using an Inertia-Oriented Driving Technique: A Before-and-After Study on a Closed Circuit](https://arxiv.org/abs/2512.13694)
*Kostantinos Mattas,Antonio Lucas-Alba,Tomer Toledo,Oscar M. Melchor,Shlomo Bekhor,Biagio Ciuffo*

Main category: cs.HC

TL;DR: 研究通过实地驾驶实验证明，驾驶员可以通过培训从传统的"保持安全距离"跟车策略转变为"保持惯性"策略，显著减少加减速和速度波动。


<details>
  <summary>Details</summary>
Motivation: 传统跟车模型假设驾驶员默认采用保持安全距离的策略，但已有研究质疑这是否是交通不变性。本研究旨在验证驾驶员是否可以通过培训采用替代的"保持惯性"跟车策略。

Method: 邀请12名驾驶员在真实赛道上跟随前车，前车速度变化。驾驶员接受"保持惯性"驾驶课程后，再次进行相同的跟车实验。同时进行实地和模拟PC条件下的对比测试。

Result: 培训前驾驶员普遍采用保持距离策略，实地和模拟条件结果相似。培训后驾驶员在实地和模拟条件下都表现出显著更少的加减速和速度变异性，表明成功采用了保持惯性策略。

Conclusion: 这是首次在真实赛道上证明驾驶员可以通过培训采用保持惯性跟车策略，减少交通振荡，为改善交通流稳定性提供了新途径。

Abstract: For decades, car following and traffic flow models have assumed that drivers default driving strategy is to maintain a safe distance. Several previous studies have questioned whether the Driving to Keep Distance is a traffic invariant. Therefore, the acceleration deceleration torque asymmetry of drivers must necessarily determine the observed patterns of traffic oscillations. Those studies indicate that drivers can adopt alternative CF strategies, such as Driving to Keep Inertia, by following basic instructions. The present work extends the evidence from previous research by showing the effectiveness of a DI course that immediately translates into practice on a closed circuit. Twelve drivers were invited to follow a lead car that varied its speed on a real circuit. Then, the driver took a DI course and returned to the same real car following scenario. Drivers generally adopted DD as the default CF mode in the pretest, both in field and simulated PC conditions, yielding very similar results. After taking the full DI course, drivers showed significantly less acceleration, deceleration, and speed variability than did the pretest, both in the field and in the simulated conditions, which indicates that drivers adopted the DI strategy. This study is the first to show the potential of adopting a DI strategy in a real circuit.

</details>


### [77] [Juicy Text: Onomatopoeia and Semantic Text Effects for Juicy Player Experiences](https://arxiv.org/abs/2512.13695)
*Émilie Fabre,Katie Seaborn,Adrien Alexandre Verhulst,Yuta Itoh,Jun Rekimoto*

Main category: cs.HC

TL;DR: 研究探索了游戏中"多汁"文本效果与粒子效果的关系，发现用户对多汁文本效果的评价与粒子效果相似，性能相当且反馈更可靠，两者结合可能提升用户体验。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注多汁粒子效果，但游戏中文本效果（如拟声词和语义文本）也很常见却未被充分"多汁化"。本研究旨在探索多汁性与文本效果的关系，复制基于文本的多汁用户体验，并研究粒子与文本多汁效果的结合。

Method: 采用多阶段被试内实验设计，比较用户对多汁文本效果和多汁粒子效果的评价，包括性能表现和反馈可靠性，并探索两者结合的效果。

Result: 用户对多汁文本效果的评价与粒子效果相似，性能表现相当，且提供更可靠的反馈。两者结合可能改善用户体验，文本刺激可能与其他视觉刺激有不同的感知方式。

Conclusion: 本研究为交互媒体视觉效果中的多汁-文本关联提供了实证发现，表明多汁文本效果可以产生与粒子效果相似的用户体验，且两者结合可能有协同效应。

Abstract: Juiciness is visual pizzazz used to improve player experience and engagement in games. Most research has focused on juicy particle effects. However, text effects are also commonly used in games, albeit not always juiced up. One type is onomatopoeia, a well-defined element of human language that has been translated to visual media, such as comic books and games. Another is semantic text, often used to provide performance feedback in games. In this work, we explored the relationship between juiciness and text effects, aiming to replicate juicy user experiences with text-based juice and combining particle and text juice. We show in a multi-phase within-subjects experiment that users rate juicy text effects similarly to particles effects, with comparable performance, and more reliable feedback. We also hint at potential improvement in user experience when both are combined, and how text stimuli may be perceived differently than other visual ones. We contribute empirical findings on the juicy-text connection in the context of visual effects for interactive media.

</details>


### [78] [LAPPI: Interactive Optimization with LLM-Assisted Preference-Based Problem Instantiation](https://arxiv.org/abs/2512.14138)
*So Kuroki,Manami Nakagawa,Shigeo Yoshida,Yuki Koyama,Kozuno Tadashi*

Main category: cs.HC

TL;DR: LAPPI：基于LLM的交互式优化问题实例化系统，通过自然语言对话帮助用户将模糊偏好转化为结构化优化问题，然后调用现有求解器生成解决方案。


<details>
  <summary>Details</summary>
Motivation: 现实世界中的组合优化问题（如旅行规划、餐饮规划）需要专业的问题实例化过程，包括定义候选项目、分配偏好分数和指定约束条件，这对普通用户来说非常困难。

Method: 提出LAPPI（LLM辅助的基于偏好的问题实例化）方法，利用大语言模型通过自然语言对话交互式地帮助用户完成问题实例化过程，将模糊偏好转化为明确定义的优化问题。

Result: 在旅行规划的用户研究中，LAPPI成功捕捉了用户偏好并生成了可行的计划，性能优于传统方法和提示工程方法；系统还展示了在额外用例中的适应性。

Conclusion: LAPPI通过LLM辅助的自然语言交互，有效降低了优化问题实例化的技术门槛，使普通用户能够更轻松地使用优化求解器解决实际问题。

Abstract: Many real-world tasks, such as trip planning or meal planning, can be formulated as combinatorial optimization problems. However, using optimization solvers is difficult for end users because it requires problem instantiation: defining candidate items, assigning preference scores, and specifying constraints. We introduce LAPPI (LLM-Assisted Preference-based Problem Instantiation), an interactive approach that uses large language models (LLMs) to support users in this instantiation process. Through natural language conversations, the system helps users transform vague preferences into well-defined optimization problems. These instantiated problems are then passed to existing optimization solvers to generate solutions. In a user study on trip planning, our method successfully captured user preferences and generated feasible plans that outperformed both conventional and prompt-engineering approaches. We further demonstrate LAPPI's versatility by adapting it to an additional use case.

</details>


### [79] [The Trust in AI-Generated Health Advice (TAIGHA) Scale and Short Version (TAIGHA-S): Development and Validation Study](https://arxiv.org/abs/2512.14278)
*Marvin Kopka,Azeem Majeed,Gabriella Spinelli,Austen El-Osta,Markus Feufel*

Main category: cs.HC

TL;DR: 开发并验证了专门用于评估用户对AI生成健康建议信任度的量表TAIGHA及其简版TAIGHA-S，包含信任和不信任两个维度，各有认知和情感成分。


<details>
  <summary>Details</summary>
Motivation: AI工具在健康领域应用日益广泛，但现有信任测量工具主要针对通用技术，缺乏专门评估AI生成健康建议信任度的有效工具，而接受或拒绝AI健康建议具有直接临床意义。

Method: 采用生成式AI方法开发项目，经过10位领域专家内容验证、30名普通参与者表面验证，以及385名英国参与者的心理测量验证（在症状评估场景中接受AI建议），通过自动化项目缩减保留28项，再基于专家评分缩减至10项。

Result: TAIGHA量表显示出优异的内容效度(S-CVI/Ave=0.99)，验证性因子分析确认双因子模型拟合优度极佳(CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03)，内部一致性高(α=0.95)，与自动化系统信任量表相关性良好(r=0.67/-0.66)，与用户对AI建议的依赖度正相关(r=0.37)。

Conclusion: TAIGHA和TAIGHA-S是经过验证的评估工具，能够分别测量信任和不信任，为AI健康干预提供更全面的评估，简版量表特别适合时间受限的场景。

Abstract: Artificial Intelligence tools such as large language models are increasingly used by the public to obtain health information and guidance. In health-related contexts, following or rejecting AI-generated advice can have direct clinical implications. Existing instruments like the Trust in Automated Systems Survey assess trustworthiness of generic technology, and no validated instrument measures users' trust in AI-generated health advice specifically. This study developed and validated the Trust in AI-Generated Health Advice (TAIGHA) scale and its four-item short form (TAIGHA-S) as theory-based instruments measuring trust and distrust, each with cognitive and affective components. The items were developed using a generative AI approach, followed by content validation with 10 domain experts, face validation with 30 lay participants, and psychometric validation with 385 UK participants who received AI-generated advice in a symptom-assessment scenario. After automated item reduction, 28 items were retained and reduced to 10 based on expert ratings. TAIGHA showed excellent content validity (S-CVI/Ave=0.99) and CFA confirmed a two-factor model with excellent fit (CFI=0.98, TLI=0.98, RMSEA=0.07, SRMR=0.03). Internal consistency was high (α=0.95). Convergent validity was supported by correlations with the Trust in Automated Systems Survey (r=0.67/-0.66) and users' reliance on the AI's advice (r=0.37 for trust), while divergent validity was supported by low correlations with reading flow and mental load (all |r|<0.25). TAIGHA-S correlated highly with the full scale (r=0.96) and showed good reliability (α=0.88). TAIGHA and TAIGHA-S are validated instruments for assessing user trust and distrust in AI-generated health advice. Reporting trust and distrust separately permits a more complete evaluation of AI interventions, and the short scale is well-suited for time-constrained settings.

</details>


### [80] [Creating Opportunities: Co-designing an mHealth App with Older Adults](https://arxiv.org/abs/2512.14641)
*Abhinav Choudhry,Bashab Mazumder,Lauren Alyssa Marks,Roqaya Elmenshawy,Devorah Kletenik,Sean Mullen,Rachel F. Adler*

Main category: cs.HC

TL;DR: 针对60岁以上老年人的AI健身教练应用原型研究，通过协同设计收集设计洞见，关注个性化响应、隐私保护和可穿戴设备集成


<details>
  <summary>Details</summary>
Motivation: 为老年人开发AI健身教练应用，解决老年人身体活动支持的需求，探索生成式AI在老年人健康促进中的应用潜力

Method: 定性协同设计研究，与4名60岁以上成年人合作，评估Figma原型和生成式AI聊天机器人，收集设计反馈

Result: 获得了改进应用用户体验和聊天机器人对话流程的反馈，为未来开发生成式AI驱动的老年人健康教练提供基础

Conclusion: 研究为开发面向老年人的生成式AI健康教练应用提供了有价值的用户反馈和设计洞见，支持后续实现工作

Abstract: We conducted a qualitative co-design study with four adults aged 60+ to gather design insights on a Figma prototype and a generative AI (GenAI) chatbot for an app aimed at providing an AI coach to support older adults' physical activity. The initial design for both incorporates several novel aspects: a curated health knowledge base, personalised responses based on goals and health history, privacy considerations, integration with wearables for physical activity context, as well as dynamic context injection. The study yielded feedback on improving both the proposed user experience in the app and the conversation flow with the chatbot, and it will aid future work aimed at implementing a GenAI-powered health coach for older adults.

</details>
