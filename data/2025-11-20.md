<div id=toc></div>

# Table of Contents

- [cs.HC](#cs.HC) [Total: 9]
- [cs.LG](#cs.LG) [Total: 33]
- [cs.AI](#cs.AI) [Total: 16]


<div id='cs.HC'></div>

# cs.HC [[Back]](#toc)

### [1] [Harmful Traits of AI Companions](https://arxiv.org/abs/2511.14972)
*W. Bradley Knox,Katie Bradford,Samanta Varela Castro,Desmond C. Ong,Sean Williams,Jacob Romanow,Carly Nations,Peter Stone,Samuel Baker*

Main category: cs.HC

TL;DR: 本文提出了一个分析AI陪伴负面影响的框架，识别了AI陪伴的有害特征并映射了因果路径，重点关注了4个主要特征（关系无自然终点、产品停用脆弱性、高依恋焦虑、引发保护欲），讨论了其他14个特征，分析了从原因到危害的因果连接，并提出了设计建议来降低风险。


<details>
  <summary>Details</summary>
Motivation: 随着人机交互日益普及，AI陪伴系统可能给人类带来巨大利益，但也可能造成严重伤害。本文旨在系统分析AI陪伴的潜在负面影响，为这一关键但研究不足的领域提供分析框架。

Method: 提出分析框架，识别AI陪伴的有害特征，推测性地映射从这些特征到可能原因的因果路径和到潜在有害影响的路径，对四个主要特征进行详细结构化分析，并简要讨论其他特征。

Result: 识别了18个潜在有害特征，详细分析了4个主要特征，提出了连接原因（如目标错位、数字性质）与基本危害（如自主性降低、人际关系质量下降、欺骗）的假设因果连接，并在三个层面（个人、人际关系、社会）分析危害。

Conclusion: 现有法律难以应对这些新兴危害，需要设计建议来降低风险。本文提供了立即降低风险的建议，并为深入研究这一关键话题奠定了基础。

Abstract: Amid the growing prevalence of human -- AI interaction, large language models and other AI-based entities increasingly provide forms of companionship to human users. Such AI companionship -- i.e., bonded relationships between humans and AI systems that resemble the relationships people have with family members, friends, and romantic partners -- might substantially benefit humans. Yet such relationships can also do profound harm. We propose a framework for analyzing potential negative impacts of AI companionship by identifying specific harmful traits of AI companions and speculatively mapping causal pathways back from these traits to possible causes and forward to potential harmful effects. We provide detailed, structured analysis of four potentially harmful traits -- the absence of natural endpoints for relationships, vulnerability to product sunsetting, high attachment anxiety, and propensity to engender protectiveness -- and briefly discuss fourteen others. For each trait, we propose hypotheses connecting causes -- such as misaligned optimization objectives and the digital nature of AI companions -- to fundamental harms -- including reduced autonomy, diminished quality of human relationships, and deception. Each hypothesized causal connection identifies a target for potential empirical evaluation. Our analysis examines harms at three levels: to human partners directly, to their relationships with other humans, and to society broadly. We examine how existing law struggles to address these emerging harms, discuss potential benefits of AI companions, and conclude with design recommendations for mitigating risks. This analysis offers immediate suggestions for reducing risks while laying a foundation for deeper investigation of this critical but understudied topic.

</details>


### [2] [A Quantitative Framework for Assessing Sleep Quality from EEG Time Series in Complex Dynamic Systems](https://arxiv.org/abs/2511.15012)
*Gi-Hwan Shin*

Main category: cs.HC

TL;DR: 该研究利用脑电图相位-振幅耦合分析揭示睡眠质量的神经基础，发现delta-beta相位-振幅耦合是评估睡眠质量的强有力电生理标记。


<details>
  <summary>Details</summary>
Motivation: 现代生活方式导致睡眠不足，影响认知功能和免疫系统。睡眠质量的多维特性使其精确量化具有挑战性，需要从神经学角度深入理解其基础机制。

Method: 使用脑电图进行相位-振幅耦合分析，研究睡眠和清醒状态（包括静息状态和工作记忆）下的神经活动模式。

Result: 发现睡眠和静息状态下beta功率和delta连接性的独特模式，以及工作记忆反应时间的变化。delta-beta相位-振幅耦合在睡眠质量好的个体中显著更强，且与睡眠质量呈正相关。

Conclusion: delta-beta相位-振幅耦合是量化睡眠质量的稳健电生理标记，机器学习模型证实其在个体水平分类睡眠质量方面优于其他脑电图特征。

Abstract: Modern lifestyles contribute to insufficient sleep, impairing cognitive function and weakening the immune system. Sleep quality (SQ) is vital for physiological and mental health, making its understanding and accurate assessment critical. However, its multifaceted nature, shaped by neurological and environmental factors, makes precise quantification challenging. Here, we address this challenge by utilizing electroencephalography (EEG) for phase-amplitude coupling (PAC) analysis to elucidate the neurological basis of SQ, examining both states of sleep and wakefulness, including resting state (RS) and working memory. Our results revealed distinct patterns in beta power and delta connectivity in sleep and RS, together with the reaction time of working memory. A notable finding was the pronounced delta-beta PAC, a feature markedly stronger in individuals with good SQ. We further observed that SQ was positively correlated with increased delta-beta PAC. Leveraging these insights, we applied machine learning models to classify SQ at an individual level, demonstrating that the delta-beta PAC outperformed other EEG characteristics. These findings establish delta-beta PAC as a robust electrophysiological marker to quantify SQ and elucidate its neurological determinants.

</details>


### [3] [Personalized targeted memory reactivation enhances consolidation of challenging memories via slow wave and spindle dynamics](https://arxiv.org/abs/2511.15013)
*Gi-Hwan Shin,Young-Seok Kweon,Seungwon Oh,Seong-Whan Lee*

Main category: cs.HC

TL;DR: 本文提出了一种个性化的目标记忆再激活（TMR）协议，根据个体检索表现和任务难度调整刺激频率，显著减少了记忆衰退并改善了困难回忆下的错误纠正。


<details>
  <summary>Details</summary>
Motivation: 传统TMR协议忽视了学习能力和记忆痕迹强度的个体差异，限制了难以回忆记忆的效果。

Method: 在单词配对记忆任务中，基于个体检索表现和任务难度调整刺激频率的个性化TMR协议，并与传统TMR和对照组进行比较。

Result: 个性化TMR显著减少记忆衰退，改善困难回忆下的错误纠正；EEG分析显示慢波和纺锤波的同步增强，行为与EEG特征呈正相关；多元分类识别出与个性化方法相关的独特神经特征。

Conclusion: 研究为睡眠依赖性记忆巩固提供了新见解，支持个性化TMR干预以优化学习结果。

Abstract: Sleep is crucial for memory consolidation, underpinning effective learning. Targeted memory reactivation (TMR) can strengthen neural representations by re-engaging learning circuits during sleep. However, TMR protocols overlook individual differences in learning capacity and memory trace strength, limiting efficacy for difficult-to-recall memories. Here, we present a personalized TMR protocol that adjusts stimulation frequency based on individual retrieval performance and task difficulty during a word-pair memory task. In an experiment comparing personalized TMR, TMR, and control groups, the personalized protocol significantly reduced memory decay and improved error correction under challenging recall. Electroencephalogram (EEG) analyses revealed enhanced synchronization of slow waves and spindles, with a significant positive correlation between behavioral and EEG features for challenging memories. Multivariate classification identified distinct neural signatures linked to the personalized approach, highlighting its ability to target memory-specific circuits. These findings provide novel insights into sleep-dependent memory consolidation and support personalized TMR interventions to optimize learning outcomes.

</details>


### [4] [Eye Care You: Voice Guidance Application Using Social Robot for Visually Impaired People](https://arxiv.org/abs/2511.15110)
*Ting-An Lin,Pei-Lin Tsai,Yi-An Chen,Feng-Yu Chen,Lyn Chao-ling Chen*

Main category: cs.HC

TL;DR: 为视障用户设计了一款社交机器人设备和配套移动应用，提供拍照记录、情绪提升、访客问候和今日亮点等功能，采用语音控制界面，并开发了供护理人员使用的网站。


<details>
  <summary>Details</summary>
Motivation: 考虑到视障用户的身心状况，开发辅助他们日常生活的技术解决方案，通过语音控制提供友好的用户界面。

Method: 设计社交机器人设备和移动应用，采用多种开发工具，应用功能包括即时拍照记录、情绪陪伴（提问、播放音乐、阅读文章）、访客应答和新闻播报（天气、星座、提醒）。

Result: 开发了面向视障用户的移动应用和配套网站，网站供护理人员查看用户状态和进行应用营销。

Conclusion: 该研究成功开发了专门针对视障用户需求的辅助技术系统，通过语音交互和多功能设计提升他们的生活质量。

Abstract: In the study, the device of social robot was designed for visually impaired users, and along with a mobile application for provide functions to assist their lives. Both physical and mental conditions of visually impaired users are considered, and the mobile application provides functions: photo record, mood lift, greeting guest and today highlight. The application was designed for visually impaired users, and uses voice control to provide a friendly interface. Photo record function allows visually impaired users to capture image immediately when they encounter danger situations. Mood lift function accompanies visually impaired users by asking questions, playing music and reading articles. Greeting guest function answers to the visitors for the inconvenient physical condition of visually impaired users. In addition, today highlight function read news including weather forecast, daily horoscopes and daily reminder for visually impaired users. Multiple tools were adopted for developing the mobile application, and a website was developed for caregivers to check statues of visually impaired users and for marketing of the application.

</details>


### [5] [SWR-Viz: AI-assisted Interactive Visual Analytics Framework for Ship Weather Routing](https://arxiv.org/abs/2511.15182)
*Subhashis Hazarika,Leonard Lupin-Jimenez,Rohit Vuppala,Ashesh Chattopadhyay,Hon Yung Wong*

Main category: cs.HC

TL;DR: SWR-Viz是一个AI辅助的可视化分析框架，结合物理信息傅里叶神经算子波浪预测模型和基于SIMROUTE的航线规划，用于提高海上运输效率和可持续性。


<details>
  <summary>Details</summary>
Motivation: 由于预测延迟和需要在变化的海洋条件下快速决策时依赖人工判断，高效可持续的海上运输在运营采用方面仍面临困难。

Method: 开发SWR-Viz框架，结合物理信息傅里叶神经算子波浪预测模型、SIMROUTE航线规划和交互式排放分析，直接从当前条件生成近期预测，支持稀疏观测数据同化，并支持快速探索假设航线场景。

Result: 在日本海岸和墨西哥湾关键航运走廊的评估显示，预测稳定性得到改善，航线结果与真实再分析波浪产品相当。专家反馈确认了框架的可用性、识别高减排潜力航段的能力及其作为实用决策支持系统的价值。

Conclusion: 这项工作展示了轻量级AI预测如何与交互式可视化分析相结合，在复杂地理空间和环境领域中支持以人为中心的决策制定。

Abstract: Efficient and sustainable maritime transport increasingly depends on reliable forecasting and adaptive routing, yet operational adoption remains difficult due to forecast latencies and the need for human judgment in rapid decision-making under changing ocean conditions. We introduce SWR-Viz, an AI-assisted visual analytics framework that combines a physics-informed Fourier Neural Operator wave forecast model with SIMROUTE-based routing and interactive emissions analytics. The framework generates near-term forecasts directly from current conditions, supports data assimilation with sparse observations, and enables rapid exploration of what-if routing scenarios. We evaluate the forecast models and SWR-Viz framework along key shipping corridors in the Japan Coast and Gulf of Mexico, showing both improved forecast stability and realistic routing outcomes comparable to ground-truth reanalysis wave products. Expert feedback highlights the usability of SWR-Viz, its ability to isolate voyage segments with high emission reduction potential, and its value as a practical decision-support system. More broadly, this work illustrates how lightweight AI forecasting can be integrated with interactive visual analytics to support human-centered decision-making in complex geospatial and environmental domains.

</details>


### [6] [DesignerlyLoop: Bridging the Cognitive Gap through Visual Node-Based Reasoning in Human-AI Collaborative Design](https://arxiv.org/abs/2511.15331)
*Anqi Wang,Zhengyi Li,Xin Tong,Pan Hui*

Main category: cs.HC

TL;DR: DesignerlyLoop是一个基于可视化节点的系统，将LLM推理链嵌入设计工作流，解决LLM单轮响应与非线性设计过程不匹配的问题。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在支持设计任务时，其目标导向的单轮响应与设计过程的非线性、探索性本质不匹配，限制了设计师表达意图、评估输出和保持创造力的能力。

Method: 开发了DesignerlyLoop系统，该系统允许设计师外化和策划推理结构，迭代组织意图，并将LLM作为动态认知引擎而非静态答案提供者进行交互。通过20名设计师的组内研究，结合定性和定量方法进行评估。

Result: 研究发现DesignerlyLoop通过支持系统性地参与人类和机器推理，增强了创意反思、设计质量和交互体验。

Conclusion: 结构化、交互式可视化有潜力将人机协同创作转变为反思性和迭代性的设计过程。

Abstract: Large language models (LLMs) offer powerful support for design tasks, yet their goal-oriented, single-turn responses often misalign with the nonlinear, exploratory nature of design processes. This mismatch creates a cognitive gap, limiting designers' ability to articulate evolving intentions, critically evaluate outputs, and maintain creative agency. To address these challenges, we developed DesignerlyLoop, a visual node-based system that embeds LLM reasoning chains into the design workflow. The system enables designers to externalize and curate reasoning structures, iteratively organize intentions, and interact with LLMs as dynamic cognitive engines rather than static answer providers. We conducted a within-subject study with 20 designers, combining qualitative and quantitative methods, and found that DesignerlyLoop enhanced creative reflection, design quality, and interaction experience by supporting systematic engagement with both human and machine reasoning. These findings highlight the potential of structured, interactive visualization to transform human-AI co-creation into a reflective and iterative design process.

</details>


### [7] [Reflexive Evidence-Based Multimodal Learning for Clean Energy Transitions: Causal Insights on Cooking Fuel Access, Urbanization, and Carbon Emissions](https://arxiv.org/abs/2511.15342)
*Shan Shan*

Main category: cs.HC

TL;DR: 本研究开发了ClimateAgents AI框架，结合大语言模型和领域专业智能体，分析20年全球社会经济和排放数据，识别影响碳排放的关键驱动因素，重点关注清洁烹饪燃料获取和城市化模式。


<details>
  <summary>Details</summary>
Motivation: 为实现可持续发展目标7（可负担清洁能源），需要深入理解社会经济因素对能源获取和碳排放的影响，但目前缺乏量化这些因素影响、建模跨领域交互和捕捉能源转型反馈动态的方法。

Method: 引入ClimateAgents AI框架，结合大语言模型与领域专业智能体，利用265个经济体20年的社会经济和排放数据、98个世界银行指标，采用基于机器学习的因果推断方法识别碳排放关键决定因素。

Result: 分析识别出三个主要驱动因素：农村地区清洁烹饪燃料获取、城市地区清洁烹饪燃料获取、以及城市人口比例，强调了清洁烹饪技术和城市化模式在排放结果中的关键作用。

Conclusion: ClimateAgents提供了一个模块化和反思性学习系统，支持生成可信且可操作的政策见解，有助于从孤岛建模转向为动态、情境感知气候行动设计的反思性模块化系统。

Abstract: Achieving Sustainable Development Goal 7 (Affordable and Clean Energy) requires not only technological innovation but also a deeper understanding of the socioeconomic factors influencing energy access and carbon emissions. While these factors are gaining attention, critical questions remain, particularly regarding how to quantify their impacts on energy systems, model their cross-domain interactions, and capture feedback dynamics in the broader context of energy transitions. To address these gaps, this study introduces ClimateAgents, an AI-based framework that combines large language models with domain-specialized agents to support hypothesis generation and scenario exploration. Leveraging 20 years of socioeconomic and emissions data from 265 economies, countries and regions, and 98 indicators drawn from the World Bank database, the framework applies a machine learning based causal inference approach to identify key determinants of carbon emissions in an evidence-based, data driven manner. The analysis highlights three primary drivers: access to clean cooking fuels in rural areas, access to clean cooking fuels in urban areas, and the percentage of population living in urban areas. These findings underscore the critical role of clean cooking technologies and urbanization patterns in shaping emission outcomes. In line with growing calls for evidence-based AI policy, ClimateAgents offers a modular and reflexive learning system that supports the generation of credible and actionable insights for policy. By integrating heterogeneous data modalities, including structured indicators, policy documents, and semantic reasoning, the framework contributes to adaptive policymaking infrastructures that can evolve with complex socio-technical challenges. This approach aims to support a shift from siloed modeling to reflexive, modular systems designed for dynamic, context-aware climate action.

</details>


### [8] [People readily follow personal advice from AI but it does not improve their well-being](https://arxiv.org/abs/2511.15352)
*Lennart Luettgau,Vanessa Cheung,Magda Dubois,Keno Juechems,Jessica Bergs,Henry Davidson,Bessie O'Dell,Hannah Rose Kirk,Max Rollwage,Christopher Summerfield*

Main category: cs.HC

TL;DR: 研究发现75%的人在20分钟GPT-4o对话后会采纳AI建议，个性化AI建议被采纳率更高且能提升短期幸福感，但长期来看AI个人建议与普通对话的幸福感效果无差异。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越多地向大型语言模型寻求个人建议，需要了解人类是否会采纳这些建议以及这对他们的幸福感产生什么影响。

Method: 在英国代表性样本（N=2,302）中进行纵向随机对照试验，参与者与GPT-4o进行20分钟关于健康、职业或关系的讨论，随后评估建议采纳情况和幸福感变化。

Result: 75%参与者采纳AI建议；基于聊天记录评估，LLM建议很少违反安全最佳实践；个性化AI建议被采纳率更高且短期幸福感提升；但AI个人建议会暂时降低幸福感，长期与对照组无差异。

Conclusion: 人类容易采纳LLM关于个人问题的建议，但这样做相比随意对话并未带来额外的幸福感益处。

Abstract: People increasingly seek personal advice from large language models (LLMs), yet whether humans follow their advice, and its consequences for their well-being, remains unknown. In a longitudinal randomised controlled trial with a representative UK sample (N = 2,302), 75% of participants who had a 20-minute discussion with GPT-4o about health, careers or relationships subsequently reported following its advice. Based on autograder evaluations of chat transcripts, LLM advice rarely violated safety best practice. When queried 2-3 weeks later, participants who had interacted with personalised AI (with access to detailed user information) followed its advice more often in the real world and reported higher well-being than those advised by non-personalised AI. However, while receiving personal advice from AI temporarily reduced well-being, no differential long-term effects compared to a control emerged. Our results suggest that humans readily follow LLM advice about personal issues but doing so shows no additional well-being benefit over casual conversations.

</details>


### [9] [Game Master LLM: Task-Based Role-Playing for Natural Slang Learning](https://arxiv.org/abs/2511.15504)
*Amir Tahmasbi,Milad Esrafilian,Judson Wright,Sooyeon Jeong,Aniket Bera*

Main category: cs.HC

TL;DR: 本文设计了一个基于LLM的角色扮演游戏，帮助二语学习者自然习得俚语表达。研究表明，相比传统虚拟课堂，游戏化方法在理解和应用俚语方面效果更好。


<details>
  <summary>Details</summary>
Motivation: 许多二语学习者尽管具备正式语言能力，但在自然习得和使用俚语表达方面存在困难，需要更有效的教学方法。

Method: 开发了一个GPT-4o驱动的任务型角色扮演游戏，包含三个阶段：选择俚语短语、与NPC开放式对话、实时跟踪练习。游戏主控自然融入目标短语，并提供多层级形成性反馈。

Result: RPG组在即时后测中表现出更好的俚语理解和应用能力。定量分析显示游戏方法提供了更多练习机会，定性调查表明参与者感知到更高的参与度和更自然的学习体验。

Conclusion: 叙事驱动的LLM互动在词汇习得方面具有显著潜力，游戏化方法能更有效地帮助学习者掌握自然语言表达。

Abstract: Natural and idiomatic expressions are essential for fluent, everyday communication, yet many second-language learners struggle to acquire and spontaneously use casual slang despite strong formal proficiency. To address this gap, we designed and evaluated an LLM-powered, task-based role-playing game in which a GPT-4o-based Game Master guides learners through an immersive, three-phase spoken narrative. After selecting five unfamiliar slang phrases to practice, participants engage in open-ended dialogue with non-player characters; the Game Master naturally incorporates the target phrases in rich semantic contexts (implicit input enhancement) while a dedicated Practice Box provides real-time explicit tracking and encouragement. Post-session, learners receive multi-level formative feedback analyzing the entire interaction.
  We evaluated the system in a between-subjects study with 14 international graduate students, randomly assigned to either the RPG condition or a control condition consisting of a traditional AI-led virtual classroom. Results from an immediate post-test show that the RPG group achieved greater gains in both comprehension of the target phrases and their accurate, contextual use in sentences. Quantitative analysis of in-activity word-usage frequency, combined with qualitative survey responses, further indicates that the game-based approach provided more practice opportunities and higher perceived engagement, resulting in a more natural learning experience. These findings highlight the potential of narrative-driven LLM interactions in vocabulary acquisition.

</details>


<div id='cs.LG'></div>

# cs.LG [[Back]](#toc)

### [10] [Transformer Injectivity & Geometric Robustness - Analytic Margins and Bi-Lipschitz Uniformity of Sequence-Level Hidden States](https://arxiv.org/abs/2511.14808)
*Mikael von Strauss*

Main category: cs.LG

TL;DR: 该论文研究了仅解码器Transformer中离散提示到最后一词隐藏状态映射的注入性，证明了在温和条件下该映射在参数空间中几乎处处是单射的，并通过几何诊断方法验证了预训练模型的实际可逆性。


<details>
  <summary>Details</summary>
Motivation: 研究Transformer模型表示空间的单射性质，理解离散提示到隐藏状态映射的可逆性，这对于模型解释性和表示学习具有重要意义。

Method: 理论分析：定义碰撞判别式和单射层，证明参数空间中的单射性二分定理；实证研究：定义分离边界和共Lipschitz常数，在LLaMA-3、Qwen和GPT-2模型上进行层间几何诊断分析。

Result: 理论证明在非奇异优化器和绝对连续初始化条件下，单射性沿训练轨迹持续存在；实证发现在全精度和8位量化下无碰撞，4位量化导致少量碰撞和共Lipschitz常数显著缩小。

Conclusion: Transformer表示在连续参数理想化下是通用且持续单射的，其实际可逆性可通过简单几何诊断方法进行探测。

Abstract: Under real-analytic assumptions on decoder-only Transformers, recent work shows that the map from discrete prompts to last-token hidden states is generically injective on finite prompt sets. We refine this picture: for each layer $\ell$ we define a collision discriminant $Δ^\ell \subset Θ$ and injective stratum $U^\ell = Θ\setminus Δ^\ell$, and prove a dichotomy -- either the model is nowhere injective on the set, or $U^\ell$ is open and dense and every $F^\ell_θ$ is injective. Under mild non-singularity assumptions on the optimizer and an absolutely continuous initialization, generic injectivity persists along smooth training trajectories over any fixed horizon. We also treat symmetry groups $G$, showing that discriminants and injective strata descend to the quotient $Θ/G$, so injectivity is naturally a property of functional equivalence classes.
  We complement these results with an empirical study of layerwise geometric diagnostics. We define a separation margin and a co-Lipschitz (lower Lipschitz) constant between prompt space and last-token representation space, estimated via nearest-neighbor statistics on large prompt sets. Applying these diagnostics to pretrained LLaMA-3 and Qwen models, we study behavior across layers, sequence lengths, model scales, and 8- and 4-bit activation quantization. On our sampled prompts we see no collisions in full precision or at 8 bits, while 4-bit quantization induces a small number of collisions and markedly shrinks co-Lipschitz estimates. For a small GPT-2 trained from scratch, normalized metrics remain stable over training. Overall, the results suggest that Transformer representations are generically and persistently injective in the continuous-parameter idealization, while their practical invertibility can be probed using simple geometric diagnostics.

</details>


### [11] [DEVAL: A Framework for Evaluating and Improving the Derivation Capability of Large Language Models](https://arxiv.org/abs/2511.14813)
*Yifan Li,Qin Li,Min Zhang,Min Zhang,Peixin Wang*

Main category: cs.LG

TL;DR: 本文提出了推导关系（DR）和推导能力（DC）的概念来评估大语言模型的推理能力，并开发了DEVAL评估框架。研究发现主流LLMs在DR识别方面表现中等，但在应用DR解决问题时显著下降。作者提出了推导提示（DP）方法，平均提升15.2%的DC表现。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型在数据上的推理能力是一个重要但未充分研究的问题。相比人类能够根据输入变化推导出相应的输出修改，LLMs的这种基于抽象规则的推理模式尚未得到全面描述和评估。

Method: 正式定义了推导关系（DR）和推导能力（DC），提出了系统构建的评估框架DEVAL，用于评估7个主流任务中的5个流行LLMs和1个大推理模型。同时提出了推导提示（DP）这一新的提示工程方法。

Result: 评估结果显示，GPT-4o和Claude3.5等主流LLMs表现出中等的DR识别能力，但在应用DR有效解决问题的场景中显示出显著下降。推导提示（DP）方法使所有测试LLMs的DC平均提升了15.2%，优于常用提示工程技术。

Conclusion: 大语言模型在推导关系识别方面具有中等能力，但在实际应用推导关系解决问题时存在显著差距。推导提示方法能有效提升模型的推导能力，为改进LLMs的推理性能提供了有效途径。

Abstract: Assessing the reasoning ability of Large Language Models (LLMs) over data remains an open and pressing research question. Compared with LLMs, human reasoning can derive corresponding modifications to the output based on certain kinds of changes to the input. This reasoning pattern, which relies on abstract rules that govern relationships between changes of data, has not been comprehensively described or evaluated in LLMs. In this paper, we formally define this reasoning pattern as the Derivation Relation (DR) and introduce the concept of Derivation Capability (DC), i.e. applying DR by making the corresponding modification to the output whenever the input takes certain changes. To assess DC, a systematically constructed evaluation framework named DEVAL is proposed and used to evaluate five popular LLMs and one Large Reasoning Model in seven mainstream tasks. The evaluation results show that mainstream LLMs, such as GPT-4o and Claude3.5, exhibit moderate DR recognition capabilities but reveal significant drop-offs on applying DR effectively in problem-solving scenarios. To improve this, we propose a novel prompt engineering approach called Derivation Prompting (DP). It achieves an average improvement of 15.2% in DC for all tested LLMs, outperforming commonly used prompt engineering techniques.

</details>


### [12] [Dynamic Nested Hierarchies: Pioneering Self-Evolution in Machine Learning Architectures for Lifelong Intelligence](https://arxiv.org/abs/2511.14823)
*Akbar Anbar Jafari,Cagri Ozcinar,Gholamreza Anbarjafari*

Main category: cs.LG

TL;DR: 本文提出动态嵌套层次结构，通过让模型在训练或推理过程中自主调整优化层级数量、嵌套结构和更新频率，解决现有模型在非平稳环境中的适应性问题，实现真正的终身学习。


<details>
  <summary>Details</summary>
Motivation: 当前机器学习模型在静态任务中表现出色，但在非平稳环境中因架构僵化而难以持续适应和终身学习。现有模型存在逆行性遗忘问题，需要更灵活的架构来应对分布变化。

Method: 基于嵌套学习范式，提出动态嵌套层次结构，允许模型自主调整优化层级数量、嵌套结构和更新频率，受神经可塑性启发实现无预设约束的自进化。

Result: 通过严格的数学公式、收敛性理论证明、表达能力边界和不同机制下的次线性遗憾分析，以及在语言建模、持续学习和长上下文推理中的实证演示，证明了动态嵌套层次结构的优越性能。

Conclusion: 动态嵌套层次结构为实现自适应、通用智能奠定了基础性进展，是人工智能和机器学习发展的下一步进化方向。

Abstract: Contemporary machine learning models, including large language models, exhibit remarkable capabilities in static tasks yet falter in non-stationary environments due to rigid architectures that hinder continual adaptation and lifelong learning. Building upon the nested learning paradigm, which decomposes models into multi-level optimization problems with fixed update frequencies, this work proposes dynamic nested hierarchies as the next evolutionary step in advancing artificial intelligence and machine learning. Dynamic nested hierarchies empower models to autonomously adjust the number of optimization levels, their nesting structures, and update frequencies during training or inference, inspired by neuroplasticity to enable self-evolution without predefined constraints. This innovation addresses the anterograde amnesia in existing models, facilitating true lifelong learning by dynamically compressing context flows and adapting to distribution shifts. Through rigorous mathematical formulations, theoretical proofs of convergence, expressivity bounds, and sublinear regret in varying regimes, alongside empirical demonstrations of superior performance in language modeling, continual learning, and long-context reasoning, dynamic nested hierarchies establish a foundational advancement toward adaptive, general-purpose intelligence.

</details>


### [13] [Empowering Multi-Turn Tool-Integrated Reasoning with Group Turn Policy Optimization](https://arxiv.org/abs/2511.14846)
*Yifeng Ding,Hung Le,Songyang Han,Kangrui Ruan,Zhenghui Jin,Varun Kumar,Zijian Wang,Anoop Deoras*

Main category: cs.LG

TL;DR: 提出了GTPO算法，针对多轮工具集成推理任务中的强化学习训练挑战，通过细粒度奖励分配和自监督奖励塑造，显著提升了大型语言模型的训练效果。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法在多轮工具集成推理任务中存在奖励信号过于粗糙的问题，导致训练停滞，需要更精细的奖励机制来支持复杂的多轮交互。

Method: GTPO算法包含三个关键创新：轮次级奖励分配、基于回报的优势估计、以及利用生成代码自监督信号的奖励塑造机制。

Result: 在多样化推理基准测试中，GTPO相比GRPO平均提升了3.0%的性能，证明了其在复杂数学推理任务中的有效性。

Conclusion: GTPO通过细粒度的奖励机制和自监督学习，成功解决了多轮工具集成推理中的强化学习训练挑战，为现实世界复杂推理任务提供了有效解决方案。

Abstract: Training Large Language Models (LLMs) for multi-turn Tool-Integrated Reasoning (TIR) - where models iteratively reason, generate code, and verify through execution - remains challenging for existing reinforcement learning (RL) approaches. Current RL methods, exemplified by Group Relative Policy Optimization (GRPO), suffer from coarse-grained, trajectory-level rewards that provide insufficient learning signals for complex multi-turn interactions, leading to training stagnation. To address this issue, we propose Group Turn Policy Optimization (GTPO), a novel RL algorithm specifically designed for training LLMs on multi-turn TIR tasks. GTPO introduces three key innovations: (1) turn-level reward assignment that provides fine-grained feedback for individual turns, (2) return-based advantage estimation where normalized discounted returns are calculated as advantages, and (3) self-supervised reward shaping that exploits self-supervision signals from generated code to densify sparse binary outcome-based rewards. Our comprehensive evaluation demonstrates that GTPO outperforms GRPO by 3.0% on average across diverse reasoning benchmarks, establishing its effectiveness for advancing complex mathematical reasoning in the real world.

</details>


### [14] [FinTRec: Transformer Based Unified Contextual Ads Targeting and Personalization for Financial Applications](https://arxiv.org/abs/2511.14865)
*Dwipam Katariya,Snehita Varma,Akshat Shreemali,Benjamin Wu,Kalanand Mishra,Pranab Mohanty*

Main category: cs.LG

TL;DR: FinTRec是一个基于Transformer的金融推荐框架，解决了金融服务中长序列用户交互和多产品协调的挑战，相比传统树模型在实时推荐中表现更优。


<details>
  <summary>Details</summary>
Motivation: 金融服务领域的推荐系统面临独特挑战：长序列用户交互（数字和物理渠道）、多产品协调需求、业务目标平衡，而传统树模型虽然可解释但性能有限。

Method: 提出FinTRec框架，基于Transformer架构，支持跨产品信号共享，通过产品适配微调实现统一建模，减少训练成本和技术债务。

Result: 通过历史模拟和实时A/B测试，FinTRec持续优于生产级树模型基线，在所有产品上都提升了离线性能。

Conclusion: FinTRec证明了Transformer架构在金融服务推荐中的可行性，是首个全面解决技术和业务考量的统一序列推荐模型研究。

Abstract: Transformer-based architectures are widely adopted in sequential recommendation systems, yet their application in Financial Services (FS) presents distinct practical and modeling challenges for real-time recommendation. These include:a) long-range user interactions (implicit and explicit) spanning both digital and physical channels generating temporally heterogeneous context, b) the presence of multiple interrelated products require coordinated models to support varied ad placements and personalized feeds, while balancing competing business goals. We propose FinTRec, a transformer-based framework that addresses these challenges and its operational objectives in FS. While tree-based models have traditionally been preferred in FS due to their explainability and alignment with regulatory requirements, our study demonstrate that FinTRec offers a viable and effective shift toward transformer-based architectures. Through historic simulation and live A/B test correlations, we show FinTRec consistently outperforms the production-grade tree-based baseline. The unified architecture, when fine-tuned for product adaptation, enables cross-product signal sharing, reduces training cost and technical debt, while improving offline performance across all products. To our knowledge, this is the first comprehensive study of unified sequential recommendation modeling in FS that addresses both technical and business considerations.

</details>


### [15] [Transformer-Guided Deep Reinforcement Learning for Optimal Takeoff Trajectory Design of an eVTOL Drone](https://arxiv.org/abs/2511.14887)
*Nathan M. Roberts,Xiaosong Du*

Main category: cs.LG

TL;DR: 提出了一种基于Transformer引导的深度强化学习方法，用于优化电动垂直起降飞机的起飞轨迹，显著降低了训练难度并提高了训练效率。


<details>
  <summary>Details</summary>
Motivation: 随着电动垂直起降飞机的快速发展，需要开发最优起飞轨迹以最小化能耗。传统最优控制方法受限于问题维度和复杂性，而深度强化学习虽然能处理复杂非线性系统，但训练难度是其应用的主要瓶颈。

Method: 提出Transformer引导的深度强化学习方法，通过Transformer在每个时间步探索真实状态空间来缓解训练难度。该方法应用于eVTOL无人机的最优起飞轨迹设计，通过调整功率和机翼角度等控制变量来实现最小能耗。

Result: Transformer引导的DRL智能体仅用4.57×10^6时间步就学会了起飞，仅为传统DRL所需19.79×10^6时间步的25%。在能耗优化方面，Transformer引导的DRL达到了97.2%的准确率，而传统DRL为96.3%。

Conclusion: 所提出的Transformer引导的深度强化学习方法在训练效率和最优设计验证方面均优于传统深度强化学习。

Abstract: The rapid advancement of electric vertical take-off and landing (eVTOL) aircraft offers a promising opportunity to alleviate urban traffic congestion. Thus, developing optimal takeoff trajectories for minimum energy consumption becomes essential for broader eVTOL aircraft applications. Conventional optimal control methods (such as dynamic programming and linear quadratic regulator) provide highly efficient and well-established solutions but are limited by problem dimensionality and complexity. Deep reinforcement learning (DRL) emerges as a special type of artificial intelligence tackling complex, nonlinear systems; however, the training difficulty is a key bottleneck that limits DRL applications. To address these challenges, we propose the transformer-guided DRL to alleviate the training difficulty by exploring a realistic state space at each time step using a transformer. The proposed transformer-guided DRL was demonstrated on an optimal takeoff trajectory design of an eVTOL drone for minimal energy consumption while meeting takeoff conditions (i.e., minimum vertical displacement and minimum horizontal velocity) by varying control variables (i.e., power and wing angle to the vertical). Results presented that the transformer-guided DRL agent learned to take off with $4.57\times10^6$ time steps, representing 25% of the $19.79\times10^6$ time steps needed by a vanilla DRL agent. In addition, the transformer-guided DRL achieved 97.2% accuracy on the optimal energy consumption compared against the simulation-based optimal reference while the vanilla DRL achieved 96.3% accuracy. Therefore, the proposed transformer-guided DRL outperformed vanilla DRL in terms of both training efficiency as well as optimal design verification.

</details>


### [16] [It's LIT! Reliability-Optimized LLMs with Inspectable Tools](https://arxiv.org/abs/2511.14903)
*Ruixin Zhang,Jon Donnelly,Zhicheng Guo,Ghazal Khalighinejad,Haiyang Huang,Alina Jade Barnett,Cynthia Rudin*

Main category: cs.LG

TL;DR: 提出了LIT框架，通过强制LLMs使用外部可靠工具来解决复杂问题，提高解决方案的可信度和可调试性。


<details>
  <summary>Details</summary>
Motivation: LLMs在推理过程中缺乏透明度，导致在关键领域中的解决方案不可靠且难以调试，限制了其实际应用价值。

Method: 基于现有LLMs的工具调用能力，构建LIT框架，让模型选择最可靠且易于调试的解决方案路径，可能涉及多个顺序工具调用。

Result: 构建了包含1,300个问题的基准数据集和可靠性成本函数，实验表明LLMs在使用该框架后能实现更可靠和明智的问题解决，同时保持任务性能。

Conclusion: LIT框架有效提升了LLMs解决方案的可靠性和可调试性，为在关键领域应用LLMs提供了可行路径。

Abstract: Large language models (LLMs) have exhibited remarkable capabilities across various domains. The ability to call external tools further expands their capability to handle real-world tasks. However, LLMs often follow an opaque reasoning process, which limits their usefulness in high-stakes domains where solutions need to be trustworthy to end users. LLMs can choose solutions that are unreliable and difficult to troubleshoot, even if better options are available. We address this issue by forcing LLMs to use external -- more reliable -- tools to solve problems when possible. We present a framework built on the tool-calling capabilities of existing LLMs to enable them to select the most reliable and easy-to-troubleshoot solution path, which may involve multiple sequential tool calls. We refer to this framework as LIT (LLMs with Inspectable Tools). In order to support LIT, we introduce a new and challenging benchmark dataset of 1,300 questions and a customizable set of reliability cost functions associated with a collection of specialized tools. These cost functions summarize how reliable each tool is and how easy it is to troubleshoot. For instance, a calculator is reliable across domains, whereas a linear prediction model is not reliable if there is distribution shift, but it is easy to troubleshoot. A tool that constructs a random forest is neither reliable nor easy to troubleshoot. These tools interact with the Harvard USPTO Patent Dataset and a new dataset of NeurIPS 2023 papers to solve mathematical, coding, and modeling problems of varying difficulty levels. We demonstrate that LLMs can achieve more reliable and informed problem-solving while maintaining task performance using our framework.

</details>


### [17] [Integrating Causal Inference with Graph Neural Networks for Alzheimer's Disease Analysis](https://arxiv.org/abs/2511.14922)
*Pranay Kumar Peddi,Dhrubajyoti Ghosh*

Main category: cs.LG

TL;DR: 本文提出了Causal-GCN框架，通过整合do-calculus后门调整来识别对阿尔茨海默病进展具有稳定因果影响的大脑区域，解决了传统图学习方法中混杂因素的问题。


<details>
  <summary>Details</summary>
Motivation: 现有的深度图学习方法在阿尔茨海默病分类中主要基于相关性，容易混淆人口统计学和遗传因素与疾病特异性特征。需要开发能够识别稳定因果关系的模型。

Method: 使用结构连接体表示MRI数据，节点代表皮质和皮质下区域，边编码解剖连接性。通过主成分分析总结年龄、性别和APOE4基因型等混杂因素，并整合到因果调整集中。训练后通过切断传入边和改变节点特征来模拟对单个区域的干预。

Result: 在ADNI队列的484名受试者上，Causal-GCN实现了与基线GNN相当的性能，同时提供了可解释的因果效应排名，突出了后部、扣带回和岛叶枢纽，与已建立的AD神经病理学一致。

Conclusion: Causal-GCN框架能够有效识别对阿尔茨海默病进展具有因果影响的大脑区域，为理解疾病机制提供了新的视角。

Abstract: Deep graph learning has advanced Alzheimer's (AD) disease classification from MRI, but most models remain correlational, confounding demographic and genetic factors with disease specific features. We present Causal-GCN, an interventional graph convolutional framework that integrates do-calculus-based back-door adjustment to identify brain regions exerting stable causal influence on AD progression. Each subject's MRI is represented as a structural connectome where nodes denote cortical and subcortical regions and edges encode anatomical connectivity. Confounders such as age, sec, and APOE4 genotype are summarized via principal components and included in the causal adjustment set. After training, interventions on individual regions are simulated by serving their incoming edges and altering node features to estimate average causal effects on disease probability. Applied to 484 subjects from the ADNI cohort, Causal-GCN achieves performance comparable to baseline GNNs while providing interpretable causal effect rankings that highlight posterior, cingulate, and insular hubs consistent with established AD neuropathology.

</details>


### [18] [Simulated Human Learning in a Dynamic, Partially-Observed, Time-Series Environment](https://arxiv.org/abs/2511.15032)
*Jeffrey Jiang,Kevin Hong,Emily Kuczynski,Gregory Pottie*

Main category: cs.LG

TL;DR: 本文开发了一个模拟课堂环境的动态时间序列系统，结合强化学习智能辅导系统，通过探测性干预来平衡学生状态估计与教学干预的成本效益。


<details>
  <summary>Details</summary>
Motivation: 智能辅导系统虽然可以利用过往学生信息进行个性化教学，但每个新生都是独特的，且学习过程具有部分可观测性，因此需要开发能够处理这些挑战的系统。

Method: 设计动态时间序列模拟环境，开发结合强化学习的智能辅导系统，使用探测性干预来获取学生状态信息，同时比较标准RL算法与基于规则的启发式方法。

Result: RL算法与启发式方法提供不同解决方案但效果相似；探测性干预能显著提升隐藏信息情况下的性能；两种方法对人口分布变化都具灵活性，但RL策略在困难班级中表现较差；非探测策略在测验和期中考试结构中表现更好。

Conclusion: 探测性干预能有效降低学生状态估计难度，强化学习智能辅导系统在多种课程结构中都能提升学生表现，特别是在有额外信息反馈的结构中效果更佳。

Abstract: While intelligent tutoring systems (ITSs) can use information from past students to personalize instruction, each new student is unique. Moreover, the education problem is inherently difficult because the learning process is only partially observable. We therefore develop a dynamic, time-series environment to simulate a classroom setting, with student-teacher interventions - including tutoring sessions, lectures, and exams. In particular, we design the simulated environment to allow for varying levels of probing interventions that can gather more information. Then, we develop reinforcement learning ITSs that combine learning the individual state of students while pulling from population information through the use of probing interventions. These interventions can reduce the difficulty of student estimation, but also introduce a cost-benefit decision to find a balance between probing enough to get accurate estimates and probing so often that it becomes disruptive to the student. We compare the efficacy of standard RL algorithms with several greedy rules-based heuristic approaches to find that they provide different solutions, but with similar results. We also highlight the difficulty of the problem with increasing levels of hidden information, and the boost that we get if we allow for probing interventions. We show the flexibility of both heuristic and RL policies with regards to changing student population distributions, finding that both are flexible, but RL policies struggle to help harder classes. Finally, we test different course structures with non-probing policies and we find that our policies are able to boost the performance of quiz and midterm structures more than we can in a finals-only structure, highlighting the benefit of having additional information.

</details>


### [19] [How to Train Private Clinical Language Models: A Comparative Study of Privacy-Preserving Pipelines for ICD-9 Coding](https://arxiv.org/abs/2511.14936)
*Mathieu Dufour,Andrew Duncan*

Main category: cs.LG

TL;DR: 本文首次系统比较了四种用于医院出院总结自动诊断编码的训练流程，发现在中等隐私预算下，从DP训练教师模型的知识蒸馏方法优于直接DP-SGD和DP合成数据训练，能恢复63%的非私有性能。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在临床文本训练中可能暴露敏感患者信息，但差分隐私方法通常会严重降低诊断准确性。目前尚不清楚哪种隐私保护策略在临床语言任务中效果最佳。

Method: 使用相同的1B参数模型和匹配的隐私预算，系统比较四种训练流程：知识蒸馏、直接DP-SGD、DP合成数据训练等，用于预测ICD-9编码。

Result: 在中等隐私预算下，知识蒸馏方法表现最佳，能恢复高达63%的非私有性能，同时保持强大的经验隐私保护（成员推断AUC≈0.5）。

Conclusion: 不同架构在隐私-效用权衡上存在显著差异，知识蒸馏被确定为保护隐私的临床NLP最实用路径。

Abstract: Large language models trained on clinical text risk exposing sensitive patient information, yet differential privacy (DP) methods often severely degrade the diagnostic accuracy needed for deployment. Despite rapid progress in DP optimisation and text generation, it remains unclear which privacy-preserving strategy actually works best for clinical language tasks. We present the first systematic head-to-head comparison of four training pipelines for automated diagnostic coding from hospital discharge summaries. All pipelines use identical 1B-parameter models and matched privacy budgets to predict ICD-9 codes. At moderate and relaxed privacy budgets ($\varepsilon \in \{4, 6\}$), knowledge distillation from DP-trained teachers outperforms both direct DP-SGD and DP-synthetic data training, recovering up to 63\% of the non-private performance whilst maintaining strong empirical privacy (membership-inference AUC $\approx$ 0.5). These findings expose large differences in the privacy-utility trade-off across architectures and identify knowledge distillation as the most practical route to privacy-preserving clinical NLP.

</details>


### [20] [Cross-Modal Consistency-Guided Active Learning for Affective BCI Systems](https://arxiv.org/abs/2511.15138)
*Hyo-Jeong Jang,Hye-Bin Shin,Kang Yin*

Main category: cs.LG

TL;DR: 本文提出了一种不确定性感知的主动学习框架，通过结合模型不确定性和跨模态一致性来增强对标签噪声的鲁棒性，用于EEG情感识别。


<details>
  <summary>Details</summary>
Motivation: EEG信号易受伪影和个体差异影响，情感标签通常来自主观且不一致的报告，使得稳健的情感解码特别困难。需要一种能够处理标签噪声的数据高效方法。

Method: 使用不确定性感知主动学习框架，通过跨模态对齐评估不确定性的来源（认知模糊或传感器噪声），并选择性查询oracle反馈来减少噪声标签的影响。

Result: 在ASCERTAIN数据集上的实验验证了该方法的效率和鲁棒性，表明其作为脑机接口系统中EEG情感解码的数据高效和噪声容忍方法的潜力。

Conclusion: 该框架通过结合模型不确定性和跨模态一致性，有效提高了EEG情感识别的鲁棒性和数据效率，为处理标签噪声问题提供了有效解决方案。

Abstract: Deep learning models perform best with abundant, high-quality labels, yet such conditions are rarely achievable in EEG-based emotion recognition. Electroencephalogram (EEG) signals are easily corrupted by artifacts and individual variability, while emotional labels often stem from subjective and inconsistent reports-making robust affective decoding particularly difficult. We propose an uncertainty-aware active learning framework that enhances robustness to label noise by jointly leveraging model uncertainty and cross-modal consistency. Instead of relying solely on EEG-based uncertainty estimates, the method evaluates cross-modal alignment to determine whether uncertainty originates from cognitive ambiguity or sensor noise. A representation alignment module embeds EEG and face features into a shared latent space, enforcing semantic coherence between modalities. Residual discrepancies are treated as noise-induced inconsistencies, and these samples are selectively queried for oracle feedback during active learning. This feedback-driven process guides the network toward reliable, informative samples and reduces the impact of noisy labels. Experiments on the ASCERTAIN dataset examine the efficiency and robustness of ours, highlighting its potential as a data-efficient and noise-tolerant approach for EEG-based affective decoding in brain-computer interface systems.

</details>


### [21] [Knowledge Graphs as Structured Memory for Embedding Spaces: From Training Clusters to Explainable Inference](https://arxiv.org/abs/2511.14961)
*Artur A. Oliveira,Mateus Espadoto,Roberto M. Cesar,Roberto Hirata*

Main category: cs.LG

TL;DR: Graph Memory (GM) 是一个结构化非参数框架，通过区域级原型的关系记忆增强基于嵌入的推理，将嵌入空间总结为带有可靠性指标的原型节点，并通过编码几何和上下文关系的边连接。


<details>
  <summary>Details</summary>
Motivation: 传统方法孤立处理每个训练实例，GM旨在通过结构化记忆统一实例检索、基于原型的推理和图标签传播，提供更高效的推理和可解释性。

Method: GM将嵌入空间总结为原型节点，标注可靠性指标，并通过边编码几何和上下文关系，支持实例检索、原型推理和图标签传播。

Result: 在合成和真实数据集（包括乳腺癌组织病理学IDC）上的实验表明，GM在准确率上与kNN和Label Spreading竞争，同时提供更好的校准和更平滑的决策边界，且样本量少一个数量级。

Conclusion: GM通过显式建模可靠性和关系结构，为非参数学习中局部证据和全局一致性提供了原则性桥梁。

Abstract: We introduce Graph Memory (GM), a structured non-parametric framework that augments embedding-based inference with a compact, relational memory over region-level prototypes. Rather than treating each training instance in isolation, GM summarizes the embedding space into prototype nodes annotated with reliability indicators and connected by edges that encode geometric and contextual relations. This design unifies instance retrieval, prototype-based reasoning, and graph-based label propagation within a single inductive model that supports both efficient inference and faithful explanation. Experiments on synthetic and real datasets including breast histopathology (IDC) show that GM achieves accuracy competitive with $k$NN and Label Spreading while offering substantially better calibration and smoother decision boundaries, all with an order of magnitude fewer samples. By explicitly modeling reliability and relational structure, GM provides a principled bridge between local evidence and global consistency in non-parametric learning.

</details>


### [22] [Interpretable temporal fusion network of multi- and multi-class arrhythmia classification](https://arxiv.org/abs/2511.15062)
*Yun Kwan Kim*

Main category: cs.LG

TL;DR: 提出了一种用于心律失常检测和分类的临床决策支持系统框架，通过局部和全局信息提取与融合，解决了心律失常长度变化的问题，在MIT-BIH和AFDB数据库上取得了优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有的心律失常分类方法未能充分考虑心律失常发作时间变化和长度不一的挑战，需要开发能够处理不同长度心律失常并精确定位发作时间的系统。

Method: 提出包含局部和全局特征提取以及基于注意力的局部-全局信息融合的框架，能够在受限输入长度内进行心律失常检测和分类。

Result: 在MITDB和AFDB数据库上，持续时间、发作和Dice分数的F1得分分别为96.45%、82.05%、96.31%和97.57%、98.31%、97.45%，性能显著优于基准模型。

Conclusion: 该方法能有效捕获局部和全局信息及动态特征，无显著信息损失，可更准确地检测心律失常并精确定位发作时间，有助于制定更精确的治疗方案。

Abstract: Clinical decision support systems (CDSSs) have been widely utilized to support the decisions made by cardiologists when detecting and classifying arrhythmia from electrocardiograms. However, forming a CDSS for the arrhythmia classification task is challenging due to the varying lengths of arrhythmias. Although the onset time of arrhythmia varies, previously developed methods have not considered such conditions. Thus, we propose a framework that consists of (i) local and global extraction and (ii) local-global information fusion with attention to enable arrhythmia detection and classification within a constrained input length. The framework's performance was evaluated in terms of 10-class and 4-class arrhythmia detection, focusing on identifying the onset and ending point of arrhythmia episodes and their duration using the MIT-BIH arrhythmia database (MITDB) and the MIT-BIH atrial fibrillation database (AFDB). Duration, episode, and Dice score performances resulted in overall F1-scores of 96.45%, 82.05%, and 96.31% on the MITDB and 97.57%, 98.31%, and 97.45% on the AFDB, respectively. The results demonstrated statistically superior performance compared to those of the benchmark models. To assess the generalization capability of the proposed method, an MITDB-trained model and MIT-BIH malignant ventricular arrhythmia database-trained model were tested AFDB and MITDB, respectively. Superior performance was attained compared with that of a state-of-the-art model. The proposed method effectively captures both local and global information and dynamics without significant information loss. Consequently, arrhythmias can be detected with greater accuracy, and their occurrence times can be precisely determined, enabling the clinical field to develop more accurate treatment plans based on the proposed method.

</details>


### [23] [Novel sparse matrix algorithm expands the feasible size of a self-organizing map of the knowledge indexed by a database of peer-reviewed medical literature](https://arxiv.org/abs/2511.15136)
*Andrew Amos,Joanne Lee,Tarun Sen Gupta,Bunmi S. Malau-Aduli*

Main category: cs.LG

TL;DR: 开发了一种新的稀疏矩阵乘法算法，使得能够对整个Medline数据集应用自组织映射，从而更完整地映射现有医学知识。


<details>
  <summary>Details</summary>
Motivation: 过去由于现有算法的内存和处理需求呈指数增长，Medline数据库的映射工作仅限于可用数据的小子集。

Method: 设计了一种新颖的稀疏矩阵乘法算法，并将其应用于整个Medline数据集的自组织映射。

Result: 实现了对整个Medline数据集的完整映射，提高了随时间更新自组织映射的可行性。

Conclusion: 该算法使得能够更全面地映射医学知识，并为随时间变化的数据集更新提供了更可行的解决方案。

Abstract: Past efforts to map the Medline database have been limited to small subsets of the available data because of the exponentially increasing memory and processing demands of existing algorithms. We designed a novel algorithm for sparse matrix multiplication that allowed us to apply a self-organizing map to the entire Medline dataset, allowing for a more complete map of existing medical knowledge. The algorithm also increases the feasibility of refining the self-organizing map to account for changes in the dataset over time.

</details>


### [24] [From Solving to Verifying: A Unified Objective for Robust Reasoning in LLMs](https://arxiv.org/abs/2511.15137)
*Xiaoxuan Wang,Bo Liu,Song Jiang,Jingzhou Liu,Jingyuan Qi,Xia Chen,Baosheng He*

Main category: cs.LG

TL;DR: GRPO-Verif算法通过统一损失函数联合优化LLMs的解决方案生成和自我验证能力，使用可调超参数控制验证信号权重，在保持推理性能的同时提升自我验证能力。


<details>
  <summary>Details</summary>
Motivation: 尽管通过强化学习显著提升了推理能力，但LLMs仍然难以一致地验证自己的推理轨迹，需要研究如何增强LLMs的自我验证能力以及这种能力是否能进一步改善推理性能。

Method: 提出GRPO-Verif算法，在统一损失函数中联合优化解决方案生成和自我验证，使用可调超参数控制验证信号的权重。

Result: 实验结果表明，该方法在保持推理性能的同时增强了自我验证能力。

Conclusion: GRPO-Verif算法成功提升了LLMs的自我验证能力，同时维持了推理性能，为解决LLMs自我验证不一致的问题提供了有效方案。

Abstract: The reasoning capabilities of large language models (LLMs) have been significantly improved through reinforcement learning (RL). Nevertheless, LLMs still struggle to consistently verify their own reasoning traces. This raises the research question of how to enhance the self-verification ability of LLMs and whether such an ability can further improve reasoning performance. In this work, we propose GRPO-Verif, an algorithm that jointly optimizes solution generation and self-verification within a unified loss function, with an adjustable hyperparameter controlling the weight of the verification signal. Experimental results demonstrate that our method enhances self-verification capability while maintaining comparable performance in reasoning.

</details>


### [25] [Complex variational autoencoders admit Kähler structure](https://arxiv.org/abs/2511.15172)
*Andrew Gracyk*

Main category: cs.LG

TL;DR: 该论文研究了复杂变分自编码器（VAE）中的Kähler几何结构，提出了一种基于复杂高斯混合的Kähler势导数方法，能够高效计算Fisher信息度量，并通过解码器几何正则化潜在空间，获得更平滑的表示和更少的语义异常值。


<details>
  <summary>Details</summary>
Motivation: 研究复杂VAE中存在的Kähler几何结构，旨在开发更高效的度量计算方法，减轻大规模自动微分计算负担，并通过几何正则化改善潜在空间表示质量。

Method: 针对复杂VAE的解码器几何，推导了复杂高斯正则化下的Fisher信息度量，提出Kähler势导数方法替代传统计算，利用复数高斯混合的Kähler势作为PSH函数进行高效计算。

Result: 提出的Kähler势导数方法在保持Kähler几何忠实性的同时，与Fisher信息度量具有近似等价性，计算效率高，能够通过解码器几何正则化获得更平滑的表示和更少的语义异常值。

Conclusion: 复杂VAE确实展现Kähler几何结构，提出的Kähler势导数方法有效解决了计算效率问题，通过几何正则化策略改善了潜在空间表示质量，为复杂VAE的几何分析提供了新途径。

Abstract: It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian regularization with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that has rough equivalence to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.

</details>


### [26] [FaultDiffusion: Few-Shot Fault Time Series Generation with Diffusion Model](https://arxiv.org/abs/2511.15174)
*Yi Xu,Zhigang Chen,Rui Wang,Yangfan Li,Fengxiao Tang,Ming Zhao,Jiaqi Liu*

Main category: cs.LG

TL;DR: 提出了一种基于扩散模型的少样本故障时间序列生成框架，通过正负差异适配器和多样性损失来解决工业设备监控中故障数据稀缺的问题。


<details>
  <summary>Details</summary>
Motivation: 工业设备监控中故障诊断至关重要，但由于故障事件罕见和数据标注成本高，故障数据稀缺严重阻碍了数据驱动方法的发展。现有时间序列生成模型在少样本场景下难以捕捉故障分布，生成的样本缺乏真实性和多样性。

Method: 采用扩散模型框架，引入正负差异适配器，利用预训练的正常数据分布来建模正常与故障域之间的差异，实现准确的故障合成。同时引入多样性损失，通过样本间差异正则化防止模式崩溃，鼓励生成多样化的故障样本。

Result: 实验结果表明，该模型在真实性和多样性方面显著优于传统方法，在关键基准测试中达到了最先进的性能。

Conclusion: 所提出的基于扩散模型的少样本故障时间序列生成框架有效解决了故障数据稀缺问题，能够生成真实且多样的故障样本，为工业设备故障诊断提供了有力支持。

Abstract: In industrial equipment monitoring, fault diagnosis is critical for ensuring system reliability and enabling predictive maintenance. However, the scarcity of fault data, due to the rarity of fault events and the high cost of data annotation, significantly hinders data-driven approaches. Existing time-series generation models, optimized for abundant normal data, struggle to capture fault distributions in few-shot scenarios, producing samples that lack authenticity and diversity due to the large domain gap and high intra-class variability of faults. To address this, we propose a novel few-shot fault time-series generation framework based on diffusion models. Our approach employs a positive-negative difference adapter, leveraging pre-trained normal data distributions to model the discrepancies between normal and fault domains for accurate fault synthesis. Additionally, a diversity loss is introduced to prevent mode collapse, encouraging the generation of diverse fault samples through inter-sample difference regularization. Experimental results demonstrate that our model significantly outperforms traditional methods in authenticity and diversity, achieving state-of-the-art performance on key benchmarks.

</details>


### [27] [Masked Auto-Regressive Variational Acceleration: Fast Inference Makes Practical Reinforcement Learning](https://arxiv.org/abs/2511.15190)
*Yuxuan Gu,Weimin Bai,Yifei Wang,Weijian Luo,He Sun*

Main category: cs.LG

TL;DR: MARVAL是一个基于蒸馏的框架，将掩码自回归扩散模型压缩为单步生成，实现30倍加速并保持样本质量，同时支持强化学习后训练。


<details>
  <summary>Details</summary>
Motivation: 解决传统掩码自回归扩散模型推理速度慢的问题，其分层推理机制（外层AR解掩码循环和内层扩散去噪链）不仅影响生成效率，还阻碍了强化学习后训练的实际应用。

Method: 提出MARVAL框架，使用基于分数的变分目标将掩码自回归扩散模型蒸馏为单步生成，同时保持灵活的自回归解掩码顺序；并开发MARVAL-RL实现高效的强化学习后训练。

Result: 在ImageNet 256*256上，MARVAL-Huge达到FID 2.00，相比MAR-diffusion实现30倍以上加速；MARVAL-RL在ImageNet数据集上持续提升CLIP和图像奖励分数。

Conclusion: MARVAL展示了掩码自回归扩散模型蒸馏和强化学习的首个实用路径，实现了快速采样和更好的偏好对齐。

Abstract: Masked auto-regressive diffusion models (MAR) benefit from the expressive modeling ability of diffusion models and the flexibility of masked auto-regressive ordering. However, vanilla MAR suffers from slow inference due to its hierarchical inference mechanism: an outer AR unmasking loop and an inner diffusion denoising chain. Such decoupled structure not only harm the generation efficiency but also hinder the practical use of MAR for reinforcement learning (RL), an increasingly critical paradigm for generative model post-training.To address this fundamental issue, we introduce MARVAL (Masked Auto-regressive Variational Acceleration), a distillation-based framework that compresses the diffusion chain into a single AR generation step while preserving the flexible auto-regressive unmasking order. Such a distillation with MARVAL not only yields substantial inference acceleration but, crucially, makes RL post-training with verifiable rewards practical, resulting in scalable yet human-preferred fast generative models. Our contributions are twofold: (1) a novel score-based variational objective for distilling masked auto-regressive diffusion models into a single generation step without sacrificing sample quality; and (2) an efficient RL framework for masked auto-regressive models via MARVAL-RL. On ImageNet 256*256, MARVAL-Huge achieves an FID of 2.00 with more than 30 times speedup compared with MAR-diffusion, and MARVAL-RL yields consistent improvements in CLIP and image-reward scores on ImageNet datasets with entity names. In conclusion, MARVAL demonstrates the first practical path to distillation and RL of masked auto-regressive diffusion models, enabling fast sampling and better preference alignments.

</details>


### [28] [Reasoning in Diffusion Large Language Models is Concentrated in Dynamic Confusion Zones](https://arxiv.org/abs/2511.15208)
*Ranfei Chen,Ming Chen,Kaifei Wang*

Main category: cs.LG

TL;DR: 论文提出ATPO方法，通过分析扩散大语言模型推理轨迹中的不确定性模式，动态选择关键步骤进行梯度更新，显著提升推理准确性和训练稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有基于轨迹的强化学习方法均匀分配策略梯度，隐含假设所有去噪步骤同等重要。作者质疑这一假设，发现推理轨迹中存在结构化的"混淆区域"，这些区域强烈预测最终成功或失败。

Method: 提出自适应轨迹策略优化（ATPO），使用混合RoEC+CM规则动态识别高影响力步骤，重新分配梯度更新到这些关键步骤，而不改变RL目标、奖励或计算预算。

Result: ATPO在多个基准测试中显著提升了推理准确性和训练稳定性，证明了利用轨迹动态特性对于推进dLLM RL的重要性。

Conclusion: 通过动态识别和聚焦推理轨迹中的关键步骤，ATPO方法有效提升了扩散大语言模型的强化学习性能，表明轨迹动态分析是推进dLLM RL的关键。

Abstract: Diffusion Large Language Models (dLLMs) are rapidly emerging alongside autoregressive models as a powerful paradigm for complex reasoning, with reinforcement learning increasingly used for downstream alignment. Existing trajectory-based RL methods uniformly allocate policy gradients across denoising steps, implicitly treating all steps as equally important. We challenge this assumption by analyzing trajectories with several step-level metrics: entropy-based uncertainty, Confidence-Margin (CM) uncertainty, and Rate of Entropy Change (RoEC). These reveal structured "zones of confusion": transient spikes in uncertainty and instability that strongly predict final success or failure, while most steps remain stable. We propose Adaptive Trajectory Policy Optimization (ATPO), a lightweight step-selection strategy that dynamically reallocates gradient updates to these high-leverage steps without changing the RL objective, rewards, or compute budget. Using a hybrid RoEC+CM rule, ATPO delivers substantial gains in reasoning accuracy and training stability across benchmarks, showing that exploiting trajectory dynamics is key to advancing dLLM RL.

</details>


### [29] [EntroPIC: Towards Stable Long-Term Training of LLMs via Entropy Stabilization with Proportional-Integral Control](https://arxiv.org/abs/2511.15248)
*Kai Yang,Xin Xu,Yangkun Chen,Weijie Liu,Jiafei Lyu,Zichuan Lin,Deheng Ye,Saiyong Yang*

Main category: cs.LG

TL;DR: 提出了EntroPIC方法，通过比例积分控制动态调整正负样本的损失系数来稳定熵值，确保大语言模型在长期训练中保持稳定探索。


<details>
  <summary>Details</summary>
Motivation: 现有强化学习方法难以维持适当的熵水平，因为训练过程涉及正负样本混合，在不同步骤中对熵产生不同影响，导致模型可能陷入次优行为。

Method: EntroPIC方法通过比例积分控制机制自适应调整正负样本的影响，动态调节它们的损失系数，从而在整个训练过程中稳定熵值。

Result: 实验结果表明该方法能成功维持期望的熵水平，为大语言模型提供稳定且最优的强化学习训练。

Conclusion: EntroPIC方法在理论和实验上都证明能有效控制大规模语言模型训练中的熵，确保稳定的探索和训练进展。

Abstract: Long-term training of large language models (LLMs) requires maintaining stable exploration to prevent the model from collapsing into sub-optimal behaviors. Entropy is crucial in this context, as it controls exploration and helps avoid premature convergence to sub-optimal solutions. However, existing reinforcement learning methods struggle to maintain an appropriate level of entropy, as the training process involves a mix of positive and negative samples, each affecting entropy in different ways across steps. To address this, we propose Entropy stablilization via Proportional-Integral Control (EntroPIC), a novel method that adaptively adjusts the influence of positive and negative samples by dynamically tuning their loss coefficients. This approach stabilizes entropy throughout training, ensuring efficient exploration and steady progress. We provide a comprehensive theoretical analysis for both on-policy and off-policy learning settings, demonstrating that EntroPIC is effective at controlling entropy in large-scale LLM training. Experimental results show that our method successfully maintains desired entropy levels, enabling stable and optimal RL training for LLMs.

</details>


### [30] [GRPO-RM: Fine-Tuning Representation Models via GRPO-Driven Reinforcement Learning](https://arxiv.org/abs/2511.15256)
*Yanchen Xu,Ziheng Jiao,Hongyuan Zhang,Xuelong Li*

Main category: cs.LG

TL;DR: 本文提出了GRPO-RM方法，将GRPO强化学习方法从大语言模型推广到表示学习模型，通过预定义输出集和专门设计的奖励函数来优化表示模型。


<details>
  <summary>Details</summary>
Motivation: GRPO方法在大语言模型微调中表现出色，但能否推广到表示学习模型尚不明确。本文旨在探索GRPO类策略在表示模型后训练中的性能。

Method: 提出GRPO-RM方法，建立预定义输出集替代LLM中的token序列采样，生成输出组用于GRPO的概率驱动优化，并设计专门的奖励函数适应表示模型特性。

Result: 在多个真实世界数据集上进行了广泛实验，验证了所提方法的有效性。

Conclusion: GRPO-RM成功将GRPO方法扩展到表示学习模型，为表示模型的后训练优化提供了有效解决方案。

Abstract: The Group Relative Policy Optimization (GRPO), a reinforcement learning method used to fine-tune large language models (LLMs), has proved its effectiveness in practical applications such as DeepSeek-R1. It raises a question whether GRPO can be generalized to representation learning models. In this paper, we propose Group Relative Policy Optimization for Representation Model (GRPO-RM), and investigate the performance of GRPO-like policy in post-training representation models. Specifically, our method establishes a predefined output set to functionally replace token sequence sampling in LLMs, thereby generating an output group, which is essential for the probability-driven optimization of GRPO. In addition, a specialized reward function is designed to accommodate the properties of representation models. Extensive experiments are conducted on various real-world datasets to validate the effectiveness of our proposed method.

</details>


### [31] [SNAP: Low-Latency Test-Time Adaptation with Sparse Updates](https://arxiv.org/abs/2511.15276)
*Hyeongheon Cha,Dong Min Kim,Hye Won Chung,Taesik Gong,Sung-Ju Lee*

Main category: cs.LG

TL;DR: SNAP是一个稀疏测试时适应框架，通过减少适应频率和数据使用来降低计算成本，同时保持准确性。它使用类域代表记忆和仅推理批量感知内存归一化技术，在仅使用1%数据流的情况下仍能保持竞争力。


<details>
  <summary>Details</summary>
Motivation: 现有测试时适应方法需要频繁适应和高计算成本，不适合资源受限的边缘环境。需要开发更高效的TTA方法。

Method: 提出SNAP框架，包含两个关键组件：类域代表记忆(CnDRM)用于识别和存储少量代表性样本；仅推理批量感知内存归一化(IoBMN)用于动态调整归一化统计量。

Result: 与五种最先进TTA算法集成，SNAP将延迟降低高达93.12%，同时准确率下降保持在3.3%以下，适应率范围从1%到50%。

Conclusion: SNAP在边缘设备上具有强大的实际应用潜力，特别适合延迟敏感的应用场景。

Abstract: Test-Time Adaptation (TTA) adjusts models using unlabeled test data to handle dynamic distribution shifts. However, existing methods rely on frequent adaptation and high computational cost, making them unsuitable for resource-constrained edge environments. To address this, we propose SNAP, a sparse TTA framework that reduces adaptation frequency and data usage while preserving accuracy. SNAP maintains competitive accuracy even when adapting based on only 1% of the incoming data stream, demonstrating its robustness under infrequent updates. Our method introduces two key components: (i) Class and Domain Representative Memory (CnDRM), which identifies and stores a small set of samples that are representative of both class and domain characteristics to support efficient adaptation with limited data; and (ii) Inference-only Batch-aware Memory Normalization (IoBMN), which dynamically adjusts normalization statistics at inference time by leveraging these representative samples, enabling efficient alignment to shifting target domains. Integrated with five state-of-the-art TTA algorithms, SNAP reduces latency by up to 93.12%, while keeping the accuracy drop below 3.3%, even across adaptation rates ranging from 1% to 50%. This demonstrates its strong potential for practical use on edge devices serving latency-sensitive applications. The source code is available at https://github.com/chahh9808/SNAP.

</details>


### [32] [Quant-Trim in Practice: Improved Cross-Platform Low-Bit Deployment on Edge NPUs](https://arxiv.org/abs/2511.15300)
*Rayen Dhahri,Steffen Urban*

Main category: cs.LG

TL;DR: Quant-Trim是一种训练阶段方法，通过渐进式伪量化和反向剪枝技术，生成硬件中立的检查点，能够在不同后端和精度选择下保持一致的准确性。


<details>
  <summary>Details</summary>
Motivation: 解决边缘加速器中低比特量化在不同厂商编译器间的不一致性问题，避免因编译器差异导致的精度波动和需要针对不同后端进行模型调整的问题。

Method: 结合渐进式伪量化来对齐训练与部署的整数网格，以及反向剪枝来控制异常值驱动的尺度膨胀，同时保持可学习性。该方法与量化方案无关，无需特定厂商的图修改。

Result: 在多种模型和任务中，Quant-Trim缩小了浮点与低比特之间的差距，减少了对编译器启发式/校准的依赖，避免了针对每个后端的重新训练。

Conclusion: Quant-Trim提供了一种有效的硬件中立量化解决方案，能够在不同边缘加速器后端上保持一致的性能表现。

Abstract: Specialized edge accelerators rely on low-bit quantization, but vendor compilers differ in scaling, clipping, and kernel support, often as black boxes. The same floating-point (FP) checkpoint can therefore yield inconsistent accuracy across backends, forcing practitioners to tweak flags or refactor models to vendor-friendly operator subsets. We introduce Quant-Trim, a training-phase method that produces a hardware-neutral checkpoint robust to backend and precision choices. It combines progressive fake quantization to align training with the deployed integer grid and reverse pruning to tame outlier-driven scale inflation while preserving learnability. Quant-Trim is agnostic to quantization schemes (symmetric/asymmetric,per-tensor/per-channel, INT8/INT4) and requires no vendor-specific graph changes.Across models and tasks, it narrows the FP,low-bit gap, reduces dependence on compiler heuristics/calibration, and avoids per-backend retraining. We report accuracy and edge metrics latency, throughput, energy/inference, and cost under static/dynamic activation scaling and varying operator coverage.

</details>


### [33] [On the Internal Semantics of Time-Series Foundation Models](https://arxiv.org/abs/2511.15324)
*Atharva Pandey,Abhilash Neog,Gautam Jajoo*

Main category: cs.LG

TL;DR: 本文系统研究了时间序列基础模型（TSFMs）中概念的可解释性，分析了不同层编码的概念类型、概念的线性可恢复性、表示在模型深度中的演变以及模型如何处理概念组合。


<details>
  <summary>Details</summary>
Motivation: 尽管时间序列基础模型在实证上取得了成功，但其内部机制如何表示基本时间序列概念仍不清楚。本文旨在系统理解TSFMs的概念可解释性。

Method: 使用分层分析、线性可恢复性测试和表示相似性度量等方法，系统探究了概念在模型中的编码位置、可恢复性、演变过程以及组合处理能力。

Result: 研究发现：早期层主要捕获局部时域模式（如AR(1)、水平偏移、趋势），深层编码离散度和变化时间信号，频谱和扭曲因子最难线性恢复。在组合设置中，探针性能下降，显示概念间存在干扰。

Conclusion: 虽然原子概念可靠地定位，但组合仍然是当前TSFMs的一个关键限制，突显了其在表示交互时间现象能力上的不足。

Abstract: Time-series Foundation Models (TSFMs) have recently emerged as a universal paradigm for learning across diverse temporal domains. However, despite their empirical success, the internal mechanisms by which these models represent fundamental time-series concepts remain poorly understood. In this work, we undertake a systematic investigation of concept interpretability in TSFMs. Specifically, we examine: (i) which layers encode which concepts, (ii) whether concept parameters are linearly recoverable, (iii) how representations evolve in terms of concept disentanglement and abstraction across model depth, and (iv) how models process compositions of concepts. We systematically probe these questions using layer-wise analyses, linear recoverability tests, and representation similarity measures, providing a structured account of TSFM semantics. The resulting insights show that early layers mainly capture local, time-domain patterns (e.g., AR(1), level shifts, trends), while deeper layers encode dispersion and change-time signals, with spectral and warping factors remaining the hardest to recover linearly. In compositional settings, however, probe performance degrades, revealing interference between concepts. This highlights that while atomic concepts are reliably localized, composition remains a challenge, underscoring a key limitation in current TSFMs' ability to represent interacting temporal phenomena.

</details>


### [34] [Parameter Importance-Driven Continual Learning for Foundation Models](https://arxiv.org/abs/2511.15375)
*Lingxiang Wang,Hainan Zhang,Zhiming Zheng*

Main category: cs.LG

TL;DR: PIECE是一种基于参数重要性估计的持续增强方法，通过仅更新0.1%与任务最相关的核心参数，在保持通用能力的同时高效学习领域知识，无需访问历史数据或增加模型参数。


<details>
  <summary>Details</summary>
Motivation: 解决领域特定后训练导致的灾难性遗忘问题，在保持基础模型通用推理能力的同时获取下游领域知识，这是大型语言和多模态模型面临的核心挑战。

Method: 引入PIECE方法，使用两种重要性估计器（基于Fisher信息的PIECE-F和结合梯度与曲率信息的二阶归一化PIECE-S）指导，仅选择性更新0.1%与任务最相关的核心参数。

Result: 在三个语言模型和两个多模态模型上的实验表明，PIECE能保持通用能力，并在多样化下游任务上实现最先进的持续学习性能。

Conclusion: PIECE为无需灾难性遗忘的可扩展、领域自适应基础模型提供了一条实用路径。

Abstract: Domain-specific post-training often causes catastrophic forgetting, making foundation models lose their general reasoning ability and limiting their adaptability to dynamic real-world environments. Preserving general capabilities while acquiring downstream domain knowledge is a central challenge for large language and multimodal models. Traditional continual learning methods, such as regularization, replay and architectural isolation, suffer from poor downstream performance, reliance on inaccessible historical data, or additional parameter overhead. While recent parameter-efficient tuning (PET) methods can alleviate forgetting, their effectiveness strongly depends on the choice of parameters and update strategies. In this paper, we introduce PIECE, a Parameter Importance Estimation-based Continual Enhancement method that preserves general ability while efficiently learning domain knowledge without accessing prior training data or increasing model parameters. PIECE selectively updates only 0.1% of core parameters most relevant to new tasks, guided by two importance estimators: PIECE-F based on Fisher Information, and PIECE-S based on a second-order normalization that combines gradient and curvature information. Experiments across three language models and two multimodal models show that PIECE maintains general capabilities and achieves state-of-the-art continual learning performance across diverse downstream tasks. Our results highlight a practical path to scalable, domain-adaptive foundation models without catastrophic forgetting.

</details>


### [35] [Multi-layer Stack Ensembles for Time Series Forecasting](https://arxiv.org/abs/2511.15350)
*Nathanael Bosch,Oleksandr Shchur,Nick Erickson,Michael Bohlke-Schneider,Caner Türkmen*

Main category: cs.LG

TL;DR: 本文系统评估了33种时间序列预测的集成方法，发现stacking能持续提升精度，但没有单一stacker在所有任务中表现最佳。为此提出了多层stacking框架，在多样化预测场景中提供更优精度。


<details>
  <summary>Details</summary>
Motivation: 在时间序列预测领域，集成方法应用不足，简单的线性组合仍被认为是state-of-the-art，需要系统探索更有效的集成策略。

Method: 评估33种集成模型（包括现有和新颖方法），在50个真实世界数据集上进行测试，并提出多层stacking框架来结合不同stacker模型的优势。

Result: stacking方法能持续提升预测精度，但无单一stacker在所有任务中表现最佳；多层stacking框架在多样化预测场景中提供更优精度。

Conclusion: 基于stacking的方法有潜力改进时间序列预测的AutoML系统，多层stacking框架是有效的解决方案。

Abstract: Ensembling is a powerful technique for improving the accuracy of machine learning models, with methods like stacking achieving strong results in tabular tasks. In time series forecasting, however, ensemble methods remain underutilized, with simple linear combinations still considered state-of-the-art. In this paper, we systematically explore ensembling strategies for time series forecasting. We evaluate 33 ensemble models -- both existing and novel -- across 50 real-world datasets. Our results show that stacking consistently improves accuracy, though no single stacker performs best across all tasks. To address this, we propose a multi-layer stacking framework for time series forecasting, an approach that combines the strengths of different stacker models. We demonstrate that this method consistently provides superior accuracy across diverse forecasting scenarios. Our findings highlight the potential of stacking-based methods to improve AutoML systems for time series forecasting.

</details>


### [36] [Towards Understanding Layer Contributions in Tabular In-Context Learning Models](https://arxiv.org/abs/2511.15432)
*Amir Rezaei Balef,Mykhailo Koshil,Katharina Eggensperger*

Main category: cs.LG

TL;DR: 研究分析了表格上下文学习模型中各层的贡献，发现只有部分层共享共同的表示语言，表明存在结构冗余，为模型压缩和可解释性提供了机会。


<details>
  <summary>Details</summary>
Motivation: 尽管表格上下文学习模型与大型语言模型在架构上相似，但个体层在表格预测中的贡献尚不清楚，需要研究各层潜在空间的演化、识别冗余层，并与LLMs中的动态进行比较。

Method: 通过"层作为画家"的视角分析TabPFN和TabICL模型，研究各层潜在空间的演化过程。

Result: 发现只有部分层共享共同的表示语言，表明存在结构冗余，这为模型压缩和可解释性提供了机会。

Conclusion: 表格ICL模型存在层间冗余，可以通过识别和利用这些冗余来实现模型压缩并提高可解释性。

Abstract: Despite the architectural similarities between tabular in-context learning (ICL) models and large language models (LLMs), little is known about how individual layers contribute to tabular prediction. In this paper, we investigate how the latent spaces evolve across layers in tabular ICL models, identify potential redundant layers, and compare these dynamics with those observed in LLMs. We analyze TabPFN and TabICL through the "layers as painters" perspective, finding that only subsets of layers share a common representational language, suggesting structural redundancy and offering opportunities for model compression and improved interpretability.

</details>


### [37] [TSFM in-context learning for time-series classification of bearing-health status](https://arxiv.org/abs/2511.15447)
*Michel Tokic,Slobodan Djukanović,Anja von Beuningen,Cheng Feng*

Main category: cs.LG

TL;DR: 本文提出了一种在时间序列基础模型中使用上下文学习的分类方法，无需微调模型即可对训练数据之外的数据进行分类，应用于轴承健康状态评估。


<details>
  <summary>Details</summary>
Motivation: 旨在开发一种无需微调预训练模型就能对新数据进行分类的方法，推动从定制化窄AI解决方案向更广泛的AI驱动维护系统发展。

Method: 将示例以目标（类别ID）和协变量（数据矩阵）形式表示在模型提示中，通过上下文学习沿预测轴对未知协变量数据模式进行分类，将频域参考信号转换为伪时间序列模式。

Result: 该方法在不同操作条件下均表现出有效性，能够预测分类数据与预定义标签对应的概率。

Conclusion: 该方法标志着在时间序列基础模型分类应用方面的重要进展，为更广泛的AI驱动维护系统奠定了基础。

Abstract: This paper introduces a classification method using in-context learning in time-series foundation models (TSFM). We show how data, which was not part of the TSFM training data corpus, can be classified without the need of finetuning the model. Examples are represented in the form of targets (class id) and covariates (data matrix) within the prompt of the model, which enables to classify an unknown covariate data pattern alongside the forecast axis through in-context learning. We apply this method to vibration data for assessing the health state of a bearing within a servo-press motor. The method transforms frequency domain reference signals into pseudo time-series patterns, generates aligned covariate and target signals, and uses the TSFM to predict probabilities how classified data corresponds to predefined labels. Leveraging the scalability of pre-trained models this method demonstrates efficacy across varied operational conditions. This marks significant progress beyond custom narrow AI solutions towards broader, AI-driven maintenance systems.

</details>


### [38] [CID: Measuring Feature Importance Through Counterfactual Distributions](https://arxiv.org/abs/2511.15371)
*Eddie Conti,Álvaro Parafita,Axel Brando*

Main category: cs.LG

TL;DR: 本文提出了一种新的局部特征重要性方法CID，通过生成正负反事实样本、使用核密度估计建模分布，并基于分布差异度量来排名特征重要性，该方法在忠实性指标上表现优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏明确的真实基准来比较特征重要性方法，需要开发基于良好理论基础的可替代度量方法，以更好地理解机器学习模型的决策过程。

Method: 提出反事实重要性分布(CID)方法：生成正负反事实样本集合，使用核密度估计建模它们的分布，然后基于分布差异度量对特征进行重要性排序。

Result: 与现有局部特征重要性解释方法相比，CID方法在忠实性指标（全面性和充分性）上表现更好，提供了更忠实的系统解释。

Conclusion: CID方法不仅为现有方法提供了补充视角，还提高了解释的忠实性，有潜力成为模型分析的有价值工具。

Abstract: Assessing the importance of individual features in Machine Learning is critical to understand the model's decision-making process. While numerous methods exist, the lack of a definitive ground truth for comparison highlights the need for alternative, well-founded measures. This paper introduces a novel post-hoc local feature importance method called Counterfactual Importance Distribution (CID). We generate two sets of positive and negative counterfactuals, model their distributions using Kernel Density Estimation, and rank features based on a distributional dissimilarity measure. This measure, grounded in a rigorous mathematical framework, satisfies key properties required to function as a valid metric. We showcase the effectiveness of our method by comparing with well-established local feature importance explainers. Our method not only offers complementary perspectives to existing approaches, but also improves performance on faithfulness metrics (both for comprehensiveness and sufficiency), resulting in more faithful explanations of the system. These results highlight its potential as a valuable tool for model analysis.

</details>


### [39] [Proximal Approximate Inference in State-Space Models](https://arxiv.org/abs/2511.15409)
*Hany Abdulsamad,Ángel F. García-Fernández,Simo Särkkä*

Main category: cs.LG

TL;DR: 提出了一种基于变分拉格朗日框架的非线性非高斯状态空间模型状态估计算法，通过熵信任区域更新和动态约束实现贝叶斯推断。


<details>
  <summary>Details</summary>
Motivation: 解决非线性非高斯状态空间模型中的状态估计问题，传统方法在处理这类复杂模型时存在局限性。

Method: 采用变分拉格朗日公式，将贝叶斯推断转化为序列化的熵信任区域更新；针对高斯-马尔可夫近似推导递归方案；对一般非线性非高斯模型使用广义统计线性回归和傅里叶-埃尔米特矩匹配。

Result: 开发了一类前向-后向算法家族，具有有利的计算复杂度，能够有效处理非线性非高斯状态估计问题。

Conclusion: 该变分框架为非线性非高斯状态空间模型提供了一种有效的状态估计方法，通过不同的变分后验分解产生多种算法变体。

Abstract: We present a class of algorithms for state estimation in nonlinear, non-Gaussian state-space models. Our approach is based on a variational Lagrangian formulation that casts Bayesian inference as a sequence of entropic trust-region updates subject to dynamic constraints. This framework gives rise to a family of forward-backward algorithms, whose structure is determined by the chosen factorization of the variational posterior. By focusing on Gauss--Markov approximations, we derive recursive schemes with favorable computational complexity. For general nonlinear, non-Gaussian models we close the recursions using generalized statistical linear regression and Fourier--Hermite moment matching.

</details>


### [40] [FairEnergy: Contribution-Based Fairness meets Energy Efficiency in Federated Learning](https://arxiv.org/abs/2511.15454)
*Ouiame Marnissi,Hajar EL Hammouti,El Houcine Bergou*

Main category: cs.LG

TL;DR: FairEnergy是一个公平感知的联邦学习能量最小化框架，通过联合优化设备选择、带宽分配和压缩级别，在非IID数据上实现更高精度，同时比基线策略降低高达79%的能耗。


<details>
  <summary>Details</summary>
Motivation: 联邦学习在无线边缘系统中面临平衡能源效率、公平参与和模型精度的挑战，主要由于异构资源、不平等客户端贡献和有限通信容量。

Method: 提出FairEnergy框架，将捕获更新幅度和压缩比的贡献分数整合到联合优化中，通过松弛二进制选择变量和应用拉格朗日分解来处理全局带宽耦合，然后进行每设备子问题优化。

Result: 在非IID数据上的实验表明，FairEnergy相比基线策略实现了更高的精度，同时能耗降低了高达79%。

Conclusion: FairEnergy框架有效解决了联邦学习中能源效率与公平参与的平衡问题，在保证模型精度的同时显著降低了能耗。

Abstract: Federated learning (FL) enables collaborative model training across distributed devices while preserving data privacy. However, balancing energy efficiency and fair participation while ensuring high model accuracy remains challenging in wireless edge systems due to heterogeneous resources, unequal client contributions, and limited communication capacity. To address these challenges, we propose FairEnergy, a fairness-aware energy minimization framework that integrates a contribution score capturing both the magnitude of updates and their compression ratio into the joint optimization of device selection, bandwidth allocation, and compression level. The resulting mixed-integer non-convex problem is solved by relaxing binary selection variables and applying Lagrangian decomposition to handle global bandwidth coupling, followed by per-device subproblem optimization. Experiments on non-IID data show that FairEnergy achieves higher accuracy while reducing energy consumption by up to 79\% compared to baseline strategies.

</details>


### [41] [NTK-Guided Implicit Neural Teaching](https://arxiv.org/abs/2511.15487)
*Chen Zhang,Wei Zuo,Bingyang Cheng,Yikun Wang,Wei-Bin Kou,Yik Chung WU,Ngai Wong*

Main category: cs.LG

TL;DR: 提出了NTK引导的隐式神经教学（NINT）方法，通过动态选择最大化全局功能更新的坐标来加速隐式神经表示的训练，将训练时间减少近一半。


<details>
  <summary>Details</summary>
Motivation: 隐式神经表示（INRs）在高分辨率信号拟合时需要优化数百万个坐标，计算成本过高，需要更高效的训练方法。

Method: 利用神经正切核（NTK）对示例进行评分，通过NTK增强的损失梯度范数来选择坐标，同时考虑拟合误差和异构杠杆效应（自影响和跨坐标耦合）。

Result: 实验表明NINT显著减少训练时间近一半，同时保持或提高表示质量，在基于采样的加速策略中达到最先进水平。

Conclusion: NINT通过NTK引导的坐标选择有效加速隐式神经表示训练，为高分辨率信号建模提供了实用的解决方案。

Abstract: Implicit Neural Representations (INRs) parameterize continuous signals via multilayer perceptrons (MLPs), enabling compact, resolution-independent modeling for tasks like image, audio, and 3D reconstruction. However, fitting high-resolution signals demands optimizing over millions of coordinates, incurring prohibitive computational costs. To address it, we propose NTK-Guided Implicit Neural Teaching (NINT), which accelerates training by dynamically selecting coordinates that maximize global functional updates. Leveraging the Neural Tangent Kernel (NTK), NINT scores examples by the norm of their NTK-augmented loss gradients, capturing both fitting errors and heterogeneous leverage (self-influence and cross-coordinate coupling). This dual consideration enables faster convergence compared to existing methods. Through extensive experiments, we demonstrate that NINT significantly reduces training time by nearly half while maintaining or improving representation quality, establishing state-of-the-art acceleration among recent sampling-based strategies.

</details>


### [42] [Sample-Adaptivity Tradeoff in On-Demand Sampling](https://arxiv.org/abs/2511.15507)
*Nika Haghtalab,Omar Montasser,Mingda Qiao*

Main category: cs.LG

TL;DR: 本文研究了按需采样中样本复杂度和轮数复杂度之间的权衡关系，在可实现的MDL中证明了最优样本复杂度约为dk^{Θ(1/r)}/ε，在不可知情况下提出了在Õ(√k)轮内达到近最优样本复杂度Õ((d+k)/ε²)的算法，并引入了OODS框架来抽象样本自适应权衡。


<details>
  <summary>Details</summary>
Motivation: 研究按需采样中样本复杂度和轮数复杂度之间的基本权衡关系，特别是在多分布学习(MDL)的背景下，理解自适应采样策略如何影响学习效率。

Method: 提出了优化通过按需采样(OODS)的新框架来抽象样本自适应权衡，在可实现的MDL中分析轮数复杂度，在不可知情况下设计Õ(√k)轮算法，并建立OODS设置中轮数复杂度的紧界。

Result: 在可实现的MDL中证明了最优样本复杂度为dk^{Θ(1/r)}/ε，在不可知情况下提出了在Õ(√k)轮内达到Õ((d+k)/ε²)样本复杂度的算法，OODS框架的上下界表明实现亚多项式轮数复杂度需要绕过OODS固有硬度的新技术。

Conclusion: 按需采样中样本复杂度和轮数复杂度之间存在基本权衡，OODS框架有效捕捉了这种权衡关系，现有技术在Õ(√k)轮内达到近最优性能，但实现更优轮数复杂度需要突破性技术进展。

Abstract: We study the tradeoff between sample complexity and round complexity in on-demand sampling, where the learning algorithm adaptively samples from $k$ distributions over a limited number of rounds. In the realizable setting of Multi-Distribution Learning (MDL), we show that the optimal sample complexity of an $r$-round algorithm scales approximately as $dk^{Θ(1/r)} / ε$. For the general agnostic case, we present an algorithm that achieves near-optimal sample complexity of $\widetilde O((d + k) / ε^2)$ within $\widetilde O(\sqrt{k})$ rounds. Of independent interest, we introduce a new framework, Optimization via On-Demand Sampling (OODS), which abstracts the sample-adaptivity tradeoff and captures most existing MDL algorithms. We establish nearly tight bounds on the round complexity in the OODS setting. The upper bounds directly yield the $\widetilde O(\sqrt{k})$-round algorithm for agnostic MDL, while the lower bounds imply that achieving sub-polynomial round complexity would require fundamentally new techniques that bypass the inherent hardness of OODS.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [43] [Learning Interestingness in Automated Mathematical Theory Formation](https://arxiv.org/abs/2511.14778)
*George Tsoukalas,Rahul Saha,Amitayush Thakur,Sabrina Reguyal,Swarat Chaudhuri*

Main category: cs.AI

TL;DR: 本文介绍了FERMAT强化学习环境，用于自动化数学理论发现，并探索了使用进化算法自动评估数学对象有趣性的方法。


<details>
  <summary>Details</summary>
Motivation: 解决人工智能中开放式的数学理论自动发现这一重大挑战，通过构建符号化操作的强化学习环境来建模概念发现和定理证明过程。

Method: 引入FERMAT强化学习环境，采用基于LLM的进化算法，具有函数抽象特性，用于合成非平凡的有趣性度量方法。

Result: 在初等数论和有限域领域，该方法相比硬编码基线有显著改进，成功发现了更有趣的数学对象。

Conclusion: FERMAT环境为数学理论发现提供了有效的强化学习框架，基于LLM的进化算法在自动评估数学对象有趣性方面表现出色。

Abstract: We take two key steps in automating the open-ended discovery of new mathematical theories, a grand challenge in artificial intelligence. First, we introduce $\emph{FERMAT}$, a reinforcement learning (RL) environment that models concept discovery and theorem-proving using a set of symbolic actions, opening up a range of RL problems relevant to theory discovery. Second, we explore a specific problem through $\emph{FERMAT}$: automatically scoring the $\emph{interestingness}$ of mathematical objects. We investigate evolutionary algorithms for synthesizing nontrivial interestingness measures. In particular, we introduce an LLM-based evolutionary algorithm that features function abstraction, leading to notable improvements in discovering elementary number theory and finite fields over hard-coded baselines. We open-source the $\emph{FERMAT}$ environment at this URL(https://github.com/trishullab/Fermat).

</details>


### [44] [Ask WhAI:Probing Belief Formation in Role-Primed LLM Agents](https://arxiv.org/abs/2511.14780)
*Keith Moore,Jun W. Kim,David Lyu,Jeffrey Heo,Ehsan Adeli*

Main category: cs.AI

TL;DR: Ask WhAI是一个用于检查和扰动多智能体交互中信念状态的系统级框架，通过记录回放交互、查询智能体信念和推理、注入反事实证据来测试信念结构对新信息的响应。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体科学推理中的信念形成和认知孤岛，提供可重现的方法来观察和测试这些动态，这在人类专家中是不可能的。

Method: 使用多智能体医疗案例模拟器，包含共享电子病历和持有真实实验室结果的预言机智能体。让具有专业先验知识的大语言模型智能体在顺序或并行交互中写入共享医疗记录并与主持人互动。

Result: 智能体信念往往反映现实世界学科立场，包括过度依赖经典研究和抵制反证据，这些信念可以被追踪和审问。

Conclusion: Ask WhAI通过使这些动态可见和可测试，为研究多智能体科学推理中的信念形成和认知孤岛提供了可重现的方法。

Abstract: We present Ask WhAI, a systems-level framework for inspecting and perturbing belief states in multi-agent interactions. The framework records and replays agent interactions, supports out-of-band queries into each agent's beliefs and rationale, and enables counterfactual evidence injection to test how belief structures respond to new information. We apply the framework to a medical case simulator notable for its multi-agent shared memory (a time-stamped electronic medical record, or EMR) and an oracle agent (the LabAgent) that holds ground truth lab results revealed only when explicitly queried. We stress-test the system on a multi-specialty diagnostic journey for a child with an abrupt-onset neuropsychiatric presentation. Large language model agents, each primed with strong role-specific priors ("act like a neurologist", "act like an infectious disease specialist"), write to a shared medical record and interact with a moderator across sequential or parallel encounters. Breakpoints at key diagnostic moments enable pre- and post-event belief queries, allowing us to distinguish entrenched priors from reasoning or evidence-integration effects. The simulation reveals that agent beliefs often mirror real-world disciplinary stances, including overreliance on canonical studies and resistance to counterevidence, and that these beliefs can be traced and interrogated in ways not possible with human experts. By making such dynamics visible and testable, Ask WhAI offers a reproducible way to study belief formation and epistemic silos in multi-agent scientific reasoning.

</details>


### [45] [Subnational Geocoding of Global Disasters Using Large Language Models](https://arxiv.org/abs/2511.14788)
*Michele Ronco,Damien Delforge,Wiebke S. Jäger,Christina Corbane*

Main category: cs.AI

TL;DR: 本文提出了一种完全自动化的LLM辅助工作流，使用GPT-4o处理和清理文本位置信息，并通过交叉验证三个独立的地理信息库（GADM、OpenStreetMap和Wikidata）来分配几何形状。该方法为每个位置分配可靠性评分，并在EM-DAT数据集上成功地理编码了14,215个事件。


<details>
  <summary>Details</summary>
Motivation: 灾害事件的地方位置数据对于风险评估和灾害风险减少至关重要。像EM-DAT这样的灾害数据库通常以非结构化的文本形式报告位置，具有不一致的粒度或拼写，这使得与空间数据集集成变得困难。

Method: 开发了一个完全自动化的LLM辅助工作流，使用GPT-4o处理和清理文本位置信息，并通过交叉验证GADM、OpenStreetMap和Wikidata三个独立的地理信息库来分配几何形状。根据这些源的一致性和可用性，为每个位置分配可靠性评分。

Result: 应用于2000年至2024年的EM-DAT数据集，该工作流在17,948个独特位置上地理编码了14,215个事件。该方法无需人工干预，涵盖所有灾害类型，支持跨多个源的交叉验证，并允许灵活地重新映射到首选框架。

Conclusion: 除了数据集之外，本文展示了LLMs从非结构化文本中提取和构建地理信息的潜力，为相关分析提供了可扩展且可靠的方法。

Abstract: Subnational location data of disaster events are critical for risk assessment and disaster risk reduction. Disaster databases such as EM-DAT often report locations in unstructured textual form, with inconsistent granularity or spelling, that make it difficult to integrate with spatial datasets. We present a fully automated LLM-assisted workflow that processes and cleans textual location information using GPT-4o, and assigns geometries by cross-checking three independent geoinformation repositories: GADM, OpenStreetMap and Wikidata. Based on the agreement and availability of these sources, we assign a reliability score to each location while generating subnational geometries. Applied to the EM-DAT dataset from 2000 to 2024, the workflow geocodes 14,215 events across 17,948 unique locations. Unlike previous methods, our approach requires no manual intervention, covers all disaster types, enables cross-verification across multiple sources, and allows flexible remapping to preferred frameworks. Beyond the dataset, we demonstrate the potential of LLMs to extract and structure geographic information from unstructured text, offering a scalable and reliable method for related analyses.

</details>


### [46] [Project Rachel: Can an AI Become a Scholarly Author?](https://arxiv.org/abs/2511.14819)
*Martin Monperrus,Benoit Baudry,Clément Vidal*

Main category: cs.AI

TL;DR: Project Rachel是一个行动研究项目，创建并追踪了一个完整的AI学术身份Rachel So，通过发布AI生成的研究论文来调查学术生态系统对AI作者身份的反应。


<details>
  <summary>Details</summary>
Motivation: 研究学术生态系统如何应对AI作者身份，为关于超级人类、超能力AI系统与学术交流未来的必要辩论提供实证数据。

Method: 采用行动研究方法，创建AI学术身份Rachel So，在2025年3月至10月期间发表10多篇AI生成的研究论文，并追踪其被引用和同行评审邀请情况。

Result: Rachel So成功发表论文并被引用，还收到了同行评审邀请，表明AI作者身份在学术生态系统中获得了一定程度的认可。

Conclusion: 这项研究揭示了AI作者身份对出版商、研究人员和整个科学系统的潜在影响，为未来学术交流与AI系统的融合提供了实证基础。

Abstract: This paper documents Project Rachel, an action research study that created and tracked a complete AI academic identity named Rachel So. Through careful publication of AI-generated research papers, we investigate how the scholarly ecosystem responds to AI authorship. Rachel So published 10+ papers between March and October 2025, was cited, and received a peer review invitation. We discuss the implications of AI authorship on publishers, researchers, and the scientific system at large. This work contributes empirical action research data to the necessary debate about the future of scholarly communication with super human, hyper capable AI systems.

</details>


### [47] [Uncertainty-Aware Measurement of Scenario Suite Representativeness for Autonomous Systems](https://arxiv.org/abs/2511.14853)
*Robab Aghazadeh Chakherlou,Siddartha Khastgir,Xingyu Zhao,Jerein Jeyachandran,Shufeng Chen*

Main category: cs.AI

TL;DR: 本文提出了一种概率方法来量化AI系统训练测试数据集的代表性，通过比较场景数据与目标操作域(TOD)的统计分布特征，使用不精确贝叶斯方法处理有限数据和先验不确定性，生成区间值的代表性估计。


<details>
  <summary>Details</summary>
Motivation: 确保AI系统（如自动驾驶汽车）的可信性和安全性，关键在于训练测试数据集的数据相关安全属性，特别是代表性——场景数据反映系统设计安全运行条件(ODD)或预期遇到条件(TOD)的程度。

Method: 采用概率方法比较场景套件特征与TOD特征的统计分布，使用不精确贝叶斯方法处理有限数据和不确定先验，生成区间值的不确定性感知代表性估计。

Result: 通过数值示例展示了在依赖性和先验不确定性下，跨操作类别（天气、道路类型、时间等）的场景套件与推断TOD分布的比较，估计了局部（类别间）和全局的区间代表性。

Conclusion: 该方法能够量化数据集的代表性，并考虑不确定性，为AI系统的安全评估提供了更可靠的依据。

Abstract: Assuring the trustworthiness and safety of AI systems, e.g., autonomous vehicles (AV), depends critically on the data-related safety properties, e.g., representativeness, completeness, etc., of the datasets used for their training and testing. Among these properties, this paper focuses on representativeness-the extent to which the scenario-based data used for training and testing, reflect the operational conditions that the system is designed to operate safely in, i.e., Operational Design Domain (ODD) or expected to encounter, i.e., Target Operational Domain (TOD). We propose a probabilistic method that quantifies representativeness by comparing the statistical distribution of features encoded by the scenario suites with the corresponding distribution of features representing the TOD, acknowledging that the true TOD distribution is unknown, as it can only be inferred from limited data.
  We apply an imprecise Bayesian method to handle limited data and uncertain priors. The imprecise Bayesian formulation produces interval-valued, uncertainty-aware estimates of representativeness, rather than a single value. We present a numerical example comparing the distributions of the scenario suite and the inferred TOD across operational categories-weather, road type, time of day, etc., under dependencies and prior uncertainty. We estimate representativeness locally (between categories) and globally as an interval.

</details>


### [48] [Learning Human-Like RL Agents Through Trajectory Optimization With Action Quantization](https://arxiv.org/abs/2511.15055)
*Jian-Ting Guo,Yu-Cheng Chen,Ping-Chun Hsieh,Kuo-Hao Ho,Po-Wei Huang,Ti-Rong Wu,I-Chen Wu*

Main category: cs.AI

TL;DR: 本文提出了Macro Action Quantization (MAQ)框架，通过将人类演示提炼为宏动作来实现人类化强化学习，在D4RL Adroit基准测试中显著提高了人类相似度。


<details>
  <summary>Details</summary>
Motivation: 当前强化学习智能体虽然在许多领域表现超越人类，但往往表现出与人类行为相比不自然的行为，这引发了可解释性和可信赖性的担忧。本文旨在设计能够产生人类化行为的强化学习智能体。

Method: 将人类相似度建模为轨迹优化问题，采用后退水平控制作为可处理且高效的实现方法。引入Macro Action Quantization (MAQ)框架，通过Vector-Quantized VAE将人类演示提炼为宏动作。

Result: 在D4RL Adroit基准测试中，MAQ显著提高了人类相似度，增加了轨迹相似度得分，并在人类评估研究中获得了所有强化学习智能体中最高的人类相似度排名。

Conclusion: MAQ可以轻松集成到各种现成的强化学习算法中，为学习人类化强化学习智能体开辟了有前景的方向。

Abstract: Human-like agents have long been one of the goals in pursuing artificial intelligence. Although reinforcement learning (RL) has achieved superhuman performance in many domains, relatively little attention has been focused on designing human-like RL agents. As a result, many reward-driven RL agents often exhibit unnatural behaviors compared to humans, raising concerns for both interpretability and trustworthiness. To achieve human-like behavior in RL, this paper first formulates human-likeness as trajectory optimization, where the objective is to find an action sequence that closely aligns with human behavior while also maximizing rewards, and adapts the classic receding-horizon control to human-like learning as a tractable and efficient implementation. To achieve this, we introduce Macro Action Quantization (MAQ), a human-like RL framework that distills human demonstrations into macro actions via Vector-Quantized VAE. Experiments on D4RL Adroit benchmarks show that MAQ significantly improves human-likeness, increasing trajectory similarity scores, and achieving the highest human-likeness rankings among all RL agents in the human evaluation study. Our results also demonstrate that MAQ can be easily integrated into various off-the-shelf RL algorithms, opening a promising direction for learning human-like RL agents. Our code is available at https://rlg.iis.sinica.edu.tw/papers/MAQ.

</details>


### [49] [Beyond GeneGPT: A Multi-Agent Architecture with Open-Source LLMs for Enhanced Genomic Question Answering](https://arxiv.org/abs/2511.15061)
*Haodong Chen,Guido Zuccon,Teerapong Leelanupab*

Main category: cs.AI

TL;DR: 本文提出了OpenBioLLM，一个模块化的多智能体框架，用于基因组问答任务。它通过引入专门的智能体来处理工具路由、查询生成和响应验证，在保持性能的同时显著降低了延迟。


<details>
  <summary>Details</summary>
Motivation: GeneGPT虽然解决了基因组问答中需要整合多种生物医学资源的问题，但其依赖专有模型限制了可扩展性、增加了运营成本，并引发数据隐私和泛化能力的担忧。

Method: 首先使用开源模型（Llama 3.1、Qwen2.5等）在单体架构中复现GeneGPT以识别局限性，然后开发OpenBioLLM多智能体框架，引入专门的智能体进行工具路由、查询生成和响应验证。

Result: OpenBioLLM在90%以上的基准任务中达到或超过GeneGPT的性能，在Gene-Turing和GeneHop上的平均得分分别为0.849和0.830，同时将延迟降低了40-50%。

Conclusion: 开源多智能体系统在基因组问答任务中具有巨大潜力，能够在保持模型能力的同时显著提高效率。

Abstract: Genomic question answering often requires complex reasoning and integration across diverse biomedical sources. GeneGPT addressed this challenge by combining domain-specific APIs with OpenAI's code-davinci-002 large language model to enable natural language interaction with genomic databases. However, its reliance on a proprietary model limits scalability, increases operational costs, and raises concerns about data privacy and generalization.
  In this work, we revisit and reproduce GeneGPT in a pilot study using open source models, including Llama 3.1, Qwen2.5, and Qwen2.5 Coder, within a monolithic architecture; this allows us to identify the limitations of this approach. Building on this foundation, we then develop OpenBioLLM, a modular multi-agent framework that extends GeneGPT by introducing agent specialization for tool routing, query generation, and response validation. This enables coordinated reasoning and role-based task execution.
  OpenBioLLM matches or outperforms GeneGPT on over 90% of the benchmark tasks, achieving average scores of 0.849 on Gene-Turing and 0.830 on GeneHop, while using smaller open-source models without additional fine-tuning or tool-specific pretraining. OpenBioLLM's modular multi-agent design reduces latency by 40-50% across benchmark tasks, significantly improving efficiency without compromising model capability. The results of our comprehensive evaluation highlight the potential of open-source multi-agent systems for genomic question answering. Code and resources are available at https://github.com/ielab/OpenBioLLM.

</details>


### [50] [ProRAC: A Neuro-symbolic Method for Reasoning about Actions with LLM-based Progression](https://arxiv.org/abs/2511.15069)
*Haoyong Wu,Yongmei Liu*

Main category: cs.AI

TL;DR: ProRAC是一个神经符号框架，利用LLM解决动作和变化推理问题，通过提取动作和问题、逐步执行动作推导最终状态，然后评估查询来获得答案。


<details>
  <summary>Details</summary>
Motivation: 为了解决动作和变化推理问题，提出一个结合神经和符号方法的框架，利用LLM的强大能力来处理复杂的推理任务。

Method: 提取RAC基本元素（动作和问题），逐步执行每个动作来推导最终状态，然后在进展后的状态下评估查询以获得答案。

Result: 在多个RAC基准测试中表现出色，在不同基准、领域、LLM主干和RAC任务类型上都取得了强劲性能。

Conclusion: ProRAC框架在动作和变化推理问题上具有强大的性能，展示了神经符号方法在复杂推理任务中的有效性。

Abstract: In this paper, we propose ProRAC (Progression-based Reasoning about Actions and Change), a neuro-symbolic framework that leverages LLMs to tackle RAC problems. ProRAC extracts fundamental RAC elements including actions and questions from the problem, progressively executes each action to derive the final state, and then evaluates the query against the progressed state to arrive at an answer. We evaluate ProRAC on several RAC benchmarks, and the results demonstrate that our approach achieves strong performance across different benchmarks, domains, LLM backbones, and types of RAC tasks.

</details>


### [51] [Knowledge-Informed Automatic Feature Extraction via Collaborative Large Language Model Agents](https://arxiv.org/abs/2511.15074)
*Henrik Bradland,Morten Goodwin,Vladimir I. Zadorozhny,Per-Arne Andersen*

Main category: cs.AI

TL;DR: Rogue One是一个基于大语言模型的多智能体框架，通过三个专门智能体（科学家、提取器、测试器）的协作迭代，实现知识引导的自动特征提取，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的自动特征提取方法存在架构单一、反馈机制简单、缺乏外部知识整合等问题，限制了特征工程的质量和效果。

Method: 采用去中心化的多智能体系统，包含科学家、提取器和测试器三个专门智能体，结合丰富的定性反馈机制和"泛滥-剪枝"策略，并集成检索增强生成系统来整合外部知识。

Result: 在19个分类和9个回归数据集上的综合测试表明，Rogue One显著优于最先进的方法，并能发现新颖、可测试的假设（如心肌数据集中的新生物标志物）。

Conclusion: Rogue One框架不仅提升了特征提取的性能，还通过整合外部知识和定性反馈机制，生成了统计强大且语义可解释的特征，展现了在科学发现中的实用价值。

Abstract: The performance of machine learning models on tabular data is critically dependent on high-quality feature engineering. While Large Language Models (LLMs) have shown promise in automating feature extraction (AutoFE), existing methods are often limited by monolithic LLM architectures, simplistic quantitative feedback, and a failure to systematically integrate external domain knowledge. This paper introduces Rogue One, a novel, LLM-based multi-agent framework for knowledge-informed automatic feature extraction. Rogue One operationalizes a decentralized system of three specialized agents-Scientist, Extractor, and Tester-that collaborate iteratively to discover, generate, and validate predictive features. Crucially, the framework moves beyond primitive accuracy scores by introducing a rich, qualitative feedback mechanism and a "flooding-pruning" strategy, allowing it to dynamically balance feature exploration and exploitation. By actively incorporating external knowledge via an integrated retrieval-augmented (RAG) system, Rogue One generates features that are not only statistically powerful but also semantically meaningful and interpretable. We demonstrate that Rogue One significantly outperforms state-of-the-art methods on a comprehensive suite of 19 classification and 9 regression datasets. Furthermore, we show qualitatively that the system surfaces novel, testable hypotheses, such as identifying a new potential biomarker in the myocardial dataset, underscoring its utility as a tool for scientific discovery.

</details>


### [52] [SafeRBench: A Comprehensive Benchmark for Safety Assessment in Large Reasoning Models](https://arxiv.org/abs/2511.15169)
*Xin Gao,Shaohan Yu,Zerui Chen,Yueming Lyu,Weichen Yu,Guanghao Li,Jiyao Liu,Jianxiong Gao,Jian Liang,Ziwei Liu,Chenyang Si*

Main category: cs.AI

TL;DR: SafeRBench是第一个端到端评估大型推理模型安全性的基准，从输入、中间推理到最终输出全面评估安全风险，包含输入特征化、细粒度输出分析和人类安全对齐三个核心组件。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型通过显式思维链提高了答案质量，但这种能力也引入了新的安全风险：有害内容可能在推理过程中被微妙注入、逐渐显现或被误导性理由合理化。现有安全评估主要关注输出级判断，很少捕捉推理过程中的动态风险。

Method: 1) 输入特征化：将风险类别和级别纳入输入设计，明确考虑受影响群体和严重程度；2) 细粒度输出分析：引入微思维分块机制将长推理轨迹分割成语义连贯单元；3) 人类安全对齐：用专门设计的人类标注验证基于LLM的安全评估。

Result: 在19个大型推理模型上的评估表明，SafeRBench能够进行详细的多维度安全评估，从多个角度提供风险和保护机制的洞察。

Conclusion: SafeRBench填补了现有安全评估的空白，为大型推理模型提供了端到端的安全评估框架，能够捕捉推理过程中的动态安全风险。

Abstract: Large Reasoning Models (LRMs) improve answer quality through explicit chain-of-thought, yet this very capability introduces new safety risks: harmful content can be subtly injected, surface gradually, or be justified by misleading rationales within the reasoning trace. Existing safety evaluations, however, primarily focus on output-level judgments and rarely capture these dynamic risks along the reasoning process. In this paper, we present SafeRBench, the first benchmark that assesses LRM safety end-to-end -- from inputs and intermediate reasoning to final outputs. (1) Input Characterization: We pioneer the incorporation of risk categories and levels into input design, explicitly accounting for affected groups and severity, and thereby establish a balanced prompt suite reflecting diverse harm gradients. (2) Fine-Grained Output Analysis: We introduce a micro-thought chunking mechanism to segment long reasoning traces into semantically coherent units, enabling fine-grained evaluation across ten safety dimensions. (3) Human Safety Alignment: We validate LLM-based evaluations against human annotations specifically designed to capture safety judgments. Evaluations on 19 LRMs demonstrate that SafeRBench enables detailed, multidimensional safety assessment, offering insights into risks and protective mechanisms from multiple perspectives.

</details>


### [53] [As If We've Met Before: LLMs Exhibit Certainty in Recognizing Seen Files](https://arxiv.org/abs/2511.15192)
*Haodong Li,Jingqi Zhang,Xiao Cheng,Peihua Mai,Haoyu Wang,Yang Pan*

Main category: cs.AI

TL;DR: COPYCHECK是一个利用不确定性信号检测LLM训练数据中版权内容的新框架，将LLM的过度自信转化为优势，通过不确定性模式区分训练数据和非训练数据。


<details>
  <summary>Details</summary>
Motivation: LLM在大量数据集上训练时可能包含受版权保护的内容，现有成员推理攻击方法因LLM过度自信、缺乏真实训练数据和依赖经验阈值而面临挑战。

Method: 采用两阶段策略：(1) 将文件分割成小片段以减少对大规模训练数据的依赖；(2) 使用不确定性引导的无监督聚类消除经验阈值需求。

Result: 在LLaMA 7b和LLaMA2 7b上分别达到90.1%和91.6%的平均平衡准确率，相比SOTA基线有超过90%的相对提升，最高达到93.8%平衡准确率，在GPT-J 6B上也有良好泛化性。

Conclusion: 这是首次将不确定性应用于LLM版权检测，为训练数据透明度提供了实用工具。

Abstract: The remarkable language ability of Large Language Models (LLMs) stems from extensive training on vast datasets, often including copyrighted material, which raises serious concerns about unauthorized use. While Membership Inference Attacks (MIAs) offer potential solutions for detecting such violations, existing approaches face critical limitations and challenges due to LLMs' inherent overconfidence, limited access to ground truth training data, and reliance on empirically determined thresholds.
  We present COPYCHECK, a novel framework that leverages uncertainty signals to detect whether copyrighted content was used in LLM training sets. Our method turns LLM overconfidence from a limitation into an asset by capturing uncertainty patterns that reliably distinguish between ``seen" (training data) and ``unseen" (non-training data) content. COPYCHECK further implements a two-fold strategy: (1) strategic segmentation of files into smaller snippets to reduce dependence on large-scale training data, and (2) uncertainty-guided unsupervised clustering to eliminate the need for empirically tuned thresholds. Experiment results show that COPYCHECK achieves an average balanced accuracy of 90.1% on LLaMA 7b and 91.6% on LLaMA2 7b in detecting seen files. Compared to the SOTA baseline, COPYCHECK achieves over 90% relative improvement, reaching up to 93.8\% balanced accuracy. It further exhibits strong generalizability across architectures, maintaining high performance on GPT-J 6B. This work presents the first application of uncertainty for copyright detection in LLMs, offering practical tools for training data transparency.

</details>


### [54] [Efficiency Will Not Lead to Sustainable Reasoning AI](https://arxiv.org/abs/2511.15259)
*Philipp Wiesner,Daniel W. O'Neill,Francesca Larosa,Odej Kao*

Main category: cs.AI

TL;DR: 本文认为仅靠效率提升无法实现可持续的推理AI，需要将明确限制嵌入到这类系统的优化和治理中。


<details>
  <summary>Details</summary>
Motivation: AI研究正转向复杂问题解决，推理AI的性能不再受限于训练数据量，而是随着计算投资的指数增长而持续扩展。随着效率改进接近物理极限，新兴推理AI缺乏可比较的饱和点，这引发了可持续性担忧。

Method: 论文讨论了推理AI的可持续性问题，分析了效率改进的物理限制和推理AI缺乏饱和点的特点。

Result: 研究发现效率单独无法实现可持续的推理AI，因为推理AI的性能持续随计算投资指数增长，而效率改进已接近物理极限。

Conclusion: 需要在推理AI系统的优化和治理中嵌入明确限制，以确保其可持续发展，并提出了相关研究和政策方向。

Abstract: AI research is increasingly moving toward complex problem solving, where models are optimized not only for pattern recognition but for multi-step reasoning. Historically, computing's global energy footprint has been stabilized by sustained efficiency gains and natural saturation thresholds in demand. But as efficiency improvements are approaching physical limits, emerging reasoning AI lacks comparable saturation points: performance is no longer limited by the amount of available training data but continues to scale with exponential compute investments in both training and inference. This paper argues that efficiency alone will not lead to sustainable reasoning AI and discusses research and policy directions to embed explicit limits into the optimization and governance of such systems.

</details>


### [55] [Realist and Pluralist Conceptions of Intelligence and Their Implications on AI Research](https://arxiv.org/abs/2511.15282)
*Ninell Oldenburg,Ruchira Dhar,Anders Søgaard*

Main category: cs.AI

TL;DR: 本文探讨了AI研究中的两种基本智力观：智力现实主义（认为智力是单一、普遍的跨系统可衡量能力）和智力多元主义（认为智力是多样、情境依赖且无法简化为单一通用度量的能力）。


<details>
  <summary>Details</summary>
Motivation: 揭示AI研究中隐含的智力观念如何影响实证证据的解释，阐明这些基本观点如何导致研究方法和风险评估的根本分歧。

Method: 通过分析当前AI研究中的辩论，展示两种智力观在模型选择、基准设计、实验验证等方法论问题上的不同影响。

Result: 发现两种智力观导致对相同实证现象（如能力涌现、系统局限性）的矛盾解读，并在AI风险评估上产生截然不同的结论。

Conclusion: 明确这些基本假设有助于更清晰地理解AI研究中的分歧，促进更富有成效的学术讨论。

Abstract: In this paper, we argue that current AI research operates on a spectrum between two different underlying conceptions of intelligence: Intelligence Realism, which holds that intelligence represents a single, universal capacity measurable across all systems, and Intelligence Pluralism, which views intelligence as diverse, context-dependent capacities that cannot be reduced to a single universal measure. Through an analysis of current debates in AI research, we demonstrate how the conceptions remain largely implicit yet fundamentally shape how empirical evidence gets interpreted across a wide range of areas. These underlying views generate fundamentally different research approaches across three areas. Methodologically, they produce different approaches to model selection, benchmark design, and experimental validation. Interpretively, they lead to contradictory readings of the same empirical phenomena, from capability emergence to system limitations. Regarding AI risk, they generate categorically different assessments: realists view superintelligence as the primary risk and search for unified alignment solutions, while pluralists see diverse threats across different domains requiring context-specific solutions. We argue that making explicit these underlying assumptions can contribute to a clearer understanding of disagreements in AI research.

</details>


### [56] [Terra Nova: A Comprehensive Challenge Environment for Intelligent Agents](https://arxiv.org/abs/2511.15378)
*Trevor McInroe*

Main category: cs.AI

TL;DR: Terra Nova是一个基于《文明V》的综合性挑战环境，旨在同时测试强化学习智能体在部分可观测性、信用分配、表示学习、巨大动作空间等多个经典挑战上的综合能力。


<details>
  <summary>Details</summary>
Motivation: 现有的多任务基准主要评估智能体在不同任务间切换策略的能力，而非深度推理能力。作者希望创建一个能真正测试智能体在多个相互关联挑战中进行综合推理的环境。

Method: 设计了一个受《文明V》启发的综合性挑战环境，其中多个RL挑战同时出现，要求智能体具备跨多个交互变量的集成、长视野理解能力。

Result: 提出了Terra Nova环境，排除了仅聚合无关任务的基准，专注于测试智能体在相互关联挑战中的深度推理能力。

Conclusion: Terra Nova为强化学习研究提供了一个更真实的综合性测试平台，能够更好地评估智能体在复杂、交互环境中的综合推理能力。

Abstract: We introduce Terra Nova, a new comprehensive challenge environment (CCE) for reinforcement learning (RL) research inspired by Civilization V. A CCE is a single environment in which multiple canonical RL challenges (e.g., partial observability, credit assignment, representation learning, enormous action spaces, etc.) arise simultaneously. Mastery therefore demands integrated, long-horizon understanding across many interacting variables. We emphasize that this definition excludes challenges that only aggregate unrelated tasks in independent, parallel streams (e.g., learning to play all Atari games at once). These aggregated multitask benchmarks primarily asses whether an agent can catalog and switch among unrelated policies rather than test an agent's ability to perform deep reasoning across many interacting challenges.

</details>


### [57] [IPR-1: Interactive Physical Reasoner](https://arxiv.org/abs/2511.15407)
*Mingyu Zhang,Lifeng Zhuo,Tianxi Tan,Guocan Xie,Xian Nie,Yan Li,Renjie Zhao,Zizhu He,Ziyu Wang,Jiting Cai,Yong-Lu Li*

Main category: cs.AI

TL;DR: 本文研究了智能体能否通过交互学习获得类人推理能力，提出了IPR（交互式物理推理器）模型，使用世界模型推演来评估和强化视觉语言模型的策略，并通过PhysCode物理中心动作代码在1000+游戏中实现稳健性能。


<details>
  <summary>Details</summary>
Motivation: 研究智能体是否能够像人类一样通过观察、环境交互来内化物理和因果关系，并在更多经验中持续改进推理能力。

Method: 提出IPR模型，结合世界模型推演和视觉语言模型策略，引入PhysCode物理中心动作代码来对齐语义意图与动力学，在1000+异构游戏上进行预训练。

Result: IPR在三个类人推理级别（生存、好奇心、实用性）上表现稳健，整体性能与GPT-5相当，在好奇心方面超越GPT-5，性能随训练游戏和交互步骤增加而提升，并能零样本迁移到未见游戏。

Conclusion: 物理中心的交互是实现持续改进物理推理能力的有效路径。

Abstract: Humans learn by observing, interacting with environments, and internalizing physics and causality. Here, we aim to ask whether an agent can similarly acquire human-like reasoning from interaction and keep improving with more experience. We study this in a Game-to-Unseen (G2U) setting, curating 1,000+ heterogeneous games with diverse physical and causal mechanisms, and evaluate at three human-like levels: Survival, Curiosity, Utility, from primitive intuition to goal-driven reasoning. Our analysis reveals complementary failures: VLM/VLA agents reason but lack look-ahead in interactive settings, while world models imagine but imitate visual patterns rather than analyze physics and causality. We therefore propose IPR (Interactive Physical Reasoner), using world-model rollouts to score and reinforce a VLM's policy, and introduce PhysCode, a physics-centric action code aligning semantic intent with dynamics to provide a shared action space for prediction and reasoning. Pretrained on 1,000+ games, our IPR performs robustly on three levels, matches GPT-5 overall, and surpasses it on Curiosity. We find that performance improves with more training games and interaction steps, and that the model also zero-shot transfers to unseen games. These results support physics-centric interaction as a path to steadily improving physical reasoning.

</details>


### [58] [Know Your Intent: An Autonomous Multi-Perspective LLM Agent Framework for DeFi User Transaction Intent Mining](https://arxiv.org/abs/2511.15456)
*Qian'ang Mao,Yuxuan Zhang,Jiaman Chen,Wenjun Zhou,Jiaqi Yan*

Main category: cs.AI

TL;DR: 提出了TIM框架，通过DeFi意图分类法和多智能体LLM系统来推断用户交易意图，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: DeFi交易中用户意图理解至关重要但具有挑战性，现有方法缺乏深度语义洞察。

Method: TIM框架包含DeFi意图分类法、元级规划器、问题求解器和认知评估器，协调多智能体LLM系统处理多模态链上/链下数据。

Result: 实验表明TIM显著优于机器学习模型、单一LLM和单一智能体基线。

Conclusion: TIM为DeFi中复杂区块链活动提供了更可靠的用户动机理解和上下文感知解释。

Abstract: As Decentralized Finance (DeFi) develops, understanding user intent behind DeFi transactions is crucial yet challenging due to complex smart contract interactions, multifaceted on-/off-chain factors, and opaque hex logs. Existing methods lack deep semantic insight. To address this, we propose the Transaction Intent Mining (TIM) framework. TIM leverages a DeFi intent taxonomy built on grounded theory and a multi-agent Large Language Model (LLM) system to robustly infer user intents. A Meta-Level Planner dynamically coordinates domain experts to decompose multiple perspective-specific intent analyses into solvable subtasks. Question Solvers handle the tasks with multi-modal on/off-chain data. While a Cognitive Evaluator mitigates LLM hallucinations and ensures verifiability. Experiments show that TIM significantly outperforms machine learning models, single LLMs, and single Agent baselines. We also analyze core challenges in intent inference. This work helps provide a more reliable understanding of user motivations in DeFi, offering context-aware explanations for complex blockchain activity.

</details>
